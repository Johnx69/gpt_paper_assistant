{
    "2506.11991": {
        "authors": [
            "Jiacong Wang",
            "Zijiang Kang",
            "Haochen Wang",
            "Haiyong Jiang",
            "Jiawen Li",
            "Bohong Wu",
            "Ya Wang",
            "Jiao Ran",
            "Xiao Liang",
            "Chao Feng",
            "Jun Xiao"
        ],
        "title": "VGR: Visual Grounded Reasoning",
        "abstract": "arXiv:2506.11991v1 Announce Type: new  Abstract: In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.",
        "arxiv_id": "2506.11991",
        "ARXIVID": "2506.11991",
        "COMMENT": "Matches criterion 2 as it introduces a novel multimodal large language model (MLLM) with enhanced visual reasoning capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.11558": {
        "authors": [
            "Bo-Cheng Chiu",
            "Jen-Jee Chen",
            "Yu-Chee Tseng",
            "Feng-Chi Chen"
        ],
        "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs",
        "abstract": "arXiv:2506.11558v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with GPT-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.",
        "arxiv_id": "2506.11558",
        "ARXIVID": "2506.11558",
        "COMMENT": "Matches criteria 2 and 6 as it introduces a video LLM designed for temporal reasoning and multimodal understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.11515": {
        "authors": [
            "Xiao Xu",
            "Libo Qin",
            "Wanxiang Che",
            "Min-Yen Kan"
        ],
        "title": "Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs",
        "abstract": "arXiv:2506.11515v1 Announce Type: new  Abstract: Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance across various downstream VL tasks. While BridgeTower further enhances performance by building bridges between encoders, it \\textit{(i)} suffers from ineffective layer-by-layer utilization of unimodal representations, \\textit{(ii)} restricts the flexible exploitation of different levels of unimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on traditional low-resolution datasets only with the Two-Tower VLM architecture. In this work, we propose Manager, a lightweight, efficient and effective plugin that adaptively aggregates insights from different levels of pre-trained unimodal experts to facilitate more comprehensive VL alignment and fusion. First, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel VLM that introduces the manager in each cross-modal layer. Whether with or without VL pre-training, ManagerTower outperforms previous strong baselines and achieves superior performance on 4 downstream VL tasks. Moreover, we extend our exploration to the latest Multimodal Large Language Model (MLLM) architecture. We demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot performance of LLaVA-OV across different categories of capabilities, images, and resolutions on 20 downstream datasets, whether the multi-grid algorithm is enabled or not. In-depth analysis reveals that both our manager and the multi-grid algorithm can be viewed as a plugin that improves the visual representation by capturing more diverse visual details from two orthogonal perspectives (depth and width). Their synergy can mitigate the semantic ambiguity caused by the multi-grid algorithm and further improve performance. Code and models are available at https://github.com/LooperXX/ManagerTower.",
        "arxiv_id": "2506.11515",
        "ARXIVID": "2506.11515",
        "COMMENT": "Matches criterion 2 as it explores Multimodal Large Language Models (MLLMs) and Vision-Language Models (VLMs), proposing a novel plugin for better alignment and fusion.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.11148": {
        "authors": [
            "Melvin Wong",
            "Yueming Lyu",
            "Thiago Rios",
            "Stefan Menzel",
            "Yew-Soon Ong"
        ],
        "title": "LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs",
        "abstract": "arXiv:2506.11148v1 Announce Type: new  Abstract: The emergence of generative artificial intelligence (GenAI) and large language models (LLMs) has revolutionized the landscape of digital content creation in different modalities. However, its potential use in Physical AI for engineering design, where the production of physically viable artifacts is paramount, remains vastly underexplored. The absence of physical knowledge in existing LLM-to-3D models often results in outputs detached from real-world physical constraints. To address this gap, we introduce LLM-to-Phy3D, a physically conform online 3D object generation that enables existing LLM-to-3D models to produce physically conforming 3D objects on the fly. LLM-to-Phy3D introduces a novel online black-box refinement loop that empowers large language models (LLMs) through synergistic visual and physics-based evaluations. By delivering directional feedback in an iterative refinement process, LLM-to-Phy3D actively drives the discovery of prompts that yield 3D artifacts with enhanced physical performance and greater geometric novelty relative to reference objects, marking a substantial contribution to AI-driven generative design. Systematic evaluations of LLM-to-Phy3D, supported by ablation studies in vehicle design optimization, reveal various LLM improvements gained by 4.5% to 106.7% in producing physically conform target domain 3D designs over conventional LLM-to-3D models. The encouraging results suggest the potential general use of LLM-to-Phy3D in Physical AI for scientific and engineering applications.",
        "arxiv_id": "2506.11148",
        "ARXIVID": "2506.11148",
        "COMMENT": "Matches criterion 5 as it introduces a method for integrating physical constraints into LLM-to-3D object generation, combining vision and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.11302": {
        "authors": [
            "H\\'ector Carri\\'on",
            "Yutong Bai",
            "V\\'ictor A. Hern\\'andez Castro",
            "Kishan Panaganti",
            "Ayush Zenith",
            "Matthew Trang",
            "Tony Zhang",
            "Pietro Perona",
            "Jitendra Malik"
        ],
        "title": "TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy",
        "abstract": "arXiv:2506.11302v1 Announce Type: new  Abstract: World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.",
        "arxiv_id": "2506.11302",
        "ARXIVID": "2506.11302",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new spatio-temporal dataset (STRIDE) and a generative world model (TARDIS) for embodied reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.11571": {
        "authors": [
            "Jiachen Yu",
            "Yufei Zhan",
            "Ziheng Wu",
            "Yousong Zhu",
            "Jinqiao Wang",
            "Minghui Qiu"
        ],
        "title": "VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?",
        "abstract": "arXiv:2506.11571v1 Announce Type: new  Abstract: Recent extensive works have demonstrated that by introducing long CoT, the capabilities of MLLMs to solve complex problems can be effectively enhanced. However, the reasons for the effectiveness of such paradigms remain unclear. It is challenging to analysis with quantitative results how much the model's specific extraction of visual cues and its subsequent so-called reasoning during inference process contribute to the performance improvements. Therefore, evaluating the faithfulness of MLLMs' reasoning to visual information is crucial. To address this issue, we first present a cue-driven automatic and controllable editing pipeline with the help of GPT-Image-1. It enables the automatic and precise editing of specific visual cues based on the instruction. Furthermore, we introduce VFaith-Bench, the first benchmark to evaluate MLLMs' visual reasoning capabilities and analyze the source of such capabilities with an emphasis on the visual faithfulness. Using the designed pipeline, we constructed comparative question-answer pairs by altering the visual cues in images that are crucial for solving the original reasoning problem, thereby changing the question's answer. By testing similar questions with images that have different details, the average accuracy reflects the model's visual reasoning ability, while the difference in accuracy before and after editing the test set images effectively reveals the relationship between the model's reasoning ability and visual perception. We further designed specific metrics to expose this relationship. VFaith-Bench includes 755 entries divided into five distinct subsets, along with an additional human-labeled perception task. We conducted in-depth testing and analysis of existing mainstream flagship models and prominent open-source model series/reasoning models on VFaith-Bench, further investigating the underlying factors of their reasoning capabilities.",
        "arxiv_id": "2506.11571",
        "ARXIVID": "2506.11571",
        "COMMENT": "Matches criteria 2 and 5 as it evaluates reasoning capabilities of multimodal large language models with a focus on visual faithfulness.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.11621": {
        "authors": [
            "Xu Wang",
            "Shengeng Tang",
            "Lechao Cheng",
            "Feng Li",
            "Shuo Wang",
            "Richang Hong"
        ],
        "title": "SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation",
        "abstract": "arXiv:2506.11621v1 Announce Type: new  Abstract: Sign language generation aims to produce diverse sign representations based on spoken language. However, achieving realistic and naturalistic generation remains a significant challenge due to the complexity of sign language, which encompasses intricate hand gestures, facial expressions, and body movements. In this work, we introduce PHOENIX14T+, an extended version of the widely-used RWTH-PHOENIX-Weather 2014T dataset, featuring three new sign representations: Pose, Hamer and Smplerx. We also propose a novel method, SignAligner, for realistic sign language generation, consisting of three stages: text-driven pose modalities co-generation, online collaborative correction of multimodality, and realistic sign video synthesis. First, by incorporating text semantics, we design a joint sign language generator to simultaneously produce posture coordinates, gesture actions, and body movements. The text encoder, based on a Transformer architecture, extracts semantic features, while a cross-modal attention mechanism integrates these features to generate diverse sign language representations, ensuring accurate mapping and controlling the diversity of modal features. Next, online collaborative correction is introduced to refine the generated pose modalities using a dynamic loss weighting strategy and cross-modal attention, facilitating the complementarity of information across modalities, eliminating spatiotemporal conflicts, and ensuring semantic coherence and action consistency. Finally, the corrected pose modalities are fed into a pre-trained video generation network to produce high-fidelity sign language videos. Extensive experiments demonstrate that SignAligner significantly improves both the accuracy and expressiveness of the generated sign videos.",
        "arxiv_id": "2506.11621",
        "ARXIVID": "2506.11621",
        "COMMENT": "Matches criterion 5 as it integrates image/video understanding (sign language generation) with text-driven tasks, involving multimodal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.11578": {
        "authors": [
            "Byeongchan Lee",
            "Jonghoon Lee",
            "Dongyoung Kim",
            "Jaehyung Kim",
            "Jinwoo Shin"
        ],
        "title": "Collaborative LLM Inference via Planning for Efficient Reasoning",
        "abstract": "arXiv:2506.11578v1 Announce Type: new  Abstract: Large language models (LLMs) excel at complex reasoning tasks, but those with strong capabilities (e.g., whose numbers of parameters are larger than 100B) are often accessible only through paid APIs, making them too costly for applications of frequent use. In contrast, smaller open-sourced LLMs (e.g., whose numbers of parameters are less than 3B) are freely available and easy to deploy locally (e.g., under a single GPU having 8G VRAM), but lack suff icient reasoning ability. This trade-off raises a natural question: can small (free) and large (costly) models collaborate at test time to combine their strengths? We propose a test-time collaboration framework in which a planner model first generates a plan, defined as a distilled and high-level abstraction of the problem.   This plan serves as a lightweight intermediate that guides a reasoner model, which generates a complete solution. Small and large models take turns acting as planner and reasoner, exchanging plans in a multi-round cascade to collaboratively solve complex tasks. Our method achieves accuracy comparable to strong proprietary models alone, while significantly reducing reliance on paid inference. These results highlight planning as an effective prior for orchestrating cost-aware, cross-model inference under real-world deployment constraints.",
        "arxiv_id": "2506.11578",
        "ARXIVID": "2506.11578",
        "COMMENT": "Matches criterion 2 as it proposes a collaborative framework for reasoning using large language models, which involves planning and efficient reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.11737": {
        "authors": [
            "Dinh Viet Cuong",
            "Hoang-Bao Le",
            "An Pham Ngoc Nguyen",
            "Liting Zhou",
            "Cathal Gurrin"
        ],
        "title": "Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model",
        "abstract": "arXiv:2506.11737v1 Announce Type: new  Abstract: This paper addresses two main objectives. Firstly, we demonstrate the impressive performance of the LLaVA-NeXT-interleave on 22 datasets across three different tasks: Multi-Image Reasoning, Documents and Knowledge-Based Understanding and Interactive Multi-Modal Communication. Secondly, we add the Dense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and compare its performance against the standard model. We find that the standard model achieves the highest overall accuracy, excelling in vision-heavy tasks like VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows particular strength on datasets requiring deeper semantic coherence or structured change understanding such as MIT-States_PropertyCoherence and SlideVQA. Our results highlight the potential of combining powerful foundation models with plug-and-play techniques for Interleave tasks. The code is available at https://github.com/dinhvietcuong1996/icme25-inova.",
        "arxiv_id": "2506.11737",
        "ARXIVID": "2506.11737",
        "COMMENT": "Matches criterion 2 as it evaluates a multimodal large language model (LLaVA-NeXT-Interleave) for multi-image reasoning and multimodal tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.11976": {
        "authors": [
            "Constantin Venhoff",
            "Ashkan Khakzar",
            "Sonia Joseph",
            "Philip Torr",
            "Neel Nanda"
        ],
        "title": "How Visual Representations Map to Language Feature Space in Multimodal LLMs",
        "abstract": "arXiv:2506.11976v1 Announce Type: new  Abstract: Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly understood. We introduce a methodological framework that deliberately maintains a frozen large language model (LLM) and a frozen vision transformer (ViT), connected solely by training a linear adapter during visual instruction tuning. This design is fundamental to our approach: by keeping the language model frozen, we ensure it maintains its original language representations without adaptation to visual data. Consequently, the linear adapter must map visual features directly into the LLM's existing representational space rather than allowing the language model to develop specialized visual understanding through fine-tuning. Our experimental design uniquely enables the use of pre-trained sparse autoencoders (SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned with the unchanged language model and serve as a snapshot of the learned language feature-representations. Through systematic analysis of SAE reconstruction error, sparsity patterns, and feature SAE descriptions, we reveal the layer-wise progression through which visual representations gradually align with language feature representations, converging in middle-to-later layers. This suggests a fundamental misalignment between ViT outputs and early LLM layers, raising important questions about whether current adapter-based architectures optimally facilitate cross-modal representation learning.",
        "arxiv_id": "2506.11976",
        "ARXIVID": "2506.11976",
        "COMMENT": "Matches criterion 5 as it explores the integration of visual representations and large language models, focusing on their alignment mechanisms.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.11147": {
        "authors": [
            "Xiaotang Gai",
            "Jiaxiang Liu",
            "Yichen Li",
            "Zijie Meng",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "title": "3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal Analysis and Diverse Diagnostic Tasks",
        "abstract": "arXiv:2506.11147v1 Announce Type: new  Abstract: Medical Visual Question Answering (Med-VQA) holds significant potential for clinical decision support, yet existing efforts primarily focus on 2D imaging with limited task diversity. This paper presents 3D-RAD, a large-scale dataset designed to advance 3D Med-VQA using radiology CT scans. The 3D-RAD dataset encompasses six diverse VQA tasks: anomaly detection, image observation, medical computation, existence detection, static temporal diagnosis, and longitudinal temporal diagnosis. It supports both open- and closed-ended questions while introducing complex reasoning challenges, including computational tasks and multi-stage temporal analysis, to enable comprehensive benchmarking. Extensive evaluations demonstrate that existing vision-language models (VLMs), especially medical VLMs exhibit limited generalization, particularly in multi-temporal tasks, underscoring the challenges of real-world 3D diagnostic reasoning. To drive future advancements, we release a high-quality training set 3D-RAD-T of 136,195 expert-aligned samples, showing that fine-tuning on this dataset could significantly enhance model performance. Our dataset and code, aiming to catalyze multimodal medical AI research and establish a robust foundation for 3D medical visual understanding, are publicly available at https://github.com/Tang-xiaoxiao/M3D-RAD.",
        "arxiv_id": "2506.11147",
        "ARXIVID": "2506.11147",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a 3D radiology dataset for VQA tasks, including temporal analysis and diagnostic reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.11166": {
        "authors": [
            "Ji Young Byun",
            "Young-Jin Park",
            "Navid Azizan",
            "Rama Chellappa"
        ],
        "title": "Test-Time-Scaling for Zero-Shot Diagnosis with Visual-Language Reasoning",
        "abstract": "arXiv:2506.11166v1 Announce Type: new  Abstract: As a cornerstone of patient care, clinical decision-making significantly influences patient outcomes and can be enhanced by large language models (LLMs). Although LLMs have demonstrated remarkable performance, their application to visual question answering in medical imaging, particularly for reasoning-based diagnosis, remains largely unexplored. Furthermore, supervised fine-tuning for reasoning tasks is largely impractical due to limited data availability and high annotation costs. In this work, we introduce a zero-shot framework for reliable medical image diagnosis that enhances the reasoning capabilities of LLMs in clinical settings through test-time scaling. Given a medical image and a textual prompt, a vision-language model processes a medical image along with a corresponding textual prompt to generate multiple descriptions or interpretations of visual features. These interpretations are then fed to an LLM, where a test-time scaling strategy consolidates multiple candidate outputs into a reliable final diagnosis. We evaluate our approach across various medical imaging modalities -- including radiology, ophthalmology, and histopathology -- and demonstrate that the proposed test-time scaling strategy enhances diagnostic accuracy for both our and baseline methods. Additionally, we provide an empirical analysis showing that the proposed approach, which allows unbiased prompting in the first stage, improves the reliability of LLM-generated diagnoses and enhances classification accuracy.",
        "arxiv_id": "2506.11166",
        "ARXIVID": "2506.11166",
        "COMMENT": "Matches criteria 2 and 5 as it explores a vision-language model for medical image diagnosis and integrates image understanding with LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.11772": {
        "authors": [
            "Byeongchan Lee",
            "John Won",
            "Seunghyun Lee",
            "Jinwoo Shin"
        ],
        "title": "CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection",
        "abstract": "arXiv:2506.11772v1 Announce Type: new  Abstract: Anomaly detection is a complex problem due to the ambiguity in defining anomalies, the diversity of anomaly types (e.g., local and global defect), and the scarcity of training data. As such, it necessitates a comprehensive model capable of capturing both low-level and high-level features, even with limited data. To address this, we propose CLIPFUSION, a method that leverages both discriminative and generative foundation models. Specifically, the CLIP-based discriminative model excels at capturing global features, while the diffusion-based generative model effectively captures local details, creating a synergistic and complementary approach. Notably, we introduce a methodology for utilizing cross-attention maps and feature maps extracted from diffusion models specifically for anomaly detection. Experimental results on benchmark datasets (MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline methods, achieving outstanding performance in both anomaly segmentation and classification. We believe that our method underscores the effectiveness of multi-modal and multi-model fusion in tackling the multifaceted challenges of anomaly detection, providing a scalable solution for real-world applications.",
        "arxiv_id": "2506.11772",
        "ARXIVID": "2506.11772",
        "COMMENT": "Matches criterion 5 as it combines image understanding (via CLIP) and generative modeling (via diffusion models) for anomaly detection, showcasing multimodal integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.11140": {
        "authors": [
            "Jin Kim",
            "Muhammad Wahi-Anwa",
            "Sangyun Park",
            "Shawn Shin",
            "John M. Hoffman",
            "Matthew S. Brown"
        ],
        "title": "Autonomous Computer Vision Development with Agentic AI",
        "abstract": "arXiv:2506.11140v1 Announce Type: new  Abstract: Agentic Artificial Intelligence (AI) systems leveraging Large Language Models (LLMs) exhibit significant potential for complex reasoning, planning, and tool utilization. We demonstrate that a specialized computer vision system can be built autonomously from a natural language prompt using Agentic AI methods. This involved extending SimpleMind (SM), an open-source Cognitive AI environment with configurable tools for medical image analysis, with an LLM-based agent, implemented using OpenManus, to automate the planning (tool configuration) for a particular computer vision task. We provide a proof-of-concept demonstration that an agentic system can interpret a computer vision task prompt, plan a corresponding SimpleMind workflow by decomposing the task and configuring appropriate tools. From the user input prompt, \"provide sm (SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest x-ray)\"), the agent LLM was able to generate the plan (tool configuration file in YAML format), and execute SM-Learn (training) and SM-Think (inference) scripts autonomously. The computer vision agent automatically configured, trained, and tested itself on 50 chest x-ray images, achieving mean dice scores of 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows the potential for autonomous planning and tool configuration that has traditionally been performed by a data scientist in the development of computer vision applications.",
        "arxiv_id": "2506.11140",
        "ARXIVID": "2506.11140",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for autonomous computer vision development using agentic AI, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.11419": {
        "authors": [
            "Bin Sun",
            "Boao Zhang",
            "Jiayi Lu",
            "Xinjie Feng",
            "Jiachen Shang",
            "Rui Cao",
            "Mengchao Zheng",
            "Chuanye Wang",
            "Shichun Yang",
            "Yaoguang Cao",
            "Ziying Song"
        ],
        "title": "FocalAD: Local Motion Planning for End-to-End Autonomous Driving",
        "abstract": "arXiv:2506.11419v1 Announce Type: new  Abstract: In end-to-end autonomous driving,the motion prediction plays a pivotal role in ego-vehicle planning. However, existing methods often rely on globally aggregated motion features, ignoring the fact that planning decisions are primarily influenced by a small number of locally interacting agents. Failing to attend to these critical local interactions can obscure potential risks and undermine planning reliability. In this work, we propose FocalAD, a novel end-to-end autonomous driving framework that focuses on critical local neighbors and refines planning by enhancing local motion representations. Specifically, FocalAD comprises two core modules: the Ego-Local-Agents Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a graph-based ego-centric interaction representation that captures motion dynamics with local neighbors to enhance both ego planning and agent motion queries. FLA Loss increases the weights of decision-critical neighboring agents, guiding the model to prioritize those more relevant to planning. Extensive experiments show that FocalAD outperforms existing state-of-the-art methods on the open-loop nuScenes datasets and closed-loop Bench2Drive benchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD achieves even greater improvements, reducing the average colilision rate by 41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.",
        "arxiv_id": "2506.11419",
        "ARXIVID": "2506.11419",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for autonomous driving, focusing on local motion planning and interaction representation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.11768": {
        "authors": [
            "Linfeng He",
            "Meiqin Liu",
            "Qi Tang",
            "Chao Yao",
            "Yao Zhao"
        ],
        "title": "MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution",
        "abstract": "arXiv:2506.11768v1 Announce Type: new  Abstract: Video super-resolution (VSR) faces critical challenges in effectively modeling non-local dependencies across misaligned frames while preserving computational efficiency. Existing VSR methods typically rely on optical flow strategies or transformer architectures, which struggle with large motion displacements and long video sequences. To address this, we propose MambaVSR, the first state-space model framework for VSR that incorporates an innovative content-aware scanning mechanism. Unlike rigid 1D sequential processing in conventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal interactions through the Shared Compass Construction (SCC) and the Content-Aware Sequentialization (CAS). Specifically, the SCC module constructs intra-frame semantic connectivity graphs via efficient sparse attention and generates adaptive spatial scanning sequences through spectral clustering. Building upon SCC, the CAS module effectively aligns and aggregates non-local similar content across multiple frames by interleaving temporal features along the learned spatial order. To bridge global dependencies with local details, the Global-Local State Space Block (GLSSB) synergistically integrates window self-attention operations with SSM-based feature propagation, enabling high-frequency detail recovery under global dependency guidance. Extensive experiments validate MambaVSR's superiority, outperforming the Transformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer parameters.",
        "arxiv_id": "2506.11768",
        "ARXIVID": "2506.11768",
        "COMMENT": "Matches criteria 6 as it introduces a novel method for video super-resolution, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.11175": {
        "authors": [
            "Hongyu Chen",
            "Jiping Liu",
            "Yong Wang",
            "Jun Zhu",
            "Dejun Feng",
            "Yakun Xie"
        ],
        "title": "Teaching in adverse scenes: a statistically feedback-driven threshold and mask adjustment teacher-student framework for object detection in UAV images under adverse scenes",
        "abstract": "arXiv:2506.11175v1 Announce Type: new  Abstract: Unsupervised Domain Adaptation (UDA) has shown promise in effectively alleviating the performance degradation caused by domain gaps between source and target domains, and it can potentially be generalized to UAV object detection in adverse scenes. However, existing UDA studies are based on natural images or clear UAV imagery, and research focused on UAV imagery in adverse conditions is still in its infancy. Moreover, due to the unique perspective of UAVs and the interference from adverse conditions, these methods often fail to accurately align features and are influenced by limited or noisy pseudo-labels. To address this, we propose the first benchmark for UAV object detection in adverse scenes, the Statistical Feedback-Driven Threshold and Mask Adjustment Teacher-Student Framework (SF-TMAT). Specifically, SF-TMAT introduces a design called Dynamic Step Feedback Mask Adjustment Autoencoder (DSFMA), which dynamically adjusts the mask ratio and reconstructs feature maps by integrating training progress and loss feedback. This approach dynamically adjusts the learning focus at different training stages to meet the model's needs for learning features at varying levels of granularity. Additionally, we propose a unique Variance Feedback Smoothing Threshold (VFST) strategy, which statistically computes the mean confidence of each class and dynamically adjusts the selection threshold by incorporating a variance penalty term. This strategy improves the quality of pseudo-labels and uncovers potentially valid labels, thus mitigating domain bias. Extensive experiments demonstrate the superiority and generalization capability of the proposed SF-TMAT in UAV object detection under adverse scene conditions. The Code is released at https://github.com/ChenHuyoo .",
        "arxiv_id": "2506.11175",
        "ARXIVID": "2506.11175",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and novel methods for UAV object detection in adverse scenes, relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.11774": {
        "authors": [
            "Abhishek Jaiswal",
            "Armeet Singh Luthra",
            "Purav Jangir",
            "Bhavya Garg",
            "Nisheeth Srivastava"
        ],
        "title": "Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation",
        "abstract": "arXiv:2506.11774v1 Announce Type: new  Abstract: Isometric exercises appeal to individuals seeking convenience, privacy, and minimal dependence on equipments. However, such fitness training is often overdependent on unreliable digital media content instead of expert supervision, introducing serious risks, including incorrect posture, injury, and disengagement due to lack of corrective feedback. To address these challenges, we present a real-time feedback system for assessing isometric poses. Our contributions include the release of the largest multiclass isometric exercise video dataset to date, comprising over 3,600 clips across six poses with correct and incorrect variations. To support robust evaluation, we benchmark state-of-the-art models-including graph-based networks-on this dataset and introduce a novel three-part metric that captures classification accuracy, mistake localization, and model confidence. Our results enhance the feasibility of intelligent and personalized exercise training systems for home workouts. This expert-level diagnosis, delivered directly to the users, also expands the potential applications of these systems to rehabilitation, physiotherapy, and various other fitness disciplines that involve physical motion.",
        "arxiv_id": "2506.11774",
        "ARXIVID": "2506.11774",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and evaluation metrics for isometric pose evaluation, relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.11924": {
        "authors": [
            "Min-Seop Kwak",
            "Junho Kim",
            "Sangdoo Yun",
            "Dongyoon Han",
            "Taekyoung Kim",
            "Seungryong Kim",
            "Jin-Hwa Kim"
        ],
        "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation",
        "abstract": "arXiv:2506.11924v1 Announce Type: new  Abstract: We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.",
        "arxiv_id": "2506.11924",
        "ARXIVID": "2506.11924",
        "COMMENT": "Matches criterion 5 as it introduces a diffusion-based framework for aligned novel view image and geometry synthesis, combining image and geometry understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.11672": {
        "authors": [
            "Chendi Ge",
            "Xin Wang",
            "Zeyang Zhang",
            "Hong Chen",
            "Jiapei Fan",
            "Longtao Huang",
            "Hui Xue",
            "Wenwu Zhu"
        ],
        "title": "Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning",
        "abstract": "arXiv:2506.11672v1 Announce Type: new  Abstract: Continual multimodal instruction tuning is crucial for adapting Multimodal Large Language Models (MLLMs) to evolving tasks. However, most existing methods adopt a fixed architecture, struggling with adapting to new tasks due to static model capacity. We propose to evolve the architecture under parameter budgets for dynamic task adaptation, which remains unexplored and imposes two challenges: 1) task architecture conflict, where different tasks require varying layer-wise adaptations, and 2) modality imbalance, where different tasks rely unevenly on modalities, leading to unbalanced updates. To address these challenges, we propose a novel Dynamic Mixture of Curriculum LoRA Experts (D-MoLE) method, which automatically evolves MLLM's architecture with controlled parameter budgets to continually adapt to new tasks while retaining previously learned knowledge. Specifically, we propose a dynamic layer-wise expert allocator, which automatically allocates LoRA experts across layers to resolve architecture conflicts, and routes instructions layer-wisely to facilitate knowledge sharing among experts. Then, we propose a gradient-based inter-modal continual curriculum, which adjusts the update ratio of each module in MLLM based on the difficulty of each modality within the task to alleviate the modality imbalance problem. Extensive experiments show that D-MoLE significantly outperforms state-of-the-art baselines, achieving a 15% average improvement over the best baseline. To the best of our knowledge, this is the first study of continual learning for MLLMs from an architectural perspective.",
        "arxiv_id": "2506.11672",
        "ARXIVID": "2506.11672",
        "COMMENT": "Matches criterion 2 as it proposes a novel method for continual multimodal instruction tuning in MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.11436": {
        "authors": [
            "Ziyang Luo",
            "Nian Liu",
            "Xuguang Yang",
            "Salman Khan",
            "Rao Muhammad Anwer",
            "Hisham Cholakkal",
            "Fahad Shahbaz Khan",
            "Junwei Han"
        ],
        "title": "TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models",
        "abstract": "arXiv:2506.11436v1 Announce Type: new  Abstract: Audio-Visual Segmentation (AVS) faces a fundamental challenge of effectively aligning audio and visual modalities. While recent approaches leverage foundation models to address data scarcity, they often rely on single-modality knowledge or combine foundation models in an off-the-shelf manner, failing to address the cross-modal alignment challenge. In this paper, we present TAViS, a novel framework that \\textbf{couples} the knowledge of multimodal foundation models (ImageBind) for cross-modal alignment and a segmentation foundation model (SAM2) for precise segmentation. However, effectively combining these models poses two key challenges: the difficulty in transferring the knowledge between SAM2 and ImageBind due to their different feature spaces, and the insufficiency of using only segmentation loss for supervision. To address these challenges, we introduce a text-bridged design with two key components: (1) a text-bridged hybrid prompting mechanism where pseudo text provides class prototype information while retaining modality-specific details from both audio and visual inputs, and (2) an alignment supervision strategy that leverages text as a bridge to align shared semantic concepts within audio-visual modalities. Our approach achieves superior performance on single-source, multi-source, semantic datasets, and excels in zero-shot settings.",
        "arxiv_id": "2506.11436",
        "ARXIVID": "2506.11436",
        "COMMENT": "Matches criterion 2 as it explores a novel framework combining multimodal foundation models for audio-visual segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.11777": {
        "authors": [
            "Divyanshu Mishra",
            "Mohammadreza Salehi",
            "Pramit Saha",
            "Olga Patey",
            "Aris T. Papageorghiou",
            "Yuki M. Asano",
            "J. Alison Noble"
        ],
        "title": "Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation",
        "abstract": "arXiv:2506.11777v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding. Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups, and achieves superior segmentation transfer.",
        "arxiv_id": "2506.11777",
        "ARXIVID": "2506.11777",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video-based tasks like representation learning and segmentation transfer in echocardiography.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.11477": {
        "authors": [
            "Wasim Ahmad",
            "Yan-Tsung Peng",
            "Yuan-Hao Chang"
        ],
        "title": "FAME: A Lightweight Spatio-Temporal Network for Model Attribution of Face-Swap Deepfakes",
        "abstract": "arXiv:2506.11477v1 Announce Type: new  Abstract: The widespread emergence of face-swap Deepfake videos poses growing risks to digital security, privacy, and media integrity, necessitating effective forensic tools for identifying the source of such manipulations. Although most prior research has focused primarily on binary Deepfake detection, the task of model attribution -- determining which generative model produced a given Deepfake -- remains underexplored. In this paper, we introduce FAME (Fake Attribution via Multilevel Embeddings), a lightweight and efficient spatio-temporal framework designed to capture subtle generative artifacts specific to different face-swap models. FAME integrates spatial and temporal attention mechanisms to improve attribution accuracy while remaining computationally efficient. We evaluate our model on three challenging and diverse datasets: Deepfake Detection and Manipulation (DFDM), FaceForensics++, and FakeAVCeleb. Results show that FAME consistently outperforms existing methods in both accuracy and runtime, highlighting its potential for deployment in real-world forensic and information security applications.",
        "arxiv_id": "2506.11477",
        "ARXIVID": "2506.11477",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks, specifically model attribution for face-swap Deepfakes, which involves spatio-temporal reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.11820": {
        "authors": [
            "Xintong Wang",
            "Jingheng Pan",
            "Yixiao Liu",
            "Xiaohu Zhao",
            "Chenyang Lyu",
            "Minghao Wu",
            "Chris Biemann",
            "Longyue Wang",
            "Linlong Xu",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "title": "Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation",
        "abstract": "arXiv:2506.11820v1 Announce Type: new  Abstract: Vision-Language Translation (VLT) is a challenging task that requires accurately recognizing multilingual text embedded in images and translating it into the target language with the support of visual context. While recent Large Vision-Language Models (LVLMs) have demonstrated strong multilingual and visual understanding capabilities, there is a lack of systematic evaluation and understanding of their performance on VLT. In this work, we present a comprehensive study of VLT from three key perspectives: data quality, model architecture, and evaluation metrics. (1) We identify critical limitations in existing datasets, particularly in semantic and cultural fidelity, and introduce AibTrans -- a multilingual, parallel, human-verified dataset with OCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6 state-of-the-art open-source models across end-to-end and cascaded architectures, revealing their OCR dependency and contrasting generation versus reasoning behaviors. (3) We propose Density-Aware Evaluation to address metric reliability issues under varying contextual complexity, introducing the DA Score as a more robust measure of translation quality. Building upon these findings, we establish a new evaluation benchmark for VLT. Notably, we observe that fine-tuning LVLMs on high-resource language pairs degrades cross-lingual performance, and we propose a balanced multilingual fine-tuning strategy that effectively adapts LVLMs to VLT without sacrificing their generalization ability.",
        "arxiv_id": "2506.11820",
        "ARXIVID": "2506.11820",
        "COMMENT": "Matches criterion 2 as it evaluates multilingual vision-language translation and proposes a new dataset and evaluation strategy.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.11375": {
        "authors": [
            "Yitong Zhou",
            "Mingyue Cheng",
            "Qingyang Mao",
            "Yucong Luo",
            "Qi Liu",
            "Yupeng Li",
            "Xiaohan Zhang",
            "Deguang Liu",
            "Xin Li",
            "Enhong Chen"
        ],
        "title": "Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables",
        "abstract": "arXiv:2506.11375v1 Announce Type: new  Abstract: Chemical tables encode complex experimental knowledge through symbolic expressions, structured variables, and embedded molecular graphics. Existing benchmarks largely overlook this multimodal and domain-specific complexity, limiting the ability of multimodal large language models to support scientific understanding in chemistry. In this work, we introduce ChemTable, a large-scale benchmark of real-world chemical tables curated from the experimental sections of literature. ChemTable includes expert-annotated cell polygons, logical layouts, and domain-specific labels, including reagents, catalysts, yields, and graphical components and supports two core tasks: (1) Table Recognition, covering structure parsing and content extraction; and (2) Table Understanding, encompassing both descriptive and reasoning-oriented question answering grounded in table structure and domain semantics. We evaluated a range of representative multimodal models, including both open-source and closed-source models, on ChemTable and reported a series of findings with practical and conceptual insights. Although models show reasonable performance on basic layout parsing, they exhibit substantial limitations on both descriptive and inferential QA tasks compared to human performance, and we observe significant performance gaps between open-source and closed-source models across multiple dimensions. These results underscore the challenges of chemistry-aware table understanding and position ChemTable as a rigorous and realistic benchmark for advancing scientific reasoning.",
        "arxiv_id": "2506.11375",
        "ARXIVID": "2506.11375",
        "COMMENT": "Matches criterion 7 as it introduces a benchmark for multimodal LLMs in chemical table understanding, which is a domain-specific survey-like contribution.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.11380": {
        "authors": [
            "Xiaoxin Lu",
            "Ranran Haoran Zhang",
            "Yusen Zhang",
            "Rui Zhang"
        ],
        "title": "Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation",
        "abstract": "arXiv:2506.11380v1 Announce Type: new  Abstract: People get informed of a daily task plan through diverse media involving both texts and images. However, most prior research only focuses on LLM's capability of textual plan generation. The potential of large-scale models in providing text-image plans remains understudied. Generating high-quality text-image plans faces two main challenges: ensuring consistent alignment between two modalities and keeping coherence among visual steps. To address these challenges, we propose a novel framework that generates and refines text-image plans step-by-step. At each iteration, our framework (1) drafts the next textual step based on the prediction history; (2) edits the last visual step to obtain the next one; (3) extracts PDDL-like visual information; and (4) refines the draft with the extracted visual information. The textual and visual step produced in stage (4) and (2) will then serve as inputs for the next iteration. Our approach offers a plug-and-play improvement to various backbone models, such as Mistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our approach, we collect a new benchmark consisting of 1,100 tasks and their text-image pair solutions covering 11 daily topics. We also design and validate a new set of metrics to evaluate the multimodal consistency and coherence in text-image plans. Extensive experiment results show the effectiveness of our approach on a range of backbone models against competitive baselines. Our code and data are available at https://github.com/psunlpgroup/MPlanner.",
        "arxiv_id": "2506.11380",
        "ARXIVID": "2506.11380",
        "COMMENT": "Matches criterion 5 as it proposes a framework for generating text-image plans, integrating image and language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.11684": {
        "authors": [
            "Anshul Singh",
            "Chris Biemann",
            "Jan Strich"
        ],
        "title": "MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space",
        "abstract": "arXiv:2506.11684v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in interpreting visual layouts and text. However, a significant challenge remains in their ability to interpret robustly and reason over multi-tabular data presented as images, a common occurrence in real-world scenarios like web pages and digital documents. Existing benchmarks typically address single tables or non-visual data (text/structured). This leaves a critical gap: they don't assess the ability to parse diverse table images, correlate information across them, and perform multi-hop reasoning on the combined visual data. We introduce MTabVQA, a novel benchmark specifically designed for multi-tabular visual question answering to bridge that gap. MTabVQA comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning across several visually rendered table images. We provide extensive benchmark results for state-of-the-art VLMs on MTabVQA, revealing significant performance limitations. We further investigate post-training techniques to enhance these reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially improves their performance on visual multi-tabular reasoning. Code and dataset (https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval) are available online (https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E).",
        "arxiv_id": "2506.11684",
        "ARXIVID": "2506.11684",
        "COMMENT": "Matches criterion 5 as it introduces a benchmark for multi-tabular reasoning in visual space, combining image understanding and language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.11168": {
        "authors": [
            "Yanlong Chen",
            "Mattia Orlandi",
            "Pierangelo Maria Rapa",
            "Simone Benatti",
            "Luca Benini",
            "Yawei Li"
        ],
        "title": "WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture Recognition",
        "abstract": "arXiv:2506.11168v1 Announce Type: new  Abstract: Human-machine interaction, particularly in prosthetic and robotic control, has seen progress with gesture recognition via surface electromyographic (sEMG) signals.However, classifying similar gestures that produce nearly identical muscle signals remains a challenge, often reducing classification accuracy. Traditional deep learning models for sEMG gesture recognition are large and computationally expensive, limiting their deployment on resource-constrained embedded systems. In this work, we propose WaveFormer, a lightweight transformer-based architecture tailored for sEMG gesture recognition. Our model integrates time-domain and frequency-domain features through a novel learnable wavelet transform, enhancing feature extraction. In particular, the WaveletConv module, a multi-level wavelet decomposition layer with depthwise separable convolution, ensures both efficiency and compactness. With just 3.1 million parameters, WaveFormer achieves 95% classification accuracy on the EPN612 dataset, outperforming larger models. Furthermore, when profiled on a laptop equipped with an Intel CPU, INT8 quantization achieves real-time deployment with a 6.75 ms inference latency.",
        "arxiv_id": "2506.11168",
        "ARXIVID": "2506.11168",
        "COMMENT": "Matches criterion 3 as it introduces a lightweight transformer model for gesture recognition, relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.11131": {
        "authors": [
            "Tanner Schmidt",
            "Richard Newcombe"
        ],
        "title": "Segment This Thing: Foveated Tokenization for Efficient Point-Prompted Segmentation",
        "abstract": "arXiv:2506.11131v1 Announce Type: new  Abstract: This paper presents Segment This Thing (STT), a new efficient image segmentation model designed to produce a single segment given a single point prompt. Instead of following prior work and increasing efficiency by decreasing model size, we gain efficiency by foveating input images. Given an image and a point prompt, we extract a crop centered on the prompt and apply a novel variable-resolution patch tokenization in which patches are downsampled at a rate that increases with increased distance from the prompt. This approach yields far fewer image tokens than uniform patch tokenization. As a result we can drastically reduce the computational cost of segmentation without reducing model size. Furthermore, the foveation focuses the model on the region of interest, a potentially useful inductive bias. We show that our Segment This Thing model is more efficient than prior work while remaining competitive on segmentation benchmarks. It can easily run at interactive frame rates on consumer hardware and is thus a promising tool for augmented reality or robotics applications.",
        "arxiv_id": "2506.11131",
        "ARXIVID": "2506.11131",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for efficient segmentation, which could be useful in robotics or embodied AI applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.11487": {
        "authors": [
            "Chenrui Cao",
            "Liangcheng Song",
            "Zenan Li",
            "Xinyi Le",
            "Xian Zhang",
            "Hui Xue",
            "Fan Yang"
        ],
        "title": "Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models",
        "abstract": "arXiv:2506.11487v1 Announce Type: new  Abstract: Recent advancements, such as DeepSeek-Prover-V2-671B and Kimina-Prover-Preview-72B, demonstrate a prevailing trend in leveraging reinforcement learning (RL)-based large-scale training for automated theorem proving. Surprisingly, we discover that even without any training, careful neuro-symbolic coordination of existing off-the-shelf reasoning models and tactic step provers can achieve comparable performance. This paper introduces \\textbf{DSP+}, an improved version of the Draft, Sketch, and Prove framework, featuring a \\emph{fine-grained and integrated} neuro-symbolic enhancement for each phase: (1) In the draft phase, we prompt reasoning models to generate concise natural-language subgoals to benefit the sketch phase, removing thinking tokens and references to human-written proofs; (2) In the sketch phase, subgoals are autoformalized with hypotheses to benefit the proving phase, and sketch lines containing syntactic errors are masked according to predefined rules; (3) In the proving phase, we tightly integrate symbolic search methods like Aesop with step provers to establish proofs for the sketch subgoals. Experimental results show that, without any additional model training or fine-tuning, DSP+ solves 80.7\\%, 32.8\\%, and 24 out of 644 problems from miniF2F, ProofNet, and PutnamBench, respectively, while requiring fewer budgets compared to state-of-the-arts. DSP+ proves \\texttt{imo\\_2019\\_p1}, an IMO problem in miniF2F that is not solved by any prior work. Additionally, DSP+ generates proof patterns comprehensible by human experts, facilitating the identification of formalization errors; For example, eight wrongly formalized statements in miniF2F are discovered. Our results highlight the potential of classical reasoning patterns besides the RL-based training. All components will be open-sourced.",
        "arxiv_id": "2506.11487",
        "ARXIVID": "2506.11487",
        "COMMENT": "Does not match any specific criterion but discusses neuro-symbolic reasoning in theorem proving, which is tangentially related to general AI advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.11434": {
        "authors": [
            "Jie Zhu",
            "Leye Wang"
        ],
        "title": "Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection",
        "abstract": "arXiv:2506.11434v1 Announce Type: new  Abstract: Text-to-image diffusion model since its propose has significantly influenced the content creation due to its impressive generation capability. However, this capability depends on large-scale text-image datasets gathered from web platforms like social media, posing substantial challenges in copyright compliance and personal privacy leakage. Though there are some efforts devoted to explore approaches for auditing data provenance in text-to-image diffusion models, existing work has unrealistic assumptions that can obtain model internal knowledge, e.g., intermediate results, or the evaluation is not reliable. To fill this gap, we propose a completely black-box auditing framework called Feature Semantic Consistency-based Auditing (FSCA). It utilizes two types of semantic connections within the text-to-image diffusion model for auditing, eliminating the need for access to internal knowledge. To demonstrate the effectiveness of our FSCA framework, we perform extensive experiments on LAION-mi dataset and COCO dataset, and compare with eight state-of-the-art baseline approaches. The results show that FSCA surpasses previous baseline approaches across various metrics and different data distributions, showcasing the superiority of our FSCA. Moreover, we introduce a recall balance strategy and a threshold adjustment strategy, which collectively allows FSCA to reach up a user-level accuracy of 90% in a real-world auditing scenario with only 10 samples/user, highlighting its strong auditing potential in real-world applications. Our code is made available at https://github.com/JiePKU/FSCA.",
        "arxiv_id": "2506.11434",
        "ARXIVID": "2506.11434",
        "COMMENT": "Does not match any specific criteria but discusses auditing data provenance in text-to-image diffusion models, which is tangentially related to vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11661": {
        "authors": [
            "Yunhan Ren",
            "Ruihuang Li",
            "Lingbo Liu",
            "Changwen Chen"
        ],
        "title": "Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling",
        "abstract": "arXiv:2506.11661v1 Announce Type: new  Abstract: Instance segmentation of prohibited items in security X-ray images is a critical yet challenging task. This is mainly caused by the significant appearance gap between prohibited items in X-ray images and natural objects, as well as the severe overlapping among objects in X-ray images. To address these issues, we propose an occlusion-aware instance segmentation pipeline designed to identify prohibited items in X-ray images. Specifically, to bridge the representation gap, we integrate the Segment Anything Model (SAM) into our pipeline, taking advantage of its rich priors and zero-shot generalization capabilities. To address the overlap between prohibited items, we design an occlusion-aware bilayer mask decoder module that explicitly models the occlusion relationships. To supervise occlusion estimation, we manually annotated occlusion areas of prohibited items in two large-scale X-ray image segmentation datasets, PIDray and PIXray. We then reorganized these additional annotations together with the original information as two occlusion-annotated datasets, PIDray-A and PIXray-A. Extensive experimental results on these occlusion-annotated datasets demonstrate the effectiveness of our proposed method. The datasets and codes are available at: https://github.com/Ryh1218/Occ",
        "arxiv_id": "2506.11661",
        "ARXIVID": "2506.11661",
        "COMMENT": "Does not match any specific criteria but focuses on instance segmentation in X-ray images, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11493": {
        "authors": [
            "Tung-Long Vuong",
            "Hoang Phan",
            "Vy Vo",
            "Anh Bui",
            "Thanh-Toan Do",
            "Trung Le",
            "Dinh Phung"
        ],
        "title": "Preserving Clusters in Prompt Learning for Unsupervised Domain Adaptation",
        "abstract": "arXiv:2506.11493v1 Announce Type: new  Abstract: Recent approaches leveraging multi-modal pre-trained models like CLIP for Unsupervised Domain Adaptation (UDA) have shown significant promise in bridging domain gaps and improving generalization by utilizing rich semantic knowledge and robust visual representations learned through extensive pre-training on diverse image-text datasets. While these methods achieve state-of-the-art performance across benchmarks, much of the improvement stems from base pseudo-labels (CLIP zero-shot predictions) and self-training mechanisms. Thus, the training mechanism exhibits a key limitation wherein the visual embedding distribution in target domains can deviate from the visual embedding distribution in the pre-trained model, leading to misguided signals from class descriptions. This work introduces a fresh solution to reinforce these pseudo-labels and facilitate target-prompt learning, by exploiting the geometry of visual and text embeddings - an aspect that is overlooked by existing methods. We first propose to directly leverage the reference predictions (from source prompts) based on the relationship between source and target visual embeddings. We later show that there is a strong clustering behavior observed between visual and text embeddings in pre-trained multi-modal models. Building on optimal transport theory, we transform this insight into a novel strategy to enforce the clustering property in text embeddings, further enhancing the alignment in the target domain. Our experiments and ablation studies validate the effectiveness of the proposed approach, demonstrating superior performance and improved quality of target prompts in terms of representation.",
        "arxiv_id": "2506.11493",
        "ARXIVID": "2506.11493",
        "COMMENT": "Does not match any specific criteria but involves domain adaptation using multi-modal pre-trained models, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11653": {
        "authors": [
            "Emre Kavak",
            "Tom Nuno Wolf",
            "Christian Wachinger"
        ],
        "title": "DISCO: Mitigating Bias in Deep Learning with Conditional Distance Correlation",
        "abstract": "arXiv:2506.11653v1 Announce Type: new  Abstract: During prediction tasks, models can use any signal they receive to come up with the final answer - including signals that are causally irrelevant. When predicting objects from images, for example, the lighting conditions could be correlated to different targets through selection bias, and an oblivious model might use these signals as shortcuts to discern between various objects. A predictor that uses lighting conditions instead of real object-specific details is obviously undesirable. To address this challenge, we introduce a standard anti-causal prediction model (SAM) that creates a causal framework for analyzing the information pathways influencing our predictor in anti-causal settings. We demonstrate that a classifier satisfying a specific conditional independence criterion will focus solely on the direct causal path from label to image, being counterfactually invariant to the remaining variables. Finally, we propose DISCO, a novel regularization strategy that uses conditional distance correlation to optimize for conditional independence in regression tasks. We can show that DISCO achieves competitive results in different bias mitigation experiments, deeming it a valid alternative to classical kernel-based methods.",
        "arxiv_id": "2506.11653",
        "ARXIVID": "2506.11653",
        "COMMENT": "Does not match any specific criteria but focuses on bias mitigation in deep learning, which is tangentially related to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11764": {
        "authors": [
            "Muhammad Sarmad",
            "Arnt-B{\\o}rre Salberg",
            "Michael Kampffmeyer"
        ],
        "title": "DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models",
        "abstract": "arXiv:2506.11764v1 Announce Type: new  Abstract: This paper presents DiffFuSR, a modular pipeline for super-resolving all 12 spectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling distance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a diffusion-based super-resolution (SR) model trained on high-resolution RGB imagery from the NAIP and WorldStrat datasets, harmonized to simulate Sentinel-2 characteristics; and (ii) a learned fusion network that upscales the remaining multispectral bands using the super-resolved RGB image as a spatial prior. We introduce a robust degradation model and contrastive degradation encoder to support blind SR. Extensive evaluations of the proposed SR pipeline on the OpenSR benchmark demonstrate that the proposed method outperforms current SOTA baselines in terms of reflectance fidelity, spectral consistency, spatial alignment, and hallucination suppression. Furthermore, the fusion network significantly outperforms classical pansharpening approaches, enabling accurate enhancement of Sentinel-2's 20 m and 60 m bands. This study underscores the power of harmonized learning with generative priors and fusion strategies to create a modular framework for Sentinel-2 SR. Our code and models can be found at https://github.com/NorskRegnesentral/DiffFuSR.",
        "arxiv_id": "2506.11764",
        "ARXIVID": "2506.11764",
        "COMMENT": "Does not match any specific criteria but involves generative modeling for super-resolution in remote sensing, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11490": {
        "authors": [
            "Efthymia Amarantidou",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Panagiotis C. Petrantonakis"
        ],
        "title": "Composite Data Augmentations for Synthetic Image Detection Against Real-World Perturbations",
        "abstract": "arXiv:2506.11490v1 Announce Type: new  Abstract: The advent of accessible Generative AI tools enables anyone to create and spread synthetic images on social media, often with the intention to mislead, thus posing a significant threat to online information integrity. Most existing Synthetic Image Detection (SID) solutions struggle on generated images sourced from the Internet, as these are often altered by compression and other operations. To address this, our research enhances SID by exploring data augmentation combinations, leveraging a genetic algorithm for optimal augmentation selection, and introducing a dual-criteria optimization approach. These methods significantly improve model performance under real-world perturbations. Our findings provide valuable insights for developing detection models capable of identifying synthetic images across varying qualities and transformations, with the best-performing model achieving a mean average precision increase of +22.53% compared to models without augmentations. The implementation is available at github.com/efthimia145/sid-composite-data-augmentation.",
        "arxiv_id": "2506.11490",
        "ARXIVID": "2506.11490",
        "COMMENT": "Does not match any specific criterion but focuses on synthetic image detection, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11627": {
        "authors": [
            "Kuniko Paxton",
            "Koorosh Aslansefat",
            "Dhavalkumar Thakker",
            "Yiannis Papadopoulos"
        ],
        "title": "Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel Technique using Tensor Data and Bayesian Regression",
        "abstract": "arXiv:2506.11627v1 Announce Type: new  Abstract: Fairness is a critical component of Trustworthy AI. In this paper, we focus on Machine Learning (ML) and the performance of model predictions when dealing with skin color. Unlike other sensitive attributes, the nature of skin color differs significantly. In computer vision, skin color is represented as tensor data rather than categorical values or single numerical points. However, much of the research on fairness across sensitive groups has focused on categorical features such as gender and race. This paper introduces a new technique for evaluating fairness in ML for image classification tasks, specifically without the use of annotation. To address the limitations of prior work, we handle tensor data, like skin color, without classifying it rigidly. Instead, we convert it into probability distributions and apply statistical distance measures. This novel approach allows us to capture fine-grained nuances in fairness both within and across what would traditionally be considered distinct groups. Additionally, we propose an innovative training method to mitigate the latent biases present in conventional skin tone categorization. This method leverages color distance estimates calculated through Bayesian regression with polynomial functions, ensuring a more nuanced and equitable treatment of skin color in ML models.",
        "arxiv_id": "2506.11627",
        "ARXIVID": "2506.11627",
        "COMMENT": "Does not match any specific criterion but discusses fairness in ML for image classification, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11549": {
        "authors": [
            "Zhaoyang Wang",
            "Wen Lu",
            "Jie Li",
            "Lihuo He",
            "Maoguo Gong",
            "Xinbo Gao"
        ],
        "title": "EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment",
        "abstract": "arXiv:2506.11549v1 Announce Type: new  Abstract: Free-energy-guided self-repair mechanisms have shown promising results in image quality assessment (IQA), but remain under-explored in video quality assessment (VQA), where temporal dynamics and model constraints pose unique challenges. Unlike static images, video content exhibits richer spatiotemporal complexity, making perceptual restoration more difficult. Moreover, VQA systems often rely on pre-trained backbones, which limits the direct integration of enhancement modules without affecting model stability. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs-resized full-frame images and patch-based fragments-to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design.",
        "arxiv_id": "2506.11549",
        "ARXIVID": "2506.11549",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to video understanding through video quality assessment.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.12012": {
        "authors": [
            "Xiaopeng Yuan",
            "Xingjian Zhang",
            "Ke Xu",
            "Yifan Xu",
            "Lijun Yu",
            "Jindong Wang",
            "Yushun Dong",
            "Haohan Wang"
        ],
        "title": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making",
        "abstract": "arXiv:2506.12012v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used for tasks that require complex reasoning. Most benchmarks focus on final outcomes but overlook the intermediate reasoning steps - such as planning, revision, and decision making under resource constraints. We argue that measuring these internal processes is essential for understanding model behavior and improving reliability. We propose using strategic games as a natural evaluation environment: closed, rule-based systems with clear states, limited resources, and automatic feedback. We introduce a framework that evaluates LLMs along three core dimensions: planning, revision, and resource-constrained decision making. To operationalize this, we define metrics beyond win rate, including overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio. In 4320 adversarial rounds across 12 leading models, ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7 percent, a correction success rate of 78.6 percent, and an improvement slope of 0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6 percent, wins only 25.6 percent of its matches - primarily due to excessive resource use. We also observe a negative correlation between overcorrection risk rate and correction success rate (Pearson r = -0.51, p = 0.093), suggesting that more frequent edits do not always improve outcomes. Our findings highlight the value of assessing not only what LLMs decide but how they arrive at those decisions",
        "arxiv_id": "2506.12012",
        "ARXIVID": "2506.12012",
        "COMMENT": "Does not match any specific criteria but is related to general interest in reasoning processes and decision-making in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11555": {
        "authors": [
            "Yu Wang",
            "Shiwan Zhao",
            "Ming Fan",
            "Zhihu Wang",
            "Yubo Zhang",
            "Xicheng Zhang",
            "Zhengfan Wang",
            "Heyuan Huang",
            "Ting Liu"
        ],
        "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning",
        "abstract": "arXiv:2506.11555v1 Announce Type: new  Abstract: The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.",
        "arxiv_id": "2506.11555",
        "ARXIVID": "2506.11555",
        "COMMENT": "Does not match any specific criteria but is related to general interest in retrieval-augmented generation and reasoning in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11167": {
        "authors": [
            "Cheng Wang",
            "Yu Jiang",
            "Zhihao Peng",
            "Chenxin Li",
            "Changbae Bang",
            "Lin Zhao",
            "Jinglei Lv",
            "Jorge Sepulcre",
            "Carl Yang",
            "Lifang He",
            "Tianming Liu",
            "Daniel Barron",
            "Quanzheng Li",
            "Randy Hirschtick",
            "Byung-Hoon Kim",
            "Xiang Li",
            "Yixuan Yuan"
        ],
        "title": "Towards a general-purpose foundation model for fMRI analysis",
        "abstract": "arXiv:2506.11167v1 Announce Type: new  Abstract: Functional Magnetic Resonance Imaging (fMRI) is essential for studying brain function and diagnosing neurological disorders, but current analysis methods face reproducibility and transferability issues due to complex pre-processing and task-specific models. We introduce NeuroSTORM (Neuroimaging Foundation Model with Spatial-Temporal Optimized Representation Modeling), a generalizable framework that directly learns from 4D fMRI volumes and enables efficient knowledge transfer across diverse applications. NeuroSTORM is pre-trained on 28.65 million fMRI frames (>9,000 hours) from over 50,000 subjects across multiple centers and ages 5 to 100. Using a Mamba backbone and a shifted scanning strategy, it efficiently processes full 4D volumes. We also propose a spatial-temporal optimized pre-training approach and task-specific prompt tuning to improve transferability. NeuroSTORM outperforms existing methods across five tasks: age/gender prediction, phenotype prediction, disease diagnosis, fMRI-to-image retrieval, and task-based fMRI classification. It demonstrates strong clinical utility on datasets from hospitals in the U.S., South Korea, and Australia, achieving top performance in disease diagnosis and cognitive phenotype prediction. NeuroSTORM provides a standardized, open-source foundation model to improve reproducibility and transferability in fMRI-based clinical research.",
        "arxiv_id": "2506.11167",
        "ARXIVID": "2506.11167",
        "COMMENT": "Does not match any specific criteria but is related to general interest in foundation models and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.11880": {
        "authors": [
            "Alejandro Pe\\~na",
            "Julian Fierrez",
            "Aythami Morales",
            "Gonzalo Mancera",
            "Miguel Lopez",
            "Ruben Tolosana"
        ],
        "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment",
        "abstract": "arXiv:2506.11880v1 Announce Type: new  Abstract: The use of language technologies in high-stake settings is increasing in recent years, mostly motivated by the success of Large Language Models (LLMs). However, despite the great performance of LLMs, they are are susceptible to ethical concerns, such as demographic biases, accountability, or privacy. This work seeks to analyze the capacity of Transformers-based systems to learn demographic biases present in the data, using a case study on AI-based automated recruitment. We propose a privacy-enhancing framework to reduce gender information from the learning pipeline as a way to mitigate biased behaviors in the final tools. Our experiments analyze the influence of data biases on systems built on two different LLMs, and how the proposed framework effectively prevents trained systems from reproducing the bias in the data.",
        "arxiv_id": "2506.11880",
        "ARXIVID": "2506.11880",
        "COMMENT": "Does not match any specific criteria but is related to general interest in ethical concerns in AI and LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.11932": {
        "authors": [
            "Nishan Gunawardena",
            "Gough Yumu Lui",
            "Jeewani Anupama Ginige",
            "Bahman Javadi"
        ],
        "title": "Evaluating Sensitivity Parameters in Smartphone-Based Gaze Estimation: A Comparative Study of Appearance-Based and Infrared Eye Trackers",
        "abstract": "arXiv:2506.11932v1 Announce Type: new  Abstract: This study evaluates a smartphone-based, deep-learning eye-tracking algorithm by comparing its performance against a commercial infrared-based eye tracker, the Tobii Pro Nano. The aim is to investigate the feasibility of appearance-based gaze estimation under realistic mobile usage conditions. Key sensitivity factors, including age, gender, vision correction, lighting conditions, device type, and head position, were systematically analysed. The appearance-based algorithm integrates a lightweight convolutional neural network (MobileNet-V3) with a recurrent structure (Long Short-Term Memory) to predict gaze coordinates from grayscale facial images. Gaze data were collected from 51 participants using dynamic visual stimuli, and accuracy was measured using Euclidean distance. The deep learning model produced a mean error of 17.76 mm, compared to 16.53 mm for the Tobii Pro Nano. While overall accuracy differences were small, the deep learning-based method was more sensitive to factors such as lighting, vision correction, and age, with higher failure rates observed under low-light conditions among participants using glasses and in older age groups. Device-specific and positional factors also influenced tracking performance. These results highlight the potential of appearance-based approaches for mobile eye tracking and offer a reference framework for evaluating gaze estimation systems across varied usage conditions.",
        "arxiv_id": "2506.11932",
        "ARXIVID": "2506.11932",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}