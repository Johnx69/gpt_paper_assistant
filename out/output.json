{
    "2506.16940": {
        "authors": [
            "Annika Thomas",
            "Robaire Galliath",
            "Aleksander Garbuz",
            "Luke Anger",
            "Cormac O'Neill",
            "Trevor Johst",
            "Dami Thomas",
            "George Lordos",
            "Jonathan P. How"
        ],
        "title": "LunarLoc: Segment-Based Global Localization on the Moon",
        "abstract": "arXiv:2506.16940v1 Announce Type: new  Abstract: Global localization is necessary for autonomous operations on the lunar surface where traditional Earth-based navigation infrastructure, such as GPS, is unavailable. As NASA advances toward sustained lunar presence under the Artemis program, autonomous operations will be an essential component of tasks such as robotic exploration and infrastructure deployment. Tasks such as excavation and transport of regolith require precise pose estimation, but proposed approaches such as visual-inertial odometry (VIO) accumulate odometry drift over long traverses. Precise pose estimation is particularly important for upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on autonomous agents to operate over extended timescales and varied terrain. To help overcome odometry drift over long traverses, we propose LunarLoc, an approach to global localization that leverages instance segmentation for zero-shot extraction of boulder landmarks from onboard stereo imagery. Segment detections are used to construct a graph-based representation of the terrain, which is then aligned with a reference map of the environment captured during a previous session using graph-theoretic data association. This method enables accurate and drift-free global localization in visually ambiguous settings. LunarLoc achieves sub-cm level accuracy in multi-session global localization experiments, significantly outperforming the state of the art in lunar global localization. To encourage the development of further methods for global localization on the Moon, we release our datasets publicly with a playback module: https://github.com/mit-acl/lunarloc-data.",
        "arxiv_id": "2506.16940",
        "ARXIVID": "2506.16940",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods). The paper introduces a novel global localization method for lunar surface navigation, which is relevant to embodied AI and robotics.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.16806": {
        "authors": [
            "Fan Yang",
            "Yousong Zhu",
            "Xin Li",
            "Yufei Zhan",
            "Hongyin Zhao",
            "Shurong Zheng",
            "Yaowei Wang",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation",
        "abstract": "arXiv:2506.16806v1 Announce Type: new  Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editing. However, current approaches treat \"what to see\" and \"how to edit\" separately: they either perform isolated object segmentation or utilize segmentation masks merely as conditional prompts for local edit generation tasks, often relying on multiple disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM that integrates segmentation-aware perception and controllable object-centric generation within an end-to-end framework. FOCUS employs a dual-branch visual encoder to simultaneously capture global semantic context and fine-grained spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to produce discrete visual tokens that enhance generation quality. To enable accurate and controllable image editing, we propose a progressive multi-stage training pipeline, where segmentation masks are jointly optimized and used as spatial condition prompts to guide the diffusion decoder. This strategy aligns visual encoding, segmentation, and generation modules, effectively bridging segmentation-aware perception with fine-grained visual synthesis. Extensive experiments across three core tasks, including multimodal understanding, referring segmentation accuracy, and controllable image generation, demonstrate that FOCUS achieves strong performance by jointly optimizing visual perception and generative capabilities.",
        "arxiv_id": "2506.16806",
        "ARXIVID": "2506.16806",
        "COMMENT": "Matches criteria 2 and 5 closely as it explores a unified vision-language model (VLLM) for interactive editing and integrates image understanding and generation tasks with LLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.17221": {
        "authors": [
            "Zhangyang Qi",
            "Zhixiong Zhang",
            "Yizhou Yu",
            "Jiaqi Wang",
            "Hengshuang Zhao"
        ],
        "title": "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning",
        "abstract": "arXiv:2506.17221v1 Announce Type: new  Abstract: Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training.",
        "arxiv_id": "2506.17221",
        "ARXIVID": "2506.17221",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods). The paper introduces a novel framework for vision-language navigation using reinforcement fine-tuning, which is directly relevant to embodied AI and spatial reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.17113": {
        "authors": [
            "Shoubin Yu",
            "Yue Zhang",
            "Ziyang Wang",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "title": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation",
        "abstract": "arXiv:2506.17113v1 Announce Type: new  Abstract: Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.",
        "arxiv_id": "2506.17113",
        "ARXIVID": "2506.17113",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models). The paper introduces a framework for multimodal reasoning using expert models and large reasoning models, which aligns with vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.16673": {
        "authors": [
            "Ruiming Chen",
            "Junming Yang",
            "Shiyu Xia",
            "Xu Yang",
            "Jing Wang",
            "Xin Geng"
        ],
        "title": "Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge",
        "abstract": "arXiv:2506.16673v1 Announce Type: new  Abstract: CLIP (Contrastive Language-Image Pre-training) has attracted widespread attention for its multimodal generalizable knowledge, which is significant for downstream tasks. However, the computational overhead of a large number of parameters and large-scale pre-training poses challenges of pre-training a different scale of CLIP. Learngene extracts the generalizable components termed as learngene from an ancestry model and initializes diverse descendant models with it. Previous Learngene paradigms fail to handle the generalizable knowledge in multimodal scenarios. In this paper, we put forward the idea of utilizing a multimodal block to extract the multimodal generalizable knowledge, which inspires us to propose MM-LG (Multimodal Learngene), a novel framework designed to extract and leverage generalizable components from CLIP. Specifically, we first establish multimodal and unimodal blocks to extract the multimodal and unimodal generalizable knowledge in a weighted-sum manner. Subsequently, we employ these components to numerically initialize descendant models of varying scales and modalities. Extensive experiments demonstrate MM-LG's effectiveness, which achieves performance gains over existing learngene approaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and comparable or superior results to the pre-training and fine-tuning paradigm (e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG requires only around 25% of the parameter storage while reducing around 2.8 times pre-training costs for diverse model scales compared to the pre-training and fine-tuning paradigm, making it particularly suitable for efficient deployment across diverse downstream tasks.",
        "arxiv_id": "2506.16673",
        "ARXIVID": "2506.16673",
        "COMMENT": "Matches criteria 2 as it focuses on extracting multimodal generalizable knowledge from CLIP, a prominent vision-language model.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.17004": {
        "authors": [
            "Hanlin Wu",
            "Pengfei Lin",
            "Ehsan Javanmardi",
            "Naren Bao",
            "Bo Qian",
            "Hao Si",
            "Manabu Tsukada"
        ],
        "title": "A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving",
        "abstract": "arXiv:2506.17004v1 Announce Type: new  Abstract: 3D semantic occupancy prediction is an emerging perception paradigm in autonomous driving, providing a voxel-level representation of both geometric details and semantic categories. However, the perception capability of a single vehicle is inherently constrained by occlusion, restricted sensor range, and narrow viewpoints. To address these limitations, collaborative perception enables the exchange of complementary information, thereby enhancing the completeness and accuracy. In the absence of a dedicated dataset for collaborative 3D semantic occupancy prediction, we augment an existing collaborative perception dataset by replaying it in CARLA with a high-resolution semantic voxel sensor to provide dense and comprehensive occupancy annotations. In addition, we establish benchmarks with varying prediction ranges designed to systematically assess the impact of spatial extent on collaborative prediction. We further develop a baseline model that performs inter-agent feature fusion via spatial alignment and attention aggregation. Experimental results demonstrate that our baseline model consistently outperforms single-agent models, with increasing gains observed as the prediction range expands.",
        "arxiv_id": "2506.17004",
        "ARXIVID": "2506.17004",
        "COMMENT": "Matches criterion 3 as it introduces a synthetic benchmark for collaborative 3D semantic occupancy prediction in autonomous driving, addressing embodied AI challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.16353": {
        "authors": [
            "Chao He",
            "Hongxi Wei"
        ],
        "title": "MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval",
        "abstract": "arXiv:2506.16353v1 Announce Type: new  Abstract: Deep image hashing aims to enable effective large-scale image retrieval by mapping the input images into simple binary hash codes through deep neural networks. More recently, Vision Mamba with linear time complexity has attracted extensive attention from researchers by achieving outstanding performance on various computer tasks. Nevertheless, the suitability of Mamba for large-scale image retrieval tasks still needs to be explored. Towards this end, we propose a visual state space hashing model, called MambaHash. Concretely, we propose a backbone network with stage-wise architecture, in which grouped Mamba operation is introduced to model local and global information by utilizing Mamba to perform multi-directional scanning along different groups of the channel. Subsequently, the proposed channel interaction attention module is used to enhance information communication across channels. Finally, we meticulously design an adaptive feature enhancement module to increase feature diversity and enhance the visual representation capability of the model. We have conducted comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and IMAGENET. The experimental results demonstrate that compared with the state-of-the-art deep hashing methods, our proposed MambaHash has well efficiency and superior performance to effectively accomplish large-scale image retrieval tasks. Source code is available https://github.com/shuaichaochao/MambaHash.git",
        "arxiv_id": "2506.16353",
        "ARXIVID": "2506.16353",
        "COMMENT": "Matches criteria 4 as it focuses on a foundation model (MambaHash) for large-scale image retrieval in computer vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.15838": {
        "authors": [
            "Jiahao Wang",
            "Hualian Sheng",
            "Sijia Cai",
            "Weizhan Zhang",
            "Caixia Yan",
            "Yachuang Feng",
            "Bing Deng",
            "Jieping Ye"
        ],
        "title": "EchoShot: Multi-Shot Portrait Video Generation",
        "abstract": "arXiv:2506.15838v1 Announce Type: new  Abstract: Video diffusion models substantially boost the productivity of artistic workflows with high-quality portrait video generative capacity. However, prevailing pipelines are primarily constrained to single-shot creation, while real-world applications urge for multiple shots with identity consistency and flexible content controllability. In this work, we propose EchoShot, a native and scalable multi-shot framework for portrait customization built upon a foundation video diffusion model. To start with, we propose shot-aware position embedding mechanisms within video diffusion transformer architecture to model inter-shot variations and establish intricate correspondence between multi-shot visual content and their textual descriptions. This simple yet effective design enables direct training on multi-shot video data without introducing additional computational overhead. To facilitate model training within multi-shot scenario, we construct PortraitGala, a large-scale and high-fidelity human-centric video dataset featuring cross-shot identity consistency and fine-grained captions such as facial attributes, outfits, and dynamic motions. To further enhance applicability, we extend EchoShot to perform reference image-based personalized multi-shot generation and long video synthesis with infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves superior identity consistency as well as attribute-level controllability in multi-shot portrait video generation. Notably, the proposed framework demonstrates potential as a foundational paradigm for general multi-shot video modeling.",
        "arxiv_id": "2506.15838",
        "ARXIVID": "2506.15838",
        "COMMENT": "Matches criteria 6 as it focuses on multi-shot portrait video generation, which is a video understanding task.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.16730": {
        "authors": [
            "Mingrui Zhu",
            "Xiru Chen",
            "Xin Wei",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "title": "TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion",
        "abstract": "arXiv:2506.16730v1 Announce Type: new  Abstract: Infrared and visible image fusion (IVF) aims to combine complementary information from both image modalities, producing more informative and comprehensive outputs. Recently, text-guided IVF has shown great potential due to its flexibility and versatility. However, the effective integration and utilization of textual semantic information remains insufficiently studied. To tackle these challenges, we introduce textual semantics at two levels: the mask semantic level and the text semantic level, both derived from textual descriptions extracted by large Vision-Language Models (VLMs). Building on this, we propose Textual Semantic Guidance for infrared and visible image fusion, termed TeSG, which guides the image synthesis process in a way that is optimized for downstream tasks such as detection and segmentation. Specifically, TeSG consists of three core components: a Semantic Information Generator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven Attentional Fusion (TDAF) module. The SIG generates mask and text semantics based on textual descriptions. The MGCA module performs initial attention-based fusion of visual features from both infrared and visible images, guided by mask semantics. Finally, the TDAF module refines the fusion process with gated attention driven by text semantics. Extensive experiments demonstrate the competitiveness of our approach, particularly in terms of performance on downstream tasks, compared to existing state-of-the-art methods.",
        "arxiv_id": "2506.16730",
        "ARXIVID": "2506.16730",
        "COMMENT": "Matches criteria 5 as it integrates textual semantics from vision-language models into image fusion tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.17202": {
        "authors": [
            "Teng Li",
            "Quanfeng Lu",
            "Lirui Zhao",
            "Hao Li",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jun Zhang",
            "Wenqi Shao"
        ],
        "title": "UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation",
        "abstract": "arXiv:2506.17202v1 Announce Type: new  Abstract: Unified image understanding and generation has emerged as a promising paradigm in multimodal artificial intelligence. Despite recent progress, the optimal architectural design for such unified models remains an open challenge. In this work, we start by analyzing the modality alignment behaviors of task-specific expert models for understanding and generation, as well as current unified models. Our analysis reveals a crucial observation: understanding tasks benefit from a progressively increasing modality alignment across network depth, which helps build up semantic information for better comprehension; In contrast, generation tasks follow a different trend: modality alignment increases in the early layers but decreases in the deep layers to recover spatial details. These divergent alignment patterns create a fundamental conflict in fully shared Transformer backbones, where a uniform representational flow often leads to performance compromises across two tasks. Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture that shares the shallow layers for cross-task representation learning, while employing task-specific branches in deeper layers to avoid task interference. This design effectively balances shared learning and task specialization. Through extensive ablation experiments, we demonstrate that Unifork consistently outperforms conventional fully shared Transformer architectures, and achieves performance on par with or better than task-specific models.",
        "arxiv_id": "2506.17202",
        "ARXIVID": "2506.17202",
        "COMMENT": "Matches criterion 5 as it proposes a novel architecture for unified multimodal understanding and generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.16371": {
        "authors": [
            "Yunhao Hou",
            "Bochao Zou",
            "Min Zhang",
            "Ran Chen",
            "Shangdong Yang",
            "Yanmei Zhang",
            "Junbao Zhuo",
            "Siheng Chen",
            "Jiansheng Chen",
            "Huimin Ma"
        ],
        "title": "AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios",
        "abstract": "arXiv:2506.16371v1 Announce Type: new  Abstract: By sharing information across multiple agents, collaborative perception helps autonomous vehicles mitigate occlusions and improve overall perception accuracy. While most previous work focus on vehicle-to-vehicle and vehicle-to-infrastructure collaboration, with limited attention to aerial perspectives provided by UAVs, which uniquely offer dynamic, top-down views to alleviate occlusions and monitor large-scale interactive environments. A major reason for this is the lack of high-quality datasets for aerial-ground collaborative scenarios. To bridge this gap, we present AGC-Drive, the first large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The data collection platform consists of two vehicles, each equipped with five cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception. Consisting of approximately 120K LiDAR frames and 440K images, the dataset covers 14 diverse real-world driving scenarios, including urban roundabouts, highway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic interaction events, including vehicle cut-ins, cut-outs, and frequent lane changes. AGC-Drive contains 400 scenes, each with approximately 100 frames and fully annotated 3D bounding boxes covering 13 object categories. We provide benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative perception and vehicle-to-UAV collaborative perception. Additionally, we release an open-source toolkit, including spatiotemporal alignment verification tools, multi-agent visualization systems, and collaborative annotation utilities. The dataset and code are available at https://github.com/PercepX/AGC-Drive.",
        "arxiv_id": "2506.16371",
        "ARXIVID": "2506.16371",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for aerial-ground collaborative perception in autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.15940": {
        "authors": [
            "Zhongchen Zhao",
            "Chaodong Xiao",
            "Hui Lin",
            "Qi Xie",
            "Lei Zhang",
            "Deyu Meng"
        ],
        "title": "Polyline Path Masked Attention for Vision Transformer",
        "abstract": "arXiv:2506.15940v1 Announce Type: new  Abstract: Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks. Recently, Vision Transformers (ViTs) have achieved remarkable success in computer vision, leveraging the powerful global dependency modeling capability of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its significant potential in natural language processing tasks by explicitly modeling the spatial adjacency prior through the structured mask. In this paper, we propose Polyline Path Masked Attention (PPMA) that integrates the self-attention mechanism of ViTs with an enhanced structured mask of Mamba2, harnessing the complementary strengths of both architectures. Specifically, we first ameliorate the traditional structured mask of Mamba2 by introducing a 2D polyline path scanning strategy and derive its corresponding structured mask, polyline path mask, which better preserves the adjacency relationships among image tokens. Notably, we conduct a thorough theoretical analysis on the structural characteristics of the proposed polyline path mask and design an efficient algorithm for the computation of the polyline path mask. Next, we embed the polyline path mask into the self-attention mechanism of ViTs, enabling explicit modeling of spatial adjacency prior. Extensive experiments on standard benchmarks, including image classification, object detection, and segmentation, demonstrate that our model outperforms previous state-of-the-art approaches based on both state-space models and Transformers. For example, our proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%, respectively. Code is available at https://github.com/zhongchenzhao/PPMA.",
        "arxiv_id": "2506.15940",
        "ARXIVID": "2506.15940",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications). The paper proposes a novel attention mechanism for Vision Transformers, which is relevant to foundational architectures in computer vision.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.16737": {
        "authors": [
            "Liu Zongzhen",
            "Luo Hui",
            "Wang Zhixing",
            "Wei Yuxing",
            "Zuo Haorui",
            "Zhang Jianlin"
        ],
        "title": "Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection",
        "abstract": "arXiv:2506.16737v1 Announce Type: new  Abstract: Unmanned aerial vehicle (UAV) object detection plays a vital role in applications such as environmental monitoring and urban security. To improve robustness, recent studies have explored multimodal detection by fusing visible (RGB) and infrared (IR) imagery. However, due to UAV platform motion and asynchronous imaging, spatial misalignment frequently occurs between modalities, leading to weak alignment. This introduces two major challenges: semantic inconsistency at corresponding spatial locations and modality conflict during feature fusion. Existing methods often address these issues in isolation, limiting their effectiveness. In this paper, we propose Cross-modal Offset-guided Dynamic Alignment and Fusion (CoDAF), a unified framework that jointly tackles both challenges in weakly aligned UAV-based object detection. CoDAF comprises two novel modules: the Offset-guided Semantic Alignment (OSA), which estimates attention-based spatial offsets and uses deformable convolution guided by a shared semantic space to align features more precisely; and the Dynamic Attention-guided Fusion Module (DAFM), which adaptively balances modality contributions through gating and refines fused features via spatial-channel dual attention. By integrating alignment and fusion in a unified design, CoDAF enables robust UAV object detection. Experiments on standard benchmarks validate the effectiveness of our approach, with CoDAF achieving a mAP of 78.6% on the DroneVehicle dataset.",
        "arxiv_id": "2506.16737",
        "ARXIVID": "2506.16737",
        "COMMENT": "Matches criterion 5 as it proposes a cross-modal alignment and fusion technique for UAV object detection, integrating RGB and IR modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.16054": {
        "authors": [
            "Tianchen Zhao",
            "Ke Hong",
            "Xinhao Yang",
            "Xuefeng Xiao",
            "Huixia Li",
            "Feng Ling",
            "Ruiqi Xie",
            "Siqi Chen",
            "Hongyu Zhu",
            "Yichong Zhang",
            "Yu Wang"
        ],
        "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models",
        "abstract": "arXiv:2506.16054v1 Announce Type: new  Abstract: In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup.",
        "arxiv_id": "2506.16054",
        "ARXIVID": "2506.16054",
        "COMMENT": "Matches criterion 4 as it proposes a novel attention mechanism (PAROAttention) for visual generation models, improving efficiency and performance.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.16112": {
        "authors": [
            "Yuan Zhang",
            "Chun-Kai Fan",
            "Tao Huang",
            "Ming Lu",
            "Sicheng Yu",
            "Junwen Pan",
            "Kuan Cheng",
            "Qi She",
            "Shanghang Zhang"
        ],
        "title": "AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models",
        "abstract": "arXiv:2506.16112v1 Announce Type: new  Abstract: Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuristic visual prompts, such as overlaying a text-query-guided attention heatmap on the original input image. However, designing effective prompts manually is challenging and time-consuming, and it often fails to explore the benefits of different visual prompts, leading to sub-optimal performance. To this end, we propose \\textbf{AutoV} that learns to automatically select the optimal visual prompt from various candidates based on given textual queries and the input image. To train AutoV, we developed an automatic data collection and labeling pipeline that evaluates various visual prompts with a pre-trained LVLM. We input a set of visual prompts into the LVLM and rank them according to the prediction losses generated by the model. Using the ranking as a supervision signal, we train AutoV to automatically choose the optimal visual prompt from various visual prompts for LVLMs. Experimental results indicate that AutoV enhances the performance of various LVLMs across multiple popular image understanding tasks. For instance, LLaVA-OV with AutoV achieves $\\textbf{1.7}\\%$ accuracy gain on LLaVA$^{\\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\\textbf{1.9}\\%$ on MMMU, highlighting its potential as an optimal visual prompting method for LVLMs.",
        "arxiv_id": "2506.16112",
        "ARXIVID": "2506.16112",
        "COMMENT": "Matches criterion 2 as it explores visual prompts for large vision-language models (LVLMs), focusing on enhancing reasoning capabilities and performance.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.17220": {
        "authors": [
            "Jisu Nam",
            "Soowon Son",
            "Dahyun Chung",
            "Jiyoung Kim",
            "Siyoon Jin",
            "Junhwa Hur",
            "Seungryong Kim"
        ],
        "title": "Emergent Temporal Correspondences from Video Diffusion Transformers",
        "abstract": "arXiv:2506.17220v1 Announce Type: new  Abstract: Recent advancements in video diffusion models based on Diffusion Transformers (DiTs) have achieved remarkable success in generating temporally coherent videos. Yet, a fundamental question persists: how do these models internally establish and represent temporal correspondences across frames? We introduce DiffTrack, the first quantitative analysis framework designed to answer this question. DiffTrack constructs a dataset of prompt-generated video with pseudo ground-truth tracking annotations and proposes novel evaluation metrics to systematically analyze how each component within the full 3D attention mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to establishing temporal correspondences. Our analysis reveals that query-key similarities in specific, but not all, layers play a critical role in temporal matching, and that this matching becomes increasingly prominent during the denoising process. We demonstrate practical applications of DiffTrack in zero-shot point tracking, where it achieves state-of-the-art performance compared to existing vision foundation and self-supervised video models. Further, we extend our findings to motion-enhanced video generation with a novel guidance method that improves temporal consistency of generated videos without additional training. We believe our work offers crucial insights into the inner workings of video DiTs and establishes a foundation for further research and applications leveraging their temporal understanding.",
        "arxiv_id": "2506.17220",
        "ARXIVID": "2506.17220",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it analyzes temporal correspondences in video diffusion models and introduces a framework for zero-shot point tracking.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.16141": {
        "authors": [
            "Yi Chen",
            "Yuying Ge",
            "Rui Wang",
            "Yixiao Ge",
            "Junhao Cheng",
            "Ying Shan",
            "Xihui Liu"
        ],
        "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning",
        "abstract": "arXiv:2506.16141v1 Announce Type: new  Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.",
        "arxiv_id": "2506.16141",
        "ARXIVID": "2506.16141",
        "COMMENT": "Matches criterion 2 as it explores multimodal reasoning in large language models with reinforcement learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.16931": {
        "authors": [
            "Jiaqi Chen",
            "Mingfeng Fan",
            "Xuefeng Zhang",
            "Jingsong Liang",
            "Yuhong Cao",
            "Guohua Wu",
            "Guillaume Adrien Sartoretti"
        ],
        "title": "Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning",
        "abstract": "arXiv:2506.16931v1 Announce Type: new  Abstract: Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.",
        "arxiv_id": "2506.16931",
        "ARXIVID": "2506.16931",
        "COMMENT": "Matches criterion 1 as it addresses spatial reasoning in robotic task planning using multimodal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.16796": {
        "authors": [
            "Junbo Qiao",
            "Miaomiao Cai",
            "Wei Li",
            "Yutong Liu",
            "Xudong Huang",
            "Gaoqi He",
            "Jiao Xie",
            "Jie Hu",
            "Xinghao Chen",
            "Shaohui Lin"
        ],
        "title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought",
        "abstract": "arXiv:2506.16796v1 Announce Type: new  Abstract: Real-World Image Super-Resolution is one of the most challenging task in image restoration. However, existing methods struggle with an accurate understanding of degraded image content, leading to reconstructed results that are both low-fidelity and unnatural. We present RealSR-R1 in this work, which empowers the RealSR models with understanding and reasoning capabilities. Inspired by the success of Chain of Thought (CoT) in large language models (LLMs), we simulate the human process of handling degraded images and propose the VLCoT framework, which integrates vision and language reasoning. The framework aims to precisely restore image details by progressively generating more comprehensive text and higher-resolution images. To overcome the challenge of traditional supervised learning CoT failing to generalize to real-world scenarios, we introduce, for the first time, Group Relative Policy Optimization (GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO as a solution, which designs four reward functions: (1) Format reward, used to standardize the CoT process; (2) Degradation reward, to incentivize accurate degradation estimation; (3) Understanding reward, to ensure the accuracy of the generated content; and (4) Generation reward, where we propose using a visual expert model to evaluate the quality of generated images, encouraging the model to generate more realistic images. Extensive experiments demonstrate that our proposed RealSR-R1 can generate realistic details and accurately understand image content, particularly in semantically rich scenes or images with severe degradation.",
        "arxiv_id": "2506.16796",
        "ARXIVID": "2506.16796",
        "COMMENT": "Matches criterion 5 as it integrates vision and language reasoning for image super-resolution.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.15741": {
        "authors": [
            "He Zhu",
            "Tianrui Qin",
            "King Zhu",
            "Heyuan Huang",
            "Yeyi Guan",
            "Jinxiang Xia",
            "Yi Yao",
            "Hanhao Li",
            "Ningning Wang",
            "Pai Liu",
            "Tianhao Peng",
            "Xin Gui",
            "Xiaowan Li",
            "Yuhui Liu",
            "Yuchen Eleanor Jiang",
            "Jun Wang",
            "Changwang Zhang",
            "Xiangru Tang",
            "Ge Zhang",
            "Jian Yang",
            "Minghao Liu",
            "Xitong Gao",
            "Wangchunshu Zhou",
            "Jiaheng Liu"
        ],
        "title": "OAgents: An Empirical Study of Building Effective Agents",
        "abstract": "arXiv:2506.15741v1 Announce Type: new  Abstract: Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.",
        "arxiv_id": "2506.15741",
        "ARXIVID": "2506.15741",
        "COMMENT": "Matches criterion 3 as it provides a systematic empirical study and introduces a new agent framework (OAgents) for embodied AI research.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.16856": {
        "authors": [
            "Jun Fu",
            "Bin Tian",
            "Haonan Chen",
            "Shi Meng",
            "Tingting Yao"
        ],
        "title": "ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control",
        "abstract": "arXiv:2506.16856v1 Announce Type: new  Abstract: Autonomous parking plays a vital role in intelligent vehicle systems, particularly in constrained urban environments where high-precision control is required. While traditional rule-based parking systems struggle with environmental uncertainties and lack adaptability in crowded or dynamic scenes, human drivers demonstrate the ability to park intuitively without explicit modeling. Inspired by this observation, we propose a Transformer-based end-to-end framework for autonomous parking that learns from expert demonstrations. The network takes as input surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. It outputs discrete control sequences including throttle, braking, steering, and gear selection. A novel cross-attention module integrates BEV features with target points, and a GRU-based pedestrian predictor enhances safety by modeling dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both vertical and parallel parking scenarios. Experiments show our model achieves a high success rate of 96.57\\%, with average positional and orientation errors of 0.21 meters and 0.41 degrees, respectively. The ablation studies further demonstrate the effectiveness of key modules such as pedestrian prediction and goal-point attention fusion. The code and dataset will be released at: https://github.com/little-snail-f/ParkFormer.",
        "arxiv_id": "2506.16856",
        "ARXIVID": "2506.16856",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for autonomous parking using a transformer-based framework, addressing embodied AI challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.16042": {
        "authors": [
            "Reyna Abhyankar",
            "Qi Qi",
            "Yiying Zhang"
        ],
        "title": "OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents",
        "abstract": "arXiv:2506.16042v1 Announce Type: new  Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks involving desktop applications. State-of-the-art systems have focused solely on improving accuracy on leading benchmarks. However, these systems are practically unusable due to extremely high end-to-end latency (e.g., tens of minutes) for tasks that typically take humans just a few minutes to complete. To understand the cause behind this and to guide future developments of computer agents, we conduct the first study on the temporal performance of computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We find that large model calls for planning and reflection account for the majority of the overall latency, and as an agent uses more steps to complete a task, each successive step can take 3x longer than steps at the beginning of a task. We then construct OSWorld-Human, a manually annotated version of the original OSWorld dataset that contains a human-determined trajectory for each task. We evaluate 16 agents on their efficiency using OSWorld-Human and found that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than necessary.",
        "arxiv_id": "2506.16042",
        "ARXIVID": "2506.16042",
        "COMMENT": "Matches criteria 3 as it introduces a benchmark (OSWorld-Human) for evaluating computer-use agents, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.15757": {
        "authors": [
            "Ruoyu Wang",
            "Tong Yu",
            "Junda Wu",
            "Yao Liu",
            "Julian McAuley",
            "Lina Yao"
        ],
        "title": "Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation",
        "abstract": "arXiv:2506.15757v1 Announce Type: new  Abstract: Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.",
        "arxiv_id": "2506.15757",
        "ARXIVID": "2506.15757",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it proposes a method for Visual Language Navigation, enhancing spatial reasoning in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.16061": {
        "authors": [
            "Yucheng Jin",
            "Jinyan Chen",
            "Ziyue He",
            "Baojun Han",
            "Furan An"
        ],
        "title": "STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution",
        "abstract": "arXiv:2506.16061v1 Announce Type: new  Abstract: Human pose estimation in low-resolution videos presents a fundamental challenge in computer vision. Conventional methods either assume high-quality inputs or employ computationally expensive cascaded processing, which limits their deployment in resource-constrained environments. We propose STAR-Pose, a spatial-temporal adaptive super-resolution framework specifically designed for video-based human pose estimation. Our method features a novel spatial-temporal Transformer with LeakyReLU-modified linear attention, which efficiently captures long-range temporal dependencies. Moreover, it is complemented by an adaptive fusion module that integrates parallel CNN branch for local texture enhancement. We also design a pose-aware compound loss to achieve task-oriented super-resolution. This loss guides the network to reconstruct structural features that are most beneficial for keypoint localization, rather than optimizing purely for visual quality. Extensive experiments on several mainstream video HPE datasets demonstrate that STAR-Pose outperforms existing approaches. It achieves up to 5.2% mAP improvement under extremely low-resolution (64x48) conditions while delivering 2.8x to 4.4x faster inference than cascaded approaches.",
        "arxiv_id": "2506.16061",
        "ARXIVID": "2506.16061",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel method for human pose estimation in low-resolution videos, which is a video-based task.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.16802": {
        "authors": [
            "Riccardo Corvi",
            "Davide Cozzolino",
            "Ekta Prashnani",
            "Shalini De Mello",
            "Koki Nagano",
            "Luisa Verdoliva"
        ],
        "title": "Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation",
        "abstract": "arXiv:2506.16802v1 Announce Type: new  Abstract: Synthetic video generation is progressing very rapidly. The latest models can produce very realistic high-resolution videos that are virtually indistinguishable from real ones. Although several video forensic detectors have been recently proposed, they often exhibit poor generalization, which limits their applicability in a real-world scenario. Our key insight to overcome this issue is to guide the detector towards seeing what really matters. In fact, a well-designed forensic classifier should focus on identifying intrinsic low-level artifacts introduced by a generative architecture rather than relying on high-level semantic flaws that characterize a specific model. In this work, first, we study different generative architectures, searching and identifying discriminative features that are unbiased, robust to impairments, and shared across models. Then, we introduce a novel forensic-oriented data augmentation strategy based on the wavelet decomposition and replace specific frequency-related bands to drive the model to exploit more relevant forensic cues. Our novel training paradigm improves the generalizability of AI-generated video detectors, without the need for complex algorithms and large datasets that include multiple synthetic generators. To evaluate our approach, we train the detector using data from a single generative model and test it against videos produced by a wide range of other models. Despite its simplicity, our method achieves a significant accuracy improvement over state-of-the-art detectors and obtains excellent results even on very recent generative models, such as NOVA and FLUX. Code and data will be made publicly available.",
        "arxiv_id": "2506.16802",
        "ARXIVID": "2506.16802",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on detecting AI-generated videos and improving generalization in video forensic tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.16784": {
        "authors": [
            "Xiaoyu Shi",
            "Rahul Kumar Jain",
            "Yinhao Li",
            "Ruibo Hou",
            "Jingliang Cheng",
            "Jie Bai",
            "Guohua Zhao",
            "Lanfen Lin",
            "Rui Xu",
            "Yen-wei Chen"
        ],
        "title": "TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration",
        "abstract": "arXiv:2506.16784v1 Announce Type: new  Abstract: Deep learning has demonstrated remarkable success in medical image segmentation and computer-aided diagnosis. In particular, numerous advanced methods have achieved state-of-the-art performance in brain tumor segmentation from MRI scans. While recent studies in other medical imaging domains have revealed that integrating textual reports with visual data can enhance segmentation accuracy, the field of brain tumor analysis lacks a comprehensive dataset that combines radiological images with corresponding textual annotations. This limitation has hindered the exploration of multimodal approaches that leverage both imaging and textual data.   To bridge this critical gap, we introduce the TextBraTS dataset, the first publicly available volume-level multimodal dataset that contains paired MRI volumes and rich textual annotations, derived from the widely adopted BraTS2020 benchmark. Building upon this novel dataset, we propose a novel baseline framework and sequential cross-attention method for text-guided volumetric medical image segmentation. Through extensive experiments with various text-image fusion strategies and templated text formulations, our approach demonstrates significant improvements in brain tumor segmentation accuracy, offering valuable insights into effective multimodal integration techniques.   Our dataset, implementation code, and pre-trained models are publicly available at https://github.com/Jupitern52/TextBraTS.",
        "arxiv_id": "2506.16784",
        "ARXIVID": "2506.16784",
        "COMMENT": "Matches criterion 2 as it explores multimodal integration of text and visual data for medical image segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.15734": {
        "authors": [
            "Peiyuan Tang",
            "Haojie Xin",
            "Xiaodong Zhang",
            "Jun Sun",
            "Qin Xia",
            "Zijiang Yang"
        ],
        "title": "The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models",
        "abstract": "arXiv:2506.15734v1 Announce Type: new  Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across real-world applications such as code generation and chatbot assistance, ensuring their safety has become paramount. Unlike traditional Large Language Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to modify visual or textual inputs to bypass safety guardrails and trigger the generation of harmful content. Through systematic analysis of VLM behavior under attack, we identify a novel phenomenon termed ``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs may initially be compromised to produce harmful content, but eventually recognize the associated risks and attempt to self-correct. This pattern suggests that VLMs retain their underlying safety awareness but experience a temporal delay in their activation. Building on this insight, we hypothesize that VLMs' safety awareness can be proactively reactivated through carefully designed prompts. To this end, we introduce ``The Safety Reminder'', a soft prompt tuning approach that optimizes learnable prompt tokens, which are periodically injected during the text generation process to enhance safety awareness, effectively preventing harmful content generation. Additionally, our safety reminder only activates when harmful content is detected, leaving normal conversations unaffected and preserving the model's performance on benign tasks. Through comprehensive evaluation across three established safety benchmarks and one adversarial attacks, we demonstrate that our approach significantly reduces attack success rates while maintaining model utility, offering a practical solution for deploying safer VLMs in real-world applications.",
        "arxiv_id": "2506.15734",
        "ARXIVID": "2506.15734",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a safety prompt for Vision-Language Models to prevent harmful content generation.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2506.15755": {
        "authors": [
            "Xiasi Wang",
            "Tianliang Yao",
            "Simin Chen",
            "Runqi Wang",
            "Lei YE",
            "Kuofeng Gao",
            "Yi Huang",
            "Yuan Yao"
        ],
        "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service",
        "abstract": "arXiv:2506.15755v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated great potential in real-world applications. While existing research primarily focuses on improving their accuracy, the efficiency remains underexplored. Given the real-time demands of many applications and the high inference overhead of VLMs, efficiency robustness is a critical issue. However, previous studies evaluate efficiency robustness under unrealistic assumptions, requiring access to the model architecture and parameters -- an impractical scenario in ML-as-a-service settings, where VLMs are deployed via inference APIs. To address this gap, we propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness in a realistic black-box setting. VLMInferSlow incorporates fine-grained efficiency modeling tailored to VLM inference and leverages zero-order optimization to search for adversarial examples. Experimental results show that VLMInferSlow generates adversarial images with imperceptible perturbations, increasing the computational cost by up to 128.47%. We hope this research raises the community's awareness about the efficiency robustness of VLMs.",
        "arxiv_id": "2506.15755",
        "ARXIVID": "2506.15755",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it evaluates the efficiency robustness of Vision-Language Models in a novel black-box setting.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2506.16398": {
        "authors": [
            "Peixiang Huang",
            "Yanyan Huang",
            "Weiqin Zhao",
            "Junjun He",
            "Lequan Yu"
        ],
        "title": "HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis",
        "abstract": "arXiv:2506.16398v1 Announce Type: new  Abstract: Pathology is essential for cancer diagnosis, with multiple instance learning (MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural hierarchy -- patches, regions, and slides -- with distinct semantic associations. While some methods attempt to leverage this hierarchy for improved representation, they predominantly rely on Euclidean embeddings, which struggle to fully capture semantic hierarchies. To address this limitation, we propose HyperPath, a novel method that integrates knowledge from textual descriptions to guide the modeling of semantic hierarchies of WSIs in hyperbolic space, thereby enhancing WSI classification. Our approach adapts both visual and textual features extracted by pathology vision-language foundation models to the hyperbolic space. We design an Angular Modality Alignment Loss to ensure robust cross-modal alignment, while a Semantic Hierarchy Consistency Loss further refines feature hierarchies through entailment and contradiction relationships and thus enhance semantic coherence. The classification is performed with geodesic distance, which measures the similarity between entities in the hyperbolic semantic hierarchy. This eliminates the need for linear classifiers and enables a geometry-aware approach to WSI analysis. Extensive experiments show that our method achieves superior performance across tasks compared to existing methods, highlighting the potential of hyperbolic embeddings for WSI analysis.",
        "arxiv_id": "2506.16398",
        "ARXIVID": "2506.16398",
        "COMMENT": "No criteria match closely. The paper focuses on hyperbolic semantic hierarchy modeling for pathology image analysis, which is not directly related to the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16504": {
        "authors": [
            "Zeqiang Lai",
            "Yunfei Zhao",
            "Haolin Liu",
            "Zibo Zhao",
            "Qingxiang Lin",
            "Huiwen Shi",
            "Xianghui Yang",
            "Mingxin Yang",
            "Shuhui Yang",
            "Yifei Feng",
            "Sheng Zhang",
            "Xin Huang",
            "Di Luo",
            "Fan Yang",
            "Fang Yang",
            "Lifu Wang",
            "Sicong Liu",
            "Yixuan Tang",
            "Yulin Cai",
            "Zebin He",
            "Tian Liu",
            "Yuhong Liu",
            "Jie Jiang",
            "Linus",
            "Jingwei Huang",
            "Chunchao Guo"
        ],
        "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details",
        "abstract": "arXiv:2506.16504v1 Announce Type: new  Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",
        "arxiv_id": "2506.16504",
        "ARXIVID": "2506.16504",
        "COMMENT": "Does not closely match any specific criterion but is relevant to generative modeling in 3D assets, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16330": {
        "authors": [
            "Ji Zhang",
            "Jingkuan Song",
            "Lianli Gao",
            "Nicu Sebe",
            "Heng Tao Shen"
        ],
        "title": "Reliable Few-shot Learning under Dual Noises",
        "abstract": "arXiv:2506.16330v1 Announce Type: new  Abstract: Recent advances in model pre-training give rise to task adaptation-based few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic model for capturing task-specific knowledge with a few-labeled support samples of the target task.Nevertheless, existing approaches may still fail in the open world due to the inevitable in-distribution (ID) and out-of-distribution (OOD) noise from both support and query samples of the target task. With limited support samples available, i) the adverse effect of the dual noises can be severely amplified during task adaptation, and ii) the adapted model can produce unreliable predictions on query samples in the presence of the dual noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate image and region weights for support samples, based on which a clean prototype loss and a noise entropy maximization loss are proposed to achieve noise-robust task adaptation. Additionally,DETA++ employs a memory bank to store and refine clean regions for each inner-task class, based on which a Local Nearest Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping (IntraSwap) strategy to rectify ID class prototypes during task adaptation, enhancing the model's robustness to the dual noises. Extensive experiments demonstrate the effectiveness and flexibility of DETA++.",
        "arxiv_id": "2506.16330",
        "ARXIVID": "2506.16330",
        "COMMENT": "Does not match any specific criteria but focuses on few-shot learning under noise, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.17136": {
        "authors": [
            "Dongdong Meng",
            "Sheng Li",
            "Hao Wu",
            "Guoping Wang",
            "Xueqing Yan"
        ],
        "title": "Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations",
        "abstract": "arXiv:2506.17136v1 Announce Type: new  Abstract: Semi-supervised learning addresses the issue of limited annotations in medical images effectively, but its performance is often inadequate for complex backgrounds and challenging tasks. Multi-modal fusion methods can significantly improve the accuracy of medical image segmentation by providing complementary information. However, they face challenges in achieving significant improvements under semi-supervised conditions due to the challenge of effectively leveraging unlabeled data. There is a significant need to create an effective and reliable multi-modal learning strategy for leveraging unlabeled data in semi-supervised segmentation. To address these issues, we propose a novel semi-supervised multi-modal medical image segmentation approach, which leverages complementary multi-modal information to enhance performance with limited labeled data. Our approach employs a multi-stage multi-modal fusion and enhancement strategy to fully utilize complementary multi-modal information, while reducing feature discrepancies and enhancing feature sharing and alignment. Furthermore, we effectively introduce contrastive mutual learning to constrain prediction consistency across modalities, thereby facilitating the robustness of segmentation results in semi-supervised tasks. Experimental results on two multi-modal datasets demonstrate the superior performance and robustness of the proposed framework, establishing its valuable potential for solving medical image segmentation tasks in complex scenarios.",
        "arxiv_id": "2506.17136",
        "ARXIVID": "2506.17136",
        "COMMENT": "Does not match any specific criteria but focuses on semi-supervised multi-modal medical image segmentation, which is tangentially relevant to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16743": {
        "authors": [
            "Weinan Guan",
            "Wei Wang",
            "Bo Peng",
            "Ziwen He",
            "Jing Dong",
            "Haonan Cheng"
        ],
        "title": "Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention",
        "abstract": "arXiv:2506.16743v1 Announce Type: new  Abstract: With the rapid development of image generation technologies, especially the advancement of Diffusion Models, the quality of synthesized images has significantly improved, raising concerns among researchers about information security. To mitigate the malicious abuse of diffusion models, diffusion-generated image detection has proven to be an effective countermeasure.However, a key challenge for forgery detection is generalising to diffusion models not seen during training. In this paper, we address this problem by focusing on image noise. We observe that images from different diffusion models share similar noise patterns, distinct from genuine images. Building upon this insight, we introduce a novel Noise-Aware Self-Attention (NASA) module that focuses on noise regions to capture anomalous patterns. To implement a SOTA detection model, we incorporate NASA into Swin Transformer, forming an novel detection architecture NASA-Swin. Additionally, we employ a cross-modality fusion embedding to combine RGB and noise images, along with a channel mask strategy to enhance feature learning from both modalities. Extensive experiments demonstrate the effectiveness of our approach in enhancing detection capabilities for diffusion-generated images. When encountering unseen generation methods, our approach achieves the state-of-the-art performance.Our code is available at https://github.com/WeinanGuan/NASA-Swin.",
        "arxiv_id": "2506.16743",
        "ARXIVID": "2506.16743",
        "COMMENT": "Does not match any specific criteria but focuses on detecting diffusion-generated images, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.15735": {
        "authors": [
            "Robert Graham",
            "Edward Stevinson",
            "Leo Richter",
            "Alexander Chia",
            "Joseph Miller",
            "Joseph Isaac Bloom"
        ],
        "title": "ContextBench: Modifying Contexts for Targeted Latent Activation",
        "abstract": "arXiv:2506.15735v1 Announce Type: new  Abstract: Identifying inputs that trigger specific behaviours or latent features in language models could have a wide range of safety use cases. We investigate a class of methods capable of generating targeted, linguistically fluent inputs that activate specific latent features or elicit model behaviours. We formalise this approach as context modification and present ContextBench -- a benchmark with tasks assessing core method capabilities and potential safety applications. Our evaluation framework measures both elicitation strength (activation of latent features or behaviours) and linguistic fluency, highlighting how current state-of-the-art methods struggle to balance these objectives. We enhance Evolutionary Prompt Optimisation (EPO) with LLM-assistance and diffusion model inpainting, and demonstrate that these variants achieve state-of-the-art performance in balancing elicitation effectiveness and fluency.",
        "arxiv_id": "2506.15735",
        "ARXIVID": "2506.15735",
        "COMMENT": "Does not match any specific criteria but is related to language model behavior and safety applications, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16497": {
        "authors": [
            "Riccardo Ziglio",
            "Cecilia Pasquini",
            "Silvio Ranise"
        ],
        "title": "Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors",
        "abstract": "arXiv:2506.16497v1 Announce Type: new  Abstract: Face swapping manipulations in video streams represents an increasing threat in remote video communications, due to advances   in automated and real-time tools. Recent literature proposes to characterize and exploit visual artifacts introduced in video frames   by swapping algorithms when dealing with challenging physical scenes, such as face occlusions. This paper investigates the   effectiveness of this approach by benchmarking CNN-based data-driven models on two data corpora (including a newly collected   one) and analyzing generalization capabilities with respect to different acquisition sources and swapping algorithms. The results   confirm excellent performance of general-purpose CNN architectures when operating within the same data source, but a significant   difficulty in robustly characterizing occlusion-based visual cues across datasets. This highlights the need for specialized detection   strategies to deal with such artifacts.",
        "arxiv_id": "2506.16497",
        "ARXIVID": "2506.16497",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it investigates visual artifacts in face-swapping videos and benchmarks CNN-based detectors.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.15976": {
        "authors": [
            "Jingwei Zhang",
            "Xi Han",
            "Hong Qin",
            "Mahdi S. Hosseini",
            "Dimitris Samaras"
        ],
        "title": "LBMamba: Locally Bi-directional Mamba",
        "abstract": "arXiv:2506.15976v1 Announce Type: new  Abstract: Mamba, a State Space Model (SSM) that accelerates training by recasting recurrence as a parallel selective scan, has recently emerged as a linearly-scaling, efficient alternative to self-attention. Because of its unidirectional nature, each state in Mamba only has information of its previous states and is blind to states after. Current Mamba-based computer-vision methods typically overcome this limitation by augmenting Mamba's global forward scan with a global backward scan, forming a bi-directional scan that restores a full receptive field. However, this operation doubles the computational load, eroding much of the efficiency advantage that originally Mamba have. To eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM block that embeds a lightweight locally backward scan inside the forward selective scan and executes it entirely in per-thread registers. Building on LBMamba, we present LBVim, a scalable vision backbone that alternates scan directions every two layers to recover a global receptive field without extra backward sweeps. We validate the versatility of our approach on both natural images and whole slide images (WSIs). We show that our LBVim constantly offers a superior performance-throughput trade-off. That is under the same throughput, LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection dataset. We also integrate LBMamba into the SOTA pathology multiple instance learning (MIL) approach, MambaMIL, which uses single directional scan. Experiments on 3 public WSI classification datasets for show that our method achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1, 1.67% better accuracy.",
        "arxiv_id": "2506.15976",
        "ARXIVID": "2506.15976",
        "COMMENT": "Does not match any specific criteria but discusses a novel vision backbone, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16499": {
        "authors": [
            "Zexi Liu",
            "Yuzhu Cai",
            "Xinyu Zhu",
            "Yujie Zheng",
            "Runkun Chen",
            "Ying Wen",
            "Yanfeng Wang",
            "Weinan E",
            "Siheng Chen"
        ],
        "title": "ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning",
        "abstract": "arXiv:2506.16499v1 Announce Type: new  Abstract: As AI capabilities advance toward and potentially beyond human-level performance, a natural transition emerges where AI-driven development becomes more efficient than human-centric approaches. A promising pathway toward this transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate and optimize the design, training, and deployment of AI systems themselves. While LLM-based agents have shown the potential to realize AI4AI, they are often unable to fully leverage the experience accumulated by agents during the exploration of solutions in the reasoning process, leading to inefficiencies and suboptimal performance. To address this limitation, we propose ML-Master, a novel AI4AI agent that seamlessly integrates exploration and reasoning by employing a selectively scoped memory mechanism. This approach allows ML-Master to efficiently combine diverse insights from parallel solution trajectories with analytical reasoning, guiding further exploration without overwhelming the agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it achieves a 29.3% average medal rate, significantly surpassing existing methods, particularly in medium-complexity tasks, while accomplishing this superior performance within a strict 12-hour time constraint-half the 24-hour limit used by previous baselines. These results demonstrate ML-Master's potential as a powerful tool for advancing AI4AI.",
        "arxiv_id": "2506.16499",
        "ARXIVID": "2506.16499",
        "COMMENT": "Does not match any specific criteria but discusses AI-for-AI, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.16563": {
        "authors": [
            "Keyhan Najafian",
            "Farhad Maleki",
            "Lingling Jin",
            "Ian Stavness"
        ],
        "title": "From Semantic To Instance: A Semi-Self-Supervised Learning Approach",
        "abstract": "arXiv:2506.16563v1 Announce Type: new  Abstract: Instance segmentation is essential for applications such as automated monitoring of plant health, growth, and yield. However, extensive effort is required to create large-scale datasets with pixel-level annotations of each object instance for developing instance segmentation models that restrict the use of deep learning in these areas. This challenge is more significant in images with densely packed, self-occluded objects, which are common in agriculture. To address this challenge, we propose a semi-self-supervised learning approach that requires minimal manual annotation to develop a high-performing instance segmentation model. We design GLMask, an image-mask representation for the model to focus on shape, texture, and pattern while minimizing its dependence on color features. We develop a pipeline to generate semantic segmentation and then transform it into instance-level segmentation. The proposed approach substantially outperforms the conventional instance segmentation models, establishing a state-of-the-art wheat head instance segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed methodology on the general-purpose Microsoft COCO dataset, achieving a significant performance improvement of over 12.6% mAP@50. This highlights that the utility of our proposed approach extends beyond precision agriculture and applies to other domains, specifically those with similar data characteristics.",
        "arxiv_id": "2506.16563",
        "ARXIVID": "2506.16563",
        "COMMENT": "Does not closely match any specific criterion but is relevant to instance segmentation and semi-supervised learning, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15854": {
        "authors": [
            "Abdolazim Rezaei",
            "Mehdi Sookhak",
            "Ahmad Patooghy"
        ],
        "title": "Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation",
        "abstract": "arXiv:2506.15854v1 Announce Type: new  Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that often process privacy-sensitive data. Among these, roadside units play a critical role particularly through the use of AI-equipped (AIE) cameras for applications such as violation detection. However, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. While traditional techniques such as face blurring and obfuscation have been applied to mitigate privacy risks, individual privacy remains at risk, as individuals can still be tracked using other features such as their clothing. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The main idea is to convert images into semantically equivalent textual descriptions, ensuring that scene-relevant information is retained while visual privacy is preserved. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Evaluation results demonstrate significant improvements in both privacy protection and textual quality, with the Unique Word Count increasing by approximately 77\\% and Detail Density by around 50\\% compared to existing approaches.",
        "arxiv_id": "2506.15854",
        "ARXIVID": "2506.15854",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to vision-language integration through vision-to-text transformation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17137": {
        "authors": [
            "Zhuonan Liang",
            "Dongnan Liu",
            "Jianan Fan",
            "Yaxuan Song",
            "Qiang Qu",
            "Yu Yao",
            "Peng Fu",
            "Weidong Cai"
        ],
        "title": "On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting",
        "abstract": "arXiv:2506.17137v1 Announce Type: new  Abstract: Object counting models suffer when deployed across domains with differing density variety, since density shifts are inherently task-relevant and violate standard domain adaptation assumptions. To address this, we propose a theoretical framework of conditional feature alignment. We first formalize the notion of conditional divergence by partitioning each domain into subsets (e.g., object vs. background) and measuring divergences per condition. We then derive a joint error bound showing that, under discrete label spaces treated as condition sets, aligning distributions conditionally leads to tighter bounds on the combined source-target decision error than unconditional alignment. These insights motivate a general conditional adaptation principle: by preserving task-relevant variations while filtering out nuisance shifts, one can achieve superior cross-domain generalization for counting. We provide both defining conditional divergence then proving its benefit in lowering joint error and a practical adaptation strategy that preserves task-relevant information in unsupervised domain-adaptive counting. We demonstrate the effectiveness of our approach through extensive experiments on multiple counting datasets with varying density distributions. The results show that our method outperforms existing unsupervised domain adaptation methods, empirically validating the theoretical insights on conditional feature alignment.",
        "arxiv_id": "2506.17137",
        "ARXIVID": "2506.17137",
        "COMMENT": "Does not match any specific criteria. Focuses on unsupervised domain-adaptive counting, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16163": {
        "authors": [
            "Hao Li",
            "Gengrui Zhang",
            "Petter Holme",
            "Shuyue Hu",
            "Zhen Wang"
        ],
        "title": "Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior",
        "abstract": "arXiv:2506.16163v1 Announce Type: new  Abstract: Human decision-making belongs to the foundation of our society and civilization, but we are on the verge of a future where much of it will be delegated to artificial intelligence. The arrival of Large Language Models (LLMs) has transformed the nature and scope of AI-supported decision-making; however, the process by which they learn to make decisions, compared to humans, remains poorly understood. In this study, we examined the decision-making behavior of five leading LLMs across three core dimensions of real-world decision-making: uncertainty, risk, and set-shifting. Using three well-established experimental psychology tasks designed to probe these dimensions, we benchmarked LLMs against 360 newly recruited human participants. Across all tasks, LLMs often outperformed humans, approaching near-optimal performance. Moreover, the processes underlying their decisions diverged fundamentally from those of humans. On the one hand, our finding demonstrates the ability of LLMs to manage uncertainty, calibrate risk, and adapt to changes. On the other hand, this disparity highlights the risks of relying on them as substitutes for human judgment, calling for further inquiry.",
        "arxiv_id": "2506.16163",
        "ARXIVID": "2506.16163",
        "COMMENT": "Does not match any specific criteria. Focuses on decision-making behavior of LLMs compared to humans, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15929": {
        "authors": [
            "Liangyan Li",
            "Yimo Ning",
            "Kevin Le",
            "Wei Dong",
            "Yunzhe Li",
            "Jun Chen",
            "Xiaohong Liu"
        ],
        "title": "Moir\\'eXNet: Adaptive Multi-Scale Demoir\\'eing with Linear Attention Test-Time Training and Truncated Flow Matching Prior",
        "abstract": "arXiv:2506.15929v1 Announce Type: new  Abstract: This paper introduces a novel framework for image and video demoir\\'eing by integrating Maximum A Posteriori (MAP) estimation with advanced deep learning techniques. Demoir\\'eing addresses inherently nonlinear degradation processes, which pose significant challenges for existing methods.   Traditional supervised learning approaches either fail to remove moir\\'e patterns completely or produce overly smooth results. This stems from constrained model capacity and scarce training data, which inadequately represent the clean image distribution and hinder accurate reconstruction of ground-truth images. While generative models excel in image restoration for linear degradations, they struggle with nonlinear cases such as demoir\\'eing and often introduce artifacts.   To address these limitations, we propose a hybrid MAP-based framework that integrates two complementary components. The first is a supervised learning model enhanced with efficient linear attention Test-Time Training (TTT) modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\\'eing. The second is a Truncated Flow Matching Prior (TFMP) that further refines the outputs by aligning them with the clean image distribution, effectively restoring high-frequency details and suppressing artifacts. These two components combine the computational efficiency of linear attention with the refinement abilities of generative models, resulting in improved restoration performance.",
        "arxiv_id": "2506.15929",
        "ARXIVID": "2506.15929",
        "COMMENT": "Does not match any specific criteria but discusses image restoration, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.16773": {
        "authors": [
            "Shuchen Sun",
            "Ligen Shi",
            "Chang Liu",
            "Lina Wu",
            "Jun Qiu"
        ],
        "title": "Infrared and Visible Image Fusion Based on Implicit Neural Representations",
        "abstract": "arXiv:2506.16773v1 Announce Type: new  Abstract: Infrared and visible light image fusion aims to combine the strengths of both modalities to generate images that are rich in information and fulfill visual or computational requirements. This paper proposes an image fusion method based on Implicit Neural Representations (INR), referred to as INRFuse. This method parameterizes a continuous function through a neural network to implicitly represent the multimodal information of the image, breaking through the traditional reliance on discrete pixels or explicit features. The normalized spatial coordinates of the infrared and visible light images serve as inputs, and multi-layer perceptrons is utilized to adaptively fuse the features of both modalities, resulting in the output of the fused image. By designing multiple loss functions, the method jointly optimizes the similarity between the fused image and the original images, effectively preserving the thermal radiation information of the infrared image while maintaining the texture details of the visible light image. Furthermore, the resolution-independent characteristic of INR allows for the direct fusion of images with varying resolutions and achieves super-resolution reconstruction through high-density coordinate queries. Experimental results indicate that INRFuse outperforms existing methods in both subjective visual quality and objective evaluation metrics, producing fused images with clear structures, natural details, and rich information without the necessity for a training dataset.",
        "arxiv_id": "2506.16773",
        "ARXIVID": "2506.16773",
        "COMMENT": "Does not match any specific criteria but discusses image fusion, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17186": {
        "authors": [
            "Ketil Malde"
        ],
        "title": "YASMOT: Yet another stereo image multi-object tracker",
        "abstract": "arXiv:2506.17186v1 Announce Type: new  Abstract: There now exists many popular object detectors based on deep learning that can analyze images and extract locations and class labels for occurrences of objects. For image time series (i.e., video or sequences of stills), tracking objects over time and preserving object identity can help to improve object detection performance, and is necessary for many downstream tasks, including classifying and predicting behaviors, and estimating total abundances. Here we present yasmot, a lightweight and flexible object tracker that can process the output from popular object detectors and track objects over time from either monoscopic or stereoscopic camera configurations. In addition, it includes functionality to generate consensus detections from ensembles of object detectors.",
        "arxiv_id": "2506.17186",
        "ARXIVID": "2506.17186",
        "COMMENT": "Does not closely match any specific criterion but is relevant to object tracking and detection, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}