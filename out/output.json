{
    "2508.08199": {
        "authors": [
            "Peiqi He",
            "Zhenhao Zhang",
            "Yixiang Zhang",
            "Xiongjun Zhao",
            "Shaoliang Peng"
        ],
        "title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model",
        "abstract": "arXiv:2508.08199v1 Announce Type: new  Abstract: Precise spatial modeling in the operating room (OR) is foundational to many clinical tasks, supporting intraoperative awareness, hazard avoidance, and surgical decision-making. While existing approaches leverage large-scale multimodal datasets for latent-space alignment to implicitly learn spatial relationships, they overlook the 3D capabilities of MLLMs. However, this approach raises two issues: (1) Operating rooms typically lack multiple video and audio sensors, making multimodal 3D data difficult to obtain; (2) Training solely on readily available 2D data fails to capture fine-grained details in complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first large vision-language model for 3D spatial reasoning in operating rooms using only RGB modality to infer volumetric and semantic cues, enabling downstream medical tasks with detailed and holistic spatial context. Spatial-ORMLLM incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D modality inputs with rich 3D spatial knowledge extracted by the estimation algorithm and then feeds the combined features into the visual tower. By employing a unified end-to-end MLLM framework, it combines powerful spatial features with textual features to deliver robust 3D scene reasoning without any additional expert annotations or sensor inputs. Experiments on multiple benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves state-of-the-art performance and generalizes robustly to previously unseen surgical scenarios and downstream tasks.",
        "arxiv_id": "2508.08199",
        "ARXIVID": "2508.08199",
        "COMMENT": "Matches criteria 1 and 2 as it focuses on spatial reasoning in operating rooms using multimodal large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.06553": {
        "authors": [
            "Jiahao Xiao",
            "Jianbo Zhang",
            "BoWen Yan",
            "Shengyu Guo",
            "Tongrui Ye",
            "Kaiwei Zhang",
            "Zicheng Zhang",
            "Xiaohong Liu",
            "Zhengxue Cheng",
            "Lei Fan",
            "Chuyi Li",
            "Guangtao Zhai"
        ],
        "title": "Static and Plugged: Make Embodied Evaluation Simple",
        "abstract": "arXiv:2508.06553v1 Announce Type: new  Abstract: Embodied intelligence is advancing rapidly, driving the need for efficient evaluation. Current benchmarks typically rely on interactive simulated environments or real-world setups, which are costly, fragmented, and hard to scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play benchmark that enables unified evaluation using static scene representations. Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and comprehensive assessment through a simple interface. Furthermore, we evaluate 19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs), establishing the first unified static leaderboard for Embodied intelligence. Moreover, we release a subset of 200 samples from our benchmark to accelerate the development of embodied intelligence.",
        "arxiv_id": "2508.06553",
        "ARXIVID": "2508.06553",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for embodied AI evaluation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.08252": {
        "authors": [
            "Shuting He",
            "Guangquan Jie",
            "Changshuo Wang",
            "Yun Zhou",
            "Shuming Hu",
            "Guanbin Li",
            "Henghui Ding"
        ],
        "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
        "abstract": "arXiv:2508.08252v1 Announce Type: new  Abstract: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view, posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI. To support research in this area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS. To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks. Dataset and code are available at https://github.com/heshuting555/ReferSplat.",
        "arxiv_id": "2508.08252",
        "ARXIVID": "2508.08252",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 5 (Integration of Image/Video and Large Language Models) due to its focus on 3D Gaussian splatting and spatially aware modeling for natural language-based segmentation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.06571": {
        "authors": [
            "Anqing Jiang",
            "Yu Gao",
            "Yiru Wang",
            "Zhigang Sun",
            "Shuo Wang",
            "Yuwen Heng",
            "Hao Sun",
            "Shichen Tang",
            "Lijuan Zhu",
            "Jinhao Chai",
            "Jijun Wang",
            "Zichong Gu",
            "Hao Jiang",
            "Li Sun"
        ],
        "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
        "abstract": "arXiv:2508.06571v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via \\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.",
        "arxiv_id": "2508.06571",
        "ARXIVID": "2508.06571",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for vision-language-action models in autonomous driving, addressing challenges in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.06525": {
        "authors": [
            "Guoyuan An",
            "JaeYoon Kim",
            "SungEui Yoon"
        ],
        "title": "Large Language Models Facilitate Vision Reflection in Image Classification",
        "abstract": "arXiv:2508.06525v1 Announce Type: new  Abstract: This paper presents several novel findings on the explainability of vision reflection in large multimodal models (LMMs). First, we show that prompting an LMM to verify the prediction of a specialized vision model can improve recognition accuracy, even on benchmarks like ImageNet, despite prior evidence that LMMs typically underperform dedicated vision encoders. Second, we analyze the internal behavior of vision reflection and find that the vision-language connector maps visual features into explicit textual concepts, allowing the language model to reason about prediction plausibility using commonsense knowledge. We further observe that replacing a large number of vision tokens with only a few text tokens still enables LLaVA to generate similar answers, suggesting that LMMs may rely primarily on a compact set of distilled textual representations rather than raw vision features. Third, we show that a training-free connector can enhance LMM performance in fine-grained recognition tasks, without extensive feature-alignment training. Together, these findings offer new insights into the explainability of vision-language models and suggest that vision reflection is a promising strategy for achieving robust and interpretable visual recognition.",
        "arxiv_id": "2508.06525",
        "ARXIVID": "2508.06525",
        "COMMENT": "Matches criteria 2 and 5 as it explores vision reflection in multimodal large language models and their integration with vision tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.07981": {
        "authors": [
            "Fangyuan Mao",
            "Aiming Hao",
            "Jintao Chen",
            "Dongxia Liu",
            "Xiaokun Feng",
            "Jiashu Zhu",
            "Meiqi Wu",
            "Chubin Chen",
            "Jiahong Wu",
            "Xiangxiang Chu"
        ],
        "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
        "abstract": "arXiv:2508.07981v1 Announce Type: new  Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.",
        "arxiv_id": "2508.07981",
        "ARXIVID": "2508.07981",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on spatially-controllable visual effects generation with unified frameworks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.08248": {
        "authors": [
            "Shuyuan Tu",
            "Yueming Pan",
            "Yinming Huang",
            "Xintong Han",
            "Zhen Xing",
            "Qi Dai",
            "Chong Luo",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
        "abstract": "arXiv:2508.08248v1 Announce Type: new  Abstract: Current diffusion models for audio-driven avatar video generation struggle to synthesize long videos with natural audio synchronization and identity consistency. This paper presents StableAvatar, the first end-to-end video diffusion transformer that synthesizes infinite-length high-quality videos without post-processing. Conditioned on a reference image and audio, StableAvatar integrates tailored training and inference modules to enable infinite-length video generation. We observe that the main reason preventing existing models from generating long videos lies in their audio modeling. They typically rely on third-party off-the-shelf extractors to obtain audio embeddings, which are then directly injected into the diffusion model via cross-attention. Since current diffusion backbones lack any audio-related priors, this approach causes severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to drift away from the optimal distribution gradually. To address this, StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents error accumulation via time-step-aware modulation. During inference, we propose a novel Audio Native Guidance Mechanism to further enhance the audio synchronization by leveraging the diffusion's own evolving joint audio-latent prediction as a dynamic guidance signal. To enhance the smoothness of the infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy that fuses latent over time. Experiments on benchmarks show the effectiveness of StableAvatar both qualitatively and quantitatively.",
        "arxiv_id": "2508.08248",
        "ARXIVID": "2508.08248",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates audio and video generation with novel techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.07759": {
        "authors": [
            "Haoran Wang",
            "Zekun Li",
            "Jian Zhang",
            "Lei Qi",
            "Yinghuan Shi"
        ],
        "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild",
        "abstract": "arXiv:2508.07759v1 Announce Type: new  Abstract: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild. Consequently, reference segmentation, which leverages reference images and their corresponding masks to impart novel knowledge to the model, emerges as a promising new direction for adapting vision models. However, existing reference segmentation approaches predominantly rely on meta-learning, which still necessitates an extensive meta-training process and brings massive data and computational cost. In this study, we propose a novel approach by representing the inherent correspondence between reference-target image pairs as a pseudo video. This perspective allows the latest version of SAM, known as SAM2, which is equipped with interactive video object segmentation (iVOS) capabilities, to be adapted to downstream tasks in a lightweight manner. We term this approach Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules: the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model to construct a semantic transformation sequence, while the Test-Time Geometric Alignment (TTGA) module aligns the geometric changes within this sequence through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets, achieving segmentation performance improvements exceeding 5% over SOTA methods. Implementation is provided in the supplementary materials.",
        "arxiv_id": "2508.07759",
        "ARXIVID": "2508.07759",
        "COMMENT": "Matches criteria 5 as it adapts SAM2 for reference segmentation using a novel test-time adaptation approach, combining image and large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.07871": {
        "authors": [
            "Yanshu Li",
            "Jianjiang Yang",
            "Zhennan Shen",
            "Ligong Han",
            "Haoyan Xu",
            "Ruixiang Tang"
        ],
        "title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning",
        "abstract": "arXiv:2508.07871v1 Announce Type: new  Abstract: Modern large vision-language models (LVLMs) convert each input image into a large set of tokens, far outnumbering the text tokens. Although this improves visual perception, it introduces severe image token redundancy. Because image tokens carry sparse information, many add little to reasoning, yet greatly increase inference cost. The emerging image token pruning methods tackle this issue by identifying the most important tokens and discarding the rest. These methods can raise efficiency with only modest performance loss. However, most of them only consider single-image tasks and overlook multimodal in-context learning (ICL), where redundancy is greater and efficiency is more critical. Redundant tokens weaken the advantage of multimodal ICL for rapid domain adaptation and cause unstable performance. Applying existing pruning methods in this setting leads to large accuracy drops, exposing a clear gap and the need for new techniques. Thus, we propose Contextually Adaptive Token Pruning (CATP), a training-free pruning method targeted at multimodal ICL. CATP consists of two stages that perform progressive pruning to fully account for the complex cross-modal interactions in the input sequence. After removing 77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\% over the vanilla model on four LVLMs and eight benchmarks, exceeding all baselines remarkably. Meanwhile, it effectively improves efficiency by achieving an average reduction of 10.78\\% in inference latency. CATP enhances the practical value of multimodal ICL and lays the groundwork for future progress in interleaved image-text scenarios.",
        "arxiv_id": "2508.07871",
        "ARXIVID": "2508.07871",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on improving multimodal large language models (LVLMs) with a novel token pruning method for image-text scenarios.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.08086": {
        "authors": [
            "Zhongqi Yang",
            "Wenhang Ge",
            "Yuqi Li",
            "Jiaqi Chen",
            "Haoyuan Li",
            "Mengyin An",
            "Fei Kang",
            "Hua Xue",
            "Baixin Xu",
            "Yuyang Yin",
            "Eric Li",
            "Yang Liu",
            "Yikai Wang",
            "Hao-Xiang Guo",
            "Yahui Zhou"
        ],
        "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
        "abstract": "arXiv:2508.08086v1 Announce Type: new  Abstract: Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.",
        "arxiv_id": "2508.08086",
        "ARXIVID": "2508.08086",
        "COMMENT": "Matches criteria 1 and 6 as it focuses on spatial intelligence and 3D world generation from panoramic video data.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.06530": {
        "authors": [
            "Ming-Kun Xie",
            "Jia-Hao Xiao",
            "Gang Niu",
            "Lei Feng",
            "Zhiqiang Kou",
            "Min-Ling Zhang",
            "Masashi Sugiyama"
        ],
        "title": "What Makes \"Good\" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?",
        "abstract": "arXiv:2508.06530v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large Language Models (LLMs), have achieved impressive performance across domains. Despite the great advances in LVLMs, they still suffer from the unavailable object hallucination issue, which tends to generate objects inconsistent with the image content. The most commonly used Polling-based Object Probing Evaluation (POPE) benchmark evaluates this issue by sampling negative categories according to category-level statistics, \\textit{e.g.}, category frequencies and co-occurrence. However, with the continuous advancement of LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing object hallucination, as it employs a simplistic sampling strategy that overlooks image-specific information and restricts distractors to negative object categories only. In this paper, we introduce the Hallucination searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate the most misleading distractors (\\textit{i.e.}, non-existent objects or incorrect image descriptions) that can trigger hallucination in LVLMs, which serves as a means to more rigorously assess their immunity to hallucination. To explore the image-specific information, the content-aware hallucination searching leverages Contrastive Language-Image Pre-Training (CLIP) to approximate the predictive behavior of LVLMs by selecting negative objects with the highest predicted likelihood as distractors. To expand the scope of hallucination assessment, the description-based hallucination searching constructs highly misleading distractors by pairing true objects with false descriptions. Experimental results show that HOPE leads to a precision drop of at least 9\\% and up to 23\\% across various state-of-the-art LVLMs, significantly outperforming POPE in exposing hallucination vulnerabilities. The code is available at https://github.com/xiemk/HOPE.",
        "arxiv_id": "2508.06530",
        "ARXIVID": "2508.06530",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for evaluating object hallucination in LVLMs, which is a key challenge in vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.07863": {
        "authors": [
            "Bin Cao",
            "Sipeng Zheng",
            "Ye Wang",
            "Lujie Xia",
            "Qianshan Wei",
            "Qin Jin",
            "Jing Liu",
            "Zongqing Lu"
        ],
        "title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model",
        "abstract": "arXiv:2508.07863v1 Announce Type: new  Abstract: Human motion generation has emerged as a critical technology with transformative potential for real-world applications. However, existing vision-language-motion models (VLMMs) face significant limitations that hinder their practical deployment. We identify controllability as a main bottleneck, manifesting in five key aspects: inadequate response to diverse human commands, limited pose initialization capabilities, poor performance on long-term sequences, insufficient handling of unseen scenarios, and lack of fine-grained control over individual body parts. To overcome these limitations, we present Being-M0.5, the first real-time, controllable VLMM that achieves state-of-the-art performance across multiple motion generation tasks. Our approach is built upon HuMo100M, the largest and most comprehensive human motion dataset to date, comprising over 5 million self-collected motion sequences, 100 million multi-task instructional instances, and detailed part-level annotations that address a critical gap in existing datasets. We introduce a novel part-aware residual quantization technique for motion tokenization that enables precise, granular control over individual body parts during generation. Extensive experimental validation demonstrates Being-M0.5's superior performance across diverse motion benchmarks, while comprehensive efficiency analysis confirms its real-time capabilities. Our contributions include design insights and detailed computational analysis to guide future development of practical motion generators. We believe that HuMo100M and Being-M0.5 represent significant advances that will accelerate the adoption of motion generation technologies in real-world applications. The project page is available at https://beingbeyond.github.io/Being-M0.5.",
        "arxiv_id": "2508.07863",
        "ARXIVID": "2508.07863",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 5 (Integration of Image/Video and Large Language Models) due to its focus on vision-language-motion models and fine-grained control in motion generation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.07908": {
        "authors": [
            "Xudong Cai",
            "Shuo Wang",
            "Peng Wang",
            "Yongcai Wang",
            "Zhaoxin Fan",
            "Wanting Li",
            "Tianbao Zhang",
            "Jianrong Tao",
            "Yeying Jin",
            "Deying Li"
        ],
        "title": "Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction",
        "abstract": "arXiv:2508.07908v1 Announce Type: new  Abstract: Reconstructing dense geometry for dynamic scenes from a monocular video is a critical yet challenging task. Recent memory-based methods enable efficient online reconstruction, but they fundamentally suffer from a Memory Demand Dilemma: The memory representation faces an inherent conflict between the long-term stability required for static structures and the rapid, high-fidelity detail retention needed for dynamic motion. This conflict forces existing methods into a compromise, leading to either geometric drift in static structures or blurred, inaccurate reconstructions of dynamic objects. To address this dilemma, we propose Mem4D, a novel framework that decouples the modeling of static geometry and dynamic motion. Guided by this insight, we design a dual-memory architecture: 1) The Transient Dynamics Memory (TDM) focuses on capturing high-frequency motion details from recent frames, enabling accurate and fine-grained modeling of dynamic content; 2) The Persistent Structure Memory (PSM) compresses and preserves long-term spatial information, ensuring global consistency and drift-free reconstruction for static elements. By alternating queries to these specialized memories, Mem4D simultaneously maintains static geometry with global consistency and reconstructs dynamic elements with high fidelity. Experiments on challenging benchmarks demonstrate that our method achieves state-of-the-art or competitive performance while maintaining high efficiency. Codes will be publicly available.",
        "arxiv_id": "2508.07908",
        "ARXIVID": "2508.07908",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its novel dual-memory architecture for dynamic scene reconstruction.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.06908": {
        "authors": [
            "Jinhao Li",
            "Zijian Chen",
            "Lirong Deng",
            "Changbo Wang",
            "Guangtao Zhai"
        ],
        "title": "MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification",
        "abstract": "arXiv:2508.06908v1 Announce Type: new  Abstract: Person re-identification (ReID) aims to retrieve the images of an interested person in the gallery images, with wide applications in medical rehabilitation, abnormal behavior detection, and public security. However, traditional person ReID models suffer from uni-modal capability, leading to poor generalization ability in multi-modal data, such as RGB, thermal, infrared, sketch images, textual descriptions, etc. Recently, the emergence of multi-modal large language models (MLLMs) shows a promising avenue for addressing this problem. Despite this potential, existing methods merely regard MLLMs as feature extractors or caption generators, which do not fully unleash their reasoning, instruction-following, and cross-modal understanding capabilities. To bridge this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark specifically designed for person ReID. The MMReID-Bench includes 20,710 multi-modal queries and gallery images covering 10 different person ReID tasks. Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in delivering effective and versatile person ReID. Nevertheless, they also have limitations in handling a few modalities, particularly thermal and infrared data. We hope MMReID-Bench can facilitate the community to develop more robust and generalizable multimodal foundation models for person ReID.",
        "arxiv_id": "2508.06908",
        "ARXIVID": "2508.06908",
        "COMMENT": "Matches criterion 2 as it explores multi-modal large language models (MLLMs) and their applications.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.06632": {
        "authors": [
            "Wenpeng Xing",
            "Jie Chen",
            "Zaifeng Yang",
            "Tiancheng Zhao",
            "Gaolei Li",
            "Changting Lin",
            "Yike Guo",
            "Meng Han"
        ],
        "title": "CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition",
        "abstract": "arXiv:2508.06632v1 Announce Type: new  Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view synthesis, but challenges remain in rendering scenes with complex specular reflections and highlights. Existing approaches may produce blurry reflections due to entanglement between lighting and material properties, or encounter optimization instability when relying on physically-based inverse rendering. In this work, we present a neural rendering framework based on dynamic coefficient decomposition, aiming to improve the modeling of view-dependent appearance. Our approach decomposes complex appearance into a shared, static neural basis that encodes intrinsic material properties, and a set of dynamic coefficients generated by a Coefficient Network conditioned on view and illumination. A Dynamic Radiance Integrator then combines these components to synthesize the final radiance. Experimental results on several challenging benchmarks suggest that our method can produce sharper and more realistic specular highlights compared to existing techniques. We hope that this decomposition paradigm can provide a flexible and effective direction for modeling complex appearance in neural scene representations.",
        "arxiv_id": "2508.06632",
        "ARXIVID": "2508.06632",
        "COMMENT": "Matches criteria 6 as it focuses on improving neural rendering for complex video-based tasks, specifically view synthesis with specular reflections.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07312": {
        "authors": [
            "Min Yang",
            "Zihan Jia",
            "Zhilin Dai",
            "Sheng Guo",
            "Limin Wang"
        ],
        "title": "MobileViCLIP: An Efficient Video-Text Model for Mobile Devices",
        "abstract": "arXiv:2508.07312v1 Announce Type: new  Abstract: Efficient lightweight neural networks are with increasing attention due to their faster reasoning speed and easier deployment on mobile devices. However, existing video pre-trained models still focus on the common ViT architecture with high latency, and few works attempt to build efficient architecture on mobile devices. This paper bridges this gap by introducing temporal structural reparameterization into an efficient image-text model and training it on a large-scale high-quality video-text dataset, resulting in an efficient video-text model that can run on mobile devices with strong zero-shot classification and retrieval capabilities, termed as MobileViCLIP. In particular, in terms of inference speed on mobile devices, our MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster than InternVideo2-S14. In terms of zero-shot retrieval performance, our MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains 6.9\\% better than InternVideo2-S14 on MSR-VTT. The code is available at https://github.com/MCG-NJU/MobileViCLIP.",
        "arxiv_id": "2508.07312",
        "ARXIVID": "2508.07312",
        "COMMENT": "Matches criteria 6 as it introduces an efficient video-text model for mobile devices, focusing on video understanding and retrieval tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06964": {
        "authors": [
            "Qiwei Tian",
            "Chenhao Lin",
            "Zhengyu Zhao",
            "Qian Li",
            "Shuai Liu",
            "Chao Shen"
        ],
        "title": "Adversarial Video Promotion Against Text-to-Video Retrieval",
        "abstract": "arXiv:2508.06964v1 Announce Type: new  Abstract: Thanks to the development of cross-modal models, text-to-video retrieval (T2VR) is advancing rapidly, but its robustness remains largely unexamined. Existing attacks against T2VR are designed to push videos away from queries, i.e., suppressing the ranks of videos, while the attacks that pull videos towards selected queries, i.e., promoting the ranks of videos, remain largely unexplored. These attacks can be more impactful as attackers may gain more views/clicks for financial benefits and widespread (mis)information. To this end, we pioneer the first attack against T2VR to promote videos adversarially, dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement (MoRe) to capture the finer-grained, intricate interaction between visual and textual modalities to enhance black-box transferability. Comprehensive experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing datasets with over 10k videos, evaluated under 3 scenarios. All experiments are conducted in a multi-target setting to reflect realistic scenarios where attackers seek to promote the video regarding multiple queries simultaneously. We also evaluated our attacks for defences and imperceptibility. Overall, ViPro surpasses other baselines by over $30/10/4\\%$ for white/grey/black-box settings on average. Our work highlights an overlooked vulnerability, provides a qualitative analysis on the upper/lower bound of our attacks, and offers insights into potential counterplays. Code will be publicly available at https://github.com/michaeltian108/ViPro.",
        "arxiv_id": "2508.06964",
        "ARXIVID": "2508.06964",
        "COMMENT": "Matches criteria 6 as it explores video-based tasks, specifically adversarial attacks on text-to-video retrieval models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06982": {
        "authors": [
            "Yixin Zhu",
            "Zuoliang Zhu",
            "Milo\\v{s} Ha\\v{s}an",
            "Jian Yang",
            "Jin Xie",
            "Beibei Wang"
        ],
        "title": "WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering",
        "abstract": "arXiv:2508.06982v1 Announce Type: new  Abstract: Forward and inverse rendering have emerged as key techniques for enabling understanding and reconstruction in the context of autonomous driving (AD). However, complex weather and illumination pose great challenges to this task. The emergence of large diffusion models has shown promise in achieving reasonable results through learning from 2D priors, but these models are difficult to control and lack robustness. In this paper, we introduce WeatherDiffusion, a diffusion-based framework for forward and inverse rendering on AD scenes with various weather and lighting conditions. Our method enables authentic estimation of material properties, scene geometry, and lighting, and further supports controllable weather and illumination editing through the use of predicted intrinsic maps guided by text descriptions. We observe that different intrinsic maps should correspond to different regions of the original image. Based on this observation, we propose Intrinsic map-aware attention (MAA) to enable high-quality inverse rendering. Additionally, we introduce a synthetic dataset (\\ie WeatherSynthetic) and a real-world dataset (\\ie WeatherReal) for forward and inverse rendering on AD scenes with diverse weather and lighting. Extensive experiments show that our WeatherDiffusion outperforms state-of-the-art methods on several benchmarks. Moreover, our method demonstrates significant value in downstream tasks for AD, enhancing the robustness of object detection and image segmentation in challenging weather scenarios.",
        "arxiv_id": "2508.06982",
        "ARXIVID": "2508.06982",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on forward and inverse rendering in autonomous driving scenes under diverse weather conditions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07493": {
        "authors": [
            "Jian Chen",
            "Ming Li",
            "Jihyung Kil",
            "Chenguang Wang",
            "Tong Yu",
            "Ryan Rossi",
            "Tianyi Zhou",
            "Changyou Chen",
            "Ruiyi Zhang"
        ],
        "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding",
        "abstract": "arXiv:2508.07493v1 Announce Type: new  Abstract: Most organizational data in this world are stored as documents, and visual retrieval plays a crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on a single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual benchmark designed for question-driven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval.",
        "arxiv_id": "2508.07493",
        "ARXIVID": "2508.07493",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a multilingual benchmark for multimodal retrieval in long documents, integrating vision and language.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2508.07089": {
        "authors": [
            "Sandro Papais",
            "Letian Wang",
            "Brian Cheong",
            "Steven L. Waslander"
        ],
        "title": "ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting",
        "abstract": "arXiv:2508.07089v1 Announce Type: new  Abstract: We introduce ForeSight, a novel joint detection and forecasting framework for vision-based 3D perception in autonomous vehicles. Traditional approaches treat detection and forecasting as separate sequential tasks, limiting their ability to leverage temporal cues. ForeSight addresses this limitation with a multi-task streaming and bidirectional learning approach, allowing detection and forecasting to share query memory and propagate information seamlessly. The forecast-aware detection transformer enhances spatial reasoning by integrating trajectory predictions from a multiple hypothesis forecast memory queue, while the streaming forecast transformer improves temporal consistency using past forecasts and refined detections. Unlike tracking-based methods, ForeSight eliminates the need for explicit object association, reducing error propagation with a tracking-free model that efficiently scales across multi-frame sequences. Experiments on the nuScenes dataset show that ForeSight achieves state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous methods by 9.3%, while also attaining the best mAP and minADE among multi-view detection and forecasting models.",
        "arxiv_id": "2508.07089",
        "ARXIVID": "2508.07089",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for joint detection and forecasting in autonomous vehicles, addressing underexplored challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07313": {
        "authors": [
            "Junyu Xiong",
            "Yonghui Wang",
            "Weichao Zhao",
            "Chenyu Liu",
            "Bing Yin",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "title": "DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding",
        "abstract": "arXiv:2508.07313v1 Announce Type: new  Abstract: Understanding multi-page documents poses a significant challenge for multimodal large language models (MLLMs), as it requires fine-grained visual comprehension and multi-hop reasoning across pages. While prior work has explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs, its application to multi-page document understanding remains underexplored. In this paper, we introduce DocR1, an MLLM trained with a novel RL framework, Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the model to first retrieve relevant pages before generating answers. This training paradigm enables us to build high-quality models with limited supervision. To support this, we design a two-stage annotation pipeline and a curriculum learning strategy, based on which we construct two datasets: EviBench, a high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments across a wide range of benchmarks demonstrate that DocR1 achieves state-of-the-art performance on multi-page tasks, while consistently maintaining strong results on single-page benchmarks.",
        "arxiv_id": "2508.07313",
        "ARXIVID": "2508.07313",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on multimodal large language models for multi-page document understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07996": {
        "authors": [
            "Thinesh Thiyakesan Ponbagavathi",
            "Chengzheng Yang",
            "Alina Roitberg"
        ],
        "title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models",
        "abstract": "arXiv:2508.07996v1 Announce Type: new  Abstract: Group Activity Detection (GAD) involves recognizing social groups and their collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2, offer excellent features, but are pretrained primarily on object-centric data and remain underexplored for modeling group dynamics. While they are a promising alternative to highly task-specific GAD architectures that require full fine-tuning, our initial investigation reveals that simply swapping CNN backbones used in these methods with VFMs brings little gain, underscoring the need for structured, group-aware reasoning on top.   We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method that bridges this gap through 1) learnable group prompts to guide the VFM attention toward social configurations, and 2) a lightweight two-layer GroupContext Transformer that infers actor-group associations and collective behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which features multiple concurrent social groups, and Social-CAD, which focuses on single-group interactions. While we surpass state-of-the-art in both settings, our method is especially effective in complex multi-group scenarios, where we yield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only 10M trainable parameters. Furthermore, our experiments reveal that ProGraD produces interpretable attention maps, offering insights into actor-group reasoning. Code and models will be released.",
        "arxiv_id": "2508.07996",
        "ARXIVID": "2508.07996",
        "COMMENT": "Matches criteria 1 and 4 as it focuses on spatial intelligence and reasoning for group activity detection using Vision Foundation Models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07819": {
        "authors": [
            "Ke Ma",
            "Jun Long",
            "Hongxiao Fei",
            "Liujie Hua",
            "Yueyi Luo"
        ],
        "title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP",
        "abstract": "arXiv:2508.07819v1 Announce Type: new  Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of local inductive biases for dense prediction and their reliance on inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method integrates a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.",
        "arxiv_id": "2508.07819",
        "ARXIVID": "2508.07819",
        "COMMENT": "Matches criterion 2 as it focuses on adapting vision-language models for anomaly detection, which involves vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07570": {
        "authors": [
            "Khanh-Binh Nguyen",
            "Phuoc-Nguyen Bui",
            "Hyunseung Choo",
            "Duc Thanh Nguyen"
        ],
        "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models",
        "abstract": "arXiv:2508.07570v1 Announce Type: new  Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios.",
        "arxiv_id": "2508.07570",
        "ARXIVID": "2508.07570",
        "COMMENT": "Matches criterion 2 as it explores test-time adaptation for vision-language models, which is relevant to improving VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07011": {
        "authors": [
            "Zixiong Wang",
            "Jian Yang",
            "Yiwei Hu",
            "Milos Hasan",
            "Beibei Wang"
        ],
        "title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation",
        "abstract": "arXiv:2508.07011v1 Announce Type: new  Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The rise of high-resolution text-to-image generative models, based on diffusion transformers (DiT), suggests an opportunity to finetune them for this task. However, retargeting the models to produce multiple aligned SVBRDF maps instead of just RGB images, while achieving high efficiency and ensuring consistency across different maps, remains a challenge. In this paper, we introduce HiMat: a memory- and computation-efficient diffusion-based framework capable of generating native 4K-resolution SVBRDFs. A key challenge we address is maintaining consistency across different maps in a lightweight manner, without relying on training new VAEs or significantly altering the DiT backbone (which would damage its prior capabilities). To tackle this, we introduce the CrossStitch module, a lightweight convolutional module that captures inter-map dependencies through localized operations. Its weights are initialized such that the DiT backbone operation is unchanged before finetuning starts. HiMat enables generation with strong structural coherence and high-frequency details. Results with a large set of text prompts demonstrate the effectiveness of our approach for 4K SVBRDF generation. Further experiments suggest generalization to tasks such as intrinsic decomposition.",
        "arxiv_id": "2508.07011",
        "ARXIVID": "2508.07011",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on diffusion transformers for high-resolution SVBRDF generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07925": {
        "authors": [
            "Jin-Seop Lee",
            "SungJoon Lee",
            "Jaehan Ahn",
            "YunSeok Choi",
            "Jee-Hyong Lee"
        ],
        "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding",
        "abstract": "arXiv:2508.07925v1 Announce Type: new  Abstract: Video Temporal Grounding (VTG) aims to extract relevant video segments based on a given natural language query. Recently, zero-shot VTG methods have gained attention by leveraging pretrained vision-language models (VLMs) to localize target moments without additional training. However, existing approaches suffer from semantic fragmentation, where temporally continuous frames sharing the same semantics are split across multiple segments. When segments are fragmented, it becomes difficult to predict an accurate target moment that aligns with the text query. Also, they rely on skewed similarity distributions for localization, making it difficult to select the optimal segment. Furthermore, they heavily depend on the use of LLMs which require expensive inferences. To address these limitations, we propose a \\textit{TAG}, a simple yet effective Temporal-Aware approach for zero-shot video temporal Grounding, which incorporates temporal pooling, temporal coherence clustering, and similarity adjustment. Our proposed method effectively captures the temporal context of videos and addresses distorted similarity distributions without training. Our approach achieves state-of-the-art results on Charades-STA and ActivityNet Captions benchmark datasets without rely on LLMs. Our code is available at https://github.com/Nuetee/TAG",
        "arxiv_id": "2508.07925",
        "ARXIVID": "2508.07925",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video temporal grounding with novel zero-shot methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.08048": {
        "authors": [
            "Peng Dai",
            "Feitong Tan",
            "Qiangeng Xu",
            "Yihua Huang",
            "David Futschik",
            "Ruofei Du",
            "Sean Fanello",
            "Yinda Zhang",
            "Xiaojuan Qi"
        ],
        "title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix",
        "abstract": "arXiv:2508.08048v1 Announce Type: new  Abstract: While video generation models excel at producing high-quality monocular videos, generating 3D stereoscopic and spatial videos for immersive applications remains an underexplored challenge. We present a pose-free and training-free method that leverages an off-the-shelf monocular video generation model to produce immersive 3D videos. Our approach first warps the generated monocular video into pre-defined camera viewpoints using estimated depth information, then applies a novel \\textit{frame matrix} inpainting framework. This framework utilizes the original video generation model to synthesize missing content across different viewpoints and timestamps, ensuring spatial and temporal consistency without requiring additional model fine-tuning. Moreover, we develop a \\dualupdate~scheme that further improves the quality of video inpainting by alleviating the negative effects propagated from disoccluded areas in the latent space. The resulting multi-view videos are then adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial video synthesis. We validate the efficacy of our proposed method by conducting experiments on videos from various generative models, such as Sora, Lumiere, WALT, and Zeroscope. The experiments demonstrate that our method has a significant improvement over previous methods. Project page at: https://daipengwa.github.io/S-2VG_ProjectPage/",
        "arxiv_id": "2508.08048",
        "ARXIVID": "2508.08048",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on generating 3D stereoscopic and spatial videos using monocular video generation models, integrating video understanding and generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07918": {
        "authors": [
            "Xing Zi",
            "Jinghao Xiao",
            "Yunxiao Shi",
            "Xian Tao",
            "Jun Li",
            "Ali Braytee",
            "Mukesh Prasad"
        ],
        "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering",
        "abstract": "arXiv:2508.07918v1 Announce Type: new  Abstract: Visual Question Answering (VQA) in remote sensing (RS) is pivotal for interpreting Earth observation data. However, existing RS VQA datasets are constrained by limitations in annotation richness, question diversity, and the assessment of specific reasoning capabilities. This paper introduces RSVLM-QA dataset, a new large-scale, content-rich VQA dataset for the RS domain. RSVLM-QA is constructed by integrating data from several prominent RS segmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ an innovative dual-track annotation generation pipeline. Firstly, we leverage Large Language Models (LLMs), specifically GPT-4.1, with meticulously designed prompts to automatically generate a suite of detailed annotations including image captions, spatial relations, and semantic tags, alongside complex caption-based VQA pairs. Secondly, to address the challenging task of object counting in RS imagery, we have developed a specialized automated process that extracts object counts directly from the original segmentation data; GPT-4.1 then formulates natural language answers from these counts, which are paired with preset question templates to create counting QA pairs. RSVLM-QA comprises 13,820 images and 162,373 VQA pairs, featuring extensive annotations and diverse question types. We provide a detailed statistical analysis of the dataset and a comparison with existing RS VQA benchmarks, highlighting the superior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct benchmark experiments on Six mainstream Vision Language Models (VLMs), demonstrating that RSVLM-QA effectively evaluates and challenges the understanding and reasoning abilities of current VLMs in the RS domain. We believe RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM research communities, poised to catalyze advancements in the field.",
        "arxiv_id": "2508.07918",
        "ARXIVID": "2508.07918",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark dataset for remote sensing VQA, which involves video-based tasks and reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.08066": {
        "authors": [
            "Weitai Kang",
            "Weiming Zhuang",
            "Zhizhong Li",
            "Yan Yan",
            "Lingjuan Lyu"
        ],
        "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
        "abstract": "arXiv:2508.08066v1 Announce Type: new  Abstract: Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
        "arxiv_id": "2508.08066",
        "ARXIVID": "2508.08066",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores design choices for visual grounding in MLLMs, which is central to vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2508.08219": {
        "authors": [
            "Wentao Sun",
            "Quanyun Wu",
            "Hanqing Xu",
            "Kyle Gao",
            "Zhengsen Xu",
            "Yiping Chen",
            "Dedong Zhang",
            "Lingfei Ma",
            "John S. Zelek",
            "Jonathan Li"
        ],
        "title": "SAGOnline: Segment Any Gaussians Online",
        "abstract": "arXiv:2508.08219v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.",
        "arxiv_id": "2508.08219",
        "ARXIVID": "2508.08219",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on real-time 3D segmentation and tracking in Gaussian scenes, which is relevant for robotic and AR/VR applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07246": {
        "authors": [
            "Xin Ma",
            "Yaohui Wang",
            "Genyun Jia",
            "Xinyuan Chen",
            "Tien-Tsin Wong",
            "Cunjian Chen"
        ],
        "title": "Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers",
        "abstract": "arXiv:2508.07246v1 Announce Type: new  Abstract: Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks.",
        "arxiv_id": "2508.07246",
        "ARXIVID": "2508.07246",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks and introduces novel methodologies for video animation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06905": {
        "authors": [
            "Ruoxi Chen",
            "Dongping Chen",
            "Siyuan Wu",
            "Sinan Wang",
            "Shiyun Lang",
            "Petr Sushko",
            "Gaoyang Jiang",
            "Yao Wan",
            "Ranjay Krishna"
        ],
        "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
        "abstract": "arXiv:2508.06905v1 Announce Type: new  Abstract: Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.",
        "arxiv_id": "2508.06905",
        "ARXIVID": "2508.06905",
        "COMMENT": "Matches criterion 5 as it focuses on techniques combining image understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.07307": {
        "authors": [
            "Haiyang Guo",
            "Fei Zhu",
            "Hongbo Zhao",
            "Fanhu Zeng",
            "Wenzhuo Liu",
            "Shijie Ma",
            "Da-Han Wang",
            "Xu-Yao Zhang"
        ],
        "title": "MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark",
        "abstract": "arXiv:2508.07307v1 Announce Type: new  Abstract: Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual Learning tasks involving multiple modalities, such as vision and language. In this setting, models are expected to not only mitigate catastrophic forgetting but also handle the challenges posed by cross-modal interactions and coordination. To facilitate research in this direction, we introduce MCITlib, a comprehensive and constantly evolving code library for continual instruction tuning of Multimodal Large Language Models. In MCITlib, we have currently implemented 8 representative algorithms for Multimodal Continual Instruction Tuning and systematically evaluated them on 2 carefully selected benchmarks. MCITlib will be continuously updated to reflect advances in the Multimodal Continual Learning field. The codebase is released at https://github.com/Ghy0501/MCITlib.",
        "arxiv_id": "2508.07307",
        "ARXIVID": "2508.07307",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on multimodal continual instruction tuning for vision and language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.07624": {
        "authors": [
            "Vishakha Lall",
            "Yisi Liu"
        ],
        "title": "Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction",
        "abstract": "arXiv:2508.07624v1 Announce Type: new  Abstract: In many real-world applications involving static environments, the spatial layout of objects remains consistent across instances. However, state-of-the-art object detection models often fail to leverage this spatial prior, resulting in inconsistent predictions, missed detections, or misclassifications, particularly in cluttered or occluded scenes. In this work, we propose a graph-based post-processing pipeline that explicitly models the spatial relationships between objects to correct detection anomalies in egocentric frames. Using a graph neural network (GNN) trained on manually annotated data, our model identifies invalid object class labels and predicts corrected class labels based on their neighbourhood context. We evaluate our approach both as a standalone anomaly detection and correction framework and as a post-processing module for standard object detectors such as YOLOv7 and RT-DETR. Experiments demonstrate that incorporating this spatial reasoning significantly improves detection performance, with mAP@50 gains of up to 4%. This method highlights the potential of leveraging the environment's spatial structure to improve reliability in object detection systems.",
        "arxiv_id": "2508.07624",
        "ARXIVID": "2508.07624",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on spatial reasoning using graph-based anomaly detection in egocentric object detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.06995": {
        "authors": [
            "Huihui Xu",
            "Jin Ye",
            "Hongqiu Wang",
            "Changkai Ji",
            "Jiashi Lin",
            "Ming Hu",
            "Ziyan Huang",
            "Ying Chen",
            "Chenglong Ma",
            "Tianbin Li",
            "Lihao Liu",
            "Junjun He",
            "Lei Zhu"
        ],
        "title": "S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision",
        "abstract": "arXiv:2508.06995v1 Announce Type: new  Abstract: Recent self-supervised image segmentation models have achieved promising performance on semantic segmentation and class-agnostic instance segmentation. However, their pretraining schedule is multi-stage, requiring a time-consuming pseudo-masks generation process between each training epoch. This time-consuming offline process not only makes it difficult to scale with training dataset size, but also leads to sub-optimal solutions due to its discontinuous optimization routine. To solve these, we first present a novel pseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer of UniAP can identify groups of similar nodes in parallel, allowing to generate both semantic-level and instance-level and multi-granular pseudo-masks within ens of milliseconds for one image. Based on the fast UniAP, we propose the Scalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a student and a momentum teacher for continuous pretraining. A novel segmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is proposed to pretrain S2-UniSeg to learn the local-to-global correspondences. Under the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving notable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on COCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image subset of SA-1B, S2-UniSeg further achieves performance gains on all four benchmarks. Our code and pretrained models are available at https://github.com/bio-mlhui/S2-UniSeg",
        "arxiv_id": "2508.06995",
        "ARXIVID": "2508.06995",
        "COMMENT": "Matches criterion 4 as it focuses on a vision foundation model and its applications in segmentation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.06756": {
        "authors": [
            "Somayeh Farahani",
            "Marjaneh Hejazi",
            "Antonio Di Ieva",
            "Sidong Liu"
        ],
        "title": "FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI",
        "abstract": "arXiv:2508.06756v1 Announce Type: new  Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is essential for effective glioma management. Traditional methods rely on invasive tissue sampling, which may fail to capture a tumor's spatial heterogeneity. While deep learning models have shown promise in molecular profiling, their performance is often limited by scarce annotated data. In contrast, foundation deep learning models offer a more generalizable approach for glioma imaging biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch signals associated with IDH mutation. The model was trained and validated on a diverse, multi-center cohort of 1705 glioma patients from six public datasets. Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE and CMD modules are essential for improving predictive accuracy. By integrating large-scale pretraining and task-specific fine-tuning, FoundBioNet enables generalizable glioma characterization. This approach enhances diagnostic accuracy and interpretability, with the potential to enable more personalized patient care.",
        "arxiv_id": "2508.06756",
        "ARXIVID": "2508.06756",
        "COMMENT": "Matches criteria 4 as it focuses on a foundation model for medical imaging, specifically glioma characterization using multi-parametric MRI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.06851": {
        "authors": [
            "Pengfei Zhou",
            "Xiaopeng Peng",
            "Fanrui Zhang",
            "Zhaopan Xu",
            "Jiaxin Ai",
            "Yansheng Qiu",
            "Chuanhao Li",
            "Zhen Li",
            "Ming Li",
            "Yukang Feng",
            "Jianwen Sun",
            "Haoquan Zhang",
            "Zizhen Li",
            "Xiaofeng Mao",
            "Zekai Li",
            "Wangbo Zhao",
            "Kai Wang",
            "Xiaojun Chang",
            "Wenqi Shao",
            "Yang You",
            "Kaipeng Zhang"
        ],
        "title": "MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams",
        "abstract": "arXiv:2508.06851v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs), which integrate language and visual cues for problem-solving, are crucial for advancing artificial general intelligence (AGI). However, current benchmarks for measuring the intelligence of MLLMs suffer from limited scale, narrow coverage, and unstructured knowledge, offering only static and undifferentiated evaluations. To bridge this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark built from real-world K-12 exams spanning six disciplines with 141K instances and 6,225 knowledge points organized in a six-layer taxonomy. Covering five question formats with difficulty and year annotations, it enables comprehensive evaluation to capture the extent to which MLLMs perform over four dimensions: 1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts, and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation framework that introduces unfamiliar visual, textual, and question form shifts to challenge model generalization while improving benchmark objectivity and longevity by mitigating data contamination. We further evaluate knowledge-point reference-augmented generation (KP-RAG) to examine the role of knowledge in problem-solving. Key findings reveal limitations in current MLLMs in multiple aspects and provide guidance for enhancing model robustness, interpretability, and AI-assisted education.",
        "arxiv_id": "2508.06851",
        "ARXIVID": "2508.06851",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark for evaluating multimodal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.06832": {
        "authors": [
            "Haifeng Li",
            "Wang Guo",
            "Haiyang Wu",
            "Mengwei Wu",
            "Jipeng Zhang",
            "Qing Zhu",
            "Yu Liu",
            "Xin Huang",
            "Chao Tao"
        ],
        "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges",
        "abstract": "arXiv:2508.06832v1 Announce Type: new  Abstract: The mainstream paradigm of remote sensing image interpretation has long been dominated by vision-centered models, which rely on visual features for semantic understanding. However, these models face inherent limitations in handling multi-modal reasoning, semantic abstraction, and interactive decision-making. While recent advances have introduced Large Language Models (LLMs) into remote sensing workflows, existing studies primarily focus on downstream applications, lacking a unified theoretical framework that explains the cognitive role of language. This review advocates a paradigm shift from vision-centered to language-centered remote sensing interpretation. Drawing inspiration from the Global Workspace Theory (GWT) of human cognition, We propose a language-centered framework for remote sensing interpretation that treats LLMs as the cognitive central hub integrating perceptual, task, knowledge and action spaces to enable unified understanding, reasoning, and decision-making. We first explore the potential of LLMs as the central cognitive component in remote sensing interpretation, and then summarize core technical challenges, including unified multimodal representation, knowledge association, and reasoning and decision-making. Furthermore, we construct a global workspace-driven interpretation mechanism and review how language-centered solutions address each challenge. Finally, we outline future research directions from four perspectives: adaptive alignment of multimodal data, task understanding under dynamic knowledge constraints, trustworthy reasoning, and autonomous interaction. This work aims to provide a conceptual foundation for the next generation of remote sensing interpretation systems and establish a roadmap toward cognition-driven intelligent geospatial analysis.",
        "arxiv_id": "2508.06832",
        "ARXIVID": "2508.06832",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive review of language-centered remote sensing image interpretation.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2508.07804": {
        "authors": [
            "Bao Li",
            "Xiaomei Zhang",
            "Miao Xu",
            "Zhaoxin Fan",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "title": "Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning",
        "abstract": "arXiv:2508.07804v1 Announce Type: new  Abstract: Generating 3D human poses from multimodal inputs such as images or text requires models to capture both rich spatial and semantic correspondences. While pose-specific multimodal large language models (MLLMs) have shown promise in this task, they are typically trained with supervised objectives such as SMPL parameter regression or token-level prediction, which struggle to model the inherent ambiguity and achieve task-specific alignment required for accurate 3D pose generation. To address these limitations, we propose Pose-RFT, a reinforcement fine-tuning framework tailored for 3D human pose generation in MLLMs. We formulate the task as a hybrid action reinforcement learning problem that jointly optimizes discrete language prediction and continuous pose generation. To this end, we introduce HyGRPO, a hybrid reinforcement learning algorithm that performs group-wise reward normalization over sampled responses to guide joint optimization of discrete and continuous actions. Pose-RFT further incorporates task-specific reward functions to guide optimization towards spatial alignment in image-to-pose generation and semantic consistency in text-to-pose generation. Extensive experiments on multiple pose generation benchmarks demonstrate that Pose-RFT significantly improves performance over existing pose-specific MLLMs, validating the effectiveness of hybrid action reinforcement fine-tuning for 3D pose generation.",
        "arxiv_id": "2508.07804",
        "ARXIVID": "2508.07804",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on multimodal large language models for 3D pose generation and introduces a novel reinforcement fine-tuning framework.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.07905": {
        "authors": [
            "Yongtao Ge",
            "Kangyang Xie",
            "Guangkai Xu",
            "Mingyu Liu",
            "Li Ke",
            "Longtao Huang",
            "Hui Xue",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "Generative Video Matting",
        "abstract": "arXiv:2508.07905v1 Announce Type: new  Abstract: Video matting has traditionally been limited by the lack of high-quality ground-truth data. Most existing video matting datasets provide only human-annotated imperfect alpha and foreground annotations, which must be composited to background images or videos during the training stage. Thus, the generalization capability of previous methods in real-world scenarios is typically poor. In this work, we propose to solve the problem from two perspectives. First, we emphasize the importance of large-scale pre-training by pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also develop a scalable synthetic data generation pipeline that can render diverse human bodies and fine-grained hairs, yielding around 200 video clips with a 3-second duration for fine-tuning. Second, we introduce a novel video matting approach that can effectively leverage the rich priors from pre-trained video diffusion models. This architecture offers two key advantages. First, strong priors play a critical role in bridging the domain gap between synthetic and real-world scenes. Second, unlike most existing methods that process video matting frame-by-frame and use an independent decoder to aggregate temporal information, our model is inherently designed for video, ensuring strong temporal consistency. We provide a comprehensive quantitative evaluation across three benchmark datasets, demonstrating our approach's superior performance, and present comprehensive qualitative results in diverse real-world scenes, illustrating the strong generalization capability of our method. The code is available at https://github.com/aim-uofa/GVM.",
        "arxiv_id": "2508.07905",
        "ARXIVID": "2508.07905",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel video matting approach leveraging video diffusion models and evaluates on video datasets.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.06511": {
        "authors": [
            "He Feng",
            "Yongjia Ma",
            "Donglin Di",
            "Lei Fan",
            "Tonghua Su",
            "Xiangqian Wu"
        ],
        "title": "DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation",
        "abstract": "arXiv:2508.06511v1 Announce Type: new  Abstract: Portrait animation aims to synthesize talking videos from a static reference face, conditioned on audio and style frame cues (e.g., emotion and head poses), while ensuring precise lip synchronization and faithful reproduction of speaking styles. Existing diffusion-based portrait animation methods primarily focus on lip synchronization or static emotion transformation, often overlooking dynamic styles such as head movements. Moreover, most of these methods rely on a dual U-Net architecture, which preserves identity consistency but incurs additional computational overhead. To this end, we propose DiTalker, a unified DiT-based framework for speaking style-controllable portrait animation. We design a Style-Emotion Encoding Module that employs two separate branches: a style branch extracting identity-specific style information (e.g., head poses and movements), and an emotion branch extracting identity-agnostic emotion features. We further introduce an Audio-Style Fusion Module that decouples audio and speaking styles via two parallel cross-attention layers, using these features to guide the animation process. To enhance the quality of results, we adopt and modify two optimization constraints: one to improve lip synchronization and the other to preserve fine-grained identity and background details. Extensive experiments demonstrate the superiority of DiTalker in terms of lip synchronization and speaking style controllability. Project Page: https://thenameishope.github.io/DiTalker/",
        "arxiv_id": "2508.06511",
        "ARXIVID": "2508.06511",
        "COMMENT": "Matches criterion 5 as it integrates audio, style, and emotion for portrait animation, showcasing techniques that combine image understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.07355": {
        "authors": [
            "Qilin Zhang",
            "Olaf Wysocki",
            "Boris Jutzi"
        ],
        "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction",
        "abstract": "arXiv:2508.07355v1 Announce Type: new  Abstract: Recent advances in Gaussian Splatting (GS) have demonstrated its effectiveness in photo-realistic rendering and 3D reconstruction. Among these, 2D Gaussian Splatting (2DGS) is particularly suitable for surface reconstruction due to its flattened Gaussian representation and integrated normal regularization. However, its performance often degrades in large-scale and complex urban scenes with frequent occlusions, leading to incomplete building reconstructions. We propose GS4Buildings, a novel prior-guided Gaussian Splatting method leveraging the ubiquity of semantic 3D building models for robust and scalable building surface reconstruction. Instead of relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic 3D building models. Moreover, we generate prior depth and normal maps from the planar building geometry and incorporate them into the optimization process, providing strong geometric guidance for surface consistency and structural accuracy. We also introduce an optional building-focused mode that limits reconstruction to building regions, achieving a 71.8% reduction in Gaussian primitives and enabling a more efficient and compact representation. Experiments on urban datasets demonstrate that GS4Buildings improves reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These results highlight the potential of semantic building model integration to advance GS-based reconstruction toward real-world urban applications such as smart cities and digital twins. Our project is available: https://github.com/zqlin0521/GS4Buildings.",
        "arxiv_id": "2508.07355",
        "ARXIVID": "2508.07355",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D building reconstruction, which is relevant to embodied/robotic AI and urban applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.07146": {
        "authors": [
            "Yu Liu",
            "Zhijie Liu",
            "Xiao Ren",
            "You-Fu Li",
            "He Kong"
        ],
        "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction",
        "abstract": "arXiv:2508.07146v1 Announce Type: new  Abstract: Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.",
        "arxiv_id": "2508.07146",
        "ARXIVID": "2508.07146",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for pedestrian trajectory prediction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.08227": {
        "authors": [
            "Zhiqiang Wu",
            "Zhaomang Sun",
            "Tong Zhou",
            "Bingtao Fu",
            "Ji Cong",
            "Yitong Dong",
            "Huaqi Zhang",
            "Xuan Tang",
            "Mingsong Chen",
            "Xian Wei"
        ],
        "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution",
        "abstract": "arXiv:2508.08227v1 Announce Type: new  Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM) generative models show promising potential for one-step Real-World Image Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a Low-Quality (LQ) image latent distribution at the initial timestep. However, a fundamental gap exists between the LQ image latent distribution and the Gaussian noisy latent distribution, limiting the effective utilization of generative priors. We observe that the noisy latent distribution at DDPM/FM mid-timesteps aligns more closely with the LQ image latent distribution. Based on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a universal framework applicable to DDPM/FM-based generative models. OMGSR injects the LQ image latent distribution at a pre-computed mid-timestep, incorporating the proposed Latent Distribution Refinement loss to alleviate the latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to eliminate checkerboard artifacts in image generation. Within this framework, we instantiate OMGSR for DDPM/FM-based generative models with two variants: OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate that OMGSR-S/F achieves balanced/excellent performance across quantitative and qualitative metrics at 512-resolution. Notably, OMGSR-F establishes overwhelming dominance in all reference metrics. We further train a 1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which yields excellent results, especially in the details of the image generation. We also generate 2k-resolution images by the 1k-resolution OMGSR-F using our two-stage Tiled VAE & Diffusion.",
        "arxiv_id": "2508.08227",
        "ARXIVID": "2508.08227",
        "COMMENT": "Matches criterion 4 as it focuses on improving image super-resolution using generative models, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.06924": {
        "authors": [
            "Shihao Yuan",
            "Yahui Liu",
            "Yang Yue",
            "Jingyuan Zhang",
            "Wangmeng Zuo",
            "Qi Wang",
            "Fuzheng Zhang",
            "Guorui Zhou"
        ],
        "title": "AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning",
        "abstract": "arXiv:2508.06924v1 Announce Type: new  Abstract: Inspired by the success of reinforcement learning (RL) in refining large language models (LLMs), we propose AR-GRPO, an approach to integrate online RL training into autoregressive (AR) image generation models. We adapt the Group Relative Policy Optimization (GRPO) algorithm to refine the vanilla autoregressive models' outputs by carefully designed reward functions that evaluate generated images across multiple quality dimensions, including perceptual quality, realism, and semantic fidelity. We conduct comprehensive experiments on both class-conditional (i.e., class-to-image) and text-conditional (i.e., text-to-image) image generation tasks, demonstrating that our RL-enhanced framework significantly improves both the image quality and human preference of generated images compared to the standard AR baselines. Our results show consistent improvements across various evaluation metrics, establishing the viability of RL-based optimization for AR image generation and opening new avenues for controllable and high-quality image synthesis. The source codes and models are available at: https://github.com/Kwai-Klear/AR-GRPO.",
        "arxiv_id": "2508.06924",
        "ARXIVID": "2508.06924",
        "COMMENT": "Matches criterion 5 as it integrates image generation tasks with reinforcement learning, which could be relevant for combining image understanding and generation with LLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.07407": {
        "authors": [
            "Jinyuan Fang",
            "Yanwen Peng",
            "Xi Zhang",
            "Yingxu Wang",
            "Xinhao Yi",
            "Guibin Zhang",
            "Yi Xu",
            "Bin Wu",
            "Siwei Liu",
            "Zihao Li",
            "Zhaochun Ren",
            "Nikos Aletras",
            "Xi Wang",
            "Han Zhou",
            "Zaiqiao Meng"
        ],
        "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems",
        "abstract": "arXiv:2508.07407v1 Announce Type: new  Abstract: Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.",
        "arxiv_id": "2508.07407",
        "ARXIVID": "2508.07407",
        "COMMENT": "Matches criteria 7 as it is a comprehensive survey on self-evolving AI agents, which includes vision-related aspects.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2508.08189": {
        "authors": [
            "Weijia Wu",
            "Chen Gao",
            "Joya Chen",
            "Kevin Qinghong Lin",
            "Qingwei Meng",
            "Yiming Zhang",
            "Yuke Qiu",
            "Hong Zhou",
            "Mike Zheng Shou"
        ],
        "title": "Reinforcement Learning in Vision: A Survey",
        "abstract": "arXiv:2508.08189v1 Announce Type: new  Abstract: Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
        "arxiv_id": "2508.08189",
        "ARXIVID": "2508.08189",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive survey on reinforcement learning in vision, covering trends and challenges.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.07585": {
        "authors": [
            "Yu-Huan Wu",
            "Wei Liu",
            "Zi-Xuan Zhu",
            "Zizhou Wang",
            "Yong Liu",
            "Liangli Zhen"
        ],
        "title": "GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm",
        "abstract": "arXiv:2508.07585v1 Announce Type: new  Abstract: Recent salient object detection (SOD) models predominantly rely on heavyweight backbones, incurring substantial computational cost and hindering their practical application in various real-world settings, particularly on edge devices. This paper presents GAPNet, a lightweight network built on the granularity-aware paradigm for both image and video SOD. We assign saliency maps of different granularities to supervise the multi-scale decoder side-outputs: coarse object locations for high-level outputs and fine-grained object boundaries for low-level outputs. Specifically, our decoder is built with granularity-aware connections which fuse high-level features of low granularity and low-level features of high granularity, respectively. To support these connections, we design granular pyramid convolution (GPC) and cross-scale attention (CSA) modules for efficient fusion of low-scale and high-scale features, respectively. On top of the encoder, a self-attention module is built to learn global information, enabling accurate object localization with negligible computational cost. Unlike traditional U-Net-based approaches, our proposed method optimizes feature utilization and semantic interpretation while applying appropriate supervision at each processing stage. Extensive experiments show that the proposed method achieves a new state-of-the-art performance among lightweight image and video SOD models. Code is available at https://github.com/yuhuan-wu/GAPNet.",
        "arxiv_id": "2508.07585",
        "ARXIVID": "2508.07585",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a lightweight framework for video salient object detection with novel granularity-aware modules.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.07165": {
        "authors": [
            "Zelin Qiu",
            "Xi Wang",
            "Zhuoyao Xie",
            "Juan Zhou",
            "Yu Wang",
            "Lingjie Yang",
            "Xinrui Jiang",
            "Juyoung Bae",
            "Moo Hyun Son",
            "Qiang Ye",
            "Dexuan Chen",
            "Rui Zhang",
            "Tao Li",
            "Neeraj Ramesh Mahboobani",
            "Varut Vardhanabhuti",
            "Xiaohui Duan",
            "Yinghua Zhao",
            "Hao Chen"
        ],
        "title": "Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications",
        "abstract": "arXiv:2508.07165v1 Announce Type: new  Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable versatility, enabling the distinct visualization of different tissue types. Nevertheless, the inherent heterogeneity among MRI sequences poses significant challenges to the generalization capability of deep learning models. These challenges undermine model performance when faced with varying acquisition parameters, thereby severely restricting their clinical utility. In this study, we present PRISM, a foundation model PRe-trained with large-scale multI-Sequence MRI. We collected a total of 64 datasets from both public and private sources, encompassing a wide range of whole-body anatomical structures, with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI scans from 34 datasets (8 public and 26 private) were curated to construct the largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a novel pretraining paradigm that disentangles anatomically invariant features from sequence-specific variations in MRI, while preserving high-level semantic representations. We established a benchmark comprising 44 downstream tasks, including disease diagnosis, image segmentation, registration, progression prediction, and report generation. These tasks were evaluated on 32 public datasets and 5 private cohorts. PRISM consistently outperformed both non-pretrained models and existing foundation models, achieving first-rank results in 39 out of 44 downstream benchmarks with statistical significance improvements. These results underscore its ability to learn robust and generalizable representations across unseen data acquired under diverse MRI protocols. PRISM provides a scalable framework for multi-sequence MRI analysis, thereby enhancing the translational potential of AI in radiology. It delivers consistent performance across diverse imaging protocols, reinforcing its clinical applicability.",
        "arxiv_id": "2508.07165",
        "ARXIVID": "2508.07165",
        "COMMENT": "Does not match any specific criterion but is related to foundation models in medical imaging, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.07747": {
        "authors": [
            "Junhyuk So",
            "Juncheol Shin",
            "Hyunho Kook",
            "Eunhyeok Park"
        ],
        "title": "Grouped Speculative Decoding for Autoregressive Image Generation",
        "abstract": "arXiv:2508.07747v1 Announce Type: new  Abstract: Recently, autoregressive (AR) image models have demonstrated remarkable generative capabilities, positioning themselves as a compelling alternative to diffusion models. However, their sequential nature leads to long inference times, limiting their practical scalability. In this work, we introduce Grouped Speculative Decoding (GSD), a novel, training-free acceleration method for AR image models. While recent studies have explored Speculative Decoding (SD) as a means to speed up AR image generation, existing approaches either provide only modest acceleration or require additional training. Our in-depth analysis reveals a fundamental difference between language and image tokens: image tokens exhibit inherent redundancy and diversity, meaning multiple tokens can convey valid semantics. However, traditional SD methods are designed to accept only a single most-likely token, which fails to leverage this difference, leading to excessive false-negative rejections. To address this, we propose a new SD strategy that evaluates clusters of visually valid tokens rather than relying on a single target token. Additionally, we observe that static clustering based on embedding distance is ineffective, which motivates our dynamic GSD approach. Extensive experiments show that GSD accelerates AR image models by an average of 3.7x while preserving image quality-all without requiring any additional training. The source code is available at https://github.com/junhyukso/GSD",
        "arxiv_id": "2508.07747",
        "ARXIVID": "2508.07747",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for images, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.07656": {
        "authors": [
            "Yimin Fu",
            "Zhunga Liu",
            "Dongxiu Guo",
            "Longfei Wang"
        ],
        "title": "Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels",
        "abstract": "arXiv:2508.07656v1 Announce Type: new  Abstract: The acquisition of high-quality labeled synthetic aperture radar (SAR) data is challenging due to the demanding requirement for expert knowledge. Consequently, the presence of unreliable noisy labels is unavoidable, which results in performance degradation of SAR automatic target recognition (ATR). Existing research on learning with noisy labels mainly focuses on image data. However, the non-intuitive visual characteristics of SAR data are insufficient to achieve noise-robust learning. To address this problem, we propose collaborative learning of scattering and deep features (CLSDF) for SAR ATR with noisy labels. Specifically, a multi-model feature fusion framework is designed to integrate scattering and deep features. The attributed scattering centers (ASCs) are treated as dynamic graph structure data, and the extracted physical characteristics effectively enrich the representation of deep image features. Then, the samples with clean and noisy labels are divided by modeling the loss distribution with multiple class-wise Gaussian Mixture Models (GMMs). Afterward, the semi-supervised learning of two divergent branches is conducted based on the data divided by each other. Moreover, a joint distribution alignment strategy is introduced to enhance the reliability of co-guessed labels. Extensive experiments have been done on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, and the results show that the proposed method can achieve state-of-the-art performance under different operating conditions with various label noises.",
        "arxiv_id": "2508.07656",
        "ARXIVID": "2508.07656",
        "COMMENT": "Does not match any specific criteria but focuses on SAR target recognition with noisy labels, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07795": {
        "authors": [
            "Hongrui Zheng",
            "Yuezun Li",
            "Liejun Wang",
            "Yunfeng Diao",
            "Zhiqing Guo"
        ],
        "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake",
        "abstract": "arXiv:2508.07795v1 Announce Type: new  Abstract: Active defense strategies have been developed to counter the threat of deepfake technology. However, a primary challenge is their lack of persistence, as their effectiveness is often short-lived. Attackers can bypass these defenses by simply collecting protected samples and retraining their models. This means that static defenses inevitably fail when attackers retrain their models, which severely limits practical use. We argue that an effective defense not only distorts forged content but also blocks the model's ability to adapt, which occurs when attackers retrain their models on protected images. To achieve this, we propose an innovative Two-Stage Defense Framework (TSDF). Benefiting from the intensity separation mechanism designed in this paper, the framework uses dual-function adversarial perturbations to perform two roles. First, it can directly distort the forged results. Second, it acts as a poisoning vehicle that disrupts the data preparation process essential for an attacker's retraining pipeline. By poisoning the data source, TSDF aims to prevent the attacker's model from adapting to the defensive perturbations, thus ensuring the defense remains effective long-term. Comprehensive experiments show that the performance of traditional interruption methods degrades sharply when it is subjected to adversarial retraining. However, our framework shows a strong dual defense capability, which can improve the persistence of active defense. Our code will be available at https://github.com/vpsg-research/TSDF.",
        "arxiv_id": "2508.07795",
        "ARXIVID": "2508.07795",
        "COMMENT": "Does not match any specific criteria but focuses on adversarial defense against deepfake, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.06696": {
        "authors": [
            "Tianqin Li",
            "George Liu",
            "Tai Sing Lee"
        ],
        "title": "Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision",
        "abstract": "arXiv:2508.06696v1 Announce Type: new  Abstract: Despite remarkable progress in computer vision, modern recognition systems remain limited by their dependence on rich, redundant visual inputs. In contrast, humans can effortlessly understand sparse, minimal representations like line drawings - suggesting that structure, rather than appearance, underlies efficient visual understanding. In this work, we propose using line drawings as a structure-first pretraining modality to induce more compact and generalizable visual representations. We show that models pretrained on line drawings develop stronger shape bias, more focused attention, and greater data efficiency across classification, detection, and segmentation tasks. Notably, these models also exhibit lower intrinsic dimensionality, requiring significantly fewer principal components to capture representational variance - echoing the similar observation in low dimensional efficient representation in the brain. Beyond performance improvements, line drawing pretraining produces more compressible representations, enabling better distillation into lightweight student models. Students distilled from line-pretrained teachers consistently outperform those trained from color-supervised teachers, highlighting the benefits of structurally compact knowledge. Finally, we demonstrate that the pretraining with line-drawing can also be extended to unsupervised setting via our proposed method \"learning to draw\". Together, our results support the view that structure-first visual learning fosters efficiency, generalization, and human-aligned inductive biases - offering a simple yet powerful strategy for building more robust and adaptable vision systems.",
        "arxiv_id": "2508.06696",
        "ARXIVID": "2508.06696",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in computer vision and efficient visual representation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07537": {
        "authors": [
            "Xiaoming Li",
            "Wangmeng Zuo",
            "Chen Change Loy"
        ],
        "title": "Enhanced Generative Structure Prior for Chinese Text Image Super-resolution",
        "abstract": "arXiv:2508.07537v1 Announce Type: new  Abstract: Faithful text image super-resolution (SR) is challenging because each character has a unique structure and usually exhibits diverse font styles and layouts. While existing methods primarily focus on English text, less attention has been paid to more complex scripts like Chinese. In this paper, we introduce a high-quality text image SR framework designed to restore the precise strokes of low-resolution (LR) Chinese characters. Unlike methods that rely on character recognition priors to regularize the SR task, we propose a novel structure prior that offers structure-level guidance to enhance visual quality. Our framework incorporates this structure prior within a StyleGAN model, leveraging its generative capabilities for restoration. To maintain the integrity of character structures while accommodating various font styles and layouts, we implement a codebook-based mechanism that restricts the generative space of StyleGAN. Each code in the codebook represents the structure of a specific character, while the vector $w$ in StyleGAN controls the character's style, including typeface, orientation, and location. Through the collaborative interaction between the codebook and style, we generate a high-resolution structure prior that aligns with LR characters both spatially and structurally. Experiments demonstrate that this structure prior provides robust, character-specific guidance, enabling the accurate restoration of clear strokes in degraded characters, even for real-world LR Chinese text with irregular layouts. Our code and pre-trained models will be available at https://github.com/csxmli2016/MARCONetPlusPlus",
        "arxiv_id": "2508.07537",
        "ARXIVID": "2508.07537",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling for text image super-resolution, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.08117": {
        "authors": [
            "Xudong Han",
            "Pengcheng Fang",
            "Yueying Tian",
            "Jianhui Yu",
            "Xiaohao Cai",
            "Daniel Roggen",
            "Philip Birch"
        ],
        "title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking",
        "abstract": "arXiv:2508.08117v1 Announce Type: new  Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged by occlusions and depth ambiguity, issues that conventional tracking-by-detection (TBD) methods struggle to resolve owing to a lack of geometric awareness. To address these limitations, we introduce GRASPTrack, a novel depth-aware MOT framework that integrates monocular depth estimation and instance segmentation into a standard TBD pipeline to generate high-fidelity 3D point clouds from 2D detections, thereby enabling explicit 3D geometric reasoning. These 3D point clouds are then voxelized to enable a precise and robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To further enhance tracking robustness, our approach incorporates Depth-aware Adaptive Noise Compensation, which dynamically adjusts the Kalman filter process noise based on occlusion severity for more reliable state estimation. Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which extends the motion direction consistency from the image plane into 3D space to improve motion-based association cues, particularly for objects with complex trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack benchmarks demonstrate that our method achieves competitive performance, significantly improving tracking robustness in complex scenes with frequent occlusions and intricate motion patterns.",
        "arxiv_id": "2508.08117",
        "ARXIVID": "2508.08117",
        "COMMENT": "Does not match any specific criterion but is related to multi-object tracking with geometric reasoning, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07616": {
        "authors": [
            "Aswin RRV",
            "Jacob Dineen",
            "Divij Handa",
            "Md Nayem Uddin",
            "Mihir Parmar",
            "Chitta Baral",
            "Ben Zhou"
        ],
        "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation",
        "abstract": "arXiv:2508.07616v1 Announce Type: new  Abstract: Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.",
        "arxiv_id": "2508.07616",
        "ARXIVID": "2508.07616",
        "COMMENT": "Does not match any specific criterion but is related to improving reasoning in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07950": {
        "authors": [
            "Chen Shen",
            "Wanqing Zhang",
            "Kehan Li",
            "Erwen Huang",
            "Haitao Bi",
            "Aiying Fan",
            "Yiwen Shen",
            "Hongmei Dong",
            "Ji Zhang",
            "Yuming Shao",
            "Zengjia Liu",
            "Xinshe Liu",
            "Tao Li",
            "Chunxia Yan",
            "Shuanliang Fan",
            "Di Wu",
            "Jianhua Ma",
            "Bin Cong",
            "Zhenyuan Wang",
            "Chunfeng Lian"
        ],
        "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis",
        "abstract": "arXiv:2508.07950v1 Announce Type: new  Abstract: Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.",
        "arxiv_id": "2508.07950",
        "ARXIVID": "2508.07950",
        "COMMENT": "Does not match any specific criterion but is related to large language models in a specific application domain.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.06753": {
        "authors": [
            "Evangelos Georganas",
            "Dhiraj Kalamkar",
            "Alexander Heinecke"
        ],
        "title": "Pushing the Envelope of LLM Inference on AI-PC",
        "abstract": "arXiv:2508.06753v1 Announce Type: new  Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the perplexity and end-task performance of their full-precision counterparts using the same model size, is ushering in a new era of LLM inference for resource-constrained environments such as edge devices and AI PCs. While these quantization advances promise models that are more cost-effective in terms of latency, memory, throughput, and energy consumption, the computational efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp) used to deploy them remains underexplored. In this work, we take a bottom-up approach: we first design and implement 1-bit and 2-bit microkernels optimized for modern CPUs, achieving peak computational efficiency across a variety of CPU platforms. We integrate these microkernels into a state-of-the-art LLM inference framework, namely PyTorch-TPP, and present end-to-end inference results with 2-bit models that outperform the current SOTA runtime bitnet.cpp by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model inference. Our optimized runtime advances the state of LLM inference on AI PCs and edge devices, paving the way for efficient deployment of ultra-low-bit LLM models.",
        "arxiv_id": "2508.06753",
        "ARXIVID": "2508.06753",
        "COMMENT": "Does not match any specific criteria but is related to efficient inference for ultra-low-bit LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07743": {
        "authors": [
            "Markus Fritzsche",
            "Elliot Gestrin",
            "Jendrik Seipp"
        ],
        "title": "Symmetry-Aware Transformer Training for Automated Planning",
        "abstract": "arXiv:2508.07743v1 Announce Type: new  Abstract: While transformers excel in many settings, their application in the field of automated planning is limited. Prior work like PlanGPT, a state-of-the-art decoder-only transformer, struggles with extrapolation from easy to hard planning problems. This in turn stems from problem symmetries: planning tasks can be represented with arbitrary variable names that carry no meaning beyond being identifiers. This causes a combinatorial explosion of equivalent representations that pure transformers cannot efficiently learn from. We propose a novel contrastive learning objective to make transformers symmetry-aware and thereby compensate for their lack of inductive bias. Combining this with architectural improvements, we show that transformers can be efficiently trained for either plan-generation or heuristic-prediction. Our results across multiple planning domains demonstrate that our symmetry-aware training effectively and efficiently addresses the limitations of PlanGPT.",
        "arxiv_id": "2508.07743",
        "ARXIVID": "2508.07743",
        "COMMENT": "Does not match any specific criteria but is related to transformers and automated planning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07022": {
        "authors": [
            "Shengtao Wen",
            "Haodong Chen",
            "Yadong Wang",
            "Zhongying Pan",
            "Xiang Chen",
            "Yu Tian",
            "Bo Qian",
            "Dong Liang",
            "Sheng-Jun Huang"
        ],
        "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA",
        "abstract": "arXiv:2508.07022v1 Announce Type: new  Abstract: Knowledge editing (KE) provides a scalable approach for updating factual knowledge in large language models without full retraining. While previous studies have demonstrated effectiveness in general domains and medical QA tasks, little attention has been paid to KE in multimodal medical scenarios. Unlike text-only settings, medical KE demands integrating updated knowledge with visual reasoning to support safe and interpretable clinical decisions. To address this gap, we propose MultiMedEdit, the first benchmark tailored to evaluating KE in clinical multimodal tasks. Our framework spans both understanding and reasoning task types, defines a three-dimensional metric suite (reliability, generality, and locality), and supports cross-paradigm comparisons across general and domain-specific models. We conduct extensive experiments under single-editing and lifelong-editing settings. Results suggest that current methods struggle with generalization and long-tail reasoning, particularly in complex clinical workflows. We further present an efficiency analysis (e.g., edit latency, memory footprint), revealing practical trade-offs in real-world deployment across KE paradigms. Overall, MultiMedEdit not only reveals the limitations of current approaches but also provides a solid foundation for developing clinically robust knowledge editing techniques in the future.",
        "arxiv_id": "2508.07022",
        "ARXIVID": "2508.07022",
        "COMMENT": "Does not match any specific criteria. Focuses on knowledge editing in medical VQA, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07607": {
        "authors": [
            "Jian Ma",
            "Xujie Zhu",
            "Zihao Pan",
            "Qirong Peng",
            "Xu Guo",
            "Chen Chen",
            "Haonan Lu"
        ],
        "title": "X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning",
        "abstract": "arXiv:2508.07607v1 Announce Type: new  Abstract: Existing open-source datasets for arbitrary-instruction image editing remain suboptimal, while a plug-and-play editing module compatible with community-prevalent generative models is notably absent. In this paper, we first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse editing tasks, including subject-driven generation. We utilize the industry-leading unified image generation models and expert models to construct the data. Meanwhile, we design reasonable editing instructions with the VLM and implement various scoring mechanisms to filter the data. As a result, we construct 3.7 million high-quality data with balanced categories. Second, to better integrate seamlessly with community image generation models, we design task-aware MoE-LoRA training based on FLUX.1, with only 8\\% of the parameters of the full model. To further improve the final performance, we utilize the internal representations of the diffusion model and define positive/negative samples based on image editing types to introduce contrastive learning. Extensive experiments demonstrate that the model's editing performance is competitive among many excellent models. Additionally, the constructed dataset exhibits substantial advantages over existing open-source datasets. The open-source code, checkpoints, and datasets for X2Edit can be found at the following link: https://github.com/OPPO-Mente-Lab/X2Edit.",
        "arxiv_id": "2508.07607",
        "ARXIVID": "2508.07607",
        "COMMENT": "Does not match any specific criteria. Focuses on arbitrary-instruction image editing and dataset creation, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.08053": {
        "authors": [
            "Runchuan Zhu",
            "Bowen Jiang",
            "Lingrui Mei",
            "Fangkai Yang",
            "Lu Wang",
            "Haoxiang Gao",
            "Fengshuo Bai",
            "Pu Zhao",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning",
        "abstract": "arXiv:2508.08053v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have sparked growing interest in agentic workflows, which are structured sequences of LLM invocations intended to solve complex tasks. However, existing approaches often rely on static templates or manually designed workflows, which limit adaptability to diverse tasks and hinder scalability. We propose AdaptFlow, a natural language-based meta-learning framework inspired by model-agnostic meta-learning (MAML). AdaptFlow learns a generalizable workflow initialization that enables rapid subtask-level adaptation. It employs a bi-level optimization scheme: the inner loop refines the workflow for a specific subtask using LLM-generated feedback, while the outer loop updates the shared initialization to perform well across tasks. This setup allows AdaptFlow to generalize effectively to unseen tasks by adapting the initialized workflow through language-guided modifications. Evaluated across question answering, code generation, and mathematical reasoning benchmarks, AdaptFlow consistently outperforms both manually crafted and automatically searched baselines, achieving state-of-the-art results with strong generalization across tasks and models. The source code and data are available at https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.",
        "arxiv_id": "2508.08053",
        "ARXIVID": "2508.08053",
        "COMMENT": "Does not match any specific criteria. Focuses on meta-learning for workflow optimization, not spatial intelligence, embodied agents, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07083": {
        "authors": [
            "Yueyu Hu",
            "Ran Gong",
            "Tingyu Fan",
            "Yao Wang"
        ],
        "title": "TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree",
        "abstract": "arXiv:2508.07083v1 Announce Type: new  Abstract: 3D visual content streaming is a key technology for emerging 3D telepresence and AR/VR applications. One fundamental element underlying the technology is a versatile 3D representation that is capable of producing high-quality renders and can be efficiently compressed at the same time. Existing 3D representations like point clouds, meshes and 3D Gaussians each have limitations in terms of rendering quality, surface definition, and compressibility. In this paper, we present the Textured Surfel Octree (TeSO), a novel 3D representation that is built from point clouds but addresses the aforementioned limitations. It represents a 3D scene as cube-bounded surfels organized on an octree, where each surfel is further associated with a texture patch. By approximating a smooth surface with a large surfel at a coarser level of the octree, it reduces the number of primitives required to represent the 3D scene, and yet retains the high-frequency texture details through the texture map attached to each surfel. We further propose a compression scheme to encode the geometry and texture efficiently, leveraging the octree structure. The proposed textured surfel octree combined with the compression scheme achieves higher rendering quality at lower bit-rates compared to multiple point cloud and 3D Gaussian-based baselines.",
        "arxiv_id": "2508.07083",
        "ARXIVID": "2508.07083",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D point cloud representation and compression, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.07409": {
        "authors": [
            "Junyao Gao",
            "Jiaxing Li",
            "Wenran Liu",
            "Yanhong Zeng",
            "Fei Shen",
            "Kai Chen",
            "Yanan Sun",
            "Cairong Zhao"
        ],
        "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
        "abstract": "arXiv:2508.07409v1 Announce Type: new  Abstract: In this paper, we propose \\textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.",
        "arxiv_id": "2508.07409",
        "ARXIVID": "2508.07409",
        "COMMENT": "Does not match any specific criteria. Focuses on 4D character animation, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.06551": {
        "authors": [
            "Ye Tao"
        ],
        "title": "Slice or the Whole Pie? Utility Control for AI Models",
        "abstract": "arXiv:2508.06551v1 Announce Type: new  Abstract: Training deep neural networks (DNNs) has become an increasingly resource-intensive task, requiring large volumes of labeled data, substantial computational power, and considerable fine-tuning efforts to achieve optimal performance across diverse use cases. Although pre-trained models offer a useful starting point, adapting them to meet specific user needs often demands extensive customization, and infrastructure overhead. This challenge grows when a single model must support diverse appli-cations with differing requirements for performance. Traditional solutions often involve training multiple model versions to meet varying requirements, which can be inefficient and difficult to maintain. In order to overcome this challenge, we propose NNObfuscator, a novel utility control mechanism that enables AI models to dynamically modify their performance according to predefined conditions. It is different from traditional methods that need separate models for each user. Instead, NNObfuscator allows a single model to be adapted in real time, giving you controlled access to multiple levels of performance. This mechanism enables model owners set up tiered access, ensuring that free-tier users receive a baseline level of performance while premium users benefit from enhanced capabilities. The approach improves resource allocation, reduces unnecessary computation, and supports sustainable business models in AI deployment. To validate our approach, we conducted experiments on multiple tasks, including image classification, semantic segmentation, and text to image generation, using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable Diffusion. Experimental results show that NNObfuscator successfully makes model more adaptable, so that a single trained model can handle a broad range of tasks without requiring a lot of changes.",
        "arxiv_id": "2508.06551",
        "ARXIVID": "2508.06551",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning and model adaptability.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.08136": {
        "authors": [
            "Yitong Yang",
            "Yinglin Wang",
            "Changshuo Wang",
            "Huajie Wang",
            "Shuting He"
        ],
        "title": "FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting",
        "abstract": "arXiv:2508.08136v1 Announce Type: new  Abstract: The success of 3DGS in generative and editing applications has sparked growing interest in 3DGS-based style transfer. However, current methods still face two major challenges: (1) multi-view inconsistency often leads to style conflicts, resulting in appearance smoothing and distortion; and (2) heavy reliance on VGG features, which struggle to disentangle style and content from style images, often causing content leakage and excessive stylization. To tackle these issues, we introduce \\textbf{FantasyStyle}, a 3DGS-based style transfer framework, and the first to rely entirely on diffusion model distillation. It comprises two key components: (1) \\textbf{Multi-View Frequency Consistency}. We enhance cross-view consistency by applying a 3D filter to multi-view noisy latent, selectively reducing low-frequency components to mitigate stylized prior conflicts. (2) \\textbf{Controllable Stylized Distillation}. To suppress content leakage from style images, we introduce negative guidance to exclude undesired content. In addition, we identify the limitations of Score Distillation Sampling and Delta Denoising Score in 3D style transfer and remove the reconstruction term accordingly. Building on these insights, we propose a controllable stylized distillation that leverages negative guidance to more effectively optimize the 3D Gaussians. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art approaches, achieving higher stylization quality and visual realism across various scenes and styles.",
        "arxiv_id": "2508.08136",
        "ARXIVID": "2508.08136",
        "COMMENT": "Does not closely match any specific criterion but involves 3D style transfer, which is tangentially related to vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07790": {
        "authors": [
            "Alessandro Abate",
            "Thom Badings",
            "Giuseppe De Giacomo",
            "Francesco Fabiano"
        ],
        "title": "Best-Effort Policies for Robust Markov Decision Processes",
        "abstract": "arXiv:2508.07790v1 Announce Type: new  Abstract: We study the common generalization of Markov decision processes (MDPs) with sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal in RMDPs is to compute a policy that maximizes the expected return under an adversarial choice of the transition probabilities. If the uncertainty in the probabilities is independent between the states, known as s-rectangularity, such optimal robust policies can be computed efficiently using robust value iteration. However, there might still be multiple optimal robust policies, which, while equivalent with respect to the worst-case, reflect different expected returns under non-adversarial choices of the transition probabilities. Hence, we propose a refined policy selection criterion for RMDPs, drawing inspiration from the notions of dominance and best-effort in game theory. Instead of seeking a policy that only maximizes the worst-case expected return, we additionally require the policy to achieve a maximal expected return under different (i.e., not fully adversarial) transition probabilities. We call such a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE policies always exist, characterize their structure, and present an algorithm to compute them with a small overhead compared to standard robust value iteration. ORBE policies offer a principled tie-breaker among optimal robust policies. Numerical experiments show the feasibility of our approach.",
        "arxiv_id": "2508.07790",
        "ARXIVID": "2508.07790",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in reinforcement learning and decision-making.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06899": {
        "authors": [
            "Yanchen Deng",
            "Xinrun Wang",
            "Bo An"
        ],
        "title": "GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization",
        "abstract": "arXiv:2508.06899v1 Announce Type: new  Abstract: Local search is an important class of incomplete algorithms for solving Distributed Constraint Optimization Problems (DCOPs) but it often converges to poor local optima. While GDBA provides a comprehensive rule set to escape premature convergence, its empirical benefits remain marginal on general-valued problems. In this work, we systematically examine GDBA and identify three factors that potentially lead to its inferior performance, i.e., over-aggressive constraint violation conditions, unbounded penalty accumulation, and uncoordinated penalty updates. To address these issues, we propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs that incorporates an adaptive violation condition to selectively penalize constraints with high cost, a penalty evaporation mechanism to control the magnitude of penalization, and a synchronization scheme for coordinated penalty updates. We theoretically show that the penalty values are bounded, and agents play a potential game in our DGLS. Our extensive empirical results on various standard benchmarks demonstrate the great superiority of DGLS over state-of-the-art baselines. Particularly, compared to Damped Max-sum with high damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance on general-valued problems, and outperforms it by significant margins (\\textbf{3.77\\%--66.3\\%}) on structured problems in terms of anytime results.",
        "arxiv_id": "2508.06899",
        "ARXIVID": "2508.06899",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in optimization and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06640": {
        "authors": [
            "Zheyuan Zhang",
            "Weihao Tang",
            "Hong Chen"
        ],
        "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors",
        "abstract": "arXiv:2508.06640v1 Announce Type: new  Abstract: Micro-expression recognition (MER) is a highly challenging task in affective computing. With the reduced-sized micro-expression (ME) input that contains key information based on key-frame indexes, key-frame-based methods have significantly improved the performance of MER. However, most of these methods focus on improving the performance with relatively accurate key-frame indexes, while ignoring the difficulty of obtaining accurate key-frame indexes and the objective existence of key-frame index errors, which impedes them from moving towards practical applications. In this paper, we propose CausalNet, a novel framework to achieve robust MER facing key-frame index errors while maintaining accurate recognition. To enhance robustness, CausalNet takes the representation of the entire ME sequence as the input. To address the information redundancy brought by the complete ME range input and maintain accurate recognition, first, the Causal Motion Position Learning Module (CMPLM) is proposed to help the model locate the muscle movement areas related to Action Units (AUs), thereby reducing the attention to other redundant areas. Second, the Causal Attention Block (CAB) is proposed to deeply learn the causal relationships between the muscle contraction and relaxation movements in MEs. Empirical experiments have demonstrated that on popular ME benchmarks, the CausalNet has achieved robust MER under different levels of key-frame index noise. Meanwhile, it has surpassed state-of-the-art (SOTA) methods on several standard MER benchmarks when using the provided annotated key-frames. Code is available at https://github.com/tony19980810/CausalNet.",
        "arxiv_id": "2508.06640",
        "ARXIVID": "2508.06640",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in computer vision and affective computing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07628": {
        "authors": [
            "Daniel Essien",
            "Suresh Neethirajan"
        ],
        "title": "Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization",
        "abstract": "arXiv:2508.07628v1 Announce Type: new  Abstract: The future of poultry production depends on a paradigm shift replacing subjective, labor-intensive welfare checks with data-driven, intelligent monitoring ecosystems. Traditional welfare assessments-limited by human observation and single-sensor data-cannot fully capture the complex, multidimensional nature of laying hen welfare in modern farms. Multimodal Artificial Intelligence (AI) offers a breakthrough, integrating visual, acoustic, environmental, and physiological data streams to reveal deeper insights into avian welfare dynamics. This investigation highlights multimodal As transformative potential, showing that intermediate (feature-level) fusion strategies achieve the best balance between robustness and performance under real-world poultry conditions, and offer greater scalability than early or late fusion approaches. Key adoption barriers include sensor fragility in harsh farm environments, high deployment costs, inconsistent behavioral definitions, and limited cross-farm generalizability. To address these, we introduce two novel evaluation tools - the Domain Transfer Score (DTS) to measure model adaptability across diverse farm settings, and the Data Reliability Index (DRI) to assess sensor data quality under operational constraints. We also propose a modular, context-aware deployment framework designed for laying hen environments, enabling scalable and practical integration of multimodal sensing. This work lays the foundation for a transition from reactive, unimodal monitoring to proactive, precision-driven welfare systems that unite productivity with ethical, science based animal care.",
        "arxiv_id": "2508.07628",
        "ARXIVID": "2508.07628",
        "COMMENT": "Does not match any specific criteria but is related to multimodal AI systems for animal welfare, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07539": {
        "authors": [
            "Yuki Shigeyasu",
            "Shota Harada",
            "Akihiko Yoshizawa",
            "Kazuhiro Terada",
            "Naoki Nakazima",
            "Mariyo Kurata",
            "Hiroyuki Abe",
            "Tetsuo Ushiku",
            "Ryoma Bise"
        ],
        "title": "Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning",
        "abstract": "arXiv:2508.07539v1 Announce Type: new  Abstract: In this paper, we address domain shifts in pathological images by focusing on shifts within whole slide images~(WSIs), such as patient characteristics and tissue thickness, rather than shifts between hospitals. Traditional approaches rely on multi-hospital data, but data collection challenges often make this impractical. Therefore, the proposed domain generalization method captures and leverages intra-hospital domain shifts by clustering WSI-level features from non-tumor regions and treating these clusters as domains. To mitigate domain shift, we apply contrastive learning to reduce feature gaps between WSI pairs from different clusters. The proposed method introduces a two-stage contrastive learning approach WSI-level and patch-level contrastive learning to minimize these gaps effectively.",
        "arxiv_id": "2508.07539",
        "ARXIVID": "2508.07539",
        "COMMENT": "Does not match any specific criteria but is related to domain generalization in pathological image segmentation, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07875": {
        "authors": [
            "Shuo Han",
            "Ahmed Karam Eldaly",
            "Solomon Sunday Oyelere"
        ],
        "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images",
        "abstract": "arXiv:2508.07875v1 Announce Type: new  Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer, and early, accurate diagnosis is critical to improving patient survival rates by guiding treatment decisions. Combining medical expertise with artificial intelligence (AI) holds significant promise for enhancing the precision and efficiency of IDC detection. In this work, we propose a human-in-the-loop (HITL) deep learning system designed to detect IDC in histopathology images. The system begins with an initial diagnosis provided by a high-performance EfficientNetV2S model, offering feedback from AI to the human expert. Medical professionals then review the AI-generated results, correct any misclassified images, and integrate the revised labels into the training dataset, forming a feedback loop from the human back to the AI. This iterative process refines the model's performance over time. The EfficientNetV2S model itself achieves state-of-the-art performance compared to existing methods in the literature, with an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system further improves the model's accuracy using four experimental groups with misclassified images. These results demonstrate the potential of this collaborative approach to enhance AI performance in diagnostic systems. This work contributes to advancing automated, efficient, and highly accurate methods for IDC detection through human-AI collaboration, offering a promising direction for future AI-assisted medical diagnostics.",
        "arxiv_id": "2508.07875",
        "ARXIVID": "2508.07875",
        "COMMENT": "Does not match any specific criteria but is related to AI-assisted medical diagnostics, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06988": {
        "authors": [
            "Fangmin Zhao",
            "Weichao Zeng",
            "Zhenhang Li",
            "Dongbao Yang",
            "Yu Zhou"
        ],
        "title": "TADoc: Robust Time-Aware Document Image Dewarping",
        "abstract": "arXiv:2508.06988v1 Announce Type: new  Abstract: Flattening curved, wrinkled, and rotated document images captured by portable photographing devices, termed document image dewarping, has become an increasingly important task with the rise of digital economy and online working. Although many methods have been proposed recently, they often struggle to achieve satisfactory results when confronted with intricate document structures and higher degrees of deformation in real-world scenarios. Our main insight is that, unlike other document restoration tasks (e.g., deblurring), dewarping in real physical scenes is a progressive motion rather than a one-step transformation. Based on this, we have undertaken two key initiatives. Firstly, we reformulate this task, modeling it for the first time as a dynamic process that encompasses a series of intermediate states. Secondly, we design a lightweight framework called TADoc (Time-Aware Document Dewarping Network) to address the geometric distortion of document images. In addition, due to the inadequacy of OCR metrics for document images containing sparse text, the comprehensiveness of evaluation is insufficient. To address this shortcoming, we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the effectiveness of document dewarping in downstream tasks. Extensive experiments and in-depth evaluations have been conducted and the results indicate that our model possesses strong robustness, achieving superiority on several benchmarks with different document types and degrees of distortion.",
        "arxiv_id": "2508.06988",
        "ARXIVID": "2508.06988",
        "COMMENT": "Does not match any specific criteria but is related to document image dewarping and geometric transformations.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07041": {
        "authors": [
            "Junkai Liu",
            "Nay Aung",
            "Theodoros N. Arvanitis",
            "Stefan K. Piechnik",
            "Joao A C Lima",
            "Steffen E. Petersen",
            "Le Zhang"
        ],
        "title": "SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging",
        "abstract": "arXiv:2508.07041v1 Announce Type: new  Abstract: Magnetic resonance imaging (MRI) provides detailed soft-tissue characteristics that assist in disease diagnosis and screening. However, the accuracy of clinical practice is often hindered by missing or unusable slices due to various factors. Volumetric MRI synthesis methods have been developed to address this issue by imputing missing slices from available ones. The inherent 3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR), poses significant challenges for missing slice imputation approaches, including (1) the difficulty of modeling local inter-slice correlations and dependencies of volumetric slices, and (2) the limited exploration of crucial 3D spatial information and global context. In this study, to mitigate these issues, we present Spatial-Aware Graph Completion Network (SAGCNet) to overcome the dependency on complete volumetric data, featuring two main innovations: (1) a volumetric slice graph completion module that incorporates the inter-slice relationships into a graph structure, and (2) a volumetric spatial adapter component that enables our model to effectively capture and utilize various forms of 3D spatial context. Extensive experiments on cardiac MRI datasets demonstrate that SAGCNet is capable of synthesizing absent CMR slices, outperforming competitive state-of-the-art MRI synthesis methods both quantitatively and qualitatively. Notably, our model maintains superior performance even with limited slice data.",
        "arxiv_id": "2508.07041",
        "ARXIVID": "2508.07041",
        "COMMENT": "Does not match any specific criteria but is related to MRI synthesis and spatial-aware networks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06951": {
        "authors": [
            "Harry Walsh",
            "Ed Fish",
            "Ozge Mercanoglu Sincan",
            "Mohamed Ilyes Lakhal",
            "Richard Bowden",
            "Neil Fox",
            "Bencie Woll",
            "Kepeng Wu",
            "Zecheng Li",
            "Weichao Zhao",
            "Haodong Wang",
            "Wengang Zhou",
            "Houqiang Li",
            "Shengeng Tang",
            "Jiayi He",
            "Xu Wang",
            "Ruobei Zhang",
            "Yaxiong Wang",
            "Lechao Cheng",
            "Meryem Tasyurek",
            "Tugce Kiziltepe",
            "Hacer Yalim Keles"
        ],
        "title": "SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work",
        "abstract": "arXiv:2508.06951v1 Announce Type: new  Abstract: Sign Language Production (SLP) is the task of generating sign language video from spoken language inputs. The field has seen a range of innovations over the last few years, with the introduction of deep learning-based approaches providing significant improvements in the realism and naturalness of generated outputs. However, the lack of standardized evaluation metrics for SLP approaches hampers meaningful comparisons across different systems. To address this, we introduce the first Sign Language Production Challenge, held as part of the third SLRTP Workshop at CVPR 2025. The competition's aims are to evaluate architectures that translate from spoken language sentences to a sequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a range of metrics. For our evaluation data, we use the RWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche Gebardensprache (DGS) weather broadcast dataset. In addition, we curate a custom hidden test set from a similar domain of discourse. This paper presents the challenge design and the winning methodologies. The challenge attracted 33 participants who submitted 231 solutions, with the top-performing team achieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach utilized a retrieval-based framework and a pre-trained language model. As part of the workshop, we release a standardized evaluation network, including high-quality skeleton extraction-based keypoints establishing a consistent baseline for the SLP field, which will enable future researchers to compare their work against a broader range of methods.",
        "arxiv_id": "2508.06951",
        "ARXIVID": "2508.06951",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and evaluation in sign language production.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07596": {
        "authors": [
            "Shahroz Tariq",
            "Simon S. Woo",
            "Priyanka Singh",
            "Irena Irmalasari",
            "Saakshi Gupta",
            "Dev Gupta"
        ],
        "title": "From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users",
        "abstract": "arXiv:2508.07596v1 Announce Type: new  Abstract: The proliferation of deepfake technologies poses urgent challenges and serious risks to digital integrity, particularly within critical sectors such as forensics, journalism, and the legal system. While existing detection systems have made significant progress in classification accuracy, they typically function as black-box models, offering limited transparency and minimal support for human reasoning. This lack of interpretability hinders their usability in real-world decision-making contexts, especially for non-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to Explanation), a novel multimodal framework that integrates visual, semantic, and narrative layers of explanation to make deepfake detection interpretable and accessible. The framework consists of three modular components: (1) a deepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual captioning module that generates natural language summaries of manipulated regions, and (3) a narrative refinement module that uses a fine-tuned Large Language Model (LLM) to produce context-aware, user-sensitive explanations. We instantiate and evaluate the framework on the DF40 benchmark, the most diverse deepfake dataset to date. Experiments demonstrate that our system achieves competitive detection performance while providing high-quality explanations aligned with Grad-CAM activations. By unifying prediction and explanation in a coherent, human-aligned pipeline, this work offers a scalable approach to interpretable deepfake detection, advancing the broader vision of trustworthy and transparent AI systems in adversarial media environments.",
        "arxiv_id": "2508.07596",
        "ARXIVID": "2508.07596",
        "COMMENT": "Does not match any specific criteria. Focuses on explainable deepfake detection, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.08141": {
        "authors": [
            "Nicholas Klein",
            "Hemlata Tak",
            "James Fullwood",
            "Krishna Regmi",
            "Leonidas Spinoulas",
            "Ganesh Sivaraman",
            "Tianxiang Chen",
            "Elie Khoury"
        ],
        "title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization",
        "abstract": "arXiv:2508.08141v1 Announce Type: new  Abstract: The field of visual and audio generation is burgeoning with new state-of-the-art methods. This rapid proliferation of new techniques underscores the need for robust solutions for detecting synthetic content in videos. In particular, when fine-grained alterations via localized manipulations are performed in visual, audio, or both domains, these subtle modifications add challenges to the detection algorithms. This paper presents solutions for the problems of deepfake video classification and localization. The methods were submitted to the ACM 1M Deepfakes Detection Challenge, achieving the best performance in the temporal localization task and a top four ranking in the classification task for the TestA split of the evaluation dataset.",
        "arxiv_id": "2508.08141",
        "ARXIVID": "2508.08141",
        "COMMENT": "Does not match any specific criteria. Focuses on deepfake detection and localization, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07353": {
        "authors": [
            "Rubing Chen",
            "Jiaxin Wu",
            "Jian Wang",
            "Xulu Zhang",
            "Wenqi Fan",
            "Chenghua Lin",
            "Xiao-Yong Wei",
            "Qing Li"
        ],
        "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach",
        "abstract": "arXiv:2508.07353v1 Announce Type: new  Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities of large language models (LLMs), highlighting the need for effective and efficient benchmark construction. Existing domain-specific benchmarks primarily focus on the scaling law, relying on massive corpora for supervised fine-tuning or generating extensive question sets for broad coverage. However, the impact of corpus and question-answer (QA) set design on the precision and recall of domain-specific LLMs remains unexplored. In this paper, we address this gap and demonstrate that the scaling law is not always the optimal principle for benchmark construction in specific domains. Instead, we propose Comp-Comp, an iterative benchmarking framework based on a comprehensiveness-compactness principle. Here, comprehensiveness ensures semantic recall of the domain, while compactness enhances precision, guiding both corpus and QA set construction. To validate our framework, we conducted a case study in a well-renowned university, resulting in the creation of XUBench, a large-scale and comprehensive closed-domain benchmark. Although we use the academic domain as the case in this work, our Comp-Comp framework is designed to be extensible beyond academia, providing valuable insights for benchmark construction across various domains.",
        "arxiv_id": "2508.07353",
        "ARXIVID": "2508.07353",
        "COMMENT": "Does not match any specific criteria. Focuses on domain-specific LLM benchmark construction, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06857": {
        "authors": [
            "Mengxue Jia",
            "Zhihua Allen-Zhao",
            "You Zhao",
            "Sanyang Liu"
        ],
        "title": "A Joint Sparse Self-Representation Learning Method for Multiview Clustering",
        "abstract": "arXiv:2508.06857v1 Announce Type: new  Abstract: Multiview clustering (MC) aims to group samples using consistent and complementary information across various views. The subspace clustering, as a fundamental technique of MC, has attracted significant attention. In this paper, we propose a novel joint sparse self-representation learning model for MC, where a featured difference is the extraction of view-specific local information by introducing cardinality (i.e., $\\ell_0$-norm) constraints instead of Graph-Laplacian regularization. Specifically, under each view, cardinality constraints directly restrict the samples used in the self-representation stage to extract reliable local and global structure information, while the low-rank constraint aids in revealing a global coherent structure in the consensus affinity matrix during merging. The attendant challenge is that Augmented Lagrange Method (ALM)-based alternating minimization algorithms cannot guarantee convergence when applied directly to our nonconvex, nonsmooth model, thus resulting in poor generalization ability. To address it, we develop an alternating quadratic penalty (AQP) method with global convergence, where two subproblems are iteratively solved by closed-form solutions. Empirical results on six standard datasets demonstrate the superiority of our model and AQP method, compared to eight state-of-the-art algorithms.",
        "arxiv_id": "2508.06857",
        "ARXIVID": "2508.06857",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning and clustering methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06968": {
        "authors": [
            "Ulas Gunes",
            "Matias Turkulainen",
            "Juho Kannala",
            "Esa Rahtu"
        ],
        "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View",
        "abstract": "arXiv:2508.06968v1 Announce Type: new  Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180 degree. Our study covers both indoor and outdoor scenes captured with 200 degree fisheye cameras and analyzes how each method handles extreme distortion in real world settings. We evaluate performance under varying fields of view (200 degree, 160 degree, and 120 degree) to study the tradeoff between peripheral distortion and spatial coverage. Fisheye-GS benefits from field of view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable across all settings and maintains high perceptual quality at the full 200 degree view. To address the limitations of SfM-based initialization, which often fails under strong distortion, we also propose a depth-based strategy using UniK3D predictions from only 2-3 fisheye images per scene. Although UniK3D is not trained on real fisheye data, it produces dense point clouds that enable reconstruction quality on par with SfM, even in difficult scenes with fog, glare, or sky. Our results highlight the practical viability of fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and distortion-heavy image inputs.",
        "arxiv_id": "2508.06968",
        "ARXIVID": "2508.06968",
        "COMMENT": "Does not match any specific criteria but is related to general interest in 3D reconstruction and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07548": {
        "authors": [
            "Takehiro Yamane",
            "Itaru Tsuge",
            "Susumu Saito",
            "Ryoma Bise"
        ],
        "title": "Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning",
        "abstract": "arXiv:2508.07548v1 Announce Type: new  Abstract: This paper proposes a novel pseudo-labeling method for medical image segmentation that can perform learning on ``individual images'' to select effective pseudo-labels. We introduce Positive and Unlabeled Learning (PU learning), which uses only positive and unlabeled data for binary classification problems, to obtain the appropriate metric for discriminating foreground and background regions on each unlabeled image. Our PU learning makes us easy to select pseudo-labels for various background regions. The experimental results show the effectiveness of our method.",
        "arxiv_id": "2508.07548",
        "ARXIVID": "2508.07548",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning and medical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.08069": {
        "authors": [
            "Xiaoxiao Cui",
            "Yiran Li",
            "Kai He",
            "Shanzhi Jiang",
            "Mengli Xue",
            "Wentao Li",
            "Junhong Leng",
            "Zhi Liu",
            "Lizhen Cui",
            "Shuo Li"
        ],
        "title": "Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition",
        "abstract": "arXiv:2508.08069v1 Announce Type: new  Abstract: Multi-label classification (MLC) of medical images aims to identify multiple diseases and holds significant clinical potential. A critical step is to learn class-specific features for accurate diagnosis and improved interpretability effectively. However, current works focus primarily on causal attention to learn class-specific features, yet they struggle to interpret the true cause due to the inadvertent attention to class-irrelevant features. To address this challenge, we propose a new structural causal model (SCM) that treats class-specific attention as a mixture of causal, spurious, and noisy factors, and a novel Information Bottleneck-based Causal Attention (IBCA) that is capable of learning the discriminative class-specific attention for MLC of medical images. Specifically, we propose learning Gaussian mixture multi-label spatial attention to filter out class-irrelevant information and capture each class-specific attention pattern. Then a contrastive enhancement-based causal intervention is proposed to gradually mitigate the spurious attention and reduce noise information by aligning multi-head attention with the Gaussian mixture multi-label spatial. Quantitative and ablation results on Endo and MuReD show that IBCA outperforms all methods. Compared to the second-best results for each metric, IBCA achieves improvements of 6.35\\% in CR, 7.72\\% in OR, and 5.02\\% in mAP for MuReD, 1.47\\% in CR, and 1.65\\% in CF1, and 1.42\\% in mAP for Endo.",
        "arxiv_id": "2508.08069",
        "ARXIVID": "2508.08069",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning and interpretability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.07771": {
        "authors": [
            "Lei Wang",
            "Shiming Chen",
            "Guo-Sen Xie",
            "Ziming Hong",
            "Chaojian Yu",
            "Qinmu Peng",
            "Xinge You"
        ],
        "title": "Prototype-Guided Curriculum Learning for Zero-Shot Learning",
        "abstract": "arXiv:2508.07771v1 Announce Type: new  Abstract: In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge transfer from seen to unseen classes by learning a visual-semantic mapping from seen-class images to class-level semantic prototypes (e.g., attributes). However, these semantic prototypes are manually defined and may introduce noisy supervision for two main reasons: (i) instance-level mismatch: variations in perspective, occlusion, and annotation bias will cause discrepancies between individual sample and the class-level semantic prototypes; and (ii) class-level imprecision: the manually defined semantic prototypes may not accurately reflect the true semantics of the class. Consequently, the visual-semantic mapping will be misled, reducing the effectiveness of knowledge transfer to unseen classes. In this work, we propose a prototype-guided curriculum learning framework (dubbed as CLZSL), which mitigates instance-level mismatches through a Prototype-Guided Curriculum Learning (PCL) module and addresses class-level imprecision via a Prototype Update (PUP) module. Specifically, the PCL module prioritizes samples with high cosine similarity between their visual mappings and the class-level semantic prototypes, and progressively advances to less-aligned samples, thereby reducing the interference of instance-level mismatches to achieve accurate visual-semantic mapping. Besides, the PUP module dynamically updates the class-level semantic prototypes by leveraging the visual mappings learned from instances, thereby reducing class-level imprecision and further improving the visual-semantic mapping. Experiments were conducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the effectiveness of our method.",
        "arxiv_id": "2508.07771",
        "ARXIVID": "2508.07771",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.08165": {
        "authors": [
            "Yan Wang",
            "Da-Wei Zhou",
            "Han-Jia Ye"
        ],
        "title": "Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning",
        "abstract": "arXiv:2508.08165v1 Announce Type: new  Abstract: Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Existing pre-trained model-based CIL methods often freeze the pre-trained network and adapt to incremental tasks using additional lightweight modules such as adapters. However, incorrect module selection during inference hurts performance, and task-specific modules often overlook shared general knowledge, leading to errors on distinguishing between similar classes across tasks. To address the aforementioned challenges, we propose integrating Task-Specific and Universal Adapters (TUNA) in this paper. Specifically, we train task-specific adapters to capture the most crucial features relevant to their respective tasks and introduce an entropy-based selection mechanism to choose the most suitable adapter. Furthermore, we leverage an adapter fusion strategy to construct a universal adapter, which encodes the most discriminative features shared across tasks. We combine task-specific and universal adapter predictions to harness both specialized and general knowledge during inference. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of our approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA",
        "arxiv_id": "2508.08165",
        "ARXIVID": "2508.08165",
        "COMMENT": "Does not closely match any specific criterion but involves class-incremental learning, which is tangentially related to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.07528": {
        "authors": [
            "Xiaotong Ji",
            "Ryoma Bise",
            "Seiichi Uchida"
        ],
        "title": "Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module",
        "abstract": "arXiv:2508.07528v1 Announce Type: new  Abstract: In medical image processing, accurate diagnosis is of paramount importance. Leveraging machine learning techniques, particularly top-rank learning, shows significant promise by focusing on the most crucial instances. However, challenges arise from noisy labels and class-ambiguous instances, which can severely hinder the top-rank objective, as they may be erroneously placed among the top-ranked instances. To address these, we propose a novel approach that enhances toprank learning by integrating a rejection module. Cooptimized with the top-rank loss, this module identifies and mitigates the impact of outliers that hinder training effectiveness. The rejection module functions as an additional branch, assessing instances based on a rejection function that measures their deviation from the norm. Through experimental validation on a medical dataset, our methodology demonstrates its efficacy in detecting and mitigating outliers, improving the reliability and accuracy of medical image diagnoses.",
        "arxiv_id": "2508.07528",
        "ARXIVID": "2508.07528",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in medical image processing and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.08004": {
        "authors": [
            "Anqi Xiao",
            "Weichen Yu",
            "Hongyuan Yu"
        ],
        "title": "Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition",
        "abstract": "arXiv:2508.08004v1 Announce Type: new  Abstract: Automatic data augmentation (AutoDA) plays an important role in enhancing the generalization of neural networks. However, mainstream AutoDA methods often encounter two challenges: either the search process is excessively time-consuming, hindering practical application, or the performance is suboptimal due to insufficient policy adaptation during training. To address these issues, we propose Sample-aware RandAugment (SRA), an asymmetric, search-free AutoDA method that dynamically adjusts augmentation policies while maintaining straightforward implementation. SRA incorporates a heuristic scoring module that evaluates the complexity of the original training data, enabling the application of tailored augmentations for each sample. Additionally, an asymmetric augmentation strategy is employed to maximize the potential of this scoring module. In multiple experimental settings, SRA narrows the performance gap between search-based and search-free AutoDA methods, achieving a state-of-the-art Top-1 accuracy of 78.31\\% on ImageNet with ResNet-50. Notably, SRA demonstrates good compatibility with existing augmentation pipelines and solid generalization across new tasks, without requiring hyperparameter tuning. The pretrained models leveraging SRA also enhance recognition in downstream object detection tasks. SRA represents a promising step towards simpler, more effective, and practical AutoDA designs applicable to a variety of future tasks. Our code is available at \\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment",
        "arxiv_id": "2508.08004",
        "ARXIVID": "2508.08004",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.07170": {
        "authors": [
            "Yunpeng Shi",
            "Lei Chen",
            "Xiaolu Shen",
            "Yanju Guo"
        ],
        "title": "Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection",
        "abstract": "arXiv:2508.07170v1 Announce Type: new  Abstract: In the domain of computer vision, multi-scale feature extraction is vital for tasks such as salient object detection. However, achieving this capability in lightweight networks remains challenging due to the trade-off between efficiency and performance. This paper proposes a novel lightweight multi-scale feature extraction layer, termed the LMF layer, which employs depthwise separable dilated convolutions in a fully connected structure. By integrating multiple LMF layers, we develop LMFNet, a lightweight network tailored for salient object detection. Our approach significantly reduces the number of parameters while maintaining competitive performance. Here, we show that LMFNet achieves state-of-the-art or comparable results on five benchmark datasets with only 0.81M parameters, outperforming several traditional and lightweight models in terms of both efficiency and accuracy. Our work not only addresses the challenge of multi-scale learning in lightweight networks but also demonstrates the potential for broader applications in image processing tasks. The related code files are available at https://github.com/Shi-Yun-peng/LMFNet",
        "arxiv_id": "2508.07170",
        "ARXIVID": "2508.07170",
        "COMMENT": "Does not match any specific criteria but is related to lightweight networks and salient object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}