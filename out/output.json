{
    "2507.13820": {
        "authors": [
            "Jun Xie",
            "Zhaoran Zhao",
            "Xiongjun Guan",
            "Yingjian Zhu",
            "Hongzhu Yi",
            "Xinming Wang",
            "Feng Chen",
            "Zhepeng Wang"
        ],
        "title": "Team of One: Cracking Complex Video QA with Model Synergy",
        "abstract": "arXiv:2507.13820v1 Announce Type: new  Abstract: We propose a novel framework for open-ended video question answering that enhances reasoning depth and robustness in complex real-world scenarios, as benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models (Video-LMMs) often exhibit limited contextual understanding, weak temporal modeling, and poor generalization to ambiguous or compositional queries. To address these challenges, we introduce a prompting-and-response integration mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs) via structured chains of thought, each tailored to distinct reasoning pathways. An external Large Language Model (LLM) serves as an evaluator and integrator, selecting and fusing the most reliable responses. Extensive experiments demonstrate that our method significantly outperforms existing baselines across all evaluation metrics, showcasing superior generalization and robustness. Our approach offers a lightweight, extensible strategy for advancing multimodal reasoning without requiring model retraining, setting a strong foundation for future Video-LMM development.",
        "arxiv_id": "2507.13820",
        "ARXIVID": "2507.13820",
        "COMMENT": "Matches criterion 6 (Video Understanding) and criterion 2 (Visual and Multimodal Large Language Models) as it proposes a novel framework for video question answering using multiple Video-Language Models and reasoning pathways.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.13362": {
        "authors": [
            "Binbin Ji",
            "Siddharth Agrawal",
            "Qiance Tang",
            "Yvonne Wu"
        ],
        "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning",
        "abstract": "arXiv:2507.13362v1 Announce Type: new  Abstract: This study investigates the spatial reasoning capabilities of vision-language models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement learning. We begin by evaluating the impact of different prompting strategies and find that simple CoT formats, where the model generates a reasoning step before the answer, not only fail to help, but can even harm the model's original performance. In contrast, structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly improves spatial reasoning accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune models using Group Relative Policy Optimization (GRPO) on the SAT dataset and evaluate their performance on CVBench. Compared to supervised fine-tuning (SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates superior robustness under out-of-distribution (OOD) conditions. In particular, we find that SFT overfits to surface-level linguistic patterns and may degrade performance when test-time phrasing changes (e.g., from \"closer to\" to \"farther from\"). GRPO, on the other hand, generalizes more reliably and maintains stable performance under such shifts. Our findings provide insights into how reinforcement learning and structured prompting improve the spatial reasoning capabilities and generalization behavior of modern VLMs. All code is open source at: https://github.com/Yvonne511/spatial-vlm-investigator",
        "arxiv_id": "2507.13362",
        "ARXIVID": "2507.13362",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 2 (Visual and Multimodal Large Language Models) due to its focus on improving spatial reasoning in vision-language models using novel methods like Chain-of-Thought prompting and reinforcement learning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.13929": {
        "authors": [
            "Hsiang-Hui Hung",
            "Huu-Phu Do",
            "Yung-Hui Li",
            "Ching-Chun Huang"
        ],
        "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
        "abstract": "arXiv:2507.13929v1 Announce Type: new  Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering novel views at arbitrary viewpoints and at arbitrary times, even with few input views. For real-world applications, it is expensive to collect multiple views and inefficient to re-optimize for unseen scenes. Moreover, as the digital realm, particularly the metaverse, strives for increasingly immersive experiences, the ability to model 3D environments that naturally transition between day and night becomes paramount. While current techniques based on Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing novel views, the exploration of NeRF's potential for temporal 3D scene modeling remains limited, with no dedicated datasets available for this purpose. To this end, our approach harnesses the strengths of multi-view stereo, neural radiance fields, and disentanglement strategies across diverse datasets. This equips our model with the capability for generalizability in a few-shot setting, allows us to construct an implicit content radiance field for scene representation, and further enables the building of neural radiance fields at any arbitrary time. Finally, we synthesize novel views of that time via volume rendering. Experiments show that TimeNeRF can render novel views in a few-shot setting without per-scene optimization. Most notably, it excels in creating realistic novel views that transition smoothly across different times, adeptly capturing intricate natural scene changes from dawn to dusk.",
        "arxiv_id": "2507.13929",
        "ARXIVID": "2507.13929",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on temporal 3D scene modeling and rendering novel views across time, which is a novel approach to video-based tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14137": {
        "authors": [
            "Shashanka Venkataramanan",
            "Valentinos Pariza",
            "Mohammadreza Salehi",
            "Lukas Knobel",
            "Spyros Gidaris",
            "Elias Ramzi",
            "Andrei Bursuc",
            "Yuki M. Asano"
        ],
        "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
        "abstract": "arXiv:2507.14137v1 Announce Type: new  Abstract: We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.",
        "arxiv_id": "2507.14137",
        "ARXIVID": "2507.14137",
        "COMMENT": "Matches criterion 4 as it focuses on a vision foundation model and its architecture, training, and applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13739": {
        "authors": [
            "Junsu Kim",
            "Yunhoe Ku",
            "Seungryul Baek"
        ],
        "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning",
        "abstract": "arXiv:2507.13739v1 Announce Type: new  Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.",
        "arxiv_id": "2507.13739",
        "ARXIVID": "2507.13739",
        "COMMENT": "Matches criterion 5 as it explores the integration of image generation tasks with large language models using a diffusion model.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13677": {
        "authors": [
            "Chuheng Wei",
            "Ziye Qin",
            "Walter Zimmer",
            "Guoyuan Wu",
            "Matthew J. Barth"
        ],
        "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors",
        "abstract": "arXiv:2507.13677v1 Announce Type: new  Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often operate under heterogeneous sensor configurations due to cost constraints and deployment variability across vehicles and infrastructure. This heterogeneity poses significant challenges for feature fusion and perception reliability. To address these issues, we propose HeCoFuse, a unified framework designed for cooperative perception across mixed sensor setups where nodes may carry Cameras (C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that adaptively weights features through a combination of channel-wise and spatial attention, HeCoFuse can tackle critical challenges such as cross-modality feature misalignment and imbalanced representation quality. In addition, an adaptive spatial resolution adjustment module is employed to balance computational cost and fusion effectiveness. To enhance robustness across different configurations, we further implement a cooperative learning strategy that dynamically adjusts fusion type based on available modalities. Experiments on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22% 3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine heterogeneous sensor configurations. These results, validated by our first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust performance across diverse sensor deployments.",
        "arxiv_id": "2507.13677",
        "ARXIVID": "2507.13677",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for cooperative perception with heterogeneous sensors, addressing challenges in real-world V2X systems.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.13708": {
        "authors": [
            "Sofia Jamil",
            "Bollampalli Areen Reddy",
            "Raghvendra Kumar",
            "Sriparna Saha",
            "Koustava Goswami",
            "K. J. Joseph"
        ],
        "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement",
        "abstract": "arXiv:2507.13708v1 Announce Type: new  Abstract: Recent advancements in text-to-image diffusion models have achieved remarkable success in generating realistic and diverse visual content. A critical factor in this process is the model's ability to accurately interpret textual prompts. However, these models often struggle with creative expressions, particularly those involving complex, abstract, or highly descriptive language. In this work, we introduce a novel training-free approach tailored to improve image generation for a unique form of creative language: poetic verse, which frequently features layered, abstract, and dual meanings. Our proposed PoemTale Diffusion approach aims to minimise the information that is lost during poetic text-to-image conversion by integrating a multi stage prompt refinement loop into Language Models to enhance the interpretability of poetic texts. To support this, we adapt existing state-of-the-art diffusion models by modifying their self-attention mechanisms with a consistent self-attention technique to generate multiple consistent images, which are then collectively used to convey the poem's meaning. Moreover, to encourage research in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting of 1111 poems sourced from multiple online and offline resources. We engaged a panel of poetry experts for qualitative assessments. The results from both human and quantitative evaluations validate the efficacy of our method and contribute a novel perspective to poem-to-image generation with enhanced information capture in the generated images.",
        "arxiv_id": "2507.13708",
        "ARXIVID": "2507.13708",
        "COMMENT": "Matches criterion 5 as it focuses on poem-to-image generation using diffusion models and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.13857": {
        "authors": [
            "Max van den Hoven",
            "Kishaan Jeeveswaran",
            "Pieter Piscaer",
            "Thijs Wensveen",
            "Elahe Arani",
            "Bahram Zonooz"
        ],
        "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
        "abstract": "arXiv:2507.13857v1 Announce Type: new  Abstract: Monocular 3D lane detection is essential for autonomous driving, but challenging due to the inherent lack of explicit spatial information. Multi-modal approaches rely on expensive depth sensors, while methods incorporating fully-supervised depth networks rely on ground-truth depth data that is impractical to collect at scale. Additionally, existing methods assume that camera parameters are available, limiting their applicability in scenarios like crowdsourced high-definition (HD) lane mapping. To address these limitations, we propose Depth3DLane, a novel dual-pathway framework that integrates self-supervised monocular depth estimation to provide explicit structural information, without the need for expensive sensors or additional ground-truth depth data. Leveraging a self-supervised depth network to obtain a point cloud representation of the scene, our bird's-eye view pathway extracts explicit spatial information, while our front view pathway simultaneously extracts rich semantic information. Depth3DLane then uses 3D lane anchors to sample features from both pathways and infer accurate 3D lane geometry. Furthermore, we extend the framework to predict camera parameters on a per-frame basis and introduce a theoretically motivated fitting procedure to enhance stability on a per-segment basis. Extensive experiments demonstrate that Depth3DLane achieves competitive performance on the OpenLane benchmark dataset. Furthermore, experimental results show that using learned parameters instead of ground-truth parameters allows Depth3DLane to be applied in scenarios where camera calibration is infeasible, unlike previous methods.",
        "arxiv_id": "2507.13857",
        "ARXIVID": "2507.13857",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for monocular 3D lane detection, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.13628": {
        "authors": [
            "Masahiro Ogawa",
            "Qi An",
            "Atsushi Yamashita"
        ],
        "title": "Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation",
        "abstract": "arXiv:2507.13628v1 Announce Type: new  Abstract: Separating moving and static objects from a moving camera viewpoint is essential for 3D reconstruction, autonomous navigation, and scene understanding in robotics. Existing approaches often rely primarily on optical flow, which struggles to detect moving objects in complex, structured scenes involving camera motion. To address this limitation, we propose Focus of Expansion Likelihood and Segmentation (FoELS), a method based on the core idea of integrating both optical flow and texture information. FoELS computes the focus of expansion (FoE) from optical flow and derives an initial motion likelihood from the outliers of the FoE computation. This likelihood is then fused with a segmentation-based prior to estimate the final moving probability. The method effectively handles challenges including complex structured scenes, rotational camera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016 dataset and real-world traffic videos demonstrate its effectiveness and state-of-the-art performance.",
        "arxiv_id": "2507.13628",
        "ARXIVID": "2507.13628",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for moving object detection in robotics, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.13405": {
        "authors": [
            "Ishant Chintapatla",
            "Kazuma Choji",
            "Naaisha Agarwal",
            "Andrew Lin",
            "Hannah You",
            "Charles Duong",
            "Kevin Zhu",
            "Sean O'Brien",
            "Vasu Sharma"
        ],
        "title": "COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark",
        "abstract": "arXiv:2507.13405v1 Announce Type: new  Abstract: Recently, many benchmarks and datasets have been developed to evaluate Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and models have shown significant accuracy improvements. However, these benchmarks rarely test the model's ability to accurately complete visual entailment, for instance, accepting or refuting a hypothesis based on the image. To address this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a benchmark of 5608 image and synthetically generated true/false statement pairs, with images derived from the CrowdHuman dataset, to provoke visual entailment reasoning on challenging crowded images. Our results show that even the top-performing VLMs achieve accuracy below 80%, with other models performing substantially worse (39.98%-69.95%). This significant performance gap reveals key limitations in VLMs' ability to reason over certain types of image-question pairs in crowded scenes.",
        "arxiv_id": "2507.13405",
        "ARXIVID": "2507.13405",
        "COMMENT": "Matches criterion 2 as it introduces a new benchmark for visual question answering with a focus on visual entailment reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.13403": {
        "authors": [
            "Morteza Bodaghi",
            "Majid Hosseini",
            "Raju Gottumukkala",
            "Ravi Teja Bhupatiraju",
            "Iftikhar Ahmad",
            "Moncef Gabbouj"
        ],
        "title": "UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data",
        "abstract": "arXiv:2507.13403v1 Announce Type: new  Abstract: In this study, we present a comprehensive public dataset for driver drowsiness detection, integrating multimodal signals of facial, behavioral, and biometric indicators. Our dataset includes 3D facial video using a depth camera, IR camera footage, posterior videos, and biometric signals such as heart rate, electrodermal activity, blood oxygen saturation, skin temperature, and accelerometer data. This data set provides grip sensor data from the steering wheel and telemetry data from the American truck simulator game to provide more information about drivers' behavior while they are alert and drowsy. Drowsiness levels were self-reported every four minutes using the Karolinska Sleepiness Scale (KSS). The simulation environment consists of three monitor setups, and the driving condition is completely like a car. Data were collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully alert and when they exhibited signs of sleepiness. Unlike other datasets, our multimodal dataset has a continuous duration of 40 minutes for each data collection session per subject, contributing to a total length of 1,400 minutes, and we recorded gradual changes in the driver state rather than discrete alert/drowsy labels. This study aims to create a comprehensive multimodal dataset of driver drowsiness that captures a wider range of physiological, behavioral, and driving-related signals. The dataset will be available upon request to the corresponding author.",
        "arxiv_id": "2507.13403",
        "ARXIVID": "2507.13403",
        "COMMENT": "Matches criterion 3 as it introduces a new multimodal dataset for embodied AI, focusing on driver drowsiness detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.13868": {
        "authors": [
            "Francesco Ortu",
            "Zhijing Jin",
            "Diego Doimo",
            "Alberto Cazzaniga"
        ],
        "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
        "abstract": "arXiv:2507.13868v1 Announce Type: new  Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources to address complex tasks, often encountering conflicts between their internal parametric knowledge and external information. Knowledge conflicts can result in hallucinations and unreliable responses, but the mechanisms governing such interactions remain unknown. To address this gap, we analyze the mechanisms that VLMs use to resolve cross-modal conflicts by introducing a dataset of multimodal counterfactual queries that deliberately contradict internal commonsense knowledge. We localize with logit inspection a small set of heads that control the conflict. Moreover, by modifying these heads, we can steer the model towards its internal knowledge or the visual inputs. Finally, we show that attention from such heads pinpoints localized image regions driving visual overrides, outperforming gradient-based attribution in precision.",
        "arxiv_id": "2507.13868",
        "ARXIVID": "2507.13868",
        "COMMENT": "Matches criterion 2 as it explores mechanisms in vision-language models to resolve cross-modal conflicts, which is relevant to visual and multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.13401": {
        "authors": [
            "Shreya Kadambi",
            "Risheek Garrepalli",
            "Shubhankar Borse",
            "Munawar Hyatt",
            "Fatih Porikli"
        ],
        "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing",
        "abstract": "arXiv:2507.13401v1 Announce Type: new  Abstract: Despite the remarkable success of diffusion models in text-to-image generation, their effectiveness in grounded visual editing and compositional control remains challenging. Motivated by advances in self-supervised learning and in-context generative modeling, we propose a series of simple yet powerful design choices that significantly enhance diffusion model capacity for structured, controllable generation and editing. We introduce Masking-Augmented Diffusion with Inference-Time Scaling (MADI), a framework that improves the editability, compositionality and controllability of diffusion models through two core innovations. First, we introduce Masking-Augmented gaussian Diffusion (MAgD), a novel training strategy with dual corruption process which combines standard denoising score matching and masked reconstruction by masking noisy input from forward process. MAgD encourages the model to learn discriminative and compositional visual representations, thus enabling localized and structure-aware editing. Second, we introduce an inference-time capacity scaling mechanism based on Pause Tokens, which act as special placeholders inserted into the prompt for increasing computational capacity at inference time. Our findings show that adopting expressive and dense prompts during training further enhances performance, particularly for MAgD. Together, these contributions in MADI substantially enhance the editability of diffusion models, paving the way toward their integration into more general-purpose, in-context generative diffusion architectures.",
        "arxiv_id": "2507.13401",
        "ARXIVID": "2507.13401",
        "COMMENT": "Matches criterion 5 as it introduces techniques for visual editing using diffusion models, combining image understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.13812": {
        "authors": [
            "Yingying Zhang",
            "Lixiang Ru",
            "Kang Wu",
            "Lei Yu",
            "Lei Liang",
            "Yansheng Li",
            "Jingdong Chen"
        ],
        "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
        "abstract": "arXiv:2507.13812v1 Announce Type: new  Abstract: The multi-modal remote sensing foundation model (MM-RSFM) has significantly advanced various Earth observation tasks, such as urban planning, environmental monitoring, and natural disaster management. However, most existing approaches generally require the training of separate backbone networks for each data modality, leading to redundancy and inefficient parameter utilization. Moreover, prevalent pre-training methods typically apply self-supervised learning (SSL) techniques from natural images without adequately accommodating the characteristics of remote sensing (RS) images, such as the complicated semantic distribution within a single RS image. In this work, we present SkySense V2, a unified MM-RSFM that employs a single transformer backbone to handle multiple modalities. This backbone is pre-trained with a novel SSL strategy tailored to the distinct traits of RS data. In particular, SkySense V2 incorporates an innovative adaptive patch merging module and learnable modality prompt tokens to address challenges related to varying resolutions and limited feature diversity across modalities. In additional, we incorporate the mixture of experts (MoE) module to further enhance the performance of the foundation model. SkySense V2 demonstrates impressive generalization abilities through an extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense by an average of 1.8 points.",
        "arxiv_id": "2507.13812",
        "ARXIVID": "2507.13812",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model for multi-modal remote sensing, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.13397": {
        "authors": [
            "Kaiyuan Zhai",
            "Juan Chen",
            "Chao Wang",
            "Zeyi Xu"
        ],
        "title": "InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction",
        "abstract": "arXiv:2507.13397v1 Announce Type: new  Abstract: Accurate pedestrian trajectory prediction is crucial for intelligent applications, yet it remains highly challenging due to the complexity of interactions among pedestrians. Previous methods have primarily relied on relative positions to model pedestrian interactions; however, they tend to overlook specific interaction patterns such as paired walking or conflicting behaviors, limiting the prediction accuracy in crowded scenarios. To address this issue, we propose InSyn (Interaction-Synchronization Network), a novel Transformer-based model that explicitly captures diverse interaction patterns (e.g., walking in sync or conflicting) while effectively modeling direction-sensitive social behaviors. Additionally, we introduce a training strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue of initial-step divergence in numerical time-series prediction. Experiments on the ETH and UCY datasets demonstrate that our model outperforms recent baselines significantly, especially in high-density scenarios. Furthermore, the SSOS strategy proves effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%.",
        "arxiv_id": "2507.13397",
        "ARXIVID": "2507.13397",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for pedestrian trajectory prediction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.13737": {
        "authors": [
            "Ye Tian",
            "Xiaoyuan Ren",
            "Zihao Wang",
            "Onat Gungor",
            "Xiaofan Yu",
            "Tajana Rosing"
        ],
        "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs",
        "abstract": "arXiv:2507.13737v1 Announce Type: new  Abstract: Rich and context-aware activity logs facilitate user behavior analysis and health monitoring, making them a key research focus in ubiquitous computing. The remarkable semantic understanding and generation capabilities of Large Language Models (LLMs) have recently created new opportunities for activity log generation. However, existing methods continue to exhibit notable limitations in terms of accuracy, efficiency, and semantic richness. To address these challenges, we propose DailyLLM. To the best of our knowledge, this is the first log generation and summarization system that comprehensively integrates contextual activity information across four dimensions: location, motion, environment, and physiology, using only sensors commonly available on smartphones and smartwatches. To achieve this, DailyLLM introduces a lightweight LLM-based framework that integrates structured prompting with efficient feature extraction to enable high-level activity understanding. Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art (SOTA) log generation methods and can be efficiently deployed on personal computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM achieves a 17% improvement in log generation BERTScore precision compared to the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference speed.",
        "arxiv_id": "2507.13737",
        "ARXIVID": "2507.13737",
        "COMMENT": "Matches criterion 2 as it explores the use of LLMs in multi-modal sensor data integration, which aligns with visual and multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.14095": {
        "authors": [
            "Yung-Hong Sun",
            "Ting-Hung Lin",
            "Jiangang Chen",
            "Hongrui Jiang",
            "Yu Hen Hu"
        ],
        "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected {\\delta}-Overlap Graphs",
        "abstract": "arXiv:2507.14095v1 Announce Type: new  Abstract: Multi-view multi-object association is a fundamental step in 3D reconstruction pipelines, enabling consistent grouping of object instances across multiple camera views. Existing methods often rely on appearance features or geometric constraints such as epipolar consistency. However, these approaches can fail when objects are visually indistinguishable or observations are corrupted by noise. We propose C-DOG, a training-free framework that serves as an intermediate module bridging object detection (or pose estimation) and 3D reconstruction, without relying on visual features. It combines connected delta-overlap graph modeling with epipolar geometry to robustly associate detections across views. Each 2D observation is represented as a graph node, with edges weighted by epipolar consistency. A delta-neighbor-overlap clustering step identifies strongly consistent groups while tolerating noise and partial connectivity. To further improve robustness, we incorporate Interquartile Range (IQR)-based filtering and a 3D back-projection error criterion to eliminate inconsistent observations. Extensive experiments on synthetic benchmarks demonstrate that C-DOG outperforms geometry-based baselines and remains robust under challenging conditions, including high object density, without visual features, and limited camera overlap, making it well-suited for scalable 3D reconstruction in real-world scenarios.",
        "arxiv_id": "2507.14095",
        "ARXIVID": "2507.14095",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for multi-view multi-object association, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.13359": {
        "authors": [
            "Yang Zhou",
            "Junjie Li",
            "CongYang Ou",
            "Dawei Yan",
            "Haokui Zhang",
            "Xizhe Xue"
        ],
        "title": "Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives",
        "abstract": "arXiv:2507.13359v1 Announce Type: new  Abstract: Due to its extensive applications, aerial image object detection has long been a hot topic in computer vision. In recent years, advancements in Unmanned Aerial Vehicles (UAV) technology have further propelled this field to new heights, giving rise to a broader range of application requirements. However, traditional UAV aerial object detection methods primarily focus on detecting predefined categories, which significantly limits their applicability. The advent of cross-modal text-image alignment (e.g., CLIP) has overcome this limitation, enabling open-vocabulary object detection (OVOD), which can identify previously unseen objects through natural language descriptions. This breakthrough significantly enhances the intelligence and autonomy of UAVs in aerial scene understanding. This paper presents a comprehensive survey of OVOD in the context of UAV aerial scenes. We begin by aligning the core principles of OVOD with the unique characteristics of UAV vision, setting the stage for a specialized discussion. Building on this foundation, we construct a systematic taxonomy that categorizes existing OVOD methods for aerial imagery and provides a comprehensive overview of the relevant datasets. This structured review enables us to critically dissect the key challenges and open problems at the intersection of these fields. Finally, based on this analysis, we outline promising future research directions and application prospects. This survey aims to provide a clear road map and a valuable reference for both newcomers and seasoned researchers, fostering innovation in this rapidly evolving domain. We keep tracing related works at https://github.com/zhouyang2002/OVOD-in-UVA-imagery",
        "arxiv_id": "2507.13359",
        "ARXIVID": "2507.13359",
        "COMMENT": "Matches criterion 7 as it is a survey paper on open-vocabulary object detection in UAV imagery, which is a vision-focused survey.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.14067": {
        "authors": [
            "Shuliang Liu",
            "Qi Zheng",
            "Jesse Jiaxi Xu",
            "Yibo Yan",
            "He Geng",
            "Aiwei Liu",
            "Peijie Jiang",
            "Jia Liu",
            "Yik-Cheung Tam",
            "Xuming Hu"
        ],
        "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
        "abstract": "arXiv:2507.14067v1 Announce Type: new  Abstract: Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking",
        "arxiv_id": "2507.14067",
        "ARXIVID": "2507.14067",
        "COMMENT": "Matches criterion 2 as it focuses on watermarking for vision-language models, which is relevant to VLLMs and MLLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.14032": {
        "authors": [
            "Lam Nguyen",
            "Erika Barcelos",
            "Roger French",
            "Yinghui Wu"
        ],
        "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
        "abstract": "arXiv:2507.14032v1 Announce Type: new  Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the communication overhead from invoking LLMs. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented LLMs significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale.",
        "arxiv_id": "2507.14032",
        "ARXIVID": "2507.14032",
        "COMMENT": "Does not closely match any specific criterion but involves the integration of large language models with knowledge retrieval, which is tangentially related to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13719": {
        "authors": [
            "Daniele Pannone",
            "Alessia Castronovo",
            "Maurizio Mancini",
            "Gian Luca Foresti",
            "Claudio Piciarelli",
            "Rossana Gabrieli",
            "Muhammad Yasir Bilal",
            "Danilo Avola"
        ],
        "title": "Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction",
        "abstract": "arXiv:2507.13719v1 Announce Type: new  Abstract: This paper presents an innovative augmented reality pipeline tailored for museum environments, aimed at recognizing artworks and generating accurate 3D models from single images. By integrating two complementary pre-trained depth estimation models, i.e., GLPN for capturing global scene structure and Depth-Anything for detailed local reconstruction, the proposed approach produces optimized depth maps that effectively represent complex artistic features. These maps are then converted into high-quality point clouds and meshes, enabling the creation of immersive AR experiences. The methodology leverages state-of-the-art neural network architectures and advanced computer vision techniques to overcome challenges posed by irregular contours and variable textures in artworks. Experimental results demonstrate significant improvements in reconstruction accuracy and visual realism, making the system a highly robust tool for museums seeking to enhance visitor engagement through interactive digital content.",
        "arxiv_id": "2507.13719",
        "ARXIVID": "2507.13719",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to computer vision applications in augmented reality and 3D reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13404": {
        "authors": [
            "Delin An",
            "Pan Du",
            "Jian-Xun Wang",
            "Chaoli Wang"
        ],
        "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation",
        "abstract": "arXiv:2507.13404v1 Announce Type: new  Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.",
        "arxiv_id": "2507.13404",
        "ARXIVID": "2507.13404",
        "COMMENT": "Does not match any specific criterion but is related to diffusion models for medical imaging, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13373": {
        "authors": [
            "Xiaojian Lin",
            "Wenxin Zhang",
            "Yuchu Jiang",
            "Wangyu Wu",
            "Yiran Guo",
            "Kangxu Wang",
            "Zongzheng Zhang",
            "Guijin Wang",
            "Lei Jin",
            "Hao Zhao"
        ],
        "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection",
        "abstract": "arXiv:2507.13373v1 Announce Type: new  Abstract: Hierarchical feature representations play a pivotal role in computer vision, particularly in object detection for autonomous driving. Multi-level semantic understanding is crucial for accurately identifying pedestrians, vehicles, and traffic signs in dynamic environments. However, existing architectures, such as YOLO and DETR, struggle to maintain feature consistency across different scales while balancing detection precision and computational efficiency. To address these challenges, we propose Butter, a novel object detection framework designed to enhance hierarchical feature representations for improving detection robustness. Specifically, Butter introduces two key innovations: Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which refines multi-scale feature consistency by leveraging adaptive frequency filtering to enhance structural and boundary precision, and Progressive Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively integrates multi-level features to mitigate semantic gaps and strengthen hierarchical feature learning. Through extensive experiments on BDD100K, KITTI, and Cityscapes, Butter demonstrates superior feature representation capabilities, leading to notable improvements in detection accuracy while reducing model complexity. By focusing on hierarchical feature refinement and integration, Butter provides an advanced approach to object detection that achieves a balance between accuracy, deployability, and computational efficiency in real-time autonomous driving scenarios. Our model and implementation are publicly available at https://github.com/Aveiro-Lin/Butter, facilitating further research and validation within the autonomous driving community.",
        "arxiv_id": "2507.13373",
        "ARXIVID": "2507.13373",
        "COMMENT": "Does not match any specific criterion but is generally relevant to hierarchical feature learning in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13386": {
        "authors": [
            "Yang Zhang",
            "Er Jin",
            "Yanfei Dong",
            "Yixuan Wu",
            "Philip Torr",
            "Ashkan Khakzar",
            "Johannes Stegmaier",
            "Kenji Kawaguchi"
        ],
        "title": "Minimalist Concept Erasure in Generative Models",
        "abstract": "arXiv:2507.13386v1 Announce Type: new  Abstract: Recent advances in generative models have demonstrated remarkable capabilities in producing high-quality images, but their reliance on large-scale unlabeled data has raised significant safety and copyright concerns. Efforts to address these issues by erasing unwanted concepts have shown promise. However, many existing erasure methods involve excessive modifications that compromise the overall utility of the model. In this work, we address these issues by formulating a novel minimalist concept erasure objective based \\emph{only} on the distributional distance of final generation outputs. Building on our formulation, we derive a tractable loss for differentiable optimization that leverages backpropagation through all generation steps in an end-to-end manner. We also conduct extensive analysis to show theoretical connections with other models and methods. To improve the robustness of the erasure, we incorporate neuron masking as an alternative to model fine-tuning. Empirical evaluations on state-of-the-art flow-matching models demonstrate that our method robustly erases concepts without degrading overall model performance, paving the way for safer and more responsible generative models.",
        "arxiv_id": "2507.13386",
        "ARXIVID": "2507.13386",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling and safety in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13599": {
        "authors": [
            "Chengxu Liu",
            "Lu Qi",
            "Jinshan Pan",
            "Xueming Qian",
            "Ming-Hsuan Yang"
        ],
        "title": "Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model",
        "abstract": "arXiv:2507.13599v1 Announce Type: new  Abstract: Since acquiring large amounts of realistic blurry-sharp image pairs is difficult and expensive, learning blind image deblurring from unpaired data is a more practical and promising solution. Unfortunately, dominant approaches rely heavily on adversarial learning to bridge the gap from blurry domains to sharp domains, ignoring the complex and unpredictable nature of real-world blur patterns. In this paper, we propose a novel diffusion model (DM)-based framework, dubbed \\ours, for image deblurring by learning spatially varying texture prior from unpaired data. In particular, \\ours performs DM to generate the prior knowledge that aids in recovering the textures of blurry images. To implement this, we propose a Texture Prior Encoder (TPE) that introduces a memory mechanism to represent the image textures and provides supervision for DM training. To fully exploit the generated texture priors, we present the Texture Transfer Transformer layer (TTformer), in which a novel Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes spatially varying blurring through adaptive filtering. Furthermore, we implement a wavelet-based adversarial loss to preserve high-frequency texture details. Extensive evaluations show that \\ours provides a promising unsupervised deblurring solution and outperforms SOTA methods in widely-used benchmarks.",
        "arxiv_id": "2507.13599",
        "ARXIVID": "2507.13599",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and image deblurring, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13779": {
        "authors": [
            "Durgesh Singh",
            "Ahc\\`ene Boubekki",
            "Robert Jenssen",
            "Michael Kampffmeyer"
        ],
        "title": "SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering",
        "abstract": "arXiv:2507.13779v1 Announce Type: new  Abstract: Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA) enhance the model performance by exploiting information from labeled and unlabeled data. The clustering assumption has proven advantageous for learning with limited supervision and states that data points belonging to the same cluster in a high-dimensional space should be assigned to the same category. Recent works have utilized different training mechanisms to implicitly enforce this assumption for the SSL and UDA. In this work, we take a different approach by explicitly involving a differentiable clustering module which is extended to leverage the supervised data to compute its centroids. We demonstrate the effectiveness of our straightforward end-to-end training strategy for SSL and UDA over extensive experiments and highlight its benefits, especially in low supervision regimes, both as a standalone model and as a regularizer for existing approaches.",
        "arxiv_id": "2507.13779",
        "ARXIVID": "2507.13779",
        "COMMENT": "Does not match any specific criterion but is relevant to semi-supervised learning and domain adaptation, which are tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13789": {
        "authors": [
            "Kyriakos Flouris",
            "Moritz Halter",
            "Yolanne Y. R. Lee",
            "Samuel Castonguay",
            "Luuk Jacobs",
            "Pietro Dirix",
            "Jonathan Nestmann",
            "Sebastian Kozerke",
            "Ender Konukoglu"
        ],
        "title": "Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI",
        "abstract": "arXiv:2507.13789v1 Announce Type: new  Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding treatment. While magnetic resonance flow imaging enables time-resolved volumetric blood velocity measurements, its low spatiotemporal resolution and signal-to-noise ratio limit its diagnostic utility. To address this, we propose the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that enhances both spatial and temporal resolution with the ability to predict wall shear stress (WSS) directly from clinical imaging data. LoFNO integrates Laplacian eigenvectors as geometric priors for improved structural awareness on irregular, unseen geometries and employs an Enhanced Deep Super-Resolution Network (EDSR) layer for robust upsampling. By combining geometric priors with neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow data, achieving superior velocity and WSS predictions compared to interpolation and alternative deep learning methods, enabling more precise cerebrovascular diagnostics.",
        "arxiv_id": "2507.13789",
        "ARXIVID": "2507.13789",
        "COMMENT": "Does not match any specific criterion but is relevant to spatiotemporal modeling in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13360": {
        "authors": [
            "Le-Anh Tran",
            "Chung Nguyen Tran",
            "Ngoc-Luu Nguyen",
            "Nhan Cach Dang",
            "Jordi Carrabina",
            "David Castells-Rufas",
            "Minh Son Nguyen"
        ],
        "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance",
        "abstract": "arXiv:2507.13360v1 Announce Type: new  Abstract: This paper introduces a novel deep learning framework for low-light image enhancement, named the Encoder-Decoder Network with Illumination Guidance (EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination map, derived from Bright Channel Prior (BCP), as a guidance input. This illumination guidance helps the network focus on underexposed regions, effectively steering the enhancement process. To further improve the model's representational power, a Spatial Pyramid Pooling (SPP) module is incorporated to extract multi-scale contextual features, enabling better handling of diverse lighting conditions. Additionally, the Swish activation function is employed to ensure smoother gradient propagation during training. EDNIG is optimized within a Generative Adversarial Network (GAN) framework using a composite loss function that combines adversarial loss, pixel-wise mean squared error (MSE), and perceptual loss. Experimental results show that EDNIG achieves competitive performance compared to state-of-the-art methods in quantitative metrics and visual quality, while maintaining lower model complexity, demonstrating its suitability for real-world applications. The source code for this work is available at https://github.com/tranleanh/ednig.",
        "arxiv_id": "2507.13360",
        "ARXIVID": "2507.13360",
        "COMMENT": "Does not match any specific criterion but is related to low-light image enhancement, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.13706": {
        "authors": [
            "\\'Angel F. Garc\\'ia-Fern\\'andez",
            "Jinhao Gu",
            "Lennart Svensson",
            "Yuxuan Xia",
            "Jan Krej\\v{c}\\'i",
            "Oliver Kost",
            "Ond\\v{r}ej Straka"
        ],
        "title": "GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms",
        "abstract": "arXiv:2507.13706v1 Announce Type: new  Abstract: This paper introduces two quasi-metrics for performance assessment of multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an extension of the generalised optimal subpattern assignment (GOSPA) metric and measures the discrepancy between sets of objects. The other quasi-metric is an extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy between sets of trajectories. Similar to the GOSPA-based metrics, these quasi-metrics include costs for localisation error for properly detected objects, the number of false objects and the number of missed objects. The T-GOSPA quasi-metric also includes a track switching cost. Differently from the GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of penalising missed and false objects with different costs, and the localisation costs are not required to be symmetric. These properties can be useful in MOT evaluation in certain applications. The performance of several Bayesian MOT algorithms is assessed with the T-GOSPA quasi-metric via simulations.",
        "arxiv_id": "2507.13706",
        "ARXIVID": "2507.13706",
        "COMMENT": "Does not match any specific criterion but is related to evaluation metrics for multi-object tracking, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.14013": {
        "authors": [
            "Ji-Yan Wu",
            "Zheng Yong Poh",
            "Anoop C. Patil",
            "Bongsoo Park",
            "Giovanni Volpe",
            "Daisuke Urano"
        ],
        "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
        "abstract": "arXiv:2507.14013v1 Announce Type: new  Abstract: Accurate detection of nutrient deficiency in plant leaves is essential for precision agriculture, enabling early intervention in fertilization, disease, and stress management. This study presents a deep learning framework for leaf anomaly segmentation using multispectral imaging and an enhanced YOLOv5 model with a transformer-based attention head. The model is tailored for processing nine-channel multispectral input and uses self-attention mechanisms to better capture subtle, spatially-distributed symptoms. The plants in the experiments were grown under controlled nutrient stress conditions for evaluation. We carry out extensive experiments to benchmark the proposed model against the baseline YOLOv5. Extensive experiments show that the proposed model significantly outperforms the baseline YOLOv5, with an average Dice score and IoU (Intersection over Union) improvement of about 12%. In particular, this model is effective in detecting challenging symptoms like chlorosis and pigment accumulation. These results highlight the promise of combining multi-spectral imaging with spectral-spatial feature learning for advancing plant phenotyping and precision agriculture.",
        "arxiv_id": "2507.14013",
        "ARXIVID": "2507.14013",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision applications in agriculture.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.14042": {
        "authors": [
            "Qiankun Ma",
            "Ziyao Zhang",
            "Chi Su",
            "Jie Chen",
            "Zhen Song",
            "Hairong Zheng",
            "Wen Gao"
        ],
        "title": "Training-free Token Reduction for Vision Mamba",
        "abstract": "arXiv:2507.14042v1 Announce Type: new  Abstract: Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs) due to its ability to efficiently capture long-range dependencies with linear computational complexity. While token reduction, an effective compression technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision Mamba's efficiency is essential for enabling broader applications. However, we find that directly applying existing token reduction techniques for ViTs to Vision Mamba leads to significant performance degradation. This is primarily because Mamba is a sequence model without attention mechanisms, whereas most token reduction techniques for ViTs rely on attention mechanisms for importance measurement and overlook the order of compressed tokens. In this paper, we investigate a Mamba structure-aware importance score to evaluate token importance in a simple and effective manner. Building on this score, we further propose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction framework. Without the need for training or additional tuning parameters, our method can be seamlessly integrated as a plug-and-play component across various Mamba models. Extensive experiments demonstrate that our approach significantly reduces computational workload while minimizing performance impact across various tasks and multiple backbones. Notably, MTR reduces FLOPs by approximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet performance without retraining.",
        "arxiv_id": "2507.14042",
        "ARXIVID": "2507.14042",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and efficiency in model design.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.13371": {
        "authors": [
            "Yeming Cai",
            "Yang Wang",
            "Zhenglin Li"
        ],
        "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation",
        "abstract": "arXiv:2507.13371v1 Announce Type: new  Abstract: This paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model to enhance medical rehabilitation. It tackles data noise and missing data caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety. Utilizing temporal sequence modeling, our framework denoises and completes motion capture data, improving robustness. Evaluations on stroke and orthopedic rehabilitation datasets show superior performance in data reconstruction and anomaly detection, providing a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.",
        "arxiv_id": "2507.13371",
        "ARXIVID": "2507.13371",
        "COMMENT": "Does not match any specific criterion but is relevant to motion capture and anomaly detection in medical rehabilitation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.13372": {
        "authors": [
            "Yeming Cai",
            "Zhenglin Li",
            "Yang Wang"
        ],
        "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks",
        "abstract": "arXiv:2507.13372v1 Announce Type: new  Abstract: Breast cancer is a leading cause of death among women globally, and early detection is critical for improving survival rates. This paper introduces an innovative framework that integrates Vision Transformers (ViT) and Graph Neural Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset. Our framework leverages ViT's ability to capture global image features and GNN's strength in modeling structural relationships, achieving an accuracy of 84.2%, outperforming traditional methods. Additionally, interpretable attention heatmaps provide insights into the model's decision-making process, aiding radiologists in clinical settings.",
        "arxiv_id": "2507.13372",
        "ARXIVID": "2507.13372",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}