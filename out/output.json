{
    "2506.20670": {
        "authors": [
            "Jinming Wu",
            "Zihao Deng",
            "Wei Li",
            "Yiding Liu",
            "Bo You",
            "Bo Li",
            "Zejun Ma",
            "Ziwei Liu"
        ],
        "title": "MMSearch-R1: Incentivizing LMMs to Search",
        "abstract": "arXiv:2506.20670v1 Announce Type: new  Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.",
        "arxiv_id": "2506.20670",
        "ARXIVID": "2506.20670",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel reinforcement learning framework for multimodal large language models (LMMs) with integration of image and text search tools.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.20590": {
        "authors": [
            "Chaojun Ni",
            "Jie Li",
            "Haoyun Li",
            "Hengyu Liu",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Guosheng Zhao",
            "Boyuan Wang",
            "Chenxin Li",
            "Guan Huang",
            "Wenjun Mei"
        ],
        "title": "WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration",
        "abstract": "arXiv:2506.20590v1 Announce Type: new  Abstract: Interactive 3D scene generation from a single image has gained significant attention due to its potential to create immersive virtual worlds. However, a key challenge in current 3D generation methods is the limited explorability, which cannot render high-quality images during larger maneuvers beyond the original viewpoint, particularly when attempting to move forward into unseen areas. To address this challenge, we propose WonderFree, the first model that enables users to interactively generate 3D worlds with the freedom to explore from arbitrary angles and directions. Specifically, we decouple this challenge into two key subproblems: novel view quality, which addresses visual artifacts and floating issues in novel views, and cross-view consistency, which ensures spatial consistency across different viewpoints. To enhance rendering quality in novel views, we introduce WorldRestorer, a data-driven video restoration model designed to eliminate floaters and artifacts. In addition, a data collection pipeline is presented to automatically gather training data for WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D scene generation. Furthermore, to improve cross-view consistency, we propose ConsistView, a multi-view joint restoration mechanism that simultaneously restores multiple perspectives while maintaining spatiotemporal coherence. Experimental results demonstrate that WonderFree not only enhances rendering quality across diverse viewpoints but also significantly improves global coherence and consistency. These improvements are confirmed by CLIP-based metrics and a user study showing a 77.20% preference for WonderFree over WonderWorld enabling a seamless and immersive 3D exploration experience. The code, model, and data will be publicly available.",
        "arxiv_id": "2506.20590",
        "ARXIVID": "2506.20590",
        "COMMENT": "Matches criteria 4 and 6 as it focuses on improving 3D scene exploration and rendering quality, which is relevant to video understanding and vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.20601": {
        "authors": [
            "Rui Huang",
            "Guangyao Zhai",
            "Zuria Bauer",
            "Marc Pollefeys",
            "Federico Tombari",
            "Leonidas Guibas",
            "Gao Huang",
            "Francis Engelmann"
        ],
        "title": "Video Perception Models for 3D Scene Synthesis",
        "abstract": "arXiv:2506.20601v1 Announce Type: new  Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant manual effort. Automating this process could greatly benefit fields such as architectural design, robotics simulation, virtual reality, and gaming. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors of modern image generation models. However, current LLMs demonstrate limited 3D spatial reasoning ability, which restricts their ability to generate realistic and coherent 3D scenes. Meanwhile, image generation-based methods often suffer from constraints in viewpoint selection and multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For more precise analysis, we further introduce First-Person View Score (FPVScore) for coherence and plausibility evaluation, utilizing continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios. The code will be released.",
        "arxiv_id": "2506.20601",
        "ARXIVID": "2506.20601",
        "COMMENT": "Matches criteria 1 and 3 as it focuses on 3D scene synthesis using video perception models, which involves spatial reasoning and embodied AI challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.20254": {
        "authors": [
            "Kun Yuan",
            "Tingxuan Chen",
            "Shi Li",
            "Joel L. Lavanchy",
            "Christian Heiliger",
            "Ege \\\"Ozsoy",
            "Yiming Huang",
            "Long Bai",
            "Nassir Navab",
            "Vinkle Srivastav",
            "Hongliang Ren",
            "Nicolas Padoy"
        ],
        "title": "Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement",
        "abstract": "arXiv:2506.20254v1 Announce Type: new  Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institution-specific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multi-modal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data. Code is available at https://github.com/CAMMA-public/SPA",
        "arxiv_id": "2506.20254",
        "ARXIVID": "2506.20254",
        "COMMENT": "Matches criteria 3 as it introduces a lightweight framework for surgical phase recognition, addressing challenges in embodied AI and task-specific adaptation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.20531": {
        "authors": [
            "Wenbin Gan",
            "Minh-Son Dao",
            "Koji Zettsu"
        ],
        "title": "Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios",
        "abstract": "arXiv:2506.20531v1 Announce Type: new  Abstract: Driving in safety-critical scenarios requires quick, context-aware decision-making grounded in both situational understanding and experiential reasoning. Large Language Models (LLMs), with their powerful general-purpose reasoning capabilities, offer a promising foundation for such decision-making. However, their direct application to autonomous driving remains limited due to challenges in domain adaptation, contextual grounding, and the lack of experiential knowledge needed to make reliable and interpretable decisions in dynamic, high-risk environments. To address this gap, this paper presents a Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for evasive maneuver decision-making in complex risk scenarios. Our approach integrates semantic scene understanding from dashcam video inputs with the retrieval of relevant past driving cases, enabling LLMs to generate maneuver recommendations that are both context-sensitive and human-aligned. Experiments across multiple open-source LLMs show that our framework improves decision accuracy, justification quality, and alignment with human expert behavior. Risk-aware prompting strategies further enhance performance across diverse risk types, while similarity-based case retrieval consistently outperforms random sampling in guiding in-context learning. Case studies further demonstrate the framework's robustness in challenging real-world conditions, underscoring its potential as an adaptive and trustworthy decision-support tool for intelligent driving systems.",
        "arxiv_id": "2506.20531",
        "ARXIVID": "2506.20531",
        "COMMENT": "Matches criteria 3 as it introduces a novel framework for decision-making in safety-critical driving scenarios, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.20388": {
        "authors": [
            "Shen Tan",
            "Xin Zhang",
            "Liangxiu Han",
            "Huaguo Huang",
            "Han Wang"
        ],
        "title": "A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management",
        "abstract": "arXiv:2506.20388v1 Announce Type: new  Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB) is crucial for supporting local livelihoods and carbon sequestration initiatives like the China Certified Emission Reduction (CCER) program. High-resolution canopy height maps (CHMs) are essential for this, but standard lidar-based methods are expensive. While deep learning with RGB imagery offers an alternative, accurately extracting canopy height features remains challenging. To address this, we developed a novel model for high-resolution CHM generation using a Large Vision Foundation Model (LVFM). Our model integrates a feature extractor, a self-supervised feature enhancement module to preserve spatial details, and a height estimator. Tested in Beijing's Fangshan District using 1-meter Google Earth imagery, our model outperformed existing methods, including conventional CNNs. It achieved a mean absolute error of 0.09 m, a root mean square error of 0.24 m, and a correlation of 0.78 against lidar-based CHMs. The resulting CHMs enabled over 90% success in individual tree detection, high accuracy in AGB estimation, and effective tracking of plantation growth, demonstrating strong generalization to non-training areas. This approach presents a promising, scalable tool for evaluating carbon sequestration in both plantations and natural forests.",
        "arxiv_id": "2506.20388",
        "ARXIVID": "2506.20388",
        "COMMENT": "This paper matches criterion 4 as it focuses on a Vision Foundation Model (LVFM) and its application to generating high-resolution canopy height maps for precision forestry management.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20279": {
        "authors": [
            "Changliang Xia",
            "Chengyou Jia",
            "Zhuohang Dang",
            "Minnan Luo"
        ],
        "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios",
        "abstract": "arXiv:2506.20279v1 Announce Type: new  Abstract: Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj",
        "arxiv_id": "2506.20279",
        "ARXIVID": "2506.20279",
        "COMMENT": "Matches criterion 4 as it focuses on dense prediction tasks and proposes a unified framework leveraging generative models' visual priors, which aligns with vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20168": {
        "authors": [
            "Zhentao He",
            "Can Zhang",
            "Ziheng Wu",
            "Zhenghao Chen",
            "Yufei Zhan",
            "Yifan Li",
            "Zhao Zhang",
            "Xian Wang",
            "Minghui Qiu"
        ],
        "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models",
        "abstract": "arXiv:2506.20168v1 Announce Type: new  Abstract: Recent advancements in multimodal large language models have enhanced document understanding by integrating textual and visual information. However, existing models exhibit incompleteness within their paradigm in real-world scenarios, particularly under visual degradation. In such conditions, the current response paradigm often fails to adequately perceive visual degradation and ambiguity, leading to overreliance on linguistic priors or misaligned visual-textual reasoning. This difficulty in recognizing uncertainty frequently results in the generation of hallucinatory content, especially when a precise answer is not feasible. To better demonstrate and analyze this phenomenon and problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR hallucination in degraded document understanding. This dataset includes test samples spanning identity cards and invoices, with simulated real-world degradations for OCR reliability. This setup allows for evaluating models' capacity, under degraded input, to distinguish reliable visual information and answer accordingly, thereby highlighting the challenge of avoiding hallucination on uncertain data. To achieve vision-faithful reasoning and thereby avoid the aforementioned issues, we further introduce a GRPO-based framework featuring a novel reward mechanism. By incorporating a self-awareness of visual uncertainty and an analysis method that initiates refusal to answer to increase task difficulty within our supervised fine-tuning and reinforcement learning framework, we successfully mitigated hallucinations in ambiguous regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model achieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA and there is no significant performance drop in standard tasks, highlighting both effectiveness and robustness.",
        "arxiv_id": "2506.20168",
        "ARXIVID": "2506.20168",
        "COMMENT": "Matches criterion 2 as it addresses multimodal large language models (MLLMs) and their challenges, specifically OCR hallucinations in degraded document understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20103": {
        "authors": [
            "Jiahao Lin",
            "Weixuan Peng",
            "Bojia Zi",
            "Yifeng Gao",
            "Xianbiao Qi",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos",
        "abstract": "arXiv:2506.20103v1 Announce Type: new  Abstract: Recent advances in deep generative models have led to significant progress in video generation, yet the fidelity of AI-generated videos remains limited. Synthesized content often exhibits visual artifacts such as temporally inconsistent motion, physically implausible trajectories, unnatural object deformations, and local blurring that undermine realism and user trust. Accurate detection and spatial localization of these artifacts are crucial for both automated quality control and for guiding the development of improved generative models. However, the research community currently lacks a comprehensive benchmark specifically designed for artifact localization in AI generated videos. Existing datasets either restrict themselves to video or frame level detection or lack the fine-grained spatial annotations necessary for evaluating localization methods. To address this gap, we introduce BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with meticulously annotated, pixel-level masks highlighting regions of visual corruption. Each annotation is validated through detailed human inspection to ensure high quality ground truth. Our experiments show that training state of the art artifact detection models and multi modal large language models (MLLMs) on BrokenVideos significantly improves their ability to localize corrupted regions. Through extensive evaluation, we demonstrate that BrokenVideos establishes a critical foundation for benchmarking and advancing research on artifact localization in generative video models. The dataset is available at: https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.",
        "arxiv_id": "2506.20103",
        "ARXIVID": "2506.20103",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark dataset (BrokenVideos) for artifact localization in AI-generated videos, which is relevant to video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20664": {
        "authors": [
            "Andrei Lupu",
            "Timon Willi",
            "Jakob Foerster"
        ],
        "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
        "abstract": "arXiv:2506.20664v1 Announce Type: new  Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the \"mental\" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.   We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents.",
        "arxiv_id": "2506.20664",
        "ARXIVID": "2506.20664",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Decrypto) for multi-agent reasoning and theory of mind, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.20294": {
        "authors": [
            "Shunqi Mao",
            "Wei Guo",
            "Chaoyi Zhang",
            "Weidong Cai"
        ],
        "title": "Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations",
        "abstract": "arXiv:2506.20294v1 Announce Type: new  Abstract: Diffusion models have shown strong performance in conditional generation by progressively denoising Gaussian noise toward a target data distribution. This denoising process can be interpreted as a form of hill climbing in a learned latent space, where the model iteratively refines the sample toward regions of higher probability. However, diffusion models often converge to local optima that are locally visually coherent yet globally inconsistent or conditionally misaligned, due to latent space complexity and suboptimal initialization. Prior efforts attempted to address this by strengthening guidance signals or manipulating the initial noise distribution. We introduce Controlled Random Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect and escape such local maxima during conditional generation. The method first identifies potential local maxima using a reward model. Upon detection, it injects noise and reverts to a previous, noisier state to escape the current optimization plateau. The reward model then evaluates candidate trajectories, accepting only those that offer improvement, while progressively deeper retreat enables stronger escapes when nearby alternatives fail. This controlled random zigzag process allows dynamic alternation between forward refinement and backward exploration, enhancing both alignment and visual quality in the generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and compatible with existing diffusion frameworks. Experimental results show that Ctrl-Z Sampling substantially improves generation quality with only around 7.6X increase in function evaluations.",
        "arxiv_id": "2506.20294",
        "ARXIVID": "2506.20294",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of generative modeling and diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.20151": {
        "authors": [
            "Haipeng Fan",
            "Shiyuan Zhang",
            "Baohunesitu",
            "Zihang Guo",
            "Huaiwen Zhang"
        ],
        "title": "EAR: Erasing Concepts from Unified Autoregressive Models",
        "abstract": "arXiv:2506.20151v1 Announce Type: new  Abstract: Autoregressive (AR) models have achieved unified and strong performance across both visual understanding and image generation tasks. However, removing undesired concepts from AR models while maintaining overall generation quality remains an open challenge. In this paper, we propose Erasure Autoregressive Model (EAR), a fine-tuning method for effective and utility-preserving concept erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation (WGA) strategy to align patch-level decoding with erasure objectives, and Thresholded Loss Masking (TLM) strategy to protect content unrelated to the target concept during fine-tuning. Furthermore, we propose a novel benchmark, Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more rigorous and comprehensive foundation for evaluating concept erasure in AR models. Specifically, we first employ structured templates across diverse large language models (LLMs) to pre-generate a large-scale corpus of target-replacement concept prompt pairs. Subsequently, we generate images from these prompts and subject them to rigorous filtering via a visual classifier to ensure concept fidelity and alignment. Extensive experimental results conducted on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR achieves marked improvements in both erasure effectiveness and model utility preservation. Code is available at: https://github.com/immc-lab/ear/",
        "arxiv_id": "2506.20151",
        "ARXIVID": "2506.20151",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of generative modeling and vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.20563": {
        "authors": [
            "Lei Zhu",
            "Jun Zhou",
            "Rick Siow Mong Goh",
            "Yong Liu"
        ],
        "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation",
        "abstract": "arXiv:2506.20563v1 Announce Type: new  Abstract: Vision Transformer has recently gained tremendous popularity in medical image segmentation task due to its superior capability in capturing long-range dependencies. However, transformer requires a large amount of labeled data to be effective, which hinders its applicability in annotation scarce semi-supervised learning scenario where only limited labeled data is available. State-of-the-art semi-supervised learning methods propose combinatorial CNN-Transformer learning to cross teach a transformer with a convolutional neural network, which achieves promising results. However, it remains a challenging task to effectively train the transformer with limited labeled data. In this paper, we propose an adversarial masked image modeling method to fully unleash the potential of transformer for semi-supervised medical image segmentation. The key challenge in semi-supervised learning with transformer lies in the lack of sufficient supervision signal. To this end, we propose to construct an auxiliary masked domain from original domain with masked image modeling and train the transformer to predict the entire segmentation mask with masked inputs to increase supervision signal. We leverage the original labels from labeled data and pseudo-labels from unlabeled data to learn the masked domain. To further benefit the original domain from masked domain, we provide a theoretical analysis of our method from a multi-domain learning perspective and devise a novel adversarial training loss to reduce the domain gap between the original and masked domain, which boosts semi-supervised learning performance. We also extend adversarial masked image modeling to CNN network. Extensive experiments on three public medical image segmentation datasets demonstrate the effectiveness of our method, where our method outperforms existing methods significantly. Our code is publicly available at https://github.com/zlheui/AdvMIM.",
        "arxiv_id": "2506.20563",
        "ARXIVID": "2506.20563",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.19955": {
        "authors": [
            "Yiming Ma",
            "Victor Sanchez",
            "Tanaya Guha"
        ],
        "title": "EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression",
        "abstract": "arXiv:2506.19955v1 Announce Type: new  Abstract: Density map estimation has become the mainstream paradigm in crowd counting. However, most existing methods overlook the extreme sparsity of ground-truth density maps. In real-world crowd scenes, the vast majority of spatial regions (often over 95%) contain no people, leading to heavily imbalanced count distributions. Ignoring this imbalance can bias models toward overestimating dense regions and underperforming in sparse areas. Furthermore, most loss functions used in density estimation are majorly based on MSE and implicitly assume Gaussian distributions, which are ill-suited for modeling discrete, non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting framework that models the spatial distribution of counts using a Zero-Inflated Poisson (ZIP) regression formulation. Our approach replaces the traditional regression loss with the negative log-likelihood of the ZIP distribution, enabling better handling of zero-heavy distributions while preserving count accuracy. Built upon the recently proposed Enhanced Block Classification (EBC) framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of targets and ensuring training stability, while further improving performance through a more principled probabilistic loss. We also evaluate EBC-ZIP with backbones of varying computational complexity to assess its scalability. Extensive experiments on four crowd counting benchmarks demonstrate that EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.",
        "arxiv_id": "2506.19955",
        "ARXIVID": "2506.19955",
        "COMMENT": "This paper does not closely match any of the specific criteria. It focuses on crowd counting using a novel probabilistic loss function, which is not directly related to spatial intelligence, embodied agents, vision-language models, or video understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20608": {
        "authors": [
            "Barry Smith",
            "Junchao Zhang",
            "Hong Zhang",
            "Lois Curfman McInnes",
            "Murat Keceli",
            "Archit Vasan",
            "Satish Balay",
            "Toby Isaac",
            "Le Chen",
            "Venkatram Vishwanath"
        ],
        "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
        "abstract": "arXiv:2506.20608v1 Announce Type: new  Abstract: Generative AI, especially through large language models (LLMs), is transforming how technical knowledge can be accessed, reused, and extended. PETSc, a widely used numerical library for high-performance scientific computing, has accumulated a rich but fragmented knowledge base over its three decades of development, spanning source code, documentation, mailing lists, GitLab issues, Discord conversations, technical papers, and more. Much of this knowledge remains informal and inaccessible to users and new developers. To activate and utilize this knowledge base more effectively, the PETSc team has begun building an LLM-powered system that combines PETSc content with custom LLM tools -- including retrieval-augmented generation (RAG), reranking algorithms, and chatbots -- to assist users, support developers, and propose updates to formal documentation. This paper presents initial experiences designing and evaluating these tools, focusing on system architecture, using RAG and reranking for PETSc-specific information, evaluation methodologies for various LLMs and embedding models, and user interface design. Leveraging the Argonne Leadership Computing Facility resources, we analyze how LLM responses can enhance the development and use of numerical software, with an initial focus on scalable Krylov solvers. Our goal is to establish an extensible framework for knowledge-centered AI in scientific software, enabling scalable support, enriched documentation, and enhanced workflows for research and development. We conclude by outlining directions for expanding this system into a robust, evolving platform that advances software ecosystems to accelerate scientific discovery.",
        "arxiv_id": "2506.20608",
        "ARXIVID": "2506.20608",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of AI and knowledge systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20600": {
        "authors": [
            "Wengxi Li",
            "Roy Pea",
            "Nick Haber",
            "Hari Subramonyam"
        ],
        "title": "CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video",
        "abstract": "arXiv:2506.20600v1 Announce Type: new  Abstract: We introduce CogGen, a learner-centered AI architecture that transforms programming videos into interactive, adaptive learning experiences by integrating student modeling with generative AI tutoring based on the Cognitive Apprenticeship framework. The architecture consists of three components: (1) video segmentation by learning goals, (2) a conversational tutoring engine applying Cognitive Apprenticeship strategies, and (3) a student model using Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation demonstrates effective video segmentation accuracy and strong pedagogical alignment across knowledge, method, action, and interaction layers. Ablation studies confirm the necessity of each component in generating effective guidance. This work advances AI-powered tutoring by bridging structured student modeling with interactive AI conversations, offering a scalable approach to enhancing video-based programming education.",
        "arxiv_id": "2506.20600",
        "ARXIVID": "2506.20600",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of AI and education.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20381": {
        "authors": [
            "Ben Kang",
            "Xin Chen",
            "Jie Zhao",
            "Chunjuan Bo",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "title": "Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking",
        "abstract": "arXiv:2506.20381v1 Announce Type: new  Abstract: Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark, outperforming all previous efficient trackers.Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9% on the LaSOT.",
        "arxiv_id": "2506.20381",
        "ARXIVID": "2506.20381",
        "COMMENT": "Does not match any specific criteria but focuses on efficient visual tracking using transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20616": {
        "authors": [
            "Quoc-Duy Tran",
            "Anh-Tuan Vo",
            "Dinh-Khoi Vo",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "Shape2Animal: Creative Animal Generation from Natural Silhouettes",
        "abstract": "arXiv:2506.20616v1 Announce Type: new  Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous stimuli, a cognitive phenomenon known as pareidolia. This paper introduces Shape2Animal framework to mimics this imaginative capacity by reinterpreting natural object silhouettes, such as clouds, stones, or flames, as plausible animal forms. Our automated framework first performs open-vocabulary segmentation to extract object silhouette and interprets semantically appropriate animal concepts using vision-language models. It then synthesizes an animal image that conforms to the input shape, leveraging text-to-image diffusion model and seamlessly blends it into the original scene to generate visually coherent and spatially consistent compositions. We evaluated Shape2Animal on a diverse set of real-world inputs, demonstrating its robustness and creative potential. Our Shape2Animal can offer new opportunities for visual storytelling, educational content, digital art, and interactive media design. Our project page is here: https://shape2image.github.io",
        "arxiv_id": "2506.20616",
        "ARXIVID": "2506.20616",
        "COMMENT": "Does not match any specific criteria but is related to creative image generation using vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20582": {
        "authors": [
            "Rajat Rasal",
            "Avinash Kori",
            "Ben Glocker"
        ],
        "title": "Causal Representation Learning with Observational Grouping for CXR Classification",
        "abstract": "arXiv:2506.20582v1 Announce Type: new  Abstract: Identifiable causal representation learning seeks to uncover the true causal relationships underlying a data generation process. In medical imaging, this presents opportunities to improve the generalisability and robustness of task-specific latent features. This work introduces the concept of grouping observations to learn identifiable representations for disease classification in chest X-rays via an end-to-end framework. Our experiments demonstrate that these causal representations improve generalisability and robustness across multiple classification tasks when grouping is used to enforce invariance w.r.t race, sex, and imaging views.",
        "arxiv_id": "2506.20582",
        "ARXIVID": "2506.20582",
        "COMMENT": "Does not match any specific criteria but is related to causal representation learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.20222": {
        "authors": [
            "Pujing Yang",
            "Guangyi Zhang",
            "Yunlong Cai",
            "Lei Yu",
            "Guanding Yu"
        ],
        "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission",
        "abstract": "arXiv:2506.20222v1 Announce Type: new  Abstract: Event cameras asynchronously capture pixel-level intensity changes with extremely low latency. They are increasingly used in conjunction with RGB cameras for a wide range of vision-related applications. However, a major challenge in these hybrid systems lies in the transmission of the large volume of triggered events and RGB images. To address this, we propose a transmission scheme that retains efficient reconstruction performance of both sources while accomplishing real-time deblurring in parallel. Conventional RGB cameras and event cameras typically capture the same scene in different ways, often resulting in significant redundant information across their outputs. To address this, we develop a joint event and image (E-I) transmission framework to eliminate redundancy and thereby optimize channel bandwidth utilization. Our approach employs Bayesian modeling and the information bottleneck method to disentangle the shared and domain-specific information within the E-I inputs. This disentangled information bottleneck framework ensures both the compactness and informativeness of extracted shared and domain-specific information. Moreover, it adaptively allocates transmission bandwidth based on scene dynamics, i.e., more symbols are allocated to events for dynamic details or to images for static information. Simulation results demonstrate that the proposed scheme not only achieves superior reconstruction quality compared to conventional systems but also delivers enhanced deblurring performance.",
        "arxiv_id": "2506.20222",
        "ARXIVID": "2506.20222",
        "COMMENT": "Does not match any specific criteria but is related to event and RGB camera data transmission optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.20179": {
        "authors": [
            "Enzhe Zhao",
            "Zhichang Guo",
            "Yao Li",
            "Fanghui Song",
            "Boying Wu"
        ],
        "title": "Progressive Alignment Degradation Learning for Pansharpening",
        "abstract": "arXiv:2506.20179v1 Announce Type: new  Abstract: Deep learning-based pansharpening has been shown to effectively generate high-resolution multispectral (HRMS) images. To create supervised ground-truth HRMS images, synthetic data generated using the Wald protocol is commonly employed. This protocol assumes that networks trained on artificial low-resolution data will perform equally well on high-resolution data. However, well-trained models typically exhibit a trade-off in performance between reduced-resolution and full-resolution datasets. In this paper, we delve into the Wald protocol and find that its inaccurate approximation of real-world degradation patterns limits the generalization of deep pansharpening models. To address this issue, we propose the Progressive Alignment Degradation Module (PADM), which uses mutual iteration between two sub-networks, PAlignNet and PDegradeNet, to adaptively learn accurate degradation processes without relying on predefined operators. Building on this, we introduce HFreqdiff, which embeds high-frequency details into a diffusion framework and incorporates CFB and BACM modules for frequency-selective detail extraction and precise reverse process learning. These innovations enable effective integration of high-resolution panchromatic and multispectral images, significantly enhancing spatial sharpness and quality. Experiments and ablation studies demonstrate the proposed method's superior performance compared to state-of-the-art techniques.",
        "arxiv_id": "2506.20179",
        "ARXIVID": "2506.20179",
        "COMMENT": "Does not match any specific criteria but focuses on pansharpening and image fusion techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.20302": {
        "authors": [
            "Abbas Anwar",
            "Mohammad Shullar",
            "Ali Arshad Nasir",
            "Mudassir Masood",
            "Saeed Anwar"
        ],
        "title": "TDiR: Transformer based Diffusion for Image Restoration Tasks",
        "abstract": "arXiv:2506.20302v1 Announce Type: new  Abstract: Images captured in challenging environments often experience various forms of degradation, including noise, color cast, blur, and light scattering. These effects significantly reduce image quality, hindering their applicability in downstream tasks such as object detection, mapping, and classification. Our transformer-based diffusion model was developed to address image restoration tasks, aiming to improve the quality of degraded images. This model was evaluated against existing deep learning methodologies across multiple quality metrics for underwater image enhancement, denoising, and deraining on publicly available datasets. Our findings demonstrate that the diffusion model, combined with transformers, surpasses current methods in performance. The results of our model highlight the efficacy of diffusion models and transformers in improving the quality of degraded images, consequently expanding their utility in downstream tasks that require high-fidelity visual data.",
        "arxiv_id": "2506.20302",
        "ARXIVID": "2506.20302",
        "COMMENT": "Does not match any specific criteria but is related to image restoration using transformers and diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}