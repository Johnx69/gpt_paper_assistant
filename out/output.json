{
    "2507.13152": {
        "authors": [
            "Xiangyu Dong",
            "Haoran Zhao",
            "Jiang Gao",
            "Haozhou Li",
            "Xiaoguang Ma",
            "Yaoming Zhou",
            "Fuhai Chen",
            "Juan Liu"
        ],
        "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
        "abstract": "arXiv:2507.13152v1 Announce Type: new  Abstract: Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.",
        "arxiv_id": "2507.13152",
        "ARXIVID": "2507.13152",
        "COMMENT": "Matches criterion 1 and 2 as it introduces a self-evolving vision-language navigation framework based on multimodal large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.12821": {
        "authors": [
            "Lance Ying",
            "Katherine M. Collins",
            "Prafull Sharma",
            "Cedric Colas",
            "Kaiya Ivy Zhao",
            "Adrian Weller",
            "Zenna Tavares",
            "Phillip Isola",
            "Samuel J. Gershman",
            "Jacob D. Andreas",
            "Thomas L. Griffiths",
            "Francois Chollet",
            "Kelsey R. Allen",
            "Joshua B. Tenenbaum"
        ],
        "title": "Assessing adaptive world models in machines with novel games",
        "abstract": "arXiv:2507.12821v1 Announce Type: new  Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on a massive corpora of data, instead of the efficiency and efficacy of models in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this kind of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of the human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.",
        "arxiv_id": "2507.12821",
        "ARXIVID": "2507.12821",
        "COMMENT": "Matches criterion 3 as it proposes a new benchmarking paradigm for adaptive world models in novel environments, relevant to embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.12916": {
        "authors": [
            "Yifan Xu",
            "Chao Zhang",
            "Hanqi Jiang",
            "Xiaoyan Wang",
            "Ruifei Ma",
            "Yiwei Li",
            "Zihao Wu",
            "Zeju Li",
            "Xiangde Liu"
        ],
        "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models",
        "abstract": "arXiv:2507.12916v1 Announce Type: new  Abstract: Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.",
        "arxiv_id": "2507.12916",
        "ARXIVID": "2507.12916",
        "COMMENT": "Matches criteria 2 and 5 as it integrates 2D multi-view images and 3D point clouds with large language models for enhanced 3D scene understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.12816": {
        "authors": [
            "Ju-Young Oh",
            "Ho-Joong Kim",
            "Seong-Whan Lee"
        ],
        "title": "FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering",
        "abstract": "arXiv:2507.12816v1 Announce Type: new  Abstract: Video question answering (VQA) is a multimodal task that requires the interpretation of a video to answer a given question. Existing VQA methods primarily utilize question and answer (Q&A) pairs to learn the spatio-temporal characteristics of video content. However, these annotations are typically event-centric, which is not enough to capture the broader context of each video. The absence of essential details such as object types, spatial layouts, and descriptive attributes restricts the model to learning only a fragmented scene representation. This issue limits the model's capacity for generalization and higher-level reasoning. In this paper, we propose a fundamental question generation with the integration of question embeddings for video question answering (FIQ), a novel approach designed to strengthen the reasoning ability of the model by enhancing the fundamental understanding of videos. FIQ generates Q&A pairs based on descriptions extracted from videos, enriching the training data with fundamental scene information. Generated Q&A pairs enable the model to understand the primary context, leading to enhanced generalizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign module that assists task-specific question embeddings with visual features, ensuring that essential domain-specific details are preserved to increase the adaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate that our FIQ achieves state-of-the-art performance compared to existing baseline methods.",
        "arxiv_id": "2507.12816",
        "ARXIVID": "2507.12816",
        "COMMENT": "Matches criterion 6 as it focuses on video question answering, a video understanding task, with novel methodologies.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.12566": {
        "authors": [
            "Gen Luo",
            "Wenhan Dou",
            "Wenhao Li",
            "Zhaokai Wang",
            "Xue Yang",
            "Changyao Tian",
            "Hao Li",
            "Weiyun Wang",
            "Wenhai Wang",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models",
        "abstract": "arXiv:2507.12566v1 Announce Type: new  Abstract: This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.",
        "arxiv_id": "2507.12566",
        "ARXIVID": "2507.12566",
        "COMMENT": "Matches criteria 2 as it explores monolithic multimodal large language models with novel architectures and training strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12819": {
        "authors": [
            "Jeong-Woo Park",
            "Seong-Whan Lee"
        ],
        "title": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval",
        "abstract": "arXiv:2507.12819v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a gallery using a composed query consisting of a reference image and a modification text. Among various CIR approaches, training-free zero-shot methods based on pre-trained models are cost-effective but still face notable limitations. For example, sequential VLM-LLM pipelines process each modality independently, which often results in information loss and limits cross-modal interaction. In contrast, methods based on multimodal large language models (MLLMs) often focus exclusively on applying changes indicated by the text, without fully utilizing the contextual visual information from the reference image. To address these issues, we propose multi-faceted Chain-of-Thought with re-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes multi-faceted Chain-of-Thought to guide the MLLM to balance explicit modifications and contextual visual cues, generating two distinct captions: one focused on modification and the other integrating comprehensive visual-textual context. The first caption is used to filter candidate images. Subsequently, we combine these two captions and the reference image to perform multi-grained re-ranking. This two-stage approach facilitates precise retrieval by aligning with the textual modification instructions while preserving the visual context of the reference image. Through extensive experiments, MCoT-RE achieves state-of-the-art results among training-free methods, yielding improvements of up to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.",
        "arxiv_id": "2507.12819",
        "ARXIVID": "2507.12819",
        "COMMENT": "Matches criteria 5 (Integration of Image/Video and Large Language Models) as it proposes a framework combining image retrieval and multimodal LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13344": {
        "authors": [
            "Yudong Jin",
            "Sida Peng",
            "Xuan Wang",
            "Tao Xie",
            "Zhen Xu",
            "Yifan Yang",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
        ],
        "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models",
        "abstract": "arXiv:2507.13344v1 Announce Type: new  Abstract: This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ .",
        "arxiv_id": "2507.13344",
        "ARXIVID": "2507.13344",
        "COMMENT": "Matches criteria 6 (Video Understanding) due to its focus on novel-view video synthesis and spatio-temporal consistency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12823": {
        "authors": [
            "Jeong-Woo Park",
            "Young-Eun Kim",
            "Seong-Whan Lee"
        ],
        "title": "FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and Adaptive Reconciliation for Composed Image Retrieval",
        "abstract": "arXiv:2507.12823v1 Announce Type: new  Abstract: Composed image retrieval (CIR) is a vision language task that retrieves a target image using a reference image and modification text, enabling intuitive specification of desired changes. While effectively fusing visual and textual modalities is crucial, existing methods typically adopt either early or late fusion. Early fusion tends to excessively focus on explicitly mentioned textual details and neglect visual context, whereas late fusion struggles to capture fine-grained semantic alignments between image regions and textual tokens. To address these issues, we propose FAR-Net, a multi-stage fusion framework designed with enhanced semantic alignment and adaptive reconciliation, integrating two complementary modules. The enhanced semantic alignment module (ESAM) employs late fusion with cross-attention to capture fine-grained semantic relationships, while the adaptive reconciliation module (ARM) applies early fusion with uncertainty embeddings to enhance robustness and adaptability. Experiments on CIRR and FashionIQ show consistent performance gains, improving Recall@1 by up to 2.4% and Recall@50 by 1.04% over existing state-of-the-art methods, empirically demonstrating that FAR Net provides a robust and scalable solution to CIR tasks.",
        "arxiv_id": "2507.12823",
        "ARXIVID": "2507.12823",
        "COMMENT": "Matches criterion 5 as it focuses on composed image retrieval, integrating image understanding and textual modifications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13347": {
        "authors": [
            "Yifan Wang",
            "Jianjun Zhou",
            "Haoyi Zhu",
            "Wenzheng Chang",
            "Yang Zhou",
            "Zizun Li",
            "Junyi Chen",
            "Jiangmiao Pang",
            "Chunhua Shen",
            "Tong He"
        ],
        "title": "$\\pi^3$: Scalable Permutation-Equivariant Visual Geometry Learning",
        "abstract": "arXiv:2507.13347v1 Announce Type: new  Abstract: We introduce $\\pi^3$, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, $\\pi^3$ employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.",
        "arxiv_id": "2507.13347",
        "ARXIVID": "2507.13347",
        "COMMENT": "Matches criterion 1 as it presents a novel methodological improvement in spatial reasoning for visual geometry reconstruction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13145": {
        "authors": [
            "Maulana Bisyir Azhari",
            "David Hyunchul Shim"
        ],
        "title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model",
        "abstract": "arXiv:2507.13145v1 Announce Type: new  Abstract: Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2's coarse features. Furthermore, we complement DINOv2's robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.",
        "arxiv_id": "2507.13145",
        "ARXIVID": "2507.13145",
        "COMMENT": "Matches criterion 3 as it introduces DINO-VO, a feature-based visual odometry system leveraging a visual foundation model, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.13353": {
        "authors": [
            "Shihao Wang",
            "Guo Chen",
            "De-an Huang",
            "Zhiqi Li",
            "Minghan Li",
            "Guilin Li",
            "Jose M. Alvarez",
            "Lei Zhang",
            "Zhiding Yu"
        ],
        "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
        "abstract": "arXiv:2507.13353v1 Announce Type: new  Abstract: Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.",
        "arxiv_id": "2507.13353",
        "ARXIVID": "2507.13353",
        "COMMENT": "Matches criterion 6 as it introduces VideoITG for multimodal video understanding with instructed temporal grounding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12795": {
        "authors": [
            "Penglei Sun",
            "Yaoxian Song",
            "Xiangru Zhu",
            "Xiang Liu",
            "Qiang Wang",
            "Yue Liu",
            "Changqun Xia",
            "Tiefeng Li",
            "Yang Yang",
            "Xiaowen Chu"
        ],
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "abstract": "arXiv:2507.12795v1 Announce Type: new  Abstract: Scene understanding enables intelligent agents to interpret and comprehend their environment. While existing large vision-language models (LVLMs) for scene understanding have primarily focused on indoor household tasks, they face two significant limitations when applied to outdoor large-scale scene understanding. First, outdoor scenarios typically encompass larger-scale environments observed through various sensors from multiple viewpoints (e.g., bird view and terrestrial view), while existing indoor LVLMs mainly analyze single visual modalities within building-scale contexts from humanoid viewpoints. Second, existing LVLMs suffer from missing multidomain perception outdoor data and struggle to effectively integrate 2D and 3D visual information. To address the aforementioned limitations, we build the first multidomain perception outdoor scene understanding dataset, named \\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale scenarios with multi\\textbf{\\underline{V}}iew and multi\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k images and $4, 811$M point clouds with $567$k question-answering pairs from vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To effectively fuse the multimodal data in the absence of one modality, we introduce incomplete multimodal learning to model outdoor scene understanding and design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is realized by constructing a joint probabilistic distribution space rather than implementing directly explicit fusion operations (e.g., concatenation). Experimental results on three typical outdoor scene understanding tasks show City-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in question-answering tasks averagely. Our method demonstrates pragmatic and generalization performance across multiple outdoor scenes.",
        "arxiv_id": "2507.12795",
        "ARXIVID": "2507.12795",
        "COMMENT": "Matches criterion 5 as it introduces City-VLM, a model integrating 2D and 3D visual information for outdoor scene understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12952": {
        "authors": [
            "Jiaxiu Jiang",
            "Wenbo Li",
            "Jingjing Ren",
            "Yuping Qiu",
            "Yong Guo",
            "Xiaogang Xu",
            "Han Wu",
            "Wangmeng Zuo"
        ],
        "title": "LoViC: Efficient Long Video Generation with Context Compression",
        "abstract": "arXiv:2507.12952v1 Announce Type: new  Abstract: Despite recent advances in diffusion transformers (DiTs) for text-to-video generation, scaling to long-duration content remains challenging due to the quadratic complexity of self-attention. While prior efforts -- such as sparse attention and temporally autoregressive models -- offer partial relief, they often compromise temporal coherence or scalability. We introduce LoViC, a DiT-based framework trained on million-scale open-domain videos, designed to produce long, coherent videos through a segment-wise generation process. At the core of our approach is FlexFormer, an expressive autoencoder that jointly compresses video and text into unified latent representations. It supports variable-length inputs with linearly adjustable compression rates, enabled by a single query token design based on the Q-Former architecture. Additionally, by encoding temporal context through position-aware mechanisms, our model seamlessly supports prediction, retradiction, interpolation, and multi-shot generation within a unified paradigm. Extensive experiments across diverse tasks validate the effectiveness and versatility of our approach.",
        "arxiv_id": "2507.12952",
        "ARXIVID": "2507.12952",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding through long video generation and introduces a novel framework (LoViC) and methodology.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2507.12832": {
        "authors": [
            "Yuki Kondo",
            "Norimichi Ukita",
            "Riku Kanayama",
            "Yuki Yoshida",
            "Takayuki Yamaguchi",
            "Xiang Yu",
            "Guang Liang",
            "Xinyao Liu",
            "Guan-Zhang Wang",
            "Wei-Ta Chu",
            "Bing-Cheng Chuang",
            "Jia-Hua Lee",
            "Pin-Tseng Kuo",
            "I-Hsuan Chu",
            "Yi-Shein Hsiao",
            "Cheng-Han Wu",
            "Po-Yi Wu",
            "Jui-Chien Tsou",
            "Hsuan-Chi Liu",
            "Chun-Yi Lee",
            "Yuan-Fu Yang",
            "Kosuke Shigematsu",
            "Asuka Shin",
            "Ba Tran"
        ],
        "title": "MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results",
        "abstract": "arXiv:2507.12832v1 Announce Type: new  Abstract: Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.",
        "arxiv_id": "2507.12832",
        "ARXIVID": "2507.12832",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (SMOT4SB dataset) and metric (SO-HOTA) for embodied/robotic AI in UAV scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.12883": {
        "authors": [
            "Weihuang Lin",
            "Yiwei Ma",
            "Xiaoshuai Sun",
            "Shuting He",
            "Jiayi Ji",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "HRSeg: High-Resolution Visual Perception and Enhancement for Reasoning Segmentation",
        "abstract": "arXiv:2507.12883v1 Announce Type: new  Abstract: The reasoning segmentation task involves segmenting objects within an image by interpreting implicit user instructions, which may encompass subtleties such as contextual cues and open-world knowledge. Despite significant advancements made by existing approaches, they remain constrained by low perceptual resolution, as visual encoders are typically pre-trained at lower resolutions. Furthermore, simply interpolating the positional embeddings of visual encoders to enhance perceptual resolution yields only marginal performance improvements while incurring substantial computational costs. To address this, we propose HRSeg, an efficient model with high-resolution fine-grained perception. It features two key innovations: High-Resolution Perception (HRP) and High-Resolution Enhancement (HRE). The HRP module processes high-resolution images through cropping, integrating local and global features for multi-granularity quality. The HRE module enhances mask features by integrating fine-grained information from high-resolution images, refining their alignment with text features for precise segmentation. Extensive ablation studies validate the effectiveness of our modules, while comprehensive experiments on multiple benchmark datasets demonstrate HRSeg's superior performance.",
        "arxiv_id": "2507.12883",
        "ARXIVID": "2507.12883",
        "COMMENT": "Matches criteria 6 (Video Understanding) due to its focus on reasoning segmentation and high-resolution visual perception.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.12739": {
        "authors": [
            "Ijazul Haq",
            "Muhammad Saqib",
            "Yingjie Zhang"
        ],
        "title": "Transformer-based Spatial Grounding: A Comprehensive Survey",
        "abstract": "arXiv:2507.12739v1 Announce Type: new  Abstract: Spatial grounding, the process of associating natural language expressions with corresponding image regions, has rapidly advanced due to the introduction of transformer-based models, significantly enhancing multimodal representation and cross-modal alignment. Despite this progress, the field lacks a comprehensive synthesis of current methodologies, dataset usage, evaluation metrics, and industrial applicability. This paper presents a systematic literature review of transformer-based spatial grounding approaches from 2018 to 2025. Our analysis identifies dominant model architectures, prevalent datasets, and widely adopted evaluation metrics, alongside highlighting key methodological trends and best practices. This study provides essential insights and structured guidance for researchers and practitioners, facilitating the development of robust, reliable, and industry-ready transformer-based spatial grounding models.",
        "arxiv_id": "2507.12739",
        "ARXIVID": "2507.12739",
        "COMMENT": "Matches criteria 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on transformer-based spatial grounding.",
        "RELEVANCE": 9,
        "NOVELTY": 5
    },
    "2507.13326": {
        "authors": [
            "Antonio Finocchiaro",
            "Alessandro Sebastiano Catinello",
            "Michele Mazzamuto",
            "Rosario Leonardi",
            "Antonino Furnari",
            "Giovanni Maria Farinella"
        ],
        "title": "A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains",
        "abstract": "arXiv:2507.13326v1 Announce Type: new  Abstract: Hand-object interaction detection remains an open challenge in real-time applications, where intuitive user experiences depend on fast and accurate detection of interactions with surrounding objects. We propose an efficient approach for detecting hand-objects interactions from streaming egocentric vision that operates in real time. Our approach consists of an action recognition module and an object detection module for identifying active objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object. We implement our models in a cascaded architecture where the action recognition and object detection modules operate sequentially. When the action recognition predicts a contact state, it activates the object detection module, which in turn performs inference on the relevant frame to detect and classify the active object.",
        "arxiv_id": "2507.13326",
        "ARXIVID": "2507.13326",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on real-time hand-object interaction detection in egocentric vision.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.12646": {
        "authors": [
            "Kaihua Chen",
            "Tarasha Khurana",
            "Deva Ramanan"
        ],
        "title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos",
        "abstract": "arXiv:2507.12646v1 Announce Type: new  Abstract: We explore novel-view synthesis for dynamic scenes from monocular videos. Prior approaches rely on costly test-time optimization of 4D representations or do not preserve scene geometry when trained in a feed-forward manner. Our approach is based on three key insights: (1) covisible pixels (that are visible in both the input and target views) can be rendered by first reconstructing the dynamic 3D scene and rendering the reconstruction from the novel-views and (2) hidden pixels in novel views can be \"inpainted\" with feed-forward 2D video diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be self-supervised from 2D videos, allowing us to train it on a large corpus of in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot to novel test videos via test-time finetuning. We empirically verify that CogNVS outperforms almost all prior art for novel-view synthesis of dynamic scenes from monocular videos.",
        "arxiv_id": "2507.12646",
        "ARXIVID": "2507.12646",
        "COMMENT": "Matches criterion 6 as it focuses on novel-view synthesis for dynamic scenes, a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.12998": {
        "authors": [
            "Zihua Zhao",
            "Feng Hong",
            "Mengxi Chen",
            "Pengyi Chen",
            "Benyuan Liu",
            "Jiangchao Yao",
            "Ya Zhang",
            "Yanfeng Wang"
        ],
        "title": "Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning",
        "abstract": "arXiv:2507.12998v1 Announce Type: new  Abstract: The remarkable success of contrastive-learning-based multimodal models has been greatly driven by training on ever-larger datasets with expensive compute consumption. Sample selection as an alternative efficient paradigm plays an important direction to accelerate the training process. However, recent advances on sample selection either mostly rely on an oracle model to offline select a high-quality coreset, which is limited in the cold-start scenarios, or focus on online selection based on real-time model predictions, which has not sufficiently or efficiently considered the noisy correspondence. To address this dilemma, we propose a novel Differential-Informed Sample Selection (DISSect) method, which accurately and efficiently discriminates the noisy correspondence for training acceleration. Specifically, we rethink the impact of noisy correspondence on contrastive learning and propose that the differential between the predicted correlation of the current model and that of a historical model is more informative to characterize sample quality. Based on this, we construct a robust differential-based sample selection and analyze its theoretical insights. Extensive experiments on three benchmark datasets and various downstream tasks demonstrate the consistent superiority of DISSect over current state-of-the-art methods. Source code is available at: https://github.com/MediaBrain-SJTU/DISSect.",
        "arxiv_id": "2507.12998",
        "ARXIVID": "2507.12998",
        "COMMENT": "Matches criterion 2 as it proposes a novel sample selection method (DISSect) for multimodal contrastive learning, relevant to VLLMs and MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.12945": {
        "authors": [
            "Yucheng Tang",
            "Yunguan Fu",
            "Weixi Yi",
            "Yipei Wang",
            "Daniel C. Alexander",
            "Rhodri Davies",
            "Yipeng Hu"
        ],
        "title": "Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large Language Models with Cardiac MR-Based Applications",
        "abstract": "arXiv:2507.12945v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) can process and integrate information from multimodality sources, such as text and images. However, interrelationship among input modalities, uncertainties due to individual uni-modal data and potential clinical applications following such an uncertainty decomposition are yet fully understood in the context of large-scale MLLMs. In this work, we propose a multimodal uncertainty propagation model (MUPM) based on uncertainty propagation, to characterise the relationship among the uncertainties arising from image-only, text-only, and joint image-text variations in MLLM inputs. Using real clinical data consisting of cardiac MR scans and digital health records, we describe that MUPMs can be optimised robustly with a few samples. We then show that the fitted MUPMs are generalisable across different input data distributions and, perhaps surprisingly, across different downstream tasks. Such a transferability may be explained by the shared pretraining, comparatively light MLLM fine-tuning, along with the low-dimensional nature of the MUPMs. More importantly, this learned transferability, quantifying the relationship between these uncertainties, led to direct clinical applications in which uncertainties may be estimated and thus analysed robustly for varying data or even a novel set of cardiac disease prediction tasks. In addition, we show experimentally the efficiency in multimodal data required for estimating the overall uncertainty and its ability to identify redundant factors, both of which are considered practical yet clinically useful applications with the proposed MUPMs. Codes are available at https://github.com/yucheng722/MUPM.",
        "arxiv_id": "2507.12945",
        "ARXIVID": "2507.12945",
        "COMMENT": "Matches criteria 2 as it explores uncertainty propagation in multimodal large language models with clinical applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12780": {
        "authors": [
            "Yancheng Wang",
            "Yingzhen Yang"
        ],
        "title": "Compact Vision Transformer by Reduction of Kernel Complexity",
        "abstract": "arXiv:2507.12780v1 Announce Type: new  Abstract: Self-attention and transformer architectures have become foundational components in modern deep learning. Recent efforts have integrated transformer blocks into compact neural architectures for computer vision, giving rise to various efficient vision transformers. In this work, we introduce Transformer with Kernel Complexity Reduction, or KCR-Transformer, a compact transformer block equipped with differentiable channel selection, guided by a novel and sharp theoretical generalization bound. KCR-Transformer performs input/output channel selection in the MLP layers of transformer blocks to reduce the computational cost. Furthermore, we provide a rigorous theoretical analysis establishing a tight generalization bound for networks equipped with KCR-Transformer blocks. Leveraging such strong theoretical results, the channel pruning by KCR-Transformer is conducted in a generalization-aware manner, ensuring that the resulting network retains a provably small generalization error. Our KCR-Transformer is compatible with many popular and compact transformer networks, such as ViT and Swin, and it reduces the FLOPs of the vision transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in the vision transformers with KCR-Transformer blocks, leading to KCR-Transformer networks with different backbones. The resulting TCR-Transformers achieve superior performance on various computer vision tasks, achieving even better performance than the original models with even less FLOPs and parameters.",
        "arxiv_id": "2507.12780",
        "ARXIVID": "2507.12780",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) as it introduces a compact vision transformer with kernel complexity reduction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12807": {
        "authors": [
            "Yufei Peng",
            "Yonggang Zhang",
            "Yiu-ming Cheung"
        ],
        "title": "Semantic-guided Fine-tuning of Foundation Model for Long-tailed Visual Recognition",
        "abstract": "arXiv:2507.12807v1 Announce Type: new  Abstract: The variance in class-wise sample sizes within long-tailed scenarios often results in degraded performance in less frequent classes. Fortunately, foundation models, pre-trained on vast open-world datasets, demonstrate strong potential for this task due to their generalizable representation, which promotes the development of adaptive strategies on pre-trained models in long-tailed learning. Advanced fine-tuning methods typically adjust visual encoders while neglecting the semantics derived from the frozen text encoder, overlooking the visual and textual alignment. To strengthen this alignment, we propose a novel approach, Semantic-guided fine-tuning of foundation model for long-tailed visual recognition (Sage), which incorporates semantic guidance derived from textual modality into the visual fine-tuning process. Specifically, we introduce an SG-Adapter that integrates class descriptions as semantic guidance to guide the fine-tuning of the visual encoder. The introduced guidance is passesed through the attention mechanism and enables the model to focus more on semantically relevant content, strengthening the alignment between the visual and textual modalities. Due to the inconsistent class-conditional distributions neglected by the existing loss function, the resulting prediction bias causes performance improvements for the tail class less than for the head class, even when the multi-modal alignment is enhanced. To address this challenge, we propose a novel distribution mismatch-aware compensation factor, which is specifically designed to rectify the prediction bias caused by the ignored inconsistent distribution based on our theoretical analysis, and is seamlessly integrated into the loss function. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed Sage in enhancing performance in long-tailed learning.",
        "arxiv_id": "2507.12807",
        "ARXIVID": "2507.12807",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) as it focuses on fine-tuning foundation models for long-tailed visual recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.12591": {
        "authors": [
            "Trong-Thang Pham",
            "Akash Awasthi",
            "Saba Khan",
            "Esteban Duran Marti",
            "Tien-Phat Nguyen",
            "Khoa Vo",
            "Minh Tran",
            "Ngoc Son Nguyen",
            "Cuong Tran Van",
            "Yuki Ikebe",
            "Anh Totti Nguyen",
            "Anh Nguyen",
            "Zhigang Deng",
            "Carol C. Wu",
            "Hien Van Nguyen",
            "Ngan Le"
        ],
        "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling",
        "abstract": "arXiv:2507.12591v1 Announce Type: new  Abstract: Understanding radiologists' eye movement during Computed Tomography (CT) reading is crucial for developing effective interpretable computer-aided diagnosis systems. However, CT research in this area has been limited by the lack of publicly available eye-tracking datasets and the three-dimensional complexity of CT volumes. To address these challenges, we present the first publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to process CT volumes and generate radiologist-like 3D fixation sequences, overcoming the limitations of current scanpath predictors that only handle 2D inputs. Since deep learning models benefit from a pretraining step, we develop a pipeline that converts existing 2D gaze datasets into 3D gaze data to pretrain CT-Searcher. Through both qualitative and quantitative evaluations on CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a comprehensive assessment framework for 3D scanpath prediction in medical imaging.",
        "arxiv_id": "2507.12591",
        "ARXIVID": "2507.12591",
        "COMMENT": "Matches criteria 3 as it introduces a new dataset and method for 3D volumetric scanpath modeling, relevant to embodied AI benchmarks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.12884": {
        "authors": [
            "Mengxi Liu",
            "Lala Shakti Swarup Ray",
            "Sizhen Bian",
            "Ko Watanabe",
            "Ankur Bhatt",
            "Joanna Sorysz",
            "Russel Torah",
            "Bo Zhou",
            "Paul Lukowicz"
        ],
        "title": "From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation",
        "abstract": "arXiv:2507.12884v1 Announce Type: new  Abstract: We present NeckSense, a novel wearable system for head pose tracking that leverages multi-channel bio-impedance sensing with soft, dry electrodes embedded in a lightweight, necklace-style form factor. NeckSense captures dynamic changes in tissue impedance around the neck, which are modulated by head rotations and subtle muscle activations. To robustly estimate head pose, we propose a deep learning framework that integrates anatomical priors, including joint constraints and natural head rotation ranges, into the loss function design. We validate NeckSense on 7 participants using the current SOTA pose estimation model as ground truth. Our system achieves a mean per-vertex error of 25.9 mm across various head movements with a leave-one-person-out cross-validation method, demonstrating that a compact, line-of-sight-free bio-impedance wearable can deliver head-tracking performance comparable to SOTA vision-based methods.",
        "arxiv_id": "2507.12884",
        "ARXIVID": "2507.12884",
        "COMMENT": "Matches criteria 3 as it introduces a novel wearable system for head pose estimation, which is relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.13260": {
        "authors": [
            "Yiting Yang",
            "Hao Luo",
            "Yuan Sun",
            "Qingsen Yan",
            "Haokui Zhang",
            "Wei Dong",
            "Guoqing Wang",
            "Peng Wang",
            "Yang Yang",
            "Hengtao Shen"
        ],
        "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy",
        "abstract": "arXiv:2507.13260v1 Announce Type: new  Abstract: A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices.",
        "arxiv_id": "2507.13260",
        "ARXIVID": "2507.13260",
        "COMMENT": "Matches criterion 4 as it proposes a novel fine-tuning strategy (AOFT) for pre-trained vision transformers, enhancing their generalization.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.12967": {
        "authors": [
            "Keli Deng",
            "Jie Nie",
            "Yuntao Qian"
        ],
        "title": "RGB Pre-Training Enhanced Unobservable Feature Latent Diffusion Model for Spectral Reconstruction",
        "abstract": "arXiv:2507.12967v1 Announce Type: new  Abstract: Spectral reconstruction (SR) is a crucial problem in image processing that requires reconstructing hyperspectral images (HSIs) from the corresponding RGB images. A key difficulty in SR is estimating the unobservable feature, which encapsulates significant spectral information not captured by RGB imaging sensors. The solution lies in effectively constructing the spectral-spatial joint distribution conditioned on the RGB image to complement the unobservable feature. Since HSIs share a similar spatial structure with the corresponding RGB images, it is rational to capitalize on the rich spatial knowledge in RGB pre-trained models for spectral-spatial joint distribution learning. To this end, we extend the RGB pre-trained latent diffusion model (RGB-LDM) to an unobservable feature LDM (ULDM) for SR. As the RGB-LDM and its corresponding spatial autoencoder (SpaAE) already excel in spatial knowledge, the ULDM can focus on modeling spectral structure. Moreover, separating the unobservable feature from the HSI reduces the redundant spectral information and empowers the ULDM to learn the joint distribution in a compact latent space. Specifically, we propose a two-stage pipeline consisting of spectral structure representation learning and spectral-spatial joint distribution learning to transform the RGB-LDM into the ULDM. In the first stage, a spectral unobservable feature autoencoder (SpeUAE) is trained to extract and compress the unobservable feature into a 3D manifold aligned with RGB space. In the second stage, the spectral and spatial structures are sequentially encoded by the SpeUAE and the SpaAE, respectively. The ULDM is then acquired to model the distribution of the coded unobservable feature with guidance from the corresponding RGB images. Experimental results on SR and downstream relighting tasks demonstrate that our proposed method achieves state-of-the-art performance.",
        "arxiv_id": "2507.12967",
        "ARXIVID": "2507.12967",
        "COMMENT": "Matches criterion 4 as it leverages RGB pre-trained models for spectral reconstruction, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.12841": {
        "authors": [
            "Yiming Ren",
            "Zhiqiang Lin",
            "Yu Li",
            "Gao Meng",
            "Weiyun Wang",
            "Junjie Wang",
            "Zicheng Lin",
            "Jifeng Dai",
            "Yujiu Yang",
            "Wenhai Wang",
            "Ruihang Chu"
        ],
        "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning",
        "abstract": "arXiv:2507.12841v1 Announce Type: new  Abstract: Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench.",
        "arxiv_id": "2507.12841",
        "ARXIVID": "2507.12841",
        "COMMENT": "Matches criterion 2 as it focuses on controllable multimodal captioning, which involves vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.13343": {
        "authors": [
            "Yushu Wu",
            "Yanyu Li",
            "Anil Kag",
            "Ivan Skorokhodov",
            "Willi Menapace",
            "Ke Ma",
            "Arpit Sahni",
            "Ju Hu",
            "Aliaksandr Siarohin",
            "Dhritiman Sagar",
            "Yanzhi Wang",
            "Sergey Tulyakov"
        ],
        "title": "Taming Diffusion Transformer for Real-Time Mobile Video Generation",
        "abstract": "arXiv:2507.13343v1 Announce Type: new  Abstract: Diffusion Transformers (DiT) have shown strong performance in video generation tasks, but their high computational cost makes them impractical for resource-constrained devices like smartphones, and real-time generation is even more challenging. In this work, we propose a series of novel optimizations to significantly accelerate video generation and enable real-time performance on mobile platforms. First, we employ a highly compressed variational autoencoder (VAE) to reduce the dimensionality of the input data without sacrificing visual quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning strategy to shrink the model size to suit mobile platform while preserving critical performance characteristics. Third, we develop an adversarial step distillation technique tailored for DiT, which allows us to reduce the number of inference steps to four. Combined, these optimizations enable our model to achieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max, demonstrating the feasibility of real-time, high-quality video generation on mobile devices.",
        "arxiv_id": "2507.13343",
        "ARXIVID": "2507.13343",
        "COMMENT": "Matches criterion 6 as it focuses on video generation and optimization for mobile platforms.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.13314": {
        "authors": [
            "Junsu Kim",
            "Naeun Kim",
            "Jaeho Lee",
            "Incheol Park",
            "Dongyoon Han",
            "Seungryul Baek"
        ],
        "title": "Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark",
        "abstract": "arXiv:2507.13314v1 Announce Type: new  Abstract: The reasoning-based pose estimation (RPE) benchmark has emerged as a widely adopted evaluation standard for pose-aware multimodal large language models (MLLMs). Despite its significance, we identified critical reproducibility and benchmark-quality issues that hinder fair and consistent quantitative evaluations. Most notably, the benchmark utilizes different image indices from those of the original 3DPW dataset, forcing researchers into tedious and error-prone manual matching processes to obtain accurate ground-truth (GT) annotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our analysis reveals several inherent benchmark-quality limitations, including significant image redundancy, scenario imbalance, overly simplistic poses, and ambiguous textual descriptions, collectively undermining reliable evaluations across diverse scenarios. To alleviate manual effort and enhance reproducibility, we carefully refined the GT annotations through meticulous visual matching and publicly release these refined annotations as an open-source resource, thereby promoting consistent quantitative evaluations and facilitating future advancements in human pose-aware multimodal reasoning.",
        "arxiv_id": "2507.13314",
        "ARXIVID": "2507.13314",
        "COMMENT": "Matches criteria 3 as it revisits and refines a benchmark for reasoning-based pose estimation, relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.12761": {
        "authors": [
            "Hanlei Shi",
            "Leyuan Qu",
            "Yu Liu",
            "Di Gao",
            "Yuhua Zheng",
            "Taihao Li"
        ],
        "title": "Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation",
        "abstract": "arXiv:2507.12761v1 Announce Type: new  Abstract: Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic engagement.With the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional expressiveness.This study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotions--by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimization--inspired by artists' portrait painting process, a progressive guidance denoising strategy is proposed, employing a \"global emotion localization--local muscle control\" mechanism to refine micro-expression dynamics in generated videos.Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our model's zero-shot generation capability.",
        "arxiv_id": "2507.12761",
        "ARXIVID": "2507.12761",
        "COMMENT": "Matches criteria 5 as it combines image/video understanding tasks with large language models for expressive talking-head generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.12905": {
        "authors": [
            "Tomohiro Suzuki",
            "Ryota Tanaka",
            "Calvin Yeung",
            "Keisuke Fujii"
        ],
        "title": "AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and Evaluation of Monocular 3D Pose Estimation Ability",
        "abstract": "arXiv:2507.12905v1 Announce Type: new  Abstract: Monocular 3D pose estimation is a promising, flexible alternative to costly motion capture systems for sports analysis. However, its practical application is hindered by two factors: a lack of realistic sports datasets and unclear reliability for sports tasks. To address these challenges, we introduce the AthleticsPose dataset, a new public dataset featuring ``real'' motions captured from 23 athletes performing various athletics events on an athletic field. Using this dataset, we trained a representative 3D pose estimation model and performed a comprehensive evaluation. Our results show that the model trained on AthleticsPose significantly outperforms a baseline model trained on an imitated sports motion dataset, reducing MPJPE by approximately 75 %. These results show the importance of training on authentic sports motion data, as models based on imitated motions do not effectively transfer to real-world motions. Further analysis reveals that estimation accuracy is sensitive to camera view and subject scale. In case studies of kinematic indicators, the model demonstrated the potential to capture individual differences in knee angles but struggled with higher-speed metrics, such as knee-drive velocity, due to prediction biases. This work provides the research community with a valuable dataset and clarifies the potential and practical limitations of using monocular 3D pose estimation for sports motion analysis. Our dataset, code, and checkpoints are available at https://github.com/SZucchini/AthleticsPose.",
        "arxiv_id": "2507.12905",
        "ARXIVID": "2507.12905",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for monocular 3D pose estimation in sports, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.12771": {
        "authors": [
            "Min-Jeong Lee",
            "Hee-Dong Kim",
            "Seong-Whan Lee"
        ],
        "title": "Local Representative Token Guided Merging for Text-to-Image Generation",
        "abstract": "arXiv:2507.12771v1 Announce Type: new  Abstract: Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.",
        "arxiv_id": "2507.12771",
        "ARXIVID": "2507.12771",
        "COMMENT": "Matches criterion 5 as it proposes a novel token merging strategy for text-to-image generation, combining image understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.12857": {
        "authors": [
            "Shiqi Huang",
            "Shuting He",
            "Huaiyuan Qin",
            "Bihan Wen"
        ],
        "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
        "abstract": "arXiv:2507.12857v1 Announce Type: new  Abstract: Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$ ($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary $\\textbf{RE}$mote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at https://github.com/HuangShiqi128/SCORE.",
        "arxiv_id": "2507.12857",
        "ARXIVID": "2507.12857",
        "COMMENT": "Matches criterion 4 as it introduces a novel method for open-vocabulary remote sensing instance segmentation, which is a vision foundation model application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.13032": {
        "authors": [
            "Yi Xin",
            "Le Zhuo",
            "Qi Qin",
            "Siqi Luo",
            "Yuewen Cao",
            "Bin Fu",
            "Yangfan He",
            "Hongsheng Li",
            "Guangtao Zhai",
            "Xiaohong Liu",
            "Peng Gao"
        ],
        "title": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation",
        "abstract": "arXiv:2507.13032v1 Announce Type: new  Abstract: AutoRegressive (AR) models have made notable progress in image generation, with Masked AutoRegressive (MAR) models gaining attention for their efficient parallel decoding. However, MAR models have traditionally underperformed when compared to standard AR models. This study refines the MAR architecture to improve image generation quality. We begin by evaluating various image tokenizers to identify the most effective one. Subsequently, we introduce an improved Bidirectional LLaMA architecture by replacing causal attention with bidirectional attention and incorporating 2D RoPE, which together form our advanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves a FID score of 3.71, matching state-of-the-art AR models in the ImageNet 256x256 benchmark, while requiring only 8 inference steps compared to the 256 steps of AR models. Furthermore, we develop a text-driven MaskGIL model with 775M parameters for generating images from text at various resolutions. Beyond image generation, MaskGIL extends to accelerate AR-based generation and enable real-time speech-to-image conversion. Our codes and models are available at https://github.com/synbol/MaskGIL.",
        "arxiv_id": "2507.13032",
        "ARXIVID": "2507.13032",
        "COMMENT": "Matches criterion 4 as it focuses on improving foundation models for image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.12845": {
        "authors": [
            "Khang Truong",
            "Lam Pham",
            "Hieu Tang",
            "Jasmin Lampert",
            "Martin Boyer",
            "Son Phan",
            "Truong Nguyen"
        ],
        "title": "SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning",
        "abstract": "arXiv:2507.12845v1 Announce Type: new  Abstract: Image captioning has emerged as a crucial task in the intersection of computer vision and natural language processing, enabling automated generation of descriptive text from visual content. In the context of remote sensing, image captioning plays a significant role in interpreting vast and complex satellite imagery, aiding applications such as environmental monitoring, disaster assessment, and urban planning. This motivates us, in this paper, to present a transformer based network architecture for remote sensing image captioning (RSIC) in which multiple techniques of Static Expansion, Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated. We evaluate our proposed models using two benchmark remote sensing image datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the state-of-the-art systems on most of evaluation metrics, which demonstrates potential to apply for real-life remote sensing image systems.",
        "arxiv_id": "2507.12845",
        "ARXIVID": "2507.12845",
        "COMMENT": "Matches criterion 2 as it involves a transformer-based architecture for image captioning, which is a vision-language integration task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.13292": {
        "authors": [
            "Ekta Balkrishna Gavas",
            "Chinmay Hegde",
            "Nasir Memon",
            "Sudipta Banerjee"
        ],
        "title": "DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation",
        "abstract": "arXiv:2507.13292v1 Announce Type: new  Abstract: Accurate age verification can protect underage users from unauthorized access to online platforms and e-commerce sites that provide age-restricted services. However, accurate age estimation can be confounded by several factors, including facial makeup that can induce changes to alter perceived identity and age to fool both humans and machines. In this work, we propose DiffClean which erases makeup traces using a text-guided diffusion model to defend against makeup attacks. DiffClean improves age estimation (minor vs. adult accuracy by 4.8%) and face verification (TMR by 8.9% at FMR=0.01%) over competing baselines on digitally simulated and real makeup images.",
        "arxiv_id": "2507.13292",
        "ARXIVID": "2507.13292",
        "COMMENT": "This paper does not match any specific criteria but is tangentially related to vision tasks and generative modeling through its use of diffusion models for makeup removal, which could be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.12851": {
        "authors": [
            "Ziyi Wang",
            "Zhi Gao",
            "Jin Chen",
            "Qingjie Zhao",
            "Xinxiao Wu",
            "Jiebo Luo"
        ],
        "title": "Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization",
        "abstract": "arXiv:2507.12851v1 Announce Type: new  Abstract: Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP's strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods. The code is available at: https://github.com/bitPrincy/SRE-DG.",
        "arxiv_id": "2507.12851",
        "ARXIVID": "2507.12851",
        "COMMENT": "Does not match any specific criterion but is related to domain generalization, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.13074": {
        "authors": [
            "Yawen Zou",
            "Guang Li",
            "Zi Wang",
            "Chunzhi Gu",
            "Chao Zhang"
        ],
        "title": "Label-Consistent Dataset Distillation with Detector-Guided Refinement",
        "abstract": "arXiv:2507.13074v1 Announce Type: new  Abstract: Dataset distillation (DD) aims to generate a compact yet informative dataset that achieves performance comparable to the original dataset, thereby reducing demands on storage and computational resources. Although diffusion models have made significant progress in dataset distillation, the generated surrogate datasets often contain samples with label inconsistencies or insufficient structural detail, leading to suboptimal downstream performance. To address these issues, we propose a detector-guided dataset distillation framework that explicitly leverages a pre-trained detector to identify and refine anomalous synthetic samples, thereby ensuring label consistency and improving image quality. Specifically, a detector model trained on the original dataset is employed to identify anomalous images exhibiting label mismatches or low classification confidence. For each defective image, multiple candidates are generated using a pre-trained diffusion model conditioned on the corresponding image prototype and label. The optimal candidate is then selected by jointly considering the detector's confidence score and dissimilarity to existing qualified synthetic samples, thereby ensuring both label accuracy and intra-class diversity. Experimental results demonstrate that our method can synthesize high-quality representative images with richer details, achieving state-of-the-art performance on the validation set.",
        "arxiv_id": "2507.13074",
        "ARXIVID": "2507.13074",
        "COMMENT": "Does not match any specific criterion but is related to dataset distillation, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.12988": {
        "authors": [
            "Uranik Berisha",
            "Jens Mehnert",
            "Alexandru Paul Condurache"
        ],
        "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
        "abstract": "arXiv:2507.12988v1 Announce Type: new  Abstract: Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44x.",
        "arxiv_id": "2507.12988",
        "ARXIVID": "2507.12988",
        "COMMENT": "This paper does not match any specific criteria but is generally relevant to the broader interest area of efficient model compression and deployment, which could be tangentially related to embodied AI or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12956": {
        "authors": [
            "Qiang Wang",
            "Mengchao Wang",
            "Fan Jiang",
            "Yaqi Fan",
            "Yonggang Qi",
            "Mu Xu"
        ],
        "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers",
        "abstract": "arXiv:2507.12956v1 Announce Type: new  Abstract: Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/.",
        "arxiv_id": "2507.12956",
        "ARXIVID": "2507.12956",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to video-based tasks and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12758": {
        "authors": [
            "Wangzheng Shi",
            "Yinglin Zheng",
            "Yuxin Lin",
            "Jianmin Bao",
            "Ming Zeng",
            "Dong Chen"
        ],
        "title": "HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation",
        "abstract": "arXiv:2507.12758v1 Announce Type: new  Abstract: Hair transfer is increasingly valuable across domains such as social media, gaming, advertising, and entertainment. While significant progress has been made in single-image hair transfer, video-based hair transfer remains challenging due to the need for temporal consistency, spatial fidelity, and dynamic adaptability. In this work, we propose HairShifter, a novel \"Anchor Frame + Animation\" framework that unifies high-quality image hair transfer with smooth and coherent video animation. At its core, HairShifter integrates a Image Hair Transfer (IHT) module for precise per-frame transformation and a Multi-Scale Gated SPADE Decoder to ensure seamless spatial blending and temporal coherence. Our method maintains hairstyle fidelity across frames while preserving non-hair regions. Extensive experiments demonstrate that HairShifter achieves state-of-the-art performance in video hairstyle transfer, combining superior visual quality, temporal consistency, and scalability. The code will be publicly available. We believe this work will open new avenues for video-based hairstyle transfer and establish a robust baseline in this field.",
        "arxiv_id": "2507.12758",
        "ARXIVID": "2507.12758",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to video-based tasks and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12804": {
        "authors": [
            "Hoang-Son Vo",
            "Quang-Vinh Nguyen",
            "Seungwon Kim",
            "Hyung-Jeong Yang",
            "Soonja Yeom",
            "Soo-Hyung Kim"
        ],
        "title": "ATL-Diff: Audio-Driven Talking Head Generation with Early Landmarks-Guide Noise Diffusion",
        "abstract": "arXiv:2507.12804v1 Announce Type: new  Abstract: Audio-driven talking head generation requires precise synchronization between facial animations and audio signals. This paper introduces ATL-Diff, a novel approach addressing synchronization limitations while reducing noise and computational costs. Our framework features three key components: a Landmark Generation Module converting audio to facial landmarks, a Landmarks-Guide Noise approach that decouples audio by distributing noise according to landmarks, and a 3D Identity Diffusion network preserving identity characteristics. Experiments on MEAD and CREMA-D datasets demonstrate that ATL-Diff outperforms state-of-the-art methods across all metrics. Our approach achieves near real-time processing with high-quality animations, computational efficiency, and exceptional preservation of facial nuances. This advancement offers promising applications for virtual assistants, education, medical communication, and digital platforms. The source code is available at: \\href{https://github.com/sonvth/ATL-Diff}{https://github.com/sonvth/ATL-Diff}",
        "arxiv_id": "2507.12804",
        "ARXIVID": "2507.12804",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to multimodal learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13345": {
        "authors": [
            "Yukai Shi",
            "Jiarong Ou",
            "Rui Chen",
            "Haotian Yang",
            "Jiahao Wang",
            "Xin Tao",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai"
        ],
        "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
        "abstract": "arXiv:2507.13345v1 Announce Type: new  Abstract: In visual generation tasks, the responses and combinations of complex concepts often lack stability and are error-prone, which remains an under-explored area. In this paper, we attempt to explore the causal factors for poor concept responses through elaborately designed experiments. We also design a concept-wise equalization loss function (IMBA loss) to address this issue. Our proposed method is online, eliminating the need for offline dataset processing, and requires minimal code changes. In our newly proposed complex concept benchmark Inert-CompBench and two other public test sets, our method significantly enhances the concept response capability of baseline models and yields highly competitive results with only a few codes.",
        "arxiv_id": "2507.13345",
        "ARXIVID": "2507.13345",
        "COMMENT": "Does not closely match any specific criteria but is generally related to generative modeling and visual generation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12730": {
        "authors": [
            "Homare Sueyoshi",
            "Kiyoshi Nishikawa",
            "Hitoshi Kiya"
        ],
        "title": "A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique",
        "abstract": "arXiv:2507.12730v1 Announce Type: new  Abstract: We propose a privacy-preserving semantic-segmentation method for applying perceptual encryption to images used for model training in addition to test images. This method also provides almost the same accuracy as models without any encryption. The above performance is achieved using a domain-adaptation technique on the embedding structure of the Vision Transformer (ViT). The effectiveness of the proposed method was experimentally confirmed in terms of the accuracy of semantic segmentation when using a powerful semantic-segmentation model with ViT called Segmentation Transformer.",
        "arxiv_id": "2507.12730",
        "ARXIVID": "2507.12730",
        "COMMENT": "Does not match any specific criterion but is related to privacy-preserving semantic segmentation, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12755": {
        "authors": [
            "Yanchen Guan",
            "Haicheng Liao",
            "Chengyue Wang",
            "Bonan Wang",
            "Jiaxun Zhang",
            "Jia Hu",
            "Zhenning Li"
        ],
        "title": "Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation",
        "abstract": "arXiv:2507.12755v1 Announce Type: new  Abstract: Developing precise and computationally efficient traffic accident anticipation system is crucial for contemporary autonomous driving technologies, enabling timely intervention and loss prevention. In this paper, we propose an accident anticipation framework employing a dual-branch architecture that effectively integrates visual information from dashcam videos with structured textual data derived from accident reports. Furthermore, we introduce a feature aggregation method that facilitates seamless integration of multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by targeted prompt engineering strategies to produce actionable feedback and standardized accident archives. Comprehensive evaluations conducted on benchmark datasets (DAD, CCD, and A3D) validate the superior predictive accuracy, enhanced responsiveness, reduced computational overhead, and improved interpretability of our approach, thus establishing a new benchmark for state-of-the-art performance in traffic accident anticipation.",
        "arxiv_id": "2507.12755",
        "ARXIVID": "2507.12755",
        "COMMENT": "Does not match any specific criterion but is related to multimodal learning in accident anticipation, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.12806": {
        "authors": [
            "Zhiwei Liu",
            "Jielin Qiu",
            "Shiyu Wang",
            "Jianguo Zhang",
            "Zuxin Liu",
            "Roshan Ram",
            "Haolin Chen",
            "Weiran Yao",
            "Huan Wang",
            "Shelby Heinecke",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
        "abstract": "arXiv:2507.12806v1 Announce Type: new  Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \\oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.",
        "arxiv_id": "2507.12806",
        "ARXIVID": "2507.12806",
        "COMMENT": "Does not match any specific criterion but is related to evaluation frameworks for AI agents, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13110": {
        "authors": [
            "Zi Wang",
            "Katsuya Hotta",
            "Koichiro Kamide",
            "Yawen Zou",
            "Chao Zhang",
            "Jun Yu"
        ],
        "title": "3DKeyAD: High-Resolution 3D Point Cloud Anomaly Detection via Keypoint-Guided Point Clustering",
        "abstract": "arXiv:2507.13110v1 Announce Type: new  Abstract: High-resolution 3D point clouds are highly effective for detecting subtle structural anomalies in industrial inspection. However, their dense and irregular nature imposes significant challenges, including high computational cost, sensitivity to spatial misalignment, and difficulty in capturing localized structural differences. This paper introduces a registration-based anomaly detection framework that combines multi-prototype alignment with cluster-wise discrepancy analysis to enable precise 3D anomaly localization. Specifically, each test sample is first registered to multiple normal prototypes to enable direct structural comparison. To evaluate anomalies at a local level, clustering is performed over the point cloud, and similarity is computed between features from the test sample and the prototypes within each cluster. Rather than selecting cluster centroids randomly, a keypoint-guided strategy is employed, where geometrically informative points are chosen as centroids. This ensures that clusters are centered on feature-rich regions, enabling more meaningful and stable distance-based comparisons. Extensive experiments on the Real3D-AD benchmark demonstrate that the proposed method achieves state-of-the-art performance in both object-level and point-level anomaly detection, even using only raw features.",
        "arxiv_id": "2507.13110",
        "ARXIVID": "2507.13110",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and anomaly detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.13350": {
        "authors": [
            "Yichi Zhang",
            "Yici Yan",
            "Alex Schwing",
            "Zhizhen Zhao"
        ],
        "title": "Hierarchical Rectified Flow Matching with Mini-Batch Couplings",
        "abstract": "arXiv:2507.13350v1 Announce Type: new  Abstract: Flow matching has emerged as a compelling generative modeling approach that is widely used across domains. To generate data via a flow matching model, an ordinary differential equation (ODE) is numerically solved via forward integration of the modeled velocity field. To better capture the multi-modality that is inherent in typical velocity fields, hierarchical flow matching was recently introduced. It uses a hierarchy of ODEs that are numerically integrated when generating data. This hierarchy of ODEs captures the multi-modal velocity distribution just like vanilla flow matching is capable of modeling a multi-modal data distribution. While this hierarchy enables to model multi-modal velocity distributions, the complexity of the modeled distribution remains identical across levels of the hierarchy. In this paper, we study how to gradually adjust the complexity of the distributions across different levels of the hierarchy via mini-batch couplings. We show the benefits of mini-batch couplings in hierarchical rectified flow matching via compelling results on synthetic and imaging data. Code is available at https://riccizz.github.io/HRF_coupling.",
        "arxiv_id": "2507.13350",
        "ARXIVID": "2507.13350",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}