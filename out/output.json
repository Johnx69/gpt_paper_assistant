{
    "2510.03117": {
        "authors": [
            "Kaisi Guan",
            "Xihua Wang",
            "Zhengfeng Lai",
            "Xin Cheng",
            "Peng Zhang",
            "XiaoJiang Liu",
            "Ruihua Song",
            "Meng Cao"
        ],
        "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction",
        "abstract": "arXiv:2510.03117v1 Announce Type: new  Abstract: This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge\" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.",
        "arxiv_id": "2510.03117",
        "ARXIVID": "2510.03117",
        "COMMENT": "This paper matches criterion 5 (Integration of Image/Video and Large Language Models) and criterion 6 (Video Understanding) as it addresses text-to-sounding-video generation, involving both video and audio modalities aligned with text.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.02987": {
        "authors": [
            "Juntong Wang",
            "Huiyu Duan",
            "Jiarui Wang",
            "Ziheng Jia",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency",
        "abstract": "arXiv:2510.02987v1 Announce Type: new  Abstract: With the rapid advancement of large multimodal models (LMMs), recent text-to-image (T2I) models can generate high-quality images and demonstrate great alignment to short prompts. However, they still struggle to effectively understand and follow long and detailed prompts, displaying inconsistent generation. To address this challenge, we introduce LPG-Bench, a comprehensive benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench features 200 meticulously crafted prompts with an average length of over 250 words, approaching the input capacity of several leading commercial models. Using these prompts, we generate 2,600 images from 13 state-of-the-art models and further perform comprehensive human-ranked annotations. Based on LPG-Bench, we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor consistency with human preferences on long-prompt-based image generation. To address the gap, we introduce a novel zero-shot metric based on text-to-image-to-text consistency, termed TIT, for evaluating long-prompt-generated images. The core concept of TIT is to quantify T2I alignment by directly comparing the consistency between the raw prompt and the LMM-produced description on the generated image, which includes an efficient score-based instantiation TIT-Score and a large-language-model (LLM) based instantiation TIT-Score-LLM. Extensive experiments demonstrate that our framework achieves superior alignment with human judgment compared to CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT methods together offer a deeper perspective to benchmark and foster the development of T2I models. All resources will be made publicly available.",
        "arxiv_id": "2510.02987",
        "ARXIVID": "2510.02987",
        "COMMENT": "This paper matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a novel evaluation metric for text-to-image-to-text consistency, which directly involves image generation and LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.03191": {
        "authors": [
            "Denis Zavadski",
            "Nikita Philip Tatsch",
            "Carsten Rother"
        ],
        "title": "Product-Quantised Image Representation for High-Quality Image Synthesis",
        "abstract": "arXiv:2510.03191v1 Announce Type: new  Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation. In this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up to 96%. Our key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection. Finally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis.",
        "arxiv_id": "2510.03191",
        "ARXIVID": "2510.03191",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a novel product-quantised image representation for high-quality image synthesis.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.03200": {
        "authors": [
            "Luca Collorone",
            "Matteo Gioia",
            "Massimiliano Pappa",
            "Paolo Leoni",
            "Giovanni Ficarra",
            "Or Litany",
            "Indro Spinelli",
            "Fabio Galasso"
        ],
        "title": "MonSTeR: a Unified Model for Motion, Scene, Text Retrieval",
        "abstract": "arXiv:2510.03200v1 Announce Type: new  Abstract: Intention drives human movement in complex environments, but such movement can only happen if the surrounding context supports it. Despite the intuitive nature of this mechanism, existing research has not yet provided tools to evaluate the alignment between skeletal movement (motion), intention (text), and the surrounding context (scene). In this work, we introduce MonSTeR, the first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of higher-order relations, MonSTeR constructs a unified latent space by leveraging unimodal and cross-modal representations. This allows MonSTeR to capture the intricate dependencies between modalities, enabling flexible but robust retrieval across various tasks. Our results show that MonSTeR outperforms trimodal models that rely solely on unimodal representations. Furthermore, we validate the alignment of our retrieval scores with human preferences through a dedicated user study. We demonstrate the versatility of MonSTeR's latent space on zero-shot in-Scene Object Placement and Motion Captioning. Code and pre-trained models are available at github.com/colloroneluca/MonSTeR.",
        "arxiv_id": "2510.03200",
        "ARXIVID": "2510.03200",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a trimodal retrieval model combining motion, scene, and text.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.02898": {
        "authors": [
            "Lorenzo Bianchi",
            "Giacomo Pacini",
            "Fabio Carrara",
            "Nicola Messina",
            "Giuseppe Amato",
            "Fabrizio Falchi"
        ],
        "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
        "abstract": "arXiv:2510.02898v1 Announce Type: new  Abstract: Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present \\frameworkName{}, a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
        "arxiv_id": "2510.02898",
        "ARXIVID": "2510.02898",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores a novel zero-shot captioning framework leveraging patch-centric representations for vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.03198": {
        "authors": [
            "Junchao Huang",
            "Xinting Hu",
            "Boyao Han",
            "Shaoshuai Shi",
            "Zhuotao Tian",
            "Tianyu He",
            "Li Jiang"
        ],
        "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft",
        "abstract": "arXiv:2510.03198v1 Announce Type: new  Abstract: Autoregressive video diffusion models have proved effective for world modeling and interactive scene generation, with Minecraft gameplay as a representative application. To faithfully simulate play, a model must generate natural content while exploring new scenes and preserve spatial consistency when revisiting explored areas. Under limited computation budgets, it must compress and exploit historical cues within a finite context window, which exposes a trade-off: Temporal-only memory lacks long-term spatial consistency, whereas adding spatial memory strengthens consistency but may degrade new scene generation quality when the model over-relies on insufficient spatial context. We present Memory Forcing, a learning framework that pairs training protocols with a geometry-indexed spatial memory. Hybrid Training exposes distinct gameplay regimes, guiding the model to rely on temporal memory during exploration and incorporate spatial memory for revisits. Chained Forward Training extends autoregressive training with model rollouts, where chained predictions create larger pose variations and encourage reliance on spatial memory for maintaining consistency. Point-to-Frame Retrieval efficiently retrieves history by mapping currently visible points to their source frames, while Incremental 3D Reconstruction maintains and updates an explicit 3D cache. Extensive experiments demonstrate that Memory Forcing achieves superior long-term spatial consistency and generative quality across diverse environments, while maintaining computational efficiency for extended sequences.",
        "arxiv_id": "2510.03198",
        "ARXIVID": "2510.03198",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) and criterion 6 (Video Understanding) as it introduces a memory-forcing framework for consistent scene generation in Minecraft.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.02528": {
        "authors": [
            "Shuhao Fu",
            "Esther Goldberg",
            "Ying Nian Wu",
            "Hongjing Lu"
        ],
        "title": "Multimodal Function Vectors for Spatial Relations",
        "abstract": "arXiv:2510.02528v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning abilities from limited multimodal demonstrations, yet the internal mechanisms supporting such task learning remain opaque. Building on prior work of large language models, we show that a small subset of attention heads in the vision-language model OpenFlamingo-4B is responsible for transmitting representations of spatial relations. The activations of these attention heads, termed function vectors, can be extracted and manipulated to alter an LMM's performance on relational tasks. First, using both synthetic and real image datasets, we apply causal mediation analysis to identify attention heads that strongly influence relational predictions, and extract multimodal function vectors that improve zero-shot accuracy at inference time. We further demonstrate that these multimodal function vectors can be fine-tuned with a modest amount of training data, while keeping LMM parameters frozen, to significantly outperform in-context learning baselines. Finally, we show that relation-specific function vectors can be linearly combined to solve analogy problems involving novel and untrained spatial relations, highlighting the strong generalization ability of this approach. Our results show that LMMs encode spatial relational knowledge within localized internal structures, which can be systematically extracted and optimized, thereby advancing our understanding of model modularity and enhancing control over relational reasoning in LMMs.",
        "arxiv_id": "2510.02528",
        "ARXIVID": "2510.02528",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 2 (Visual and Multimodal Large Language Models) as it explores spatial relations in LMMs and introduces novel methods for relational reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.02566": {
        "authors": [
            "Qiao Feng",
            "Yiming Huang",
            "Yufu Wang",
            "Jiatao Gu",
            "Lingjie Liu"
        ],
        "title": "PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction",
        "abstract": "arXiv:2510.02566v1 Announce Type: new  Abstract: Reconstructing physically plausible human motion from monocular videos remains a challenging problem in computer vision and graphics. Existing methods primarily focus on kinematics-based pose estimation, often leading to unrealistic results due to the lack of physical constraints. To address such artifacts, prior methods have typically relied on physics-based post-processing following the initial kinematics-based motion estimation. However, this two-stage design introduces error accumulation, ultimately limiting the overall reconstruction quality. In this paper, we present PhysHMR, a unified framework that directly learns a visual-to-action policy for humanoid control in a physics-based simulator, enabling motion reconstruction that is both physically grounded and visually aligned with the input video. A key component of our approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial rays and transforms them into global space. These rays are incorporated as policy inputs, providing robust global pose guidance without depending on noisy 3D root predictions. This soft global grounding, combined with local visual features from a pretrained encoder, allows the policy to reason over both detailed pose and global positioning. To overcome the sample inefficiency of reinforcement learning, we further introduce a distillation scheme that transfers motion knowledge from a mocap-trained expert to the vision-conditioned policy, which is then refined using physically motivated reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR produces high-fidelity, physically plausible motion across diverse scenarios, outperforming prior approaches in both visual accuracy and physical realism.",
        "arxiv_id": "2510.02566",
        "ARXIVID": "2510.02566",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on physically plausible human motion reconstruction in a physics-based simulator.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.02677": {
        "authors": [
            "Zhaorun Chen",
            "Xun Liu",
            "Mintong Kang",
            "Jiawei Zhang",
            "Minzhou Pan",
            "Shuang Yang",
            "Bo Li"
        ],
        "title": "ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks",
        "abstract": "arXiv:2510.02677v1 Announce Type: new  Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces also introduce new safety vulnerabilities, making the safety evaluation challenging and critical. Existing red-teaming efforts are either restricted to a narrow set of adversarial patterns or depend heavily on manual engineering, lacking scalable exploration of emerging real-world VLM vulnerabilities. To bridge this gap, we propose ARMs, an adaptive red-teaming agent that systematically conducts comprehensive risk assessments for VLMs. Given a target harmful behavior or risk definition, ARMs automatically optimizes diverse red-teaming strategies with reasoning-enhanced multi-step orchestration, to effectively elicit harmful outputs from target VLMs. We propose 11 novel multimodal attack strategies, covering diverse adversarial patterns of VLMs (e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming algorithms into ARMs via model context protocol (MCP). To balance the diversity and effectiveness of the attack, we design a layered memory with an epsilon-greedy attack exploration algorithm. Extensive experiments on instance- and policy-based benchmarks show that ARMs achieves SOTA attack success rates, exceeding baselines by an average of 52.1% and surpassing 90% on Claude-4-Sonnet. We show that the diversity of red-teaming instances generated by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs. Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety dataset comprising over 30K red-teaming instances spanning 51 diverse risk categories, grounded in both real-world multimodal threats and regulatory risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness of VLMs while preserving their general utility, providing actionable guidance to improve multimodal safety alignment against emerging threats.",
        "arxiv_id": "2510.02677",
        "ARXIVID": "2510.02677",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a red-teaming framework for multimodal safety evaluation in vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.02571": {
        "authors": [
            "Zhiting Mei",
            "Ola Shorinwa",
            "Anirudha Majumdar"
        ],
        "title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty",
        "abstract": "arXiv:2510.02571v1 Announce Type: new  Abstract: Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.",
        "arxiv_id": "2510.02571",
        "ARXIVID": "2510.02571",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a framework for uncertainty quantification in generative video models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.03110": {
        "authors": [
            "Beibei Lin",
            "Tingting Chen",
            "Robby T. Tan"
        ],
        "title": "GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion",
        "abstract": "arXiv:2510.03110v1 Announce Type: new  Abstract: Reference-driven image completion, which restores missing regions in a target view using additional images, is particularly challenging when the target view differs significantly from the references. Existing generative methods rely solely on diffusion priors and, without geometric cues such as camera pose or depth, often produce misaligned or implausible content. We propose GeoComplete, a novel framework that incorporates explicit 3D structural guidance to enforce geometric consistency in the completed regions, setting it apart from prior image-only approaches. GeoComplete introduces two key ideas: conditioning the diffusion process on projected point clouds to infuse geometric information, and applying target-aware masking to guide the model toward relevant reference cues. The framework features a dual-branch diffusion architecture. One branch synthesizes the missing regions from the masked target, while the other extracts geometric features from the projected point cloud. Joint self-attention across branches ensures coherent and accurate completion. To address regions visible in references but absent in the target, we project the target view into each reference to detect occluded areas, which are then masked during training. This target-aware masking directs the model to focus on useful cues, enhancing performance in difficult scenarios. By integrating a geometry-aware dual-branch diffusion architecture with a target-aware masking strategy, GeoComplete offers a unified and robust solution for geometry-conditioned image completion. Experiments show that GeoComplete achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly boosting geometric accuracy while maintaining high visual quality.",
        "arxiv_id": "2510.03110",
        "ARXIVID": "2510.03110",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) and criterion 5 (Integration of Image/Video and Large Language Models) due to its geometry-aware diffusion framework for image completion.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.03122": {
        "authors": [
            "Shiyi Zhang",
            "Dong Liang",
            "Hairong Zheng",
            "Yihang Zhou"
        ],
        "title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion",
        "abstract": "arXiv:2510.03122v1 Announce Type: new  Abstract: The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models.",
        "arxiv_id": "2510.03122",
        "ARXIVID": "2510.03122",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes a novel hierarchical model for reconstructing visual information from brain activity.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2510.03049": {
        "authors": [
            "Ruotong Liao",
            "Guowen Huang",
            "Qing Cheng",
            "Thomas Seidl",
            "Daniel Cremers",
            "Volker Tresp"
        ],
        "title": "When and Where do Events Switch in Multi-Event Video Generation?",
        "abstract": "arXiv:2510.03049v1 Announce Type: new  Abstract: Text-to-video (T2V) generation has surged in response to challenging questions, especially when a long video must depict multiple sequential events with temporal coherence and controllable content. Existing methods that extend to multi-event generation omit an inspection of the intrinsic factor in event shifting. The paper aims to answer the central question: When and where multi-event prompts control event transition during T2V generation. This work introduces MEve, a self-curated prompt suite for evaluating multi-event text-to-video (T2V) generation, and conducts a systematic study of two representative model families, i.e., OpenSora and CogVideoX. Extensive experiments demonstrate the importance of early intervention in denoising steps and block-wise model layers, revealing the essential factor for multi-event video generation and highlighting the possibilities for multi-event conditioning in future models.",
        "arxiv_id": "2510.03049",
        "ARXIVID": "2510.03049",
        "COMMENT": "This paper matches criterion 6 (Video Understanding) as it focuses on multi-event video generation and explores temporal coherence and event transitions in text-to-video models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.02778": {
        "authors": [
            "Xian Zhang",
            "Zexi Wu",
            "Zinuo Li",
            "Hongming Xu",
            "Luqi Gong",
            "Farid Boussaid",
            "Naoufel Werghi",
            "Mohammed Bennamoun"
        ],
        "title": "AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding",
        "abstract": "arXiv:2510.02778v1 Announce Type: new  Abstract: Understanding long-form videos remains a significant challenge for vision--language models (VLMs) due to their extensive temporal length and high information density. Most current multimodal large language models (MLLMs) rely on uniform sampling, which often overlooks critical moments, leading to incorrect responses to queries. In parallel, many keyframe selection approaches impose rigid temporal spacing: once a frame is chosen, an exclusion window suppresses adjacent timestamps to reduce redundancy. While effective at limiting overlap, this strategy frequently misses short, fine-grained cues near important events. Other methods instead emphasize visual diversity but neglect query relevance. We propose AdaRD-Key, a training-free keyframe sampling module for query-driven long-form video understanding. AdaRD-Key maximizes a unified Relevance--Diversity Max-Volume (RD-MV) objective, combining a query-conditioned relevance score with a log-determinant diversity component to yield informative yet non-redundant frames. To handle broad queries with weak alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating mechanism; when the relevance distribution indicates weak alignment, the method seamlessly shifts into a diversity-only mode, enhancing coverage without additional supervision. Our pipeline is training-free, computationally efficient (running in real time on a single GPU), and compatible with existing VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and Video-MME demonstrate state-of-the-art performance, particularly on long-form videos. Code available at https://github.com/Xian867/AdaRD-Key.",
        "arxiv_id": "2510.02778",
        "ARXIVID": "2510.02778",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a novel keyframe sampling method for long-form video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.03160": {
        "authors": [
            "Ming Zhao",
            "Wenhui Dong",
            "Yang Zhang",
            "Xiang Zheng",
            "Zhonghao Zhang",
            "Zian Zhou",
            "Yunzhi Guan",
            "Liukun Xu",
            "Wei Peng",
            "Zhaoyang Gong",
            "Zhicheng Zhang",
            "Dachuan Li",
            "Xiaosheng Ma",
            "Yuli Ma",
            "Jianing Ni",
            "Changjiang Jiang",
            "Lixia Tian",
            "Qixin Chen",
            "Kaishun Xia",
            "Pingping Liu",
            "Tongshun Zhang",
            "Zhiqiang Liu",
            "Zhongan Bi",
            "Chenyang Si",
            "Tiansheng Sun",
            "Caifeng Shan"
        ],
        "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus",
        "abstract": "arXiv:2510.03160v1 Announce Type: new  Abstract: Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our model's outputs.",
        "arxiv_id": "2510.03160",
        "ARXIVID": "2510.03160",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark and dataset for multimodal reasoning in spine disorders.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.03104": {
        "authors": [
            "Zhiting Mei",
            "Ola Shorinwa",
            "Anirudha Majumdar"
        ],
        "title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields",
        "abstract": "arXiv:2510.03104v1 Announce Type: new  Abstract: Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grounding in distilled fields remains an open question. In principle, visual-geometry features seem very promising for spatial tasks such as pose estimation, prompting the question: Do geometry-grounded semantic features offer an edge in distilled fields? Specifically, we ask three critical questions: First, does spatial-grounding produce higher-fidelity geometry-aware semantic features? We find that image features from geometry-grounded backbones contain finer structural details compared to their counterparts. Secondly, does geometry-grounding improve semantic object localization? We observe no significant difference in this task. Thirdly, does geometry-grounding enable higher-accuracy radiance field inversion? Given the limitations of prior work and their lack of semantics integration, we propose a novel framework SPINE for inverting radiance fields without an initial guess, consisting of two core components: coarse inversion using distilled semantics, and fine inversion using photometric-based optimization. Surprisingly, we find that the pose estimation accuracy decreases with geometry-grounded features. Our results suggest that visual-only features offer greater versatility for a broader range of downstream tasks, although geometry-grounded features contain more geometric detail. Notably, our findings underscore the necessity of future research on effective strategies for geometry-grounding that augment the versatility and performance of pretrained semantic features.",
        "arxiv_id": "2510.03104",
        "ARXIVID": "2510.03104",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it investigates geometry-grounded semantic features for spatial tasks and embodied AI applications.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.02750": {
        "authors": [
            "Lihua Zhou",
            "Mao Ye",
            "Shuaifeng Li",
            "Nianxin Li",
            "Jinlin Wu",
            "Xiatian Zhu",
            "Lei Deng",
            "Hongbin Liu",
            "Jiebo Luo",
            "Zhen Lei"
        ],
        "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models",
        "abstract": "arXiv:2510.02750v1 Announce Type: new  Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved remarkable success in object recognition and detection. However, their performance often degrades under real-world distribution shifts. Test-time adaptation (TTA) aims to mitigate this issue by adapting models during inference. Existing methods either rely on computationally expensive backpropagation, which hinders real-time deployment, or focus solely on likelihood adaptation, which overlooks the critical role of the prior. Our prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for object recognition by introducing a training-free framework that incorporates adaptive priors. Building upon this foundation, we now present Bayesian Class Adaptation plus (BCA+), a unified, training-free framework for TTA for both object recognition and detection. BCA+ introduces a dynamic cache that adaptively stores and updates class embeddings, spatial scales (for detection), and, crucially, adaptive class priors derived from historical predictions. We formulate adaptation as a Bayesian inference problem, where final predictions are generated by fusing the initial VLM output with a cache-based prediction. This cache-based prediction combines a dynamically updated likelihood (measuring feature and scale similarity) and a prior (reflecting the evolving class distribution). This dual-adaptation mechanism, coupled with uncertainty-guided fusion, enables BCA+ to correct both the model's semantic understanding and its contextual confidence. As a training-free method requiring no backpropagation, BCA+ is highly efficient. Extensive experiments demonstrate that BCA+ achieves state-of-the-art performance on both recognition and detection benchmarks.",
        "arxiv_id": "2510.02750",
        "ARXIVID": "2510.02750",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a Bayesian test-time adaptation framework for vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.02912": {
        "authors": [
            "Xin Zou",
            "Di Lu",
            "Yizhou Wang",
            "Yibo Yan",
            "Yuanhuiyi Lyu",
            "Xu Zheng",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "title": "Don't Just Chase \"Highlighted Tokens\" in MLLMs: Revisiting Visual Holistic Context Retention",
        "abstract": "arXiv:2510.02912v1 Announce Type: new  Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs) suffer from considerable computational overhead due to their reliance on massive visual tokens. Recent studies have explored token pruning to alleviate this problem, which typically uses text-vision cross-attention or [\\texttt{CLS}] attention to assess and discard redundant visual tokens. In this work, we identify a critical limitation of such attention-first pruning approaches, i.e., they tend to preserve semantically similar tokens, resulting in pronounced performance drops under high pruning ratios. To this end, we propose {HoloV}, a simple yet effective, plug-and-play visual token pruning framework for efficient inference. Distinct from previous attention-first schemes, HoloV rethinks token retention from a holistic perspective. By adaptively distributing the pruning budget across different spatial crops, HoloV ensures that the retained tokens capture the global visual context rather than isolated salient features. This strategy minimizes representational collapse and maintains task-relevant information even under aggressive pruning. Experimental results demonstrate that our HoloV achieves superior performance across various tasks, MLLM architectures, and pruning ratios compared to SOTA methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\\% of the original performance after pruning 88.9\\% of visual tokens, achieving superior efficiency-accuracy trade-offs.",
        "arxiv_id": "2510.02912",
        "ARXIVID": "2510.02912",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a novel token pruning framework for MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.03153": {
        "authors": [
            "Hima Jacob Leven Suprabha",
            "Laxmi Nag Laxminarayan Nagesh",
            "Ajith Nair",
            "Alvin Reuben Amal Selvaster",
            "Ayan Khan",
            "Raghuram Damarla",
            "Sanju Hannah Samuel",
            "Sreenithi Saravana Perumal",
            "Titouan Puech",
            "Venkataramireddy Marella",
            "Vishal Sonar",
            "Alessandro Suglia",
            "Oliver Lemon"
        ],
        "title": "Improving Cooperation in Collaborative Embodied AI",
        "abstract": "arXiv:2510.03153v1 Announce Type: new  Abstract: The integration of Large Language Models (LLMs) into multiagent systems has opened new possibilities for collaborative reasoning and cooperation with AI agents. This paper explores different prompting methods and evaluates their effectiveness in enhancing agent collaborative behaviour and decision-making. We enhance CoELA, a framework designed for building Collaborative Embodied Agents that leverage LLMs for multi-agent communication, reasoning, and task coordination in shared virtual spaces. Through systematic experimentation, we examine different LLMs and prompt engineering strategies to identify optimised combinations that maximise collaboration performance. Furthermore, we extend our research by integrating speech capabilities, enabling seamless collaborative voice-based interactions. Our findings highlight the effectiveness of prompt optimisation in enhancing collaborative agent performance; for example, our best combination improved the efficiency of the system running with Gemma3 by 22% compared to the original CoELA system. In addition, the speech integration provides a more engaging user interface for iterative system development and demonstrations.",
        "arxiv_id": "2510.03153",
        "ARXIVID": "2510.03153",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on improving collaboration in embodied agents using LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.02599": {
        "authors": [
            "Hovhannes Margaryan",
            "Bo Wan",
            "Tinne Tuytelaars"
        ],
        "title": "PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization",
        "abstract": "arXiv:2510.02599v1 Announce Type: new  Abstract: This paper introduces a novel approach to aesthetic quality improvement in pre-trained text-to-image diffusion models when given a simple prompt. Our method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained text-to-image diffusion model as a backbone and optimizes the text embedding of a given simple and uncurated prompt to enhance the visual quality of the generated image. We achieve this by a tripartite objective function that improves the aesthetic fidelity of the generated image, ensures adherence to the optimized text embedding, and minimal divergence from the initial prompt. The latter is accomplished through a prompt preservation term. Additionally, PEO is training-free and backbone-independent. Quantitative and qualitative evaluations confirm the effectiveness of the proposed method, exceeding or equating the performance of state-of-the-art text-to-image and prompt adaptation methods.",
        "arxiv_id": "2510.02599",
        "ARXIVID": "2510.02599",
        "COMMENT": "This paper does not directly match any specific criterion but is related to generative modeling and aesthetic quality improvement in text-to-image diffusion models, which are of general interest to your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.02654": {
        "authors": [
            "Benjamin Yu",
            "Jackie Liu",
            "Justin Cui"
        ],
        "title": "Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models",
        "abstract": "arXiv:2510.02654v1 Announce Type: new  Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image generation. However, the deterministic nature of flow-matching models makes them poorly suited for reinforcement learning, a key tool for improving image quality and human alignment. Prior work has introduced stochasticity by perturbing latents with random noise, but such perturbations are inefficient and unstable. We propose Smart-GRPO, the first method to optimize noise perturbations for reinforcement learning in flow-matching models. Smart-GRPO employs an iterative search strategy that decodes candidate perturbations, evaluates them with a reward function, and refines the noise distribution toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves both reward optimization and visual quality compared to baseline methods. Our results suggest a practical path toward reinforcement learning in flow-matching frameworks, bridging the gap between efficient training and human-aligned generation.",
        "arxiv_id": "2510.02654",
        "ARXIVID": "2510.02654",
        "COMMENT": "This paper does not directly match any specific criterion but is tangentially related to reinforcement learning and generative modeling, which are of general interest to your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.03194": {
        "authors": [
            "Zichen Chen",
            "Jiefeng Chen",
            "Sercan \\\"O. Arik",
            "Misha Sra",
            "Tomas Pfister",
            "Jinsung Yoon"
        ],
        "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
        "abstract": "arXiv:2510.03194v1 Announce Type: new  Abstract: Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. In this paper, we reframe this challenge as a collaborative multi-agent problem. We introduce CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. We formalize this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows.",
        "arxiv_id": "2510.03194",
        "ARXIVID": "2510.03194",
        "COMMENT": "Does not closely match any specific criterion but is relevant to collaborative agentic systems and multimodal workflows.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.02789": {
        "authors": [
            "Ara Seo",
            "Bryan Sangwoo Kim",
            "Hyungjin Chung",
            "Jong Chul Ye"
        ],
        "title": "Align Your Query: Representation Alignment for Multimodality Medical Object Detection",
        "abstract": "arXiv:2510.02789v1 Announce Type: new  Abstract: Medical object detection suffers when a single detector is trained on mixed medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and disjoint representation spaces. To address this challenge, we turn to representation alignment, an approach that has proven effective for bringing features from different sources into a shared space. Specifically, we target the representations of DETR-style object queries and propose a simple, detector-agnostic framework to align them with modality context. First, we define modality tokens: compact, text-derived embeddings encoding imaging modality that are lightweight and require no extra annotations. We integrate the modality tokens into the detection process via Multimodality Context Attention (MoCA), mixing object-query representations via self-attention to propagate modality context within the query set. This preserves DETR-style architectures and adds negligible latency while injecting modality cues into object queries. We further introduce QueryREPA, a short pretraining stage that aligns query representations to their modality tokens using a task-specific contrastive objective with modality-balanced batches. Together, MoCA and QueryREPA produce modality-aware, class-faithful queries that transfer effectively to downstream training. Across diverse modalities trained altogether, the proposed approach consistently improves AP with minimal overhead and no architectural modifications, offering a practical path toward robust multimodality medical object detection. Project page: https://araseo.github.io/alignyourquery/.",
        "arxiv_id": "2510.02789",
        "ARXIVID": "2510.02789",
        "COMMENT": "Does not closely match any specific criterion but is relevant to multimodal learning in medical object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.02480": {
        "authors": [
            "Andrea Wynn",
            "Metod Jazbec",
            "Charith Peris",
            "Rinat Khaziev",
            "Anqi Liu",
            "Daniel Khashabi",
            "Eric Nalisnick"
        ],
        "title": "Safe and Efficient In-Context Learning via Risk Control",
        "abstract": "arXiv:2510.02480v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new tasks from a few in-context examples. However, this flexibility introduces safety concerns: LLMs can be influenced by incorrect or malicious demonstrations -- for example, if an adversary tampers with or injects harmful examples without a human supervisor noticing. This motivates principled designs in which the system itself includes built-in mechanisms to guard against such attacks. We propose a novel approach to limit the degree to which harmful demonstrations can degrade model performance. First, we define a baseline ``safe'' behavior for the model -- the model's performance given no in-context demonstrations (zero-shot). Next, we apply distribution-free risk control (DFRC) to control the extent to which in-context samples can decay performance below zero-shot. We achieve this by leveraging dynamic early exit prediction, ignoring later attention heads that attend the most to the unsafe inputs. Finally, we propose modifications to DFRC that allow it to both control risk for harmful inputs \\textit{and} leverage performance and efficiency gains on helpful inputs. We present both theoretical and empirical results showing that our approach can effectively control risk for harmful in-context demonstrations while simultaneously achieving substantial computational efficiency gains with helpful demonstrations.",
        "arxiv_id": "2510.02480",
        "ARXIVID": "2510.02480",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to LLM safety and efficiency improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.02611": {
        "authors": [
            "Yuheng Wu",
            "Azalia Mirhoseini",
            "Thierry Tambe"
        ],
        "title": "On the Role of Temperature Sampling in Test-Time Scaling",
        "abstract": "arXiv:2510.02611v1 Announce Type: new  Abstract: Large language models (LLMs) can improve reasoning at inference time through test-time scaling (TTS), where multiple reasoning traces are generated and the best one is selected. Prior work shows that increasing the number of samples K steadily improves accuracy. In this paper, we demonstrate that this trend does not hold indefinitely: at large K, further scaling yields no gains, and certain hard questions remain unsolved regardless of the number of traces. Interestingly, we find that different sampling temperatures solve different subsets of problems, implying that single-temperature scaling explores only part of a model's potential. We therefore propose scaling along the temperature dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3 (0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME 2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an additional 7.3 points over single-temperature TTS. Temperature scaling also enables base models to reach performance comparable to reinforcement learning (RL)-trained counterparts, without additional post-training. We further provide a comprehensive analysis of this phenomenon and design a multi-temperature voting method that reduces the overhead of temperature scaling. Overall, our findings suggest that TTS is more powerful than previously thought, and that temperature scaling offers a simple and effective way to unlock the latent potential of base models.",
        "arxiv_id": "2510.02611",
        "ARXIVID": "2510.02611",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to LLM advancements and reasoning strategies.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}