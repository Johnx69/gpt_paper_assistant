{
    "2507.11102": {
        "authors": [
            "Jie Yang",
            "Wang Zeng",
            "Sheng Jin",
            "Lumin Xu",
            "Wentao Liu",
            "Chen Qian",
            "Zhen Li",
            "Ruimao Zhang"
        ],
        "title": "KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model",
        "abstract": "arXiv:2507.11102v1 Announce Type: new  Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized image understanding by bridging textual and visual modalities. However, these models often struggle with capturing fine-grained semantic information, such as the precise identification and analysis of object keypoints. Keypoints, as structure-aware, pixel-level, and compact representations of objects, particularly articulated ones, play a crucial role in applications such as fine-grained image analysis, object retrieval, and behavior recognition. In this paper, we propose KptLLM++, a novel multimodal large language model that specifically designed for generic keypoint comprehension through the integration of diverse input modalities guided by user-defined instructions. By unifying keypoint detection across varied contexts, KptLLM++ establishes itself as an advanced interface, fostering more effective human-AI collaboration. The model is built upon a novel identify-then-detect paradigm, which first interprets keypoint semantics and subsequently localizes their precise positions through a structured chain-of-thought reasoning mechanism. To push the boundaries of performance, we have scaled up the training dataset to over 500K samples, encompassing diverse objects, keypoint categories, image styles, and scenarios with complex occlusions. This extensive scaling enables KptLLM++ to unlock its potential, achieving remarkable accuracy and generalization. Comprehensive experiments on multiple keypoint detection benchmarks demonstrate its state-of-the-art performance, underscoring its potential as a unified solution for fine-grained image understanding and its transformative implications for human-AI interaction.",
        "arxiv_id": "2507.11102",
        "ARXIVID": "2507.11102",
        "COMMENT": "Matches criterion 2 as it proposes a multimodal large language model (MLLM) for keypoint comprehension, focusing on vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.11334": {
        "authors": [
            "Yuehao Huang",
            "Liang Liu",
            "Shuangming Lei",
            "Yukai Ma",
            "Hao Su",
            "Jianbiao Mei",
            "Pengxiang Zhao",
            "Yaqing Gu",
            "Yong Liu",
            "Jiajun Lv"
        ],
        "title": "CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking",
        "abstract": "arXiv:2507.11334v1 Announce Type: new  Abstract: Mobile robots are increasingly required to navigate and interact within unknown and unstructured environments to meet human demands. Demand-driven navigation (DDN) enables robots to identify and locate objects based on implicit human intent, even when object locations are unknown. However, traditional data-driven DDN methods rely on pre-collected data for model training and decision-making, limiting their generalization capability in unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that emulates the human cognitive and learning mechanisms by integrating fast and slow thinking systems and selectively identifying key objects essential to fulfilling user demands. CogDDN identifies appropriate target objects by semantically aligning detected objects with the given instructions. Furthermore, it incorporates a dual-process decision-making module, comprising a Heuristic Process for rapid, efficient decisions and an Analytic Process that analyzes past errors, accumulates them in a knowledge base, and continuously improves performance. Chain of Thought (CoT) reasoning strengthens the decision-making process. Extensive closed-loop evaluations on the AI2Thor simulator with the ProcThor dataset show that CogDDN outperforms single-view camera-only methods by 15%, demonstrating significant improvements in navigation accuracy and adaptability. The project page is available at https://yuehaohuang.github.io/CogDDN/.",
        "arxiv_id": "2507.11334",
        "ARXIVID": "2507.11334",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for demand-driven navigation in embodied AI, addressing challenges in unstructured environments.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.10894": {
        "authors": [
            "Zongtao He",
            "Liuyi Wang",
            "Lu Chen",
            "Chengju Liu",
            "Qijun Chen"
        ],
        "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization",
        "abstract": "arXiv:2507.10894v1 Announce Type: new  Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.",
        "arxiv_id": "2507.10894",
        "ARXIVID": "2507.10894",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on generating navigation instructions for embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.10844": {
        "authors": [
            "Furkan Mumcu",
            "Michael J. Jones",
            "Anoop Cherian",
            "Yasin Yilmaz"
        ],
        "title": "LLM-Guided Agentic Object Detection for Open-World Understanding",
        "abstract": "arXiv:2507.10844v1 Announce Type: new  Abstract: Object detection traditionally relies on fixed category sets, requiring costly re-training to handle novel objects. While Open-World and Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting autonomy. We propose an LLM-guided agentic object detection (LAOD) framework that enables fully label-free, zero-shot detection by prompting a Large Language Model (LLM) to generate scene-specific object names. These are passed to an open-vocabulary detector for localization, allowing the system to adapt its goals dynamically. We introduce two new metrics, Class-Agnostic Average Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD validate our approach, showing strong performance in detecting and naming novel objects. Our method offers enhanced autonomy and adaptability for open-world understanding.",
        "arxiv_id": "2507.10844",
        "ARXIVID": "2507.10844",
        "COMMENT": "Matches criterion 2 as it explores a framework integrating LLMs for open-world object detection, which involves vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.10935": {
        "authors": [
            "Shaowen Tong",
            "Zimin Xia",
            "Alexandre Alahi",
            "Xuming He",
            "Yujiao Shi"
        ],
        "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization",
        "abstract": "arXiv:2507.10935v1 Announce Type: new  Abstract: Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.",
        "arxiv_id": "2507.10935",
        "ARXIVID": "2507.10935",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a weakly supervised framework for cross-view localization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.11245": {
        "authors": [
            "X. Feng",
            "H. Yu",
            "M. Wu",
            "S. Hu",
            "J. Chen",
            "C. Zhu",
            "J. Wu",
            "X. Chu",
            "K. Huang"
        ],
        "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models",
        "abstract": "arXiv:2507.11245v1 Announce Type: new  Abstract: With the rapid development of foundation video generation technologies, long video generation models have exhibited promising research potential thanks to expanded content creation space. Recent studies reveal that the goal of long video generation tasks is not only to extend video duration but also to accurately express richer narrative content within longer videos. However, due to the lack of evaluation benchmarks specifically designed for long video generation models, the current assessment of these models primarily relies on benchmarks with simple narrative prompts (e.g., VBench). To the best of our knowledge, our proposed NarrLV is the first benchmark to comprehensively evaluate the Narrative expression capabilities of Long Video generation models. Inspired by film narrative theory, (i) we first introduce the basic narrative unit maintaining continuous visual presentation in videos as Temporal Narrative Atom (TNA), and use its count to quantitatively measure narrative richness. Guided by three key film narrative elements influencing TNA changes, we construct an automatic prompt generation pipeline capable of producing evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based on the three progressive levels of narrative content expression, we design an effective evaluation metric using the MLLM-based question generation and answering framework. (iii) Finally, we conduct extensive evaluations on existing long video generation models and the foundation generation models. Experimental results demonstrate that our metric aligns closely with human judgments. The derived evaluation outcomes reveal the detailed capability boundaries of current video generation models in narrative content expression.",
        "arxiv_id": "2507.11245",
        "ARXIVID": "2507.11245",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for evaluating narrative capabilities in long video generation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.11522": {
        "authors": [
            "Tariq Mehmood",
            "Hamza Ahmad",
            "Muhammad Haroon Shakeel",
            "Murtaza Taj"
        ],
        "title": "CATVis: Context-Aware Thought Visualization",
        "abstract": "arXiv:2507.11522v1 Announce Type: new  Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic alignment and image quality.",
        "arxiv_id": "2507.11522",
        "ARXIVID": "2507.11522",
        "COMMENT": "Matches criterion 5 as it combines EEG-based image generation with large language models, showcasing multimodal integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.11003": {
        "authors": [
            "Yuhu Bai",
            "Jiangning Zhang",
            "Yunkang Cao",
            "Guangyuan Lu",
            "Qingdong He",
            "Xiangtai Li",
            "Guanzhong Tian"
        ],
        "title": "Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection",
        "abstract": "arXiv:2507.11003v1 Announce Type: new  Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in recent research, where the rare classes are essential and expected in many applications. This study introduces \\textbf{FiSeCLIP} for ZSAD with training-free \\textbf{CLIP}, combining the feature matching with the cross-modal alignment. Testing with the entire dataset is impractical, while batch-based testing better aligns with real industrial needs, and images within a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes other images in the same batch as reference information for the current image. However, the lack of labels for these references can introduce ambiguity, we apply text information to \\textbf{fi}lter out noisy features. In addition, we further explore CLIP's inherent potential to restore its local \\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection tasks to enable a more accurate filtering process. Our approach exhibits superior performance for both anomaly classification and segmentation on anomaly detection benchmarks, building a stronger baseline for the direction, e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by +4.6\\%$\\uparrow$/+5.7\\%$\\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.",
        "arxiv_id": "2507.11003",
        "ARXIVID": "2507.11003",
        "COMMENT": "Matches criterion 5 as it explores zero-shot anomaly detection using vision-language models, combining image understanding and language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.11539": {
        "authors": [
            "Dong Zhuo",
            "Wenzhao Zheng",
            "Jiahe Guo",
            "Yuqi Wu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Streaming 4D Visual Geometry Transformer",
        "abstract": "arXiv:2507.11539v1 Announce Type: new  Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.",
        "arxiv_id": "2507.11539",
        "ARXIVID": "2507.11539",
        "COMMENT": "Matches criterion 6 as it focuses on 4D video geometry perception and reconstruction, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.11075": {
        "authors": [
            "Chang Peng",
            "Yifei Zhou",
            "Huifeng Xi",
            "Shiqing Huang",
            "Chuangye Chen",
            "Jianming Yang",
            "Bao Yang",
            "Zhenyu Jiang"
        ],
        "title": "Joint angle model based learning to refine kinematic human pose estimation",
        "abstract": "arXiv:2507.11075v1 Announce Type: new  Abstract: Marker-free human pose estimation (HPE) has found increasing applications in various fields. Current HPE suffers from occasional errors in keypoint recognition and random fluctuation in keypoint trajectories when analyzing kinematic human poses. The performance of existing deep learning-based models for HPE refinement is considerably limited by inaccurate training datasets in which the keypoints are manually annotated. This paper proposed a novel method to overcome the difficulty through joint angle-based modeling. The key techniques include: (i) A joint angle-based model of human pose, which is robust to describe kinematic human poses; (ii) Approximating temporal variation of joint angles through high order Fourier series to get reliable \"ground truth\"; (iii) A bidirectional recurrent network is designed as a post-processing module to refine the estimation of well-established HRNet. Trained with the high-quality dataset constructed using our method, the network demonstrates outstanding performance to correct wrongly recognized joints and smooth their spatiotemporal trajectories. Tests show that joint angle-based refinement (JAR) outperforms the state-of-the-art HPE refinement network in challenging cases like figure skating and breaking.",
        "arxiv_id": "2507.11075",
        "ARXIVID": "2507.11075",
        "COMMENT": "Matches criterion 1 as it presents a novel method for refining kinematic human pose estimation, which involves spatial reasoning for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.11540": {
        "authors": [
            "Zhen Xu",
            "Hongyu Zhou",
            "Sida Peng",
            "Haotong Lin",
            "Haoyu Guo",
            "Jiahao Shao",
            "Peishan Yang",
            "Qinglin Yang",
            "Sheng Miao",
            "Xingyi He",
            "Yifan Wang",
            "Yue Wang",
            "Ruizhen Hu",
            "Yiyi Liao",
            "Xiaowei Zhou",
            "Hujun Bao"
        ],
        "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation",
        "abstract": "arXiv:2507.11540v1 Announce Type: new  Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for applications such as 3D reconstruction, free-viewpoint rendering, robotics, autonomous driving, and AR/VR technologies. Traditional methods relying on hardware sensors like LiDAR are often limited by high costs, low resolution, and environmental sensitivity, limiting their applicability in real-world scenarios. Recent advances in vision-based methods offer a promising alternative, yet they face challenges in generalization and stability due to either the low-capacity model architectures or the reliance on domain-specific and small-scale datasets. The emergence of scaling laws and foundation models in other domains has inspired the development of \"depth foundation models\": deep neural networks trained on large datasets with strong zero-shot generalization capabilities. This paper surveys the evolution of deep learning architectures and paradigms for depth estimation across the monocular, stereo, multi-view, and monocular video settings. We explore the potential of these models to address existing challenges and provide a comprehensive overview of large-scale datasets that can facilitate their development. By identifying key architectures and training strategies, we aim to highlight the path towards robust depth foundation models, offering insights into their future research and applications.",
        "arxiv_id": "2507.11540",
        "ARXIVID": "2507.11540",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on depth estimation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.10938": {
        "authors": [
            "Zhengyi Xu",
            "Haoran Wu",
            "Wen Jiang",
            "Jie Geng"
        ],
        "title": "Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing",
        "abstract": "arXiv:2507.10938v1 Announce Type: new  Abstract: Semantic change detection (SCD) extends the binary change detection task to provide not only the change locations but also the detailed \"from-to\" categories in multi-temporal remote sensing data. Such detailed semantic insights into changes offer considerable advantages for a wide array of applications. However, since SCD involves the simultaneous optimization of multiple tasks, the model is prone to negative transfer due to task-specific learning difficulties and conflicting gradient flows. To address this issue, we propose Graph Aggregation Prototype Learning for Semantic Change Detection in remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization method is designed to optimize the primary task of semantic segmentation and change detection, along with the auxiliary task of graph aggregation prototype learning. Adaptive weight allocation and gradient rotation methods are used to alleviate the conflict between training tasks and improve multi-task learning capabilities. Specifically, the graph aggregation prototype learning module constructs an interaction graph using high-level features. Prototypes serve as class proxies, enabling category-level domain alignment across time points and reducing interference from irrelevant changes. Additionally, the proposed self-query multi-level feature interaction and bi-temporal feature fusion modules further enhance multi-scale feature representation, improving performance in complex scenes. Experimental results on the SECOND and Landsat-SCD datasets demonstrate that our method achieves state-of-the-art performance, with significant improvements in accuracy and robustness for SCD task.",
        "arxiv_id": "2507.10938",
        "ARXIVID": "2507.10938",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for semantic change detection in remote sensing, which is relevant to embodied/robotic AI benchmarks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.11030": {
        "authors": [
            "Sunghyun Park",
            "Jungsoo Lee",
            "Shubhankar Borse",
            "Munawar Hayat",
            "Sungha Choi",
            "Kyuwoong Hwang",
            "Fatih Porikli"
        ],
        "title": "Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation",
        "abstract": "arXiv:2507.11030v1 Announce Type: new  Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into semantic regions based on arbitrarily given text descriptions even for classes unseen during training, it fails to understand personal texts (e.g., `my mug cup') for segmenting regions of specific interest to users. This paper addresses challenges like recognizing `my mug cup' among `multiple mug cups'. To overcome this challenge, we introduce a novel task termed \\textit{personalized open-vocabulary semantic segmentation} and propose a text prompt tuning-based plug-in method designed to recognize personal visual concepts using a few pairs of images and masks, while maintaining the performance of the original OVSS. Based on the observation that reducing false predictions is essential when applying text prompt tuning to this task, our proposed method employs `negative mask proposal' that captures visual concepts other than the personalized concept. We further improve the performance by enriching the representation of text prompts by injecting visual embeddings of the personal concept into them. This approach enhances personalized OVSS without compromising the original OVSS performance. We demonstrate the superiority of our method on our newly established benchmarks for this task, including FSS$^\\text{per}$, CUB$^\\text{per}$, and ADE$^\\text{per}$.",
        "arxiv_id": "2507.11030",
        "ARXIVID": "2507.11030",
        "COMMENT": "Matches criterion 2 as it explores open-vocabulary semantic segmentation with personalized text prompts, which involves vision-language integration.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.11153": {
        "authors": [
            "Hongfei Ye",
            "Bin Chen",
            "Wenxi Liu",
            "Yu Zhang",
            "Zhao Li",
            "Dandan Ni",
            "Hongyang Chen"
        ],
        "title": "Assessing Color Vision Test in Large Vision-language Models",
        "abstract": "arXiv:2507.11153v1 Announce Type: new  Abstract: With the widespread adoption of large vision-language models, the capacity for color vision in these models is crucial. However, the color vision abilities of large visual-language models have not yet been thoroughly explored. To address this gap, we define a color vision testing task for large vision-language models and construct a dataset \\footnote{Anonymous Github Showing some of the data https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers multiple categories of test questions and tasks of varying difficulty levels. Furthermore, we analyze the types of errors made by large vision-language models and propose fine-tuning strategies to enhance their performance in color vision tests.",
        "arxiv_id": "2507.11153",
        "ARXIVID": "2507.11153",
        "COMMENT": "Matches criterion 2 as it evaluates color vision capabilities in large vision-language models, which involves vision-language integration.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.11200": {
        "authors": [
            "Che Liu",
            "Jiazhen Pan",
            "Weixiang Shen",
            "Wenjia Bai",
            "Daniel Rueckert",
            "Rossella Arcucci"
        ],
        "title": "How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study",
        "abstract": "arXiv:2507.11200v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural image tasks and are increasingly repurposed for healthcare; however, their competence in medical tasks remains underexplored. We present a comprehensive evaluation of open-source general-purpose and medically specialised VLMs, ranging from 3B to 72B parameters, across eight benchmarks: MedXpert, OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model performance across different aspects, we first separate it into understanding and reasoning components. Three salient findings emerge. First, large general-purpose models already match or surpass medical-specific counterparts on several benchmarks, demonstrating strong zero-shot transfer from natural to medical images. Second, reasoning performance is consistently lower than understanding, highlighting a critical barrier to safe decision support. Third, performance varies widely across benchmarks, reflecting differences in task design, annotation quality, and knowledge demands. No model yet reaches the reliability threshold for clinical deployment, underscoring the need for stronger multimodal alignment and more rigorous, fine-grained evaluation protocols.",
        "arxiv_id": "2507.11200",
        "ARXIVID": "2507.11200",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models in medical tasks, focusing on their reasoning and understanding capabilities.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.10644": {
        "authors": [
            "Tatiana Petrova (SEDAN SnT",
            "University of Luxembourg",
            "Luxembourg",
            "Luxembourg)",
            "Aleksandr Puzikov (SEDAN SnT",
            "University of Luxembourg",
            "Luxembourg",
            "Luxembourg)",
            "Boris Bliznukov (SEDAN SnT",
            "University of Luxembourg",
            "Luxembourg",
            "Luxembourg)",
            "Radu State (SEDAN SnT",
            "University of Luxembourg",
            "Luxembourg",
            "Luxembourg)"
        ],
        "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents",
        "abstract": "arXiv:2507.10644v1 Announce Type: new  Abstract: The concept of the Web of Agents (WoA), which transforms the static, document-centric Web into an environment of autonomous agents acting on users' behalf, has attracted growing interest as large language models (LLMs) become more capable. However, research in this area is still fragmented across different communities. Contemporary surveys catalog the latest LLM-powered frameworks, while the rich histories of Multi-Agent Systems (MAS) and the Semantic Web are often treated as separate, legacy domains. This fragmentation obscures the intellectual lineage of modern systems and hinders a holistic understanding of the field's trajectory. We present the first comprehensive evolutionary overview of the WoA. We show that modern protocols like A2A and the MCP, are direct evolutionary responses to the well-documented limitations of earlier standards like FIPA standards and OWL-based semantic agents. To systematize this analysis, we introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism). This framework provides a unified analytical lens for comparing agent architectures across all generations, revealing a clear line of descent where others have seen a disconnect. Our analysis identifies a paradigm shift in the 'locus of intelligence': from being encoded in external data (Semantic Web) or the platform (MAS) to being embedded within the agent's core model (LLM). This shift is foundational to modern Agentic AI, enabling the scalable and adaptive systems the WoA has long envisioned. We conclude that while new protocols are essential, they are insufficient for building a robust, open, trustworthy ecosystem. Finally, we argue that the next research frontier lies in solving persistent socio-technical challenges, and we map out a new agenda focused on decentralized identity, economic models, security, and governance for the emerging WoA.",
        "arxiv_id": "2507.10644",
        "ARXIVID": "2507.10644",
        "COMMENT": "This paper does not match any specific criteria but provides a survey on the evolution of agentic AI, which is tangentially related to embodied agents and multi-agent systems.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.10855": {
        "authors": [
            "Wei Chen",
            "Jingxi Yu",
            "Zichen Miao",
            "Qiang Qiu"
        ],
        "title": "Sparse Fine-Tuning of Transformers for Generative Tasks",
        "abstract": "arXiv:2507.10855v1 Announce Type: new  Abstract: Large pre-trained transformers have revolutionized artificial intelligence across various domains, and fine-tuning remains the dominant approach for adapting these models to downstream tasks due to the cost of training from scratch. However, in existing fine-tuning methods, the updated representations are formed as a dense combination of modified parameters, making it challenging to interpret their contributions and understand how the model adapts to new tasks. In this work, we introduce a fine-tuning framework inspired by sparse coding, where fine-tuned features are represented as a sparse combination of basic elements, i.e., feature dictionary atoms. The feature dictionary atoms function as fundamental building blocks of the representation, and tuning atoms allows for seamless adaptation to downstream tasks. Sparse coefficients then serve as indicators of atom importance, identifying the contribution of each atom to the updated representation. Leveraging the atom selection capability of sparse coefficients, we first demonstrate that our method enhances image editing performance by improving text alignment through the removal of unimportant feature dictionary atoms. Additionally, we validate the effectiveness of our approach in the text-to-image concept customization task, where our method efficiently constructs the target concept using a sparse combination of feature dictionary atoms, outperforming various baseline fine-tuning methods.",
        "arxiv_id": "2507.10855",
        "ARXIVID": "2507.10855",
        "COMMENT": "Does not match any specific criteria but discusses sparse fine-tuning for generative tasks, which is tangentially related to large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.11252": {
        "authors": [
            "Guanghao Wu",
            "Chen Xu",
            "Hai Song",
            "Chong Wang",
            "Qixing Zhang"
        ],
        "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection",
        "abstract": "arXiv:2507.11252v1 Announce Type: new  Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image captions.Then, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask edges.Finally, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at https://github.com/wghr123/MFGDiffusion.",
        "arxiv_id": "2507.11252",
        "ARXIVID": "2507.11252",
        "COMMENT": "This paper does not match any specific criteria but involves image generation and multimodal filtering, which is tangentially related to generative modeling and multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.11152": {
        "authors": [
            "Duoyou Chen",
            "Yunqing Chen",
            "Can Zhang",
            "Zhou Wang",
            "Cheng Chen",
            "Ruoxiu Xiao"
        ],
        "title": "Latent Space Consistency for Sparse-View CT Reconstruction",
        "abstract": "arXiv:2507.11152v1 Announce Type: new  Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research and applications in other domains.",
        "arxiv_id": "2507.11152",
        "ARXIVID": "2507.11152",
        "COMMENT": "This paper does not match any specific criteria but involves cross-modal transformation tasks, which is tangentially related to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.11473": {
        "authors": [
            "Tomek Korbak",
            "Mikita Balesni",
            "Elizabeth Barnes",
            "Yoshua Bengio",
            "Joe Benton",
            "Joseph Bloom",
            "Mark Chen",
            "Alan Cooney",
            "Allan Dafoe",
            "Anca Dragan",
            "Scott Emmons",
            "Owain Evans",
            "David Farhi",
            "Ryan Greenblatt",
            "Dan Hendrycks",
            "Marius Hobbhahn",
            "Evan Hubinger",
            "Geoffrey Irving",
            "Erik Jenner",
            "Daniel Kokotajlo",
            "Victoria Krakovna",
            "Shane Legg",
            "David Lindner",
            "David Luan",
            "Aleksander M\\k{a}dry",
            "Julian Michael",
            "Neel Nanda",
            "Dave Orr",
            "Jakub Pachocki",
            "Ethan Perez",
            "Mary Phuong",
            "Fabien Roger",
            "Joshua Saxe",
            "Buck Shlegeris",
            "Mart\\'in Soto",
            "Eric Steinberger",
            "Jasmine Wang",
            "Wojciech Zaremba",
            "Bowen Baker",
            "Rohin Shah",
            "Vlad Mikulik"
        ],
        "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety",
        "abstract": "arXiv:2507.11473v1 Announce Type: new  Abstract: AI systems that \"think\" in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and we recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, we recommend that frontier model developers consider the impact of development decisions on CoT monitorability.",
        "arxiv_id": "2507.11473",
        "ARXIVID": "2507.11473",
        "COMMENT": "Does not match any specific criteria. Focuses on AI safety and chain-of-thought monitoring, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.11443": {
        "authors": [
            "Haoran Wang",
            "Hanyu Pei",
            "Yang Lyu",
            "Kai Zhang",
            "Li Li",
            "Feng-Lei Fan"
        ],
        "title": "COLI: A Hierarchical Efficient Compressor for Large Images",
        "abstract": "arXiv:2507.11443v1 Announce Type: new  Abstract: The escalating adoption of high-resolution, large-field-of-view imagery amplifies the need for efficient compression methodologies. Conventional techniques frequently fail to preserve critical image details, while data-driven approaches exhibit limited generalizability. Implicit Neural Representations (INRs) present a promising alternative by learning continuous mappings from spatial coordinates to pixel intensities for individual images, thereby storing network weights rather than raw pixels and avoiding the generalization problem. However, INR-based compression of large images faces challenges including slow compression speed and suboptimal compression ratios. To address these limitations, we introduce COLI (Compressor for Large Images), a novel framework leveraging Neural Representations for Videos (NeRV). First, recognizing that INR-based compression constitutes a training process, we accelerate its convergence through a pretraining-finetuning paradigm, mixed-precision training, and reformulation of the sequential loss into a parallelizable objective. Second, capitalizing on INRs' transformation of image storage constraints into weight storage, we implement Hyper-Compression, a novel post-training technique to substantially enhance compression ratios while maintaining minimal output distortion. Evaluations across two medical imaging datasets demonstrate that COLI consistently achieves competitive or superior PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while accelerating NeRV training by up to 4 times.",
        "arxiv_id": "2507.11443",
        "ARXIVID": "2507.11443",
        "COMMENT": "Does not match any specific criterion but is relevant to image compression and neural representations.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.11061": {
        "authors": [
            "Hayeon Kim",
            "Ji Ha Jang",
            "Se Young Chun"
        ],
        "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
        "abstract": "arXiv:2507.11061v1 Announce Type: new  Abstract: Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing.",
        "arxiv_id": "2507.11061",
        "ARXIVID": "2507.11061",
        "COMMENT": "Does not match any specific criterion but is relevant to 3D editing and Gaussian splatting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10923": {
        "authors": [
            "Yuhao Wang",
            "Keyan Ding",
            "Kehua Feng",
            "Zeyuan Wang",
            "Ming Qin",
            "Xiaotong Li",
            "Qiang Zhang",
            "Huajun Chen"
        ],
        "title": "Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization",
        "abstract": "arXiv:2507.10923v1 Announce Type: new  Abstract: Protein language models have emerged as powerful tools for sequence generation, offering substantial advantages in functional optimization and denovo design. However, these models also present significant risks of generating harmful protein sequences, such as those that enhance viral transmissibility or evade immune responses. These concerns underscore critical biosafety and ethical challenges. To address these issues, we propose a Knowledge-guided Preference Optimization (KPO) framework that integrates prior knowledge via a Protein Safety Knowledge Graph. This framework utilizes an efficient graph pruning strategy to identify preferred sequences and employs reinforcement learning to minimize the risk of generating harmful proteins. Experimental results demonstrate that KPO effectively reduces the likelihood of producing hazardous sequences while maintaining high functionality, offering a robust safety assurance framework for applying generative models in biotechnology.",
        "arxiv_id": "2507.10923",
        "ARXIVID": "2507.10923",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in biotechnology.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.11321": {
        "authors": [
            "Haoxuan Qu",
            "Yujun Cai",
            "Hossein Rahmani",
            "Ajay Kumar",
            "Junsong Yuan",
            "Jun Liu"
        ],
        "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction",
        "abstract": "arXiv:2507.11321v1 Announce Type: new  Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.",
        "arxiv_id": "2507.11321",
        "ARXIVID": "2507.11321",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and 3D surface reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10943": {
        "authors": [
            "Yushun Fang",
            "Lu Liu",
            "Xiang Gao",
            "Qiang Hu",
            "Ning Cao",
            "Jianghe Cui",
            "Gang Chen",
            "Xiaoyun Zhang"
        ],
        "title": "Robust ID-Specific Face Restoration via Alignment Learning",
        "abstract": "arXiv:2507.10943v1 Announce Type: new  Abstract: The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.",
        "arxiv_id": "2507.10943",
        "ARXIVID": "2507.10943",
        "COMMENT": "Does not match any specific criteria but discusses face restoration using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.11372": {
        "authors": [
            "Pierrick Leroy",
            "Antonio Mastropietro",
            "Marco Nurisso",
            "Francesco Vaccarino"
        ],
        "title": "Attributes Shape the Embedding Space of Face Recognition Models",
        "abstract": "arXiv:2507.11372v1 Announce Type: new  Abstract: Face Recognition (FR) tasks have made significant progress with the advent of Deep Neural Networks, particularly through margin-based triplet losses that embed facial images into high-dimensional feature spaces. During training, these contrastive losses focus exclusively on identity information as labels. However, we observe a multiscale geometric structure emerging in the embedding space, influenced by interpretable facial (e.g., hair color) and image attributes (e.g., contrast). We propose a geometric approach to describe the dependence or invariance of FR models to these attributes and introduce a physics-inspired alignment metric. We evaluate the proposed metric on controlled, simplified models and widely used FR models fine-tuned with synthetic data for targeted attribute augmentation. Our findings reveal that the models exhibit varying degrees of invariance across different attributes, providing insight into their strengths and weaknesses and enabling deeper interpretability. Code available here: https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs",
        "arxiv_id": "2507.11372",
        "ARXIVID": "2507.11372",
        "COMMENT": "Does not match any specific criteria but discusses interpretability in face recognition models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.11143": {
        "authors": [
            "Lam Pham",
            "Cam Le",
            "Hieu Tang",
            "Khang Truong",
            "Truong Nguyen",
            "Jasmin Lampert",
            "Alexander Schindler",
            "Martin Boyer",
            "Son Phan"
        ],
        "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images",
        "abstract": "arXiv:2507.11143v1 Announce Type: new  Abstract: In recent years, landslide disasters have reported frequently due to the extreme weather events of droughts, floods , storms, or the consequence of human activities such as deforestation, excessive exploitation of natural resources. However, automatically observing landslide is challenging due to the extremely large observing area and the rugged topography such as mountain or highland. This motivates us to propose an end-to-end deep-learning-based model which explores the remote sensing images for automatically observing landslide events. By considering remote sensing images as the input data, we can obtain free resource, observe large and rough terrains by time. To explore the remote sensing images, we proposed a novel neural network architecture which is for two tasks of landslide detection and landslide segmentation. We evaluated our proposed model on three different benchmark datasets of LandSlide4Sense, Bijie, and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23, 93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense, Nepal datasets. These experimental results prove potential to integrate our proposed model into real-life landslide observation systems.",
        "arxiv_id": "2507.11143",
        "ARXIVID": "2507.11143",
        "COMMENT": "This paper does not match any specific criteria but involves computer vision applications for landslide detection, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.11247": {
        "authors": [
            "Veronika Shilova",
            "Emmanuel Malherbe",
            "Giovanni Palma",
            "Laurent Risser",
            "Jean-Michel Loubes"
        ],
        "title": "Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone",
        "abstract": "arXiv:2507.11247v1 Announce Type: new  Abstract: Within a legal framework, fairness in datasets and models is typically assessed by dividing observations into predefined groups and then computing fairness measures (e.g., Disparate Impact or Equality of Odds with respect to gender). However, when sensitive attributes such as skin color are continuous, dividing into default groups may overlook or obscure the discrimination experienced by certain minority subpopulations. To address this limitation, we propose a fairness-based grouping approach for continuous (possibly multidimensional) sensitive attributes. By grouping data according to observed levels of discrimination, our method identifies the partition that maximizes a novel criterion based on inter-group variance in discrimination, thereby isolating the most critical subgroups.   We validate the proposed approach using multiple synthetic datasets and demonstrate its robustness under changing population distributions - revealing how discrimination is manifested within the space of sensitive attributes. Furthermore, we examine a specialized setting of monotonic fairness for the case of skin color. Our empirical results on both CelebA and FFHQ, leveraging the skin tone as predicted by an industrial proprietary algorithm, show that the proposed segmentation uncovers more nuanced patterns of discrimination than previously reported, and that these findings remain stable across datasets for a given model. Finally, we leverage our grouping model for debiasing purpose, aiming at predicting fair scores with group-by-group post-processing. The results demonstrate that our approach improves fairness while having minimal impact on accuracy, thus confirming our partition method and opening the door for industrial deployment.",
        "arxiv_id": "2507.11247",
        "ARXIVID": "2507.11247",
        "COMMENT": "This paper does not match any specific criteria but discusses fairness in datasets and models, which is tangentially related to machine learning and societal impacts.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.10846": {
        "authors": [
            "Casey Wall",
            "Longwei Wang",
            "Rodrigue Rizk",
            "KC Santosh"
        ],
        "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization",
        "abstract": "arXiv:2507.10846v1 Announce Type: new  Abstract: Interpreting the decision-making process of Convolutional Neural Networks (CNNs) is critical for deploying models in high-stakes domains. Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method for visual explanations, yet it typically focuses on the final convolutional layer or na\\\"ively averages across layers, strategies that can obscure important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a novel, human-tunable extension of Grad-CAM that generates robust and coherent saliency maps by aggregating information across all convolutional layers. To mitigate the influence of noisy or extreme attribution values, Winsor-CAM applies Winsorization, a percentile-based outlier attenuation technique. A user-controllable threshold allows for semantic-level tuning, enabling flexible exploration of model behavior across representational hierarchies. Evaluations on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable heatmaps and achieves superior performance in localization metrics, including intersection-over-union and center-of-mass alignment, when compared to Grad-CAM and uniform layer-averaging baselines. Winsor-CAM advances the goal of trustworthy AI by offering interpretable, multi-layer insights with human-in-the-loop control.",
        "arxiv_id": "2507.10846",
        "ARXIVID": "2507.10846",
        "COMMENT": "This paper does not match any specific criteria but focuses on interpretability in vision models, which is tangentially related to computer vision and trustworthy AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.11135": {
        "authors": [
            "Selma Saidi",
            "Omar Laimona",
            "Christoph Schmickler",
            "Dirk Ziegenbein"
        ],
        "title": "Collaborative Trustworthiness for Good Decision Making in Autonomous Systems",
        "abstract": "arXiv:2507.11135v1 Announce Type: new  Abstract: Autonomous systems are becoming an integral part of many application domains, like in the mobility sector. However, ensuring their safe and correct behaviour in dynamic and complex environments remains a significant challenge, where systems should autonomously make decisions e.g., about manoeuvring. We propose in this paper a general collaborative approach for increasing the level of trustworthiness in the environment of operation and improve reliability and good decision making in autonomous system. In the presence of conflicting information, aggregation becomes a major issue for trustworthy decision making based on collaborative data sharing. Unlike classical approaches in the literature that rely on consensus or majority as aggregation rule, we exploit the fact that autonomous systems have different quality attributes like perception quality. We use this criteria to determine which autonomous systems are trustworthy and borrow concepts from social epistemology to define aggregation and propagation rules, used for automated decision making. We use Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and propagation, and formulate reduction rules to reduce the size of the BDDs and allow efficient computation structures for collaborative automated reasoning.",
        "arxiv_id": "2507.11135",
        "ARXIVID": "2507.11135",
        "COMMENT": "This paper does not match any specific criteria but discusses decision-making in autonomous systems, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.11081": {
        "authors": [
            "Chang Peng",
            "Bao Yang",
            "Meiqi Li",
            "Ge Zhang",
            "Hui Sun",
            "Zhenyu Jiang"
        ],
        "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification",
        "abstract": "arXiv:2507.11081v1 Announce Type: new  Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive solution for road subsurface distress (RSD) detection. However, RSD recognition from GPR images is labor-intensive and heavily relies on inspectors' expertise. Deep learning offers the possibility for automatic RSD recognition, but its current performance is limited by two factors: Scarcity of high-quality dataset for network training and insufficient capability of network to distinguish RSD. In this study, a rigorously validated 3D GPR dataset containing 2134 samples of diverse types was constructed through field scanning. Based on the finding that the YOLO model trained with one of the three scans of GPR images exhibits varying sensitivity to specific type of RSD, we proposed a novel cross-verification strategy with outstanding accuracy in RSD recognition, achieving recall over 98.6% in field tests. The approach, integrated into an online RSD detection system, can reduce the labor of inspection by around 90%.",
        "arxiv_id": "2507.11081",
        "ARXIVID": "2507.11081",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and deep learning applications in GPR image analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.10798": {
        "authors": [
            "Asim H. Gazi",
            "Bhanu T. Gullapalli",
            "Daiqi Gao",
            "Benjamin M. Marlin",
            "Vivek Shetty",
            "Susan A. Murphy"
        ],
        "title": "Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions",
        "abstract": "arXiv:2507.10798v1 Announce Type: new  Abstract: Timely decision making is critical to the effectiveness of mobile health (mHealth) interventions. At predefined timepoints called \"decision points,\" intelligent mHealth systems such as just-in-time adaptive interventions (JITAIs) estimate an individual's biobehavioral context from sensor or survey data and determine whether and how to intervene. For interventions targeting habitual behavior (e.g., oral hygiene), effectiveness often hinges on delivering support shortly before the target behavior is likely to occur. Current practice schedules decision points at a fixed interval (e.g., one hour) before user-provided behavior times, and the fixed interval is kept the same for all individuals. However, this one-size-fits-all approach performs poorly for individuals with irregular routines, often scheduling decision points after the target behavior has already occurred, rendering interventions ineffective. In this paper, we propose SigmaScheduling, a method to dynamically schedule decision points based on uncertainty in predicted behavior times. When behavior timing is more predictable, SigmaScheduling schedules decision points closer to the predicted behavior time; when timing is less certain, SigmaScheduling schedules decision points earlier, increasing the likelihood of timely intervention. We evaluated SigmaScheduling using real-world data from 68 participants in a 10-week trial of Oralytics, a JITAI designed to improve daily toothbrushing. SigmaScheduling increased the likelihood that decision points preceded brushing events in at least 70% of cases, preserving opportunities to intervene and impact behavior. Our results indicate that SigmaScheduling can advance precision mHealth, particularly for JITAIs targeting time-sensitive, habitual behaviors such as oral hygiene or dietary habits.",
        "arxiv_id": "2507.10798",
        "ARXIVID": "2507.10798",
        "COMMENT": "Does not match any specific criterion but is relevant to decision-making systems and intelligent interventions.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}