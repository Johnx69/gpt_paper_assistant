{
    "2507.18713": {
        "authors": [
            "Yun Chen",
            "Matthew Haines",
            "Jingkang Wang",
            "Krzysztof Baron-Lis",
            "Sivabalan Manivasagam",
            "Ze Yang",
            "Raquel Urtasun"
        ],
        "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time",
        "abstract": "arXiv:2507.18713v1 Announce Type: new  Abstract: High-fidelity sensor simulation of light-based sensors such as cameras and LiDARs is critical for safe and accurate autonomy testing. Neural radiance field (NeRF)-based methods that reconstruct sensor observations via ray-casting of implicit representations have demonstrated accurate simulation of driving scenes, but are slow to train and render, hampering scale. 3D Gaussian Splatting (3DGS) has demonstrated faster training and rendering times through rasterization, but is primarily restricted to pinhole camera sensors, preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both NeRF and 3DGS couple the representation with the rendering procedure (implicit networks for ray-based evaluation, particles for rasterization), preventing interoperability, which is key for general usage. In this work, we present Sparse Local Fields (SaLF), a novel volumetric representation that supports rasterization and raytracing. SaLF represents volumes as a sparse set of 3D voxel primitives, where each voxel is a local implicit field. SaLF has fast training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS LiDAR), has adaptive pruning and densification to easily handle large scenes, and can support non-pinhole cameras and spinning LiDARs. We demonstrate that SaLF has similar realism as existing self-driving sensor simulation methods while improving efficiency and enhancing capabilities, enabling more scalable simulation. https://waabi.ai/salf/",
        "arxiv_id": "2507.18713",
        "ARXIVID": "2507.18713",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for real-time multi-sensor rendering, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18678": {
        "authors": [
            "Xingyu Miao",
            "Haoran Duan",
            "Quanhao Qian",
            "Jiuniu Wang",
            "Yang Long",
            "Ling Shao",
            "Deli Zhao",
            "Ran Xu",
            "Gongjie Zhang"
        ],
        "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting",
        "abstract": "arXiv:2507.18678v1 Announce Type: new  Abstract: Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation. In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations - including point clouds, camera poses, depth maps, and pseudo-RGBD - via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence. We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments.",
        "arxiv_id": "2507.18678",
        "ARXIVID": "2507.18678",
        "COMMENT": "Matches criterion 1 as it focuses on spatial intelligence and scalable 3D data generation for spatial reasoning tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18881": {
        "authors": [
            "Bolei Chen",
            "Jiaxu Kang",
            "Haonan Yang",
            "Ping Zhong",
            "Jianxin Wang"
        ],
        "title": "Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?",
        "abstract": "arXiv:2507.18881v1 Announce Type: new  Abstract: Since a building's floorplans are easily accessible, consistent over time, and inherently robust to changes in visual appearance, self-localization within the floorplan has attracted researchers' interest. However, since floorplans are minimalist representations of a building's structure, modal and geometric differences between visual perceptions and floorplans pose challenges to this task. While existing methods cleverly utilize 2D geometric features and pose filters to achieve promising performance, they fail to address the localization errors caused by frequent visual changes and view occlusions due to variously shaped 3D objects. To tackle these issues, this paper views the 2D Floorplan Localization (FLoc) problem from a higher dimension by injecting 3D geometric priors into the visual FLoc algorithm. For the 3D geometric prior modeling, we first model geometrically aware view invariance using multi-view constraints, i.e., leveraging imaging geometric principles to provide matching constraints between multiple images that see the same points. Then, we further model the view-scene aligned geometric priors, enhancing the cross-modal geometry-color correspondences by associating the scene's surface reconstruction with the RGB frames of the sequence. Both 3D priors are modeled through self-supervised contrastive learning, thus no additional geometric or semantic annotations are required. These 3D priors summarized in extensive realistic scenes bridge the modal gap while improving localization success without increasing the computational burden on the FLoc algorithm. Sufficient comparative studies demonstrate that our method significantly outperforms state-of-the-art methods and substantially boosts the FLoc accuracy. All data and code will be released after the anonymous review.",
        "arxiv_id": "2507.18881",
        "ARXIVID": "2507.18881",
        "COMMENT": "Matches criterion 1 as it explores spatial intelligence with 3D geometric priors for floorplan localization.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.19058": {
        "authors": [
            "Chong Xia",
            "Shengjun Zhang",
            "Fangfu Liu",
            "Chang Liu",
            "Khodchaphun Hirunyaratsameewong",
            "Yueqi Duan"
        ],
        "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment",
        "abstract": "arXiv:2507.19058v1 Announce Type: new  Abstract: Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences. Project Page: https://xiac20.github.io/ScenePainter/.",
        "arxiv_id": "2507.19058",
        "ARXIVID": "2507.19058",
        "COMMENT": "Matches criterion 6 as it focuses on 3D scene generation and addresses semantic consistency in video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.19213": {
        "authors": [
            "Hanbing Wu",
            "Ping Jiang",
            "Anyang Su",
            "Chenxu Zhao",
            "Tianyu Fu",
            "Minghui Wu",
            "Beiping Tan",
            "Huiying Li"
        ],
        "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction",
        "abstract": "arXiv:2507.19213v1 Announce Type: new  Abstract: Visual selective attention, driven by individual preferences, regulates human prioritization of visual stimuli by bridging subjective cognitive mechanisms with objective visual elements, thereby steering the semantic interpretation and hierarchical processing of dynamic visual scenes. However, existing models and datasets predominantly neglect the influence of subjective cognitive diversity on fixation behavior. Conventional saliency prediction models, typically employing segmentation approaches, rely on low-resolution imagery to generate saliency heatmaps, subsequently upscaled to native resolutions, which limiting their capacity to capture personalized attention patterns. Furthermore, MLLMs are constrained by factors such as hallucinations, making it very costly to strictly adhere to the expected format in tasks involving multiple point predictions, and achieving precise point positioning is challenging. To address these limitations, we present Subjective Personalized Attention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal dataset capturing gaze behaviors from over 4,500 participants varying in age and gender with 486 videos. Furthermore, we propose PRE-MAP, a novel eye-tracking saliency model that characterizes Personalized visual disparities through Reinforcement learning-optimized Eye-tracking, built upon MLLMs and guided by Multi-Attribute user profiles to predict Points. To ensure MLLMs produce prediction points that are both format-correct and spatially accurate, we introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired by the variability in eye movement points and Multi-Attribute profiles. Extensive experiments on SPA-ADV and other benchmarks demonstrate the effectiveness of our approach. The code and dataset are available at \\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.",
        "arxiv_id": "2507.19213",
        "ARXIVID": "2507.19213",
        "COMMENT": "Matches criterion 2 as it involves a multimodal large language model (MLLM) and explores personalized attention prediction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.19451": {
        "authors": [
            "Baijun Ye",
            "Minghui Qin",
            "Saining Zhang",
            "Moonjun Gong",
            "Shaoting Zhu",
            "Zebang Shen",
            "Luan Zhang",
            "Lu Zhang",
            "Hao Zhao",
            "Hang Zhao"
        ],
        "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting",
        "abstract": "arXiv:2507.19451v1 Announce Type: new  Abstract: Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for autonomous driving perception. Project Page: https://gs-occ3d.github.io/",
        "arxiv_id": "2507.19451",
        "ARXIVID": "2507.19451",
        "COMMENT": "Matches criterion 3. Proposes a scalable vision-only framework for occupancy reconstruction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.18763": {
        "authors": [
            "Keshav Gupta",
            "Tejas S. Stanley",
            "Pranjal Paul",
            "Arun K. Singh",
            "K. Madhava Krishna"
        ],
        "title": "Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving",
        "abstract": "arXiv:2507.18763v1 Announce Type: new  Abstract: Drivable Free-space prediction is a fundamental and crucial problem in autonomous driving. Recent works have addressed the problem by representing the entire non-obstacle road regions as the free-space. In contrast our aim is to estimate the driving corridors that are a navigable subset of the entire road region. Unfortunately, existing corridor estimation methods directly assume a BEV-centric representation, which is hard to obtain. In contrast, we frame drivable free-space corridor prediction as a pure image perception task, using only monocular camera input. However such a formulation poses several challenges as one doesn't have the corresponding data for such free-space corridor segments in the image. Consequently, we develop a novel self-supervised approach for free-space sample generation by leveraging future ego trajectories and front-view camera images, making the process of visual corridor estimation dependent on the ego trajectory. We then employ a diffusion process to model the distribution of such segments in the image. However, the existing binary mask-based representation for a segment poses many limitations. Therefore, we introduce ContourDiff, a specialized diffusion-based architecture that denoises over contour points rather than relying on binary mask representations, enabling structured and interpretable free-space predictions. We evaluate our approach qualitatively and quantitatively on both nuScenes and CARLA, demonstrating its effectiveness in accurately predicting safe multimodal navigable corridors in the image.",
        "arxiv_id": "2507.18763",
        "ARXIVID": "2507.18763",
        "COMMENT": "Matches criterion 3 as it introduces a novel self-supervised method for free-space prediction in autonomous driving, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.19292": {
        "authors": [
            "Sakuya Ota",
            "Qing Yu",
            "Kent Fujiwara",
            "Satoshi Ikehata",
            "Ikuro Sato"
        ],
        "title": "PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups",
        "abstract": "arXiv:2507.19292v1 Announce Type: new  Abstract: Generating realistic group interactions involving multiple characters remains challenging due to increasing complexity as group size expands. While existing conditional diffusion models incrementally generate motions by conditioning on previously generated characters, they rely on single shared prompts, limiting nuanced control and leading to overly simplified interactions. In this paper, we introduce Person-Interaction Noise Optimization (PINO), a novel, training-free framework designed for generating realistic and customizable interactions among groups of arbitrary size. PINO decomposes complex group interactions into semantically relevant pairwise interactions, and leverages pretrained two-person interaction diffusion models to incrementally compose group interactions. To ensure physical plausibility and avoid common artifacts such as overlapping or penetration between characters, PINO employs physics-based penalties during noise optimization. This approach allows precise user control over character orientation, speed, and spatial relationships without additional training. Comprehensive evaluations demonstrate that PINO generates visually realistic, physically coherent, and adaptable multi-person interactions suitable for diverse animation, gaming, and robotics applications.",
        "arxiv_id": "2507.19292",
        "ARXIVID": "2507.19292",
        "COMMENT": "This paper matches criterion 3 as it introduces a novel method (PINO) for generating realistic and customizable multi-person interactions, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19132": {
        "authors": [
            "Xuetian Chen",
            "Yinghao Chen",
            "Xinfeng Yuan",
            "Zhuo Peng",
            "Lu Chen",
            "Yuekeng Li",
            "Zhoujia Zhang",
            "Yingqian Huang",
            "Leyan Huang",
            "Jiaqing Liang",
            "Tianbao Xie",
            "Zhiyong Wu",
            "Qiushi Sun",
            "Biqing Qi",
            "Bowen Zhou"
        ],
        "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?",
        "abstract": "arXiv:2507.19132v1 Announce Type: new  Abstract: Computer-using agents have shown strong potential to boost human productivity and enable new application forms across platforms. While recent advances have led to usable applications, existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the need for a deeper understanding of current strengths and limitations to drive the future progress in computer-using agents research and deployment. All code, environments, baselines, and data are publicly available at https://github.com/OS-Copilot/OS-Map.",
        "arxiv_id": "2507.19132",
        "ARXIVID": "2507.19132",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark (OS-MAP) for computer-using agents, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.18743": {
        "authors": [
            "Xinjun Cheng",
            "Yiguo He",
            "Junjie Zhu",
            "Chunping Qiu",
            "Jun Wang",
            "Qiangjuan Huang",
            "Ke Yang"
        ],
        "title": "SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning",
        "abstract": "arXiv:2507.18743v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have achieved remarkable breakthroughs in the field of remote sensing in recent years. Synthetic Aperture Radar (SAR) imagery, with its all-weather capability, is essential in remote sensing, yet the lack of large-scale, high-quality SAR image-text datasets hinders its semantic understanding. In this paper, we construct SAR-Text, a large-scale and high-quality dataset consisting of over 130,000 SAR image-text pairs. To construct the SAR-Text dataset, we design the SAR-Narrator framework, which generates textual descriptions for SAR images through a multi-stage progressive transfer learning strategy. To verify the effectiveness of the SAR-TEXT dataset, we conduct experiments on three typical vision-language tasks: image-text retrieval, image captioning, and visual question answering (VQA). Specifically, we construct three representative models on SAR-TEXT: SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable improvements in retrieval performance, boosting average recall by 16.43% and 10.54% on the OSdataset-512 and HRSID test sets, respectively. In the captioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding those of the original CoCa model by more than 8x, 4x, and 10x, respectively. In the VQA task, SAR-GPT outperforms baseline and single-stage models on multiple SAR-VQA datasets, demonstrating stronger semantic understanding and reasoning ability, as further confirmed by qualitative results. It is worth noting that, as a flexible captioning tool, SAR-Narrator can be readily adopted by the community to construct larger-scale SAR image-text datasets.",
        "arxiv_id": "2507.18743",
        "ARXIVID": "2507.18743",
        "COMMENT": "Matches criterion 2. Introduces a large-scale SAR image-text dataset and explores vision-language tasks, which aligns with visual and multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19119": {
        "authors": [
            "Yanghong Liu",
            "Xingping Dong",
            "Ming Li",
            "Weixing Zhang",
            "Yidong Lou"
        ],
        "title": "PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction",
        "abstract": "arXiv:2507.19119v1 Announce Type: new  Abstract: Pedestrian trajectory prediction is crucial for autonomous driving and robotics. While existing point-based and grid-based methods expose two key limitations: insufficiently modeling human motion dynamics, as they fail to balance local motion details with long-range spatiotemporal dependencies, and the time representation lacks interaction with the frequency domain in modeling trajectory sequences. To address these challenges, we propose PatchTraj, a dynamic patch-based trajectory prediction framework that unifies time-domain and frequency-domain representations. Specifically, we decompose the trajectory into raw time sequences and frequency components, employing dynamic patch partitioning for multi-scale trajectory segmentation to capture hierarchical motion patterns. Each patch is processed by an adaptive embedding layer with scale-aware feature extraction, followed by hierarchical feature aggregation to model both fine-grained and long-range dependencies. The outputs of two branches interact via cross-modal attention, enabling complementary fusion of temporal and spectral cues. Finally, a Transformer encoder-decoder integrates both modalities to autoregressively predict future trajectories. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method achieves state-of-the-art performance with high efficiency.",
        "arxiv_id": "2507.19119",
        "ARXIVID": "2507.19119",
        "COMMENT": "Matches criterion 3. Proposes a novel method for trajectory prediction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19239": {
        "authors": [
            "Jiaru Zhong",
            "Jiahao Wang",
            "Jiahui Xu",
            "Xiaofan Li",
            "Zaiqing Nie",
            "Haibao Yu"
        ],
        "title": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception",
        "abstract": "arXiv:2507.19239v1 Announce Type: new  Abstract: Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\\% mAP and 32.8\\% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.",
        "arxiv_id": "2507.19239",
        "ARXIVID": "2507.19239",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new framework for cooperative sequential perception in autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19468": {
        "authors": [
            "Federico Baldassarre",
            "Marc Szafraniec",
            "Basile Terver",
            "Vasil Khalidov",
            "Francisco Massa",
            "Yann LeCun",
            "Patrick Labatut",
            "Maximilian Seitzer",
            "Piotr Bojanowski"
        ],
        "title": "Back to the Features: DINO as a Foundation for Video World Models",
        "abstract": "arXiv:2507.19468v1 Announce Type: new  Abstract: We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms previous models on a variety of video prediction benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong understanding of intuitive physics. Furthermore, we show that it is possible to fine-tune the predictor on observation-action trajectories. The resulting action-conditioned world model can be used for planning by simulating candidate trajectories in latent space.",
        "arxiv_id": "2507.19468",
        "ARXIVID": "2507.19468",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video prediction benchmarks and temporal dynamics in video data.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19362": {
        "authors": [
            "Yusuke Hirota",
            "Boyi Li",
            "Ryo Hachiuma",
            "Yueh-Hua Wu",
            "Boris Ivanovic",
            "Yuta Nakashima",
            "Marco Pavone",
            "Yejin Choi",
            "Yu-Chiang Frank Wang",
            "Chao-Han Huck Yang"
        ],
        "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences",
        "abstract": "arXiv:2507.19362v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have transformed image captioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations. LOTUS comprehensively evaluates various aspects, including caption quality (e.g., alignment, descriptiveness), risks (\\eg, hallucination), and societal biases (e.g., gender bias) while enabling preference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of recent LVLMs reveals no single model excels across all criteria, while correlations emerge between caption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model selection depends on user priorities.",
        "arxiv_id": "2507.19362",
        "ARXIVID": "2507.19362",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it evaluates large vision-language models for detailed image captioning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19272": {
        "authors": [
            "Marcel Simon",
            "Tae-Ho Kim",
            "Seul-Ki Yeom"
        ],
        "title": "Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception",
        "abstract": "arXiv:2507.19272v1 Announce Type: new  Abstract: Self-supervised image encoders such as DINO have recently gained significant interest for learning robust visual features without labels. However, most SSL methods train on static images and miss the temporal cues inherent in videos. We introduce a video-distilled single-image encoder trained to predict the next-frame representation from the current frame. This simple objective injects 3D spatial and temporal priors without optical flow or tracking. When pre-training on a single 2-hour video, our approach raises the mean Intersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while remaining a drop-in replacement for image-only pipelines. Our results highlight video self-distillation as a lightweight route to geometry-aware perception an essential ingredient for physically plausible world models and Physical AI.",
        "arxiv_id": "2507.19272",
        "ARXIVID": "2507.19272",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a video self-distillation method for single-image encoders, incorporating temporal cues.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19304": {
        "authors": [
            "Muhammad Ibrahim",
            "Naveed Akhtar",
            "Haitian Wang",
            "Saeed Anwar",
            "Ajmal Mian"
        ],
        "title": "Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes",
        "abstract": "arXiv:2507.19304v1 Announce Type: new  Abstract: Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object detection accuracy. To address real-world challenges in outdoor 3D object detection, fusion of LiDAR and RGB input has started gaining traction. However, effective integration of these modalities for precise object detection task still remains a largely open problem. To address that, we propose a MultiStream Detection (MuStD) network, that meticulously extracts task-relevant information from both data modalities. The network follows a three-stream structure. Its LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input while the LiDAR-Height Compression stream computes Bird's-Eye View features. An additional 3D Multimodal stream combines RGB and LiDAR features using UV mapping and polar coordinate indexing. Eventually, the features containing comprehensive spatial, textural and geometric information are carefully fused and fed to a detection head for 3D object detection. Our extensive evaluation on the challenging KITTI Object Detection Benchmark using public testing server at https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0 establishes the efficacy of our method by achieving new state-of-the-art or highly competitive results in different categories while remaining among the most efficient methods. Our code will be released through MuStD GitHub repository at https://github.com/IbrahimUWA/MuStD.git",
        "arxiv_id": "2507.19304",
        "ARXIVID": "2507.19304",
        "COMMENT": "Matches criterion 5 as it discusses a multimodal fusion approach combining LiDAR and RGB data for 3D object detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19474": {
        "authors": [
            "Ziren Gong",
            "Xiaohan Li",
            "Fabio Tosi",
            "Youmin Zhang",
            "Stefano Mattoccia",
            "Jun Wu",
            "Matteo Poggi"
        ],
        "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations",
        "abstract": "arXiv:2507.19474v1 Announce Type: new  Abstract: This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.",
        "arxiv_id": "2507.19474",
        "ARXIVID": "2507.19474",
        "COMMENT": "Matches criterion 3 as it proposes a SLAM system integrating neural implicit and explicit representations, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19253": {
        "authors": [
            "An Xiang",
            "Zixuan Huang",
            "Xitong Gao",
            "Kejiang Ye",
            "Cheng-zhong Xu"
        ],
        "title": "BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection",
        "abstract": "arXiv:2507.19253v1 Announce Type: new  Abstract: Industrial anomaly detection for 2D objects has gained significant attention and achieved progress in anomaly detection (AD) methods. However, identifying 3D depth anomalies using only 2D information is insufficient. Despite explicitly fusing depth information into RGB images or using point cloud backbone networks to extract depth features, both approaches struggle to adequately represent 3D information in multimodal scenarios due to the disparities among different modal information. Additionally, due to the scarcity of abnormal samples in industrial data, especially in multimodal scenarios, it is necessary to perform anomaly generation to simulate real-world abnormal samples. Therefore, we propose a novel unified multimodal anomaly detection framework to address these issues. Our contributions consist of 3 key aspects. (1) We extract visible depth information from 3D point cloud data simply and use 2D RGB images to represent appearance, which disentangles depth and appearance to support unified anomaly generation. (2) Benefiting from the flexible input representation, the proposed Multi-Scale Gaussian Anomaly Generator and Unified Texture Anomaly Generator can generate richer anomalies in RGB and depth. (3) All modules share parameters for both RGB and depth data, effectively bridging 2D and 3D anomaly detection. Subsequent modules can directly leverage features from both modalities without complex fusion. Experiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD and Eyecandies datasets. Code available at: https://github.com/Xantastic/BridgeNet",
        "arxiv_id": "2507.19253",
        "ARXIVID": "2507.19253",
        "COMMENT": "Matches criterion 2 as it discusses a multimodal framework integrating 2D and 3D data, which aligns with vision\u2013language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19360": {
        "authors": [
            "Chen Zhu",
            "Wangbo Zhao",
            "Huiwen Zhang",
            "Samir Khaki",
            "Yuhao Zhou",
            "Weidong Tang",
            "Shuo Wang",
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Xiaojiang Peng",
            "Kai Wang",
            "Dawei Yang"
        ],
        "title": "EA-ViT: Efficient Adaptation for Elastic Vision Transformer",
        "abstract": "arXiv:2507.19360v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have emerged as a foundational model in computer vision, excelling in generalization and adaptation to downstream tasks. However, deploying ViTs to support diverse resource constraints typically requires retraining multiple, size-specific ViTs, which is both time-consuming and energy-intensive. To address this issue, we propose an efficient ViT adaptation framework that enables a single adaptation process to generate multiple models of varying sizes for deployment on platforms with various resource constraints. Our approach comprises two stages. In the first stage, we enhance a pre-trained ViT with a nested elastic architecture that enables structural flexibility across MLP expansion ratio, number of attention heads, embedding dimension, and network depth. To preserve pre-trained knowledge and ensure stable adaptation, we adopt a curriculum-based training strategy that progressively increases elasticity. In the second stage, we design a lightweight router to select submodels according to computational budgets and downstream task demands. Initialized with Pareto-optimal configurations derived via a customized NSGA-II algorithm, the router is then jointly optimized with the backbone. Extensive experiments on multiple benchmarks demonstrate the effectiveness and versatility of EA-ViT. The code is available at https://github.com/zcxcf/EA-ViT.",
        "arxiv_id": "2507.19360",
        "ARXIVID": "2507.19360",
        "COMMENT": "Matches criterion 4 as it focuses on Vision Transformers, a type of vision foundation model, and their efficient adaptation for deployment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.19024": {
        "authors": [
            "Zhiyuan Chen (State Key Laboratory of AI Safety",
            "Institute of Computing Technology",
            "Chinese Academy of Sciences",
            "University of Chinese Academy of Sciences)",
            "Yuecong Min (State Key Laboratory of AI Safety",
            "Institute of Computing Technology",
            "Chinese Academy of Sciences",
            "University of Chinese Academy of Sciences)",
            "Jie Zhang (State Key Laboratory of AI Safety",
            "Institute of Computing Technology",
            "Chinese Academy of Sciences",
            "University of Chinese Academy of Sciences)",
            "Bei Yan (State Key Laboratory of AI Safety",
            "Institute of Computing Technology",
            "Chinese Academy of Sciences",
            "University of Chinese Academy of Sciences)",
            "Jiahao Wang (Trustworthy Technology and Engineering Laboratory",
            "Huawei)",
            "Xiaozhen Wang (Trustworthy Technology and Engineering Laboratory",
            "Huawei)",
            "Shiguang Shan (State Key Laboratory of AI Safety",
            "Institute of Computing Technology",
            "Chinese Academy of Sciences",
            "University of Chinese Academy of Sciences)"
        ],
        "title": "A Survey of Multimodal Hallucination Evaluation and Detection",
        "abstract": "arXiv:2507.19024v1 Announce Type: new  Abstract: Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm for integrating visual and textual information, supporting a wide range of multi-modal tasks. However, these models often suffer from hallucination, producing content that appears plausible but contradicts the input content or established world knowledge. This survey offers an in-depth review of hallucination evaluation benchmarks and detection methods across Image-to-Text (I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose a taxonomy of hallucination based on faithfulness and factuality, incorporating the common types of hallucinations observed in practice. Then we provide an overview of existing hallucination evaluation benchmarks for both T2I and I2T tasks, highlighting their construction process, evaluation objectives, and employed metrics. Furthermore, we summarize recent advances in hallucination detection methods, which aims to identify hallucinated content at the instance level and serve as a practical complement of benchmark-based evaluation. Finally, we highlight key limitations in current benchmarks and detection methods, and outline potential directions for future research.",
        "arxiv_id": "2507.19024",
        "ARXIVID": "2507.19024",
        "COMMENT": "Matches criterion 7 as it is a survey paper on multimodal hallucination evaluation and detection, synthesizing the state of the art.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2507.19328": {
        "authors": [
            "Kirsten W. H. Maas",
            "Danny Ruijters",
            "Nicola Pezzotti",
            "Anna Vilanova"
        ],
        "title": "NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography",
        "abstract": "arXiv:2507.19328v1 Announce Type: new  Abstract: Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary arteries from X-ray coronary angiography (CA) has the potential to improve clinical procedures. However, there are multiple challenges to be addressed, most notably, blood-vessel structure sparsity, poor background and blood vessel distinction, sparse-views, and intra-scan motion. State-of-the-art reconstruction approaches rely on time-consuming manual or error-prone automatic segmentations, limiting clinical usability. Recently, approaches based on Neural Radiance Fields (NeRF) have shown promise for automatic reconstructions in the sparse-view setting. However, they suffer from long training times due to their dependence on MLP-based representations. We propose NerT-CA, a hybrid approach of Neural and Tensorial representations for accelerated 4D reconstructions with sparse-view CA. Building on top of the previous NeRF-based work, we model the CA scene as a decomposition of low-rank and sparse components, utilizing fast tensorial fields for low-rank static reconstruction and neural fields for dynamic sparse reconstruction. Our approach outperforms previous works in both training time and reconstruction accuracy, yielding reasonable reconstructions from as few as three angiogram views. We validate our approach quantitatively and qualitatively on representative 4D phantom datasets.",
        "arxiv_id": "2507.19328",
        "ARXIVID": "2507.19328",
        "COMMENT": "Does not match any specific criteria but introduces a hybrid approach for 4D reconstruction in medical imaging, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.18838": {
        "authors": [
            "Fabio De Sousa Ribeiro",
            "Omar Todd",
            "Charles Jones",
            "Avinash Kori",
            "Raghav Mehta",
            "Ben Glocker"
        ],
        "title": "Flow Stochastic Segmentation Networks",
        "abstract": "arXiv:2507.18838v1 Announce Type: new  Abstract: We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a generative segmentation model family featuring discrete-time autoregressive and modern continuous-time flow variants. We prove fundamental limitations of the low-rank parameterisation of previous methods and show that Flow-SSNs can estimate arbitrarily high-rank pixel-wise covariances without assuming the rank or storing the distributional parameters. Flow-SSNs are also more efficient to sample from than standard diffusion-based segmentation models, thanks to most of the model capacity being allocated to learning the base distribution of the flow, constituting an expressive prior. We apply Flow-SSNs to challenging medical imaging benchmarks and achieve state-of-the-art results. Code available: https://github.com/biomedia-mira/flow-ssn.",
        "arxiv_id": "2507.18838",
        "ARXIVID": "2507.18838",
        "COMMENT": "Does not match any specific criteria but introduces a novel generative segmentation model, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.19481": {
        "authors": [
            "Byungjun Kim",
            "Shunsuke Saito",
            "Giljoo Nam",
            "Tomas Simon",
            "Jason Saragih",
            "Hanbyul Joo",
            "Junxuan Li"
        ],
        "title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars",
        "abstract": "arXiv:2507.19481v1 Announce Type: new  Abstract: We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.",
        "arxiv_id": "2507.19481",
        "ARXIVID": "2507.19481",
        "COMMENT": "This paper does not match any specific criteria but may be of general interest due to its focus on 3D avatar generation and disentangled modeling of face and hair, which could be tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18758": {
        "authors": [
            "Yifan Liu",
            "Shengjun Zhang",
            "Chensheng Dai",
            "Yang Chen",
            "Hao Liu",
            "Chen Li",
            "Yueqi Duan"
        ],
        "title": "Learning Efficient and Generalizable Human Representation with Human Gaussian Model",
        "abstract": "arXiv:2507.18758v1 Announce Type: new  Abstract: Modeling animatable human avatars from videos is a long-standing and challenging problem. While conventional methods require per-instance optimization, recent feed-forward methods have been proposed to generate 3D Gaussians with a learnable network. However, these methods predict Gaussians for each frame independently, without fully capturing the relations of Gaussians from different timestamps. To address this, we propose Human Gaussian Graph to model the connection between predicted Gaussians and human SMPL mesh, so that we can leverage information from all frames to recover an animatable human representation. Specifically, the Human Gaussian Graph contains dual layers where Gaussians are the first layer nodes and mesh vertices serve as the second layer nodes. Based on this structure, we further propose the intra-node operation to aggregate various Gaussians connected to one mesh vertex, and inter-node operation to support message passing among mesh node neighbors. Experimental results on novel view synthesis and novel pose animation demonstrate the efficiency and generalization of our method.",
        "arxiv_id": "2507.18758",
        "ARXIVID": "2507.18758",
        "COMMENT": "Does not match any specific criteria. Focuses on human representation modeling, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19140": {
        "authors": [
            "Tianyu Zou",
            "Shengwu Xiong",
            "Ruilin Yao",
            "Yi Rong"
        ],
        "title": "Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation",
        "abstract": "arXiv:2507.19140v1 Announce Type: new  Abstract: This paper studies the few-shot segmentation (FSS) task, which aims to segment objects belonging to unseen categories in a query image by learning a model on a small number of well-annotated support samples. Our analysis of two mainstream FSS paradigms reveals that the predictions made by prototype learning methods are usually conservative, while those of affinity learning methods tend to be more aggressive. This observation motivates us to balance the conservative and aggressive information captured by these two types of FSS frameworks so as to improve the segmentation performance. To achieve this, we propose a **P**rototype-**A**ffinity **H**ybrid **Net**work (PAHNet), which introduces a Prototype-guided Feature Enhancement (PFE) module and an Attention Score Calibration (ASC) module in each attention block of an affinity learning model (called affinity learner). These two modules utilize the predictions generated by a pre-trained prototype learning model (called prototype predictor) to enhance the foreground information in support and query image representations and suppress the mismatched foreground-background (FG-BG) relationships between them, respectively. In this way, the aggressiveness of the affinity learner can be effectively mitigated, thereby eventually increasing the segmentation accuracy of our PAHNet method. Experimental results show that PAHNet outperforms most recently proposed methods across 1-shot and 5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its effectiveness. The code is available at: [GitHub - tianyu-zou/PAHNet: Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation (ICCV'25)](https://github.com/tianyu-zou/PAHNet)",
        "arxiv_id": "2507.19140",
        "ARXIVID": "2507.19140",
        "COMMENT": "Does not match any specific criteria but is related to few-shot segmentation and hybrid learning methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18988": {
        "authors": [
            "Chao Wang",
            "Kejiang Chen",
            "Zijin Yang",
            "Yaofei Wang",
            "Weiming Zhang"
        ],
        "title": "AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction",
        "abstract": "arXiv:2507.18988v1 Announce Type: new  Abstract: The rapid advancement of image-generation technologies has made it possible for anyone to create photorealistic images using generative models, raising significant security concerns. To mitigate malicious use, tracing the origin of such images is essential. Reconstruction-based attribution methods offer a promising solution, but they often suffer from reduced accuracy and high computational costs when applied to state-of-the-art (SOTA) models. To address these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel training-free attribution method designed for generative models with continuous autoencoders. Unlike existing reconstruction-based approaches that rely on the value of a single reconstruction loss, AEDR performs two consecutive reconstructions using the model's autoencoder, and adopts the ratio of these two reconstruction losses as the attribution signal. This signal is further calibrated using the image homogeneity metric to improve accuracy, which inherently cancels out absolute biases caused by image complexity, with autoencoder-based reconstruction ensuring superior computational efficiency. Experiments on eight top latent diffusion models show that AEDR achieves 25.5% higher attribution accuracy than existing reconstruction-based methods, while requiring only 1% of the computational time.",
        "arxiv_id": "2507.18988",
        "ARXIVID": "2507.18988",
        "COMMENT": "Does not match any specific criteria but is related to generative model attribution and security concerns.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19071": {
        "authors": [
            "Yangyang Xu",
            "Bangzhen Liu",
            "Wenqi Shao",
            "Yong Du",
            "Shengfeng He",
            "Tingting Zhu"
        ],
        "title": "Cross-Subject Mind Decoding from Inaccurate Representations",
        "abstract": "arXiv:2507.19071v1 Announce Type: new  Abstract: Decoding stimulus images from fMRI signals has advanced with pre-trained generative models. However, existing methods struggle with cross-subject mappings due to cognitive variability and subject-specific differences. This challenge arises from sequential errors, where unidirectional mappings generate partially inaccurate representations that, when fed into diffusion models, accumulate errors and degrade reconstruction fidelity. To address this, we propose the Bidirectional Autoencoder Intertwining framework for accurate decoded representation prediction. Our approach unifies multiple subjects through a Subject Bias Modulation Module while leveraging bidirectional mapping to better capture data distributions for precise representation prediction. To further enhance fidelity when decoding representations into stimulus images, we introduce a Semantic Refinement Module to improve semantic representations and a Visual Coherence Module to mitigate the effects of inaccurate visual representations. Integrated with ControlNet and Stable Diffusion, our method outperforms state-of-the-art approaches on benchmark datasets in both qualitative and quantitative evaluations. Moreover, our framework exhibits strong adaptability to new subjects with minimal training samples.",
        "arxiv_id": "2507.19071",
        "ARXIVID": "2507.19071",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and decoding representations.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19172": {
        "authors": [
            "Jiyao Wang",
            "Xiao Yang",
            "Qingyong Hu",
            "Jiankai Tang",
            "Can Liu",
            "Dengbo He",
            "Yuntao Wang",
            "Yingcong Chen",
            "Kaishun Wu"
        ],
        "title": "PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring",
        "abstract": "arXiv:2507.19172v1 Announce Type: new  Abstract: Robust and unobtrusive in-vehicle physiological monitoring is crucial for ensuring driving safety and user experience. While remote physiological measurement (RPM) offers a promising non-invasive solution, its translation to real-world driving scenarios is critically constrained by the scarcity of comprehensive datasets. Existing resources are often limited in scale, modality diversity, the breadth of biometric annotations, and the range of captured conditions, thereby omitting inherent real-world challenges in driving. Here, we present PhysDrive, the first large-scale multimodal dataset for contactless in-vehicle physiological sensing with dedicated consideration on various modality settings and driving factors. PhysDrive collects data from 48 drivers, including synchronized RGB, near-infrared camera, and raw mmWave radar data, accompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR, and SpO2). It covers a wide spectrum of naturalistic driving conditions, including driver motions, dynamic natural light, vehicle types, and road conditions. We extensively evaluate both signal-processing and deep-learning methods on PhysDrive, establishing a comprehensive benchmark across all modalities, and release full open-source code with compatibility for mainstream public toolboxes. We envision PhysDrive will serve as a foundational resource and accelerate research on multimodal driver monitoring and smart-cockpit systems.",
        "arxiv_id": "2507.19172",
        "ARXIVID": "2507.19172",
        "COMMENT": "Does not match any specific criteria but is related to multimodal datasets and driver monitoring.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18830": {
        "authors": [
            "Shen Zhu",
            "Yinzhu Jin",
            "Tyler Spears",
            "Ifrah Zawar",
            "P. Thomas Fletcher"
        ],
        "title": "RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models",
        "abstract": "arXiv:2507.18830v1 Announce Type: new  Abstract: We propose image-to-image diffusion models that are designed to enhance the realism and details of generated brain images by introducing sharp edges, fine textures, subtle anatomical features, and imaging noise. Generative models have been widely adopted in the biomedical domain, especially in image generation applications. Latent diffusion models achieve state-of-the-art results in generating brain MRIs. However, due to latent compression, generated images from these models are overly smooth, lacking fine anatomical structures and scan acquisition noise that are typically seen in real images. This work formulates the realism enhancing and detail adding process as image-to-image diffusion models, which refines the quality of LDM-generated images. We employ commonly used metrics like FID and LPIPS for image realism assessment. Furthermore, we introduce new metrics to demonstrate the realism of images generated by RealDeal in terms of image noise distribution, sharpness, and texture.",
        "arxiv_id": "2507.18830",
        "ARXIVID": "2507.18830",
        "COMMENT": "Does not match any specific criteria but focuses on enhancing realism in brain image generation, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19059": {
        "authors": [
            "Xiaocheng Fang",
            "Jieyi Cai",
            "Huanyu Liu",
            "Wenxiu Cai",
            "Yishu Liu",
            "Bingzhi Chen"
        ],
        "title": "Revisiting DETR for Small Object Detection via Noise-Resilient Query Optimization",
        "abstract": "arXiv:2507.19059v1 Announce Type: new  Abstract: Despite advancements in Transformer-based detectors for small object detection (SOD), recent studies show that these detectors still face challenges due to inherent noise sensitivity in feature pyramid networks (FPN) and diminished query quality in existing label assignment strategies. In this paper, we propose a novel Noise-Resilient Query Optimization (NRQO) paradigm, which innovatively incorporates the Noise-Tolerance Feature Pyramid Network (NT-FPN) and the Pairwise-Similarity Region Proposal Network (PS-RPN). Specifically, NT-FPN mitigates noise during feature fusion in FPN by preserving spatial and semantic information integrity. Unlike existing label assignment strategies, PS-RPN generates a sufficient number of high-quality positive queries by enhancing anchor-ground truth matching through position and shape similarities, without the need for additional hyperparameters. Extensive experiments on multiple benchmarks consistently demonstrate the superiority of NRQO over state-of-the-art baselines.",
        "arxiv_id": "2507.19059",
        "ARXIVID": "2507.19059",
        "COMMENT": "Does not match any specific criteria but focuses on small object detection with transformers, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19175": {
        "authors": [
            "Yuki Igaue",
            "Hiroaki Aizawa"
        ],
        "title": "Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers",
        "abstract": "arXiv:2507.19175v1 Announce Type: new  Abstract: Multi-head self-attention is a distinctive feature extraction mechanism of vision transformers that computes pairwise relationships among all input patches, contributing significantly to their high performance. However, it is known to incur a quadratic computational complexity with respect to the number of patches. One promising approach to address this issue is patch pruning, which improves computational efficiency by identifying and removing redundant patches. In this work, we propose a patch pruning strategy that evaluates the importance of each patch based on the variance of attention weights across multiple attention heads. This approach is inspired by the design of multi-head self-attention, which aims to capture diverse attention patterns across different subspaces of feature representations. The proposed method can be easily applied during both training and inference, and achieves improved throughput while maintaining classification accuracy in scenarios such as fine-tuning with pre-trained models. In addition, we also found that using robust statistical measures, such as the median absolute deviation in place of variance, to assess patch importance can similarly lead to strong performance. Furthermore, by introducing overlapping patch embeddings, our method achieves better performance with comparable throughput to conventional approaches that utilize all patches.",
        "arxiv_id": "2507.19175",
        "ARXIVID": "2507.19175",
        "COMMENT": "Does not match any specific criteria but introduces a patch pruning strategy for vision transformers, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19209": {
        "authors": [
            "Xiaoyu Zhang",
            "Zhifeng Bao",
            "Hai Dong",
            "Ziwei Wang",
            "Jiajun Liu"
        ],
        "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet",
        "abstract": "arXiv:2507.19209v1 Announce Type: new  Abstract: Autonomous vehicles generate massive volumes of point cloud data, yet only a subset is relevant for specific tasks such as collision detection, traffic analysis, or congestion monitoring. Effectively querying this data is essential to enable targeted analytics. In this work, we formalize point cloud querying by defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each aligned with distinct analytical scenarios. All these queries rely heavily on accurate object counts to produce meaningful results, making precise object counting a critical component of query execution. Prior work has focused on indexing techniques for 2D video data, assuming detection models provide accurate counting information. However, when applied to 3D point cloud data, state-of-the-art detection models often fail to generate reliable object counts, leading to substantial errors in query results. To address this limitation, we propose CounterNet, a heatmap-based network designed for accurate object counting in large-scale point cloud data. Rather than focusing on accurate object localization, CounterNet detects object presence by finding object centers to improve counting accuracy. We further enhance its performance with a feature map partitioning strategy using overlapping regions, enabling better handling of both small and large objects in complex traffic scenes. To adapt to varying frame characteristics, we introduce a per-frame dynamic model selection strategy that selects the most effective configuration for each input. Evaluations on three real-world autonomous vehicle datasets show that CounterNet improves counting accuracy by 5% to 20% across object categories, resulting in more reliable query outcomes across all supported query types.",
        "arxiv_id": "2507.19209",
        "ARXIVID": "2507.19209",
        "COMMENT": "Does not match any specific criteria but focuses on point cloud data querying and object counting, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19321": {
        "authors": [
            "Viktar Dubovik",
            "{\\L}ukasz Struski",
            "Jacek Tabor",
            "Dawid Rymarczyk"
        ],
        "title": "SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence",
        "abstract": "arXiv:2507.19321v1 Announce Type: new  Abstract: Understanding the decisions made by deep neural networks is essential in high-stakes domains such as medical imaging and autonomous driving. Yet, these models often lack transparency, particularly in computer vision. Prototypical-parts-based neural networks have emerged as a promising solution by offering concept-level explanations. However, most are limited to fine-grained classification tasks, with few exceptions such as InfoDisent. InfoDisent extends prototypical models to large-scale datasets like ImageNet, but produces complex explanations.   We introduce Sparse Information Disentanglement for Explainability (SIDE), a novel method that improves the interpretability of prototypical parts through a dedicated training and pruning scheme that enforces sparsity. Combined with sigmoid activations in place of softmax, this approach allows SIDE to associate each class with only a small set of relevant prototypes. Extensive experiments show that SIDE matches the accuracy of existing methods while reducing explanation size by over $90\\%$, substantially enhancing the understandability of prototype-based explanations.",
        "arxiv_id": "2507.19321",
        "ARXIVID": "2507.19321",
        "COMMENT": "Does not match any specific criteria but focuses on explainability in computer vision, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19182": {
        "authors": [
            "Kuncheng Zou",
            "Jiahao Mai",
            "Yonggang Zhang",
            "Yuyi Wang",
            "Ond\\v{r}ej Ku\\v{z}elka",
            "Yuanhong Wang",
            "Yi Chang"
        ],
        "title": "Faster Lifting for Ordered Domains with Predecessor Relations",
        "abstract": "arXiv:2507.19182v1 Announce Type: new  Abstract: We investigate lifted inference on ordered domains with predecessor relations, where the elements of the domain respect a total (cyclic) order, and every element has a distinct (clockwise) predecessor. Previous work has explored this problem through weighted first-order model counting (WFOMC), which computes the weighted sum of models for a given first-order logic sentence over a finite domain. In WFOMC, the order constraint is typically encoded by the linear order axiom introducing a binary predicate in the sentence to impose a linear ordering on the domain elements. The immediate and second predecessor relations are then encoded by the linear order predicate. Although WFOMC with the linear order axiom is theoretically tractable, existing algorithms struggle with practical applications, particularly when the predecessor relations are involved. In this paper, we treat predecessor relations as a native part of the axiom and devise a novel algorithm that inherently supports these relations. The proposed algorithm not only provides an exponential speedup for the immediate and second predecessor relations, which are known to be tractable, but also handles the general k-th predecessor relations. The extensive experiments on lifted inference tasks and combinatorics math problems demonstrate the efficiency of our algorithm, achieving speedups of a full order of magnitude.",
        "arxiv_id": "2507.19182",
        "ARXIVID": "2507.19182",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to efficient inference in ordered domains.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18977": {
        "authors": [
            "Mehrnoosh Mirtaheri",
            "Ryan A. Rossi",
            "Sungchul Kim",
            "Kanak Mahadik",
            "Tong Yu",
            "Xiang Chen",
            "Mohammad Rostami"
        ],
        "title": "Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling",
        "abstract": "arXiv:2507.18977v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) completion models traditionally assume access to the entire graph during training. This overlooks challenges stemming from the evolving nature of TKGs, such as: (i) the model's requirement to generalize and assimilate new knowledge, and (ii) the task of managing new or unseen entities that often have sparse connections. In this paper, we present an incremental training framework specifically designed for TKGs, aiming to address entities that are either not observed during training or have sparse connections. Our approach combines a model-agnostic enhancement layer with a weighted sampling strategy, that can be augmented to and improve any existing TKG completion method. The enhancement layer leverages a broader, global definition of entity similarity, which moves beyond mere local neighborhood proximity of GNN-based methods. The weighted sampling strategy employed in training accentuates edges linked to infrequently occurring entities. We evaluate our method on two benchmark datasets, and demonstrate that our framework outperforms existing methods in total link prediction, inductive link prediction, and in addressing long-tail entities. Notably, our method achieves a 10\\% improvement and a 15\\% boost in MRR for these datasets. The results underscore the potential of our approach in mitigating catastrophic forgetting and enhancing the robustness of TKG completion methods, especially in an incremental training context",
        "arxiv_id": "2507.18977",
        "ARXIVID": "2507.18977",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to temporal knowledge graphs and incremental learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19052": {
        "authors": [
            "Hamid Abdollahi",
            "Amir Hossein Mansouri Majoumerd",
            "Amir Hossein Bagheri Baboukani",
            "Amir Abolfazl Suratgar",
            "Mohammad Bagher Menhaj"
        ],
        "title": "Probing Multimodal Fusion in the Brain: The Dominance of Audiovisual Streams in Naturalistic Encoding",
        "abstract": "arXiv:2507.19052v1 Announce Type: new  Abstract: Predicting brain activity in response to naturalistic, multimodal stimuli is a key challenge in computational neuroscience. While encoding models are becoming more powerful, their ability to generalize to truly novel contexts remains a critical, often untested, question. In this work, we developed brain encoding models using state-of-the-art visual (X-CLIP) and auditory (Whisper) feature extractors and rigorously evaluated them on both in-distribution (ID) and diverse out-of-distribution (OOD) data. Our results reveal a fundamental trade-off between model complexity and generalization: a higher-capacity attention-based model excelled on ID data, but a simpler linear model was more robust, outperforming a competitive baseline by 18\\% on the OOD set. Intriguingly, we found that linguistic features did not improve predictive accuracy, suggesting that for familiar languages, neural encoding may be dominated by the continuous visual and auditory streams over redundant textual information. Spatially, our approach showed marked performance gains in the auditory cortex, underscoring the benefit of high-fidelity speech representations. Collectively, our findings demonstrate that rigorous OOD testing is essential for building robust neuro-AI models and provides nuanced insights into how model architecture, stimulus characteristics, and sensory hierarchies shape the neural encoding of our rich, multimodal world.",
        "arxiv_id": "2507.19052",
        "ARXIVID": "2507.19052",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multimodal learning and neuroscience applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19409": {
        "authors": [
            "Toufiq Parag",
            "Ahmed Elgammal"
        ],
        "title": "Modality Agnostic Efficient Long Range Encoder",
        "abstract": "arXiv:2507.19409v1 Announce Type: new  Abstract: The long-context capability of recent large transformer models can be surmised to rely on techniques such as attention/model parallelism, as well as hardware-level optimizations. While these strategies allow input lengths to scale to millions of tokens, they do not fundamentally mitigate the quadratic computational and memory complexity of the core attention mechanism. In this paper, we address the challenge of long-context processing on a single device using generic implementations by reducing the quadratic memory footprint and inference cost. Existing approaches to extend the context length for generic single device implementations -- such as token merging and modified attentions -- are often modality specific and attain a suboptimal tradeoff between accuracy and efficiency. To overcome these limitations, we propose MAELRE (Modality Agnostic Efficient Long Range Encoder), a unified and efficient transformer architecture designed for long-range encoding across diverse modalities. MAELRE integrates token merging with attention approximation, progressively merging tokens at different stages of internal computational blocks. It employs a lightweight attention approximation when the number of tokens is large, and switches to standard dot-product attention as the sequence becomes shorter through successive aggregation. We demonstrate that MAELRE achieves superior accuracy while reducing computational cost compared to existing long-context models on classification tasks spanning multiple modalities, including text, time series, audio, and vision.",
        "arxiv_id": "2507.19409",
        "ARXIVID": "2507.19409",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to machine learning and efficient transformer architectures.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.18966": {
        "authors": [
            "Saraa Al-Saddik",
            "Manna Elizabeth Philip",
            "Ali Haidar"
        ],
        "title": "YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study",
        "abstract": "arXiv:2507.18966v1 Announce Type: new  Abstract: Accurate identification of vehicle attributes such as make, colour, and shape is critical for law enforcement and intelligence applications. This study evaluates the effectiveness of three state-of-the-art deep learning approaches YOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image dataset. This dataset was collected under challenging and unconstrained conditions by NSW Police Highway Patrol Vehicles. A multi-view inference (MVI) approach was deployed to enhance the performance of the models' predictions. To conduct the analyses, datasets with 100,000 plus images were created for each of the three metadata prediction tasks, specifically make, shape and colour. The models were tested on a separate dataset with 29,937 images belonging to 1809 number plates. Different sets of experiments have been investigated by varying the models sizes. A classification accuracy of 93.70%, 82.86%, 85.19%, and 94.86% was achieved with the best performing make, shape, colour, and colour-binary models respectively. It was concluded that there is a need to use MVI to get usable models within such complex real-world datasets. Our findings indicated that the object detection models YOLO-v11 and YOLO-World outperformed classification-only models in make and shape extraction. Moreover, smaller YOLO variants perform comparably to larger counterparts, offering substantial efficiency benefits for real-time predictions. This work provides a robust baseline for extracting vehicle metadata in real-world scenarios. Such models can be used in filtering and sorting user queries, minimising the time required to search large vehicle images datasets.",
        "arxiv_id": "2507.18966",
        "ARXIVID": "2507.18966",
        "COMMENT": "Does not match any specific criteria but is related to object detection and vehicle metadata extraction.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.19364": {
        "authors": [
            "Patrick Taillandier",
            "Jean Daniel Zucker",
            "Arnaud Grignard",
            "Benoit Gaudou",
            "Nghi Quang Huynh",
            "Alexis Drogoul"
        ],
        "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges",
        "abstract": "arXiv:2507.19364v1 Announce Type: new  Abstract: This position paper examines the use of Large Language Models (LLMs) in social simulation, analyzing both their potential and their limitations from a computational social science perspective. The first part reviews recent findings on the ability of LLMs to replicate key aspects of human cognition, including Theory of Mind reasoning and social inference, while also highlighting significant limitations such as cognitive biases, lack of true understanding, and inconsistencies in behavior. The second part surveys emerging applications of LLMs in multi-agent simulation frameworks, focusing on system architectures, scale, and validation strategies. Notable projects such as Generative Agents (Smallville) and AgentSociety are discussed in terms of their design choices, empirical grounding, and methodological innovations. Particular attention is given to the challenges of behavioral fidelity, calibration, and reproducibility in large-scale LLM-driven simulations. The final section distinguishes between contexts where LLMs, like other black-box systems, offer direct value-such as interactive simulations and serious games-and those where their use is more problematic, notably in explanatory or predictive modeling. The paper concludes by advocating for hybrid approaches that integrate LLMs into traditional agent-based modeling platforms (GAMA, Netlogo, etc), enabling modelers to combine the expressive flexibility of language-based reasoning with the transparency and analytical rigor of classical rule-based systems.",
        "arxiv_id": "2507.19364",
        "ARXIVID": "2507.19364",
        "COMMENT": "Does not match any specific criteria but discusses LLMs in agent-based simulations, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}