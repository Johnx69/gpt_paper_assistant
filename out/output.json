{
    "2510.17722": {
        "authors": [
            "Yaning Pan",
            "Zekun Wang",
            "Qianqian Xie",
            "Yongqian Wen",
            "Yuanxing Zhang",
            "Guohui Zhang",
            "Haoxuan Hu",
            "Zhiyu Pan",
            "Yibing Huang",
            "Zhidong Gan",
            "Yonghong Lin",
            "An Ping",
            "Tianhao Peng",
            "Jiaheng Liu"
        ],
        "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
        "abstract": "arXiv:2510.17722v1 Announce Type: new  Abstract: The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
        "arxiv_id": "2510.17722",
        "ARXIVID": "2510.17722",
        "COMMENT": "Matches criteria 6 as it introduces a new benchmark for video understanding in multi-turn dialogues, focusing on MLLMs and video-based tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.16333": {
        "authors": [
            "Junha Song",
            "Sangdoo Yun",
            "Dongyoon Han",
            "Jaegul Choo",
            "Byeongho Heo"
        ],
        "title": "RL makes MLLMs see better than SFT",
        "abstract": "arXiv:2510.16333v1 Announce Type: new  Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created a void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows a clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct a critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLM's post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into a simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, a PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Project page available at https://june-page.github.io/pivot/",
        "arxiv_id": "2510.16333",
        "ARXIVID": "2510.16333",
        "COMMENT": "Matches criteria 2 and 5 as it explores novel training strategies (RL vs SFT) for MLLMs and their impact on vision encoders, focusing on vision-language integration.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.17519": {
        "authors": [
            "Yongshun Zhang",
            "Zhongyi Fan",
            "Yonghang Zhang",
            "Zhangzikang Li",
            "Weifeng Chen",
            "Zhongwei Feng",
            "Chaoyue Wang",
            "Peng Hou",
            "Anxiang Zeng"
        ],
        "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
        "abstract": "arXiv:2510.17519v1 Announce Type: new  Abstract: In recent years, large-scale generative models for visual content (\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignment, the long sequences involved, and the complex spatiotemporal dependencies. To address these challenges, we present a training framework that optimizes four pillars: (i) data processing, (ii) model architecture, (iii) training strategy, and (iv) infrastructure for large-scale video generation models. These optimizations delivered significant efficiency gains and performance improvements across all stages of data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent state-of-the-art video generators overall and, on e-commerce-oriented video generation tasks, surpasses leading open-source baselines in human evaluations. More importantly, we open-source the complete stack, including model weights, Megatron-Core-based large-scale training code, and inference pipelines for video generation and enhancement. To our knowledge, this is the first public release of large-scale video generation training code that exploits Megatron-Core to achieve high training efficiency and near-linear multi-node scaling, details are available in \\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
        "arxiv_id": "2510.17519",
        "ARXIVID": "2510.17519",
        "COMMENT": "Matches criteria 5 (Integration of Image/Video and Large Language Models) and criteria 6 (Video Understanding) due to its focus on large-scale video generation models and cross-modal alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.15963": {
        "authors": [
            "Jiani Huang",
            "Amish Sethi",
            "Matthew Kuo",
            "Mayank Keoliya",
            "Neelay Velingker",
            "JungHo Jung",
            "Ser-Nam Lim",
            "Ziyang Li",
            "Mayur Naik"
        ],
        "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation",
        "abstract": "arXiv:2510.15963v1 Announce Type: new  Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, current training pipelines primarily rely on high-level vision-sound-text pairs and lack fine-grained, structured alignment between pixel-level visual content and textual semantics. To overcome this challenge, we propose ESCA, a new framework for contextualizing embodied agents through structured spatial-temporal understanding. At its core is SGClip, a novel CLIP-based, open-domain, and promptable model for generating scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic learning pipeline, which harnesses model-driven self-supervision from video-caption pairs and structured reasoning, thereby eliminating the need for human-labeled scene graph annotations. We demonstrate that SGClip supports both prompt-based inference and task-specific fine-tuning, excelling in scene graph generation and action localization benchmarks. ESCA with SGClip consistently improves both open-source and commercial MLLMs, achieving state-of-the-art performance across two embodied environments. Notably, it significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines.",
        "arxiv_id": "2510.15963",
        "ARXIVID": "2510.15963",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces scene-graph generation for embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.17023": {
        "authors": [
            "Shraman Pramanick",
            "Effrosyni Mavroudi",
            "Yale Song",
            "Rama Chellappa",
            "Lorenzo Torresani",
            "Triantafyllos Afouras"
        ],
        "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs",
        "abstract": "arXiv:2510.17023v1 Announce Type: new  Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.",
        "arxiv_id": "2510.17023",
        "ARXIVID": "2510.17023",
        "COMMENT": "Matches criterion 6 (Video Understanding) and criterion 2 (Visual and Multimodal Large Language Models) as it focuses on video temporal grounding using multimodal LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.16756": {
        "authors": [
            "Siyin Wang",
            "Wenyi Yu",
            "Xianzhao Chen",
            "Xiaohai Tian",
            "Jun Zhang",
            "Lu Lu",
            "Chao Zhang"
        ],
        "title": "End-to-end Listen, Look, Speak and Act",
        "abstract": "arXiv:2510.16756v1 Announce Type: new  Abstract: Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.",
        "arxiv_id": "2510.16756",
        "ARXIVID": "2510.16756",
        "COMMENT": "Matches criterion 2 as it introduces a novel multimodal architecture (ELLSA) integrating vision, text, speech, and action, and criterion 3 for its application to embodied AI benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.16888": {
        "authors": [
            "Zongjian Li",
            "Zheyuan Liu",
            "Qihui Zhang",
            "Bin Lin",
            "Shenghai Yuan",
            "Zhiyuan Yan",
            "Yang Ye",
            "Wangbo Yu",
            "Yuwei Niu",
            "Li Yuan"
        ],
        "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback",
        "abstract": "arXiv:2510.16888v1 Announce Type: new  Abstract: Instruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. UniWorld-V2, trained with this framework, achieves \\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available at https://github.com/PKU-YuanGroup/UniWorld-V2.",
        "arxiv_id": "2510.16888",
        "ARXIVID": "2510.16888",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a novel framework for instruction-based image editing using MLLMs and policy optimization, focusing on vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.16907": {
        "authors": [
            "Kangrui Wang",
            "Pingyue Zhang",
            "Zihan Wang",
            "Yaning Gao",
            "Linjie Li",
            "Qineng Wang",
            "Hanyang Chen",
            "Chi Wan",
            "Yiping Lu",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Ranjay Krishna",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Yejin Choi",
            "Manling Li"
        ],
        "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents",
        "abstract": "arXiv:2510.16907v1 Announce Type: new  Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to Language Model (LLM) agents, lies in the shift from textual states to complex visual observations. This transition introduces partial observability and demands robust world modeling. We ask: Can VLM agents construct internal world models through explicit visual state reasoning? To address this question, we architecturally enforce and reward the agent's reasoning process via reinforcement learning (RL), formulating it as a Partially Observable Markov Decision Process (POMDP). We find that decomposing the agent's reasoning into State Estimation (\"what is the current state?\") and Transition Modeling (\"what comes next?\") is critical for success, as demonstrated through five reasoning strategies. Our investigation into how agents represent internal beliefs reveals that the optimal representation is task-dependent: Natural Language excels at capturing semantic relationships in general tasks, while Structured formats are indispensable for precise manipulation and control. Building on these insights, we design a World Modeling Reward that provides dense, turn-level supervision for accurate state prediction, and introduce Bi-Level General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment. Through this form of visual state reasoning, a 3B-parameter model achieves a score of 0.82 across five diverse agent benchmarks, representing a 3$\\times$ improvement over its untrained counterpart (0.21) and outperforming proprietary reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5 (0.62). All experiments are conducted within our VAGEN framework, a scalable system for training and analyzing multi-turn VLM agents in diverse visual environments. Code and data are publicly available at https://vagen-ai.github.io.",
        "arxiv_id": "2510.16907",
        "ARXIVID": "2510.16907",
        "COMMENT": "Matches criterion 5 as it focuses on integrating vision tasks with large language models and reasoning strategies.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.17777": {
        "authors": [
            "Samir Khaki",
            "Junxian Guo",
            "Jiaming Tang",
            "Shang Yang",
            "Yukang Chen",
            "Konstantinos N. Plataniotis",
            "Yao Lu",
            "Song Han",
            "Zhijian Liu"
        ],
        "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
        "abstract": "arXiv:2510.17777v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.",
        "arxiv_id": "2510.17777",
        "ARXIVID": "2510.17777",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) due to its focus on efficient inference for vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.17800": {
        "authors": [
            "Jiale Cheng",
            "Yusen Liu",
            "Xinyu Zhang",
            "Yulin Fei",
            "Wenyi Hong",
            "Ruiliang Lyu",
            "Weihan Wang",
            "Zhe Su",
            "Xiaotao Gu",
            "Xiao Liu",
            "Yushi Bai",
            "Jie Tang",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
        "abstract": "arXiv:2510.17800v1 Announce Type: new  Abstract: Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.",
        "arxiv_id": "2510.17800",
        "ARXIVID": "2510.17800",
        "COMMENT": "Matches criteria 5 (Integration of Image/Video and Large Language Models) due to its focus on combining visual-text compression and LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.16785": {
        "authors": [
            "Jiazhen Liu",
            "Long Chen"
        ],
        "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs",
        "abstract": "arXiv:2510.16785v1 Announce Type: new  Abstract: Integrating diverse visual capabilities into a unified model is a significant trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion of segmentation poses a distinct set of challenges. To equip MLLMs with pixel-level segmentation abilities, prevailing methods require finetuning the model to produce specific outputs compatible with a mask decoder. This process typically alters the model's output space and compromises its intrinsic generalization, which undermines the goal of building a unified model. We introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel plug-and-play solution. LENS attaches a lightweight, trainable head to a completely frozen MLLM. By refining the spatial cues embedded in attention maps, LENS extracts keypoints and describes them into point-wise features directly compatible with the mask decoder. Extensive experiments validate our approach: LENS achieves segmentation performance competitive with or superior to that of retraining-based methods. Crucially, it does so while fully preserving the MLLM's generalization capabilities, which are significantly degraded by finetuning approaches. As such, the attachable design of LENS establishes an efficient and powerful paradigm for extending MLLMs, paving the way for truly multi-talented, unified models.",
        "arxiv_id": "2510.16785",
        "ARXIVID": "2510.16785",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a novel plug-and-play segmentation capability for MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.17790": {
        "authors": [
            "Yuhao Yang",
            "Zhen Yang",
            "Zi-Yi Dou",
            "Anh Nguyen",
            "Keen You",
            "Omar Attia",
            "Andrew Szot",
            "Michael Feng",
            "Ram Ramrakhya",
            "Alexander Toshev",
            "Chao Huang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "abstract": "arXiv:2510.17790v1 Announce Type: new  Abstract: Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.",
        "arxiv_id": "2510.17790",
        "ARXIVID": "2510.17790",
        "COMMENT": "Matches criteria 3 as it introduces a foundation model for computer-use agents with hybrid action, addressing challenges in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.16036": {
        "authors": [
            "Zewen Li",
            "Zitong Yu",
            "Qilang Ye",
            "Weicheng Xie",
            "Wei Zhuo",
            "Linlin Shen"
        ],
        "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection",
        "abstract": "arXiv:2510.16036v1 Announce Type: new  Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold the potential of detecting defective objects in Industrial Anomaly Detection (IAD). However, most traditional IAD methods lack the ability to provide multi-turn human-machine dialogues and detailed descriptions, such as the color of objects, the shape of an anomaly, or specific types of anomalies. At the same time, methods based on large pre-trained models have not fully stimulated the ability of large models in anomaly detection tasks. In this paper, we explore the combination of rich text semantics with both image-level and pixel-level information from images and propose IAD-GPT, a novel paradigm based on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate detailed anomaly prompts for specific objects. These specific prompts from the large language model (LLM) are used to activate the detection and segmentation functions of the pre-trained visual-language model (i.e., CLIP). To enhance the visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein image features interact with normal and abnormal text prompts to dynamically select enhancement pathways, which enables language models to focus on specific aspects of visual data, enhancing their ability to accurately interpret and respond to anomalies within images. Moreover, we design a Multi-Mask Fusion module to incorporate mask as expert knowledge, which enhances the LLM's perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA datasets demonstrate our state-of-the-art performance on self-supervised and few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA datasets. The codes are available at \\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.",
        "arxiv_id": "2510.16036",
        "ARXIVID": "2510.16036",
        "COMMENT": "Matches criteria 2 and 5 as it proposes a novel MLLM-based approach for industrial anomaly detection, integrating image-level and pixel-level information with text semantics.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.16134": {
        "authors": [
            "Chen Kong",
            "James Fort",
            "Aria Kang",
            "Jonathan Wittmer",
            "Simon Green",
            "Tianwei Shen",
            "Yipu Zhao",
            "Cheng Peng",
            "Gustavo Solaira",
            "Andrew Berkovich",
            "Nikhil Raina",
            "Vijay Baiyya",
            "Evgeniy Oleinik",
            "Eric Huang",
            "Fan Zhang",
            "Julian Straub",
            "Mark Schwesinger",
            "Luis Pesqueira",
            "Xiaqing Pan",
            "Jakob Julian Engel",
            "Carl Ren",
            "Mingfei Yan",
            "Richard Newcombe"
        ],
        "title": "Aria Gen 2 Pilot Dataset",
        "abstract": "arXiv:2510.16134v1 Announce Type: new  Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely access, A2PD is released incrementally with ongoing dataset enhancements. The initial release features Dia'ane, our primary subject, who records her daily activities alongside friends, each equipped with Aria Gen 2 glasses. It encompasses five primary scenarios: cleaning, cooking, eating, playing, and outdoor walking. In each of the scenarios, we provide comprehensive raw sensor data and output data from various machine perception algorithms. These data illustrate the device's ability to perceive the wearer, the surrounding environment, and interactions between the wearer and the environment, while maintaining robust performance across diverse users and conditions. The A2PD is publicly available at projectaria.com, with open-source tools and usage examples provided in Project Aria Tools.",
        "arxiv_id": "2510.16134",
        "ARXIVID": "2510.16134",
        "COMMENT": "Matches criteria 3 as it introduces a new egocentric multimodal dataset for embodied AI, which could be useful for benchmarking and advancing embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.17364": {
        "authors": [
            "Vaggelis Dorovatas",
            "Soroush Seifi",
            "Gunshi Gupta",
            "Rahaf Aljundi"
        ],
        "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
        "abstract": "arXiv:2510.17364v1 Announce Type: new  Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos in-context, provided they have full access to the video when answering queries. However, these models face challenges in streaming scenarios where hour-long videos must be processed online, and questions need timely responses. In this work, we propose a training-free approach compatible with standard Video-LLMs, leveraging three key concepts: 1) LLM-informed selection of visual tokens to identify those that the LLM has attended to and contributed to its understanding of each short clip. Our attention-based selection allows us to discard up to ~95% of unimportant visual tokens with minimal performance loss; 2) Recurrent processing of past selected tokens to generate temporally coherent understanding of each processed clip; 3) Caption-based question answering for lightweight and accurate responses. Our method achieves state-of-the-art performance on streaming video benchmarks, striking a balance between efficiency and effectiveness.",
        "arxiv_id": "2510.17364",
        "ARXIVID": "2510.17364",
        "COMMENT": "Matches criterion 5 as it discusses techniques combining video understanding and large language models (Video-LLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.17188": {
        "authors": [
            "Vaibhav Rathore",
            "Divyam Gupta",
            "Biplab Banerjee"
        ],
        "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
        "abstract": "arXiv:2510.17188v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into either seen categories** -- available during training -- or novel ones, without relying on label supervision. Most existing GCD methods assume simultaneous access to labeled and unlabeled data during training and arising from the same domain, limiting applicability in open-world scenarios involving distribution shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by requiring models to generalize to unseen domains containing novel categories, without accessing targetdomain data during training. The only prior DG-GCD method, DG2CD-Net, relies on episodic training with multiple synthetic domains and task vector aggregation, incurring high computational cost and error accumulation. We propose HIDISC, a hyperbolic representation learning framework that achieves domain and category-level generalization without episodic simulation. To expose the model to minimal but diverse domain variations, we augment the source domain using GPT-guided diffusion, avoiding overfitting while maintaining efficiency. To structure the representation space, we introduce Tangent CutMix, a curvature-aware interpolation that synthesizes pseudo-novel samples in tangent space, preserving manifold consistency. A unified loss -- combining penalized Busemann alignment, hybrid hyperbolic contrastive regularization, and adaptive outlier repulsion -- **facilitates compact, semantically structured embeddings. A learnable curvature parameter further adapts the geometry to dataset complexity. HIDISC achieves state-of-the-art results on PACS , Office-Home , and DomainNet, consistently outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.",
        "arxiv_id": "2510.17188",
        "ARXIVID": "2510.17188",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on domain generalization and novel hyperbolic representation learning for category discovery in unseen domains.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16258": {
        "authors": [
            "Claire McLean",
            "Makenzie Meendering",
            "Tristan Swartz",
            "Orri Gabbay",
            "Alexandra Olsen",
            "Rachel Jacobs",
            "Nicholas Rosen",
            "Philippe de Bree",
            "Tony Garcia",
            "Gadsden Merrill",
            "Jake Sandakly",
            "Julia Buffalini",
            "Neham Jain",
            "Steven Krenn",
            "Moneish Kumar",
            "Dejan Markovic",
            "Evonne Ng",
            "Fabian Prada",
            "Andrew Saba",
            "Siwei Zhang",
            "Vasu Agrawal",
            "Tim Godisart",
            "Alexander Richard",
            "Michael Zollhoefer"
        ],
        "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset",
        "abstract": "arXiv:2510.16258v1 Announce Type: new  Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of 500 individual hours of 3D motion data from 439 participants collected in a multi-camera collection stage, amounting to over 54 million frames of tracked 3D motion. The dataset features a wide range of single-person motion data, including prompted motions, hand gestures, and locomotion; as well as multi-person behavioral and conversational data like discussions, conversations in different emotional states, collaborative activities, and co-living scenarios in an apartment-like space. We provide tracked human motion including hand tracking and body shape, text annotations, and a separate audio track for each participant.",
        "arxiv_id": "2510.16258",
        "ARXIVID": "2510.16258",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a large-scale multimodal motion and behavior dataset for embodied AI research.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16777": {
        "authors": [
            "Junbo Li",
            "Weimin Yuan",
            "Yinuo Wang",
            "Yue Zeng",
            "Shihao Shu",
            "Cai Meng",
            "Xiangzhi Bai"
        ],
        "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation",
        "abstract": "arXiv:2510.16777v1 Announce Type: new  Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer vision, and current research typically predicts the 6D pose by establishing correspondences between 2D image features and 3D model features. However, these methods often face difficulties with textureless objects and varying illumination conditions. To overcome these limitations, we propose GS2POSE, a novel approach for 6D object pose estimation. GS2POSE formulates a pose regression algorithm inspired by the principles of Bundle Adjustment (BA). By leveraging Lie algebra, we extend the capabilities of 3DGS to develop a pose-differentiable rendering pipeline, which iteratively optimizes the pose by comparing the input image to the rendered image. Additionally, GS2POSE updates color parameters within the 3DGS model, enhancing its adaptability to changes in illumination. Compared to previous models, GS2POSE demonstrates accuracy improvements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and LineMod datasets, respectively.",
        "arxiv_id": "2510.16777",
        "ARXIVID": "2510.16777",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel method for 6D object pose estimation, tackling challenges like textureless objects.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16781": {
        "authors": [
            "Shihao Ji",
            "Zihui Song"
        ],
        "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features",
        "abstract": "arXiv:2510.16781v1 Announce Type: new  Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual Language Models (VLMs) on static images have yet to be fully translated to the video domain. Conventional video understanding models often rely on extensive, task-specific training on annotated datasets, a process that is both costly and limited in scalability. This paper introduces a novel, training-free framework for video understanding that circumvents end-to-end training by synergistically combining the rich semantic priors of pre-trained VLMs with classic machine learning algorithms for pattern discovery. Our core idea is to reframe video understanding as a self-supervised spatio-temporal clustering problem within a high-dimensional semantic feature space. The proposed pipeline first transforms a video stream into a semantic feature trajectory using the frozen visual encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal Segmentation (KTS), a robust machine learning technique, to partition the continuous feature stream into discrete, semantically coherent event segments. These segments are then subjected to unsupervised density-based clustering to identify recurring macroscopic scenes and themes throughout the video. By selecting representative keyframes from each discovered cluster and leveraging the VLM's generative capabilities for textual description, our framework automatically produces a structured, multi-modal summary of the video content. This approach provides an effective, interpretable, and model-agnostic pathway for zero-shot, automated structural analysis of video content.",
        "arxiv_id": "2510.16781",
        "ARXIVID": "2510.16781",
        "COMMENT": "Matches criteria 6 (Video Understanding) due to its focus on video-based tasks like summarization and classification using a novel training-free framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17771": {
        "authors": [
            "Zhining Liu",
            "Ziyi Chen",
            "Hui Liu",
            "Chen Luo",
            "Xianfeng Tang",
            "Suhang Wang",
            "Joy Zeng",
            "Zhenwei Dai",
            "Zhan Shi",
            "Tianxin Wei",
            "Benoit Dumoulin",
            "Hanghang Tong"
        ],
        "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs",
        "abstract": "arXiv:2510.17771v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term ``seeing but not believing'' that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.",
        "arxiv_id": "2510.17771",
        "ARXIVID": "2510.17771",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it investigates visual attention and reasoning in VLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17664": {
        "authors": [
            "Ling Liu",
            "Jun Tian",
            "Li Yi"
        ],
        "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
        "abstract": "arXiv:2510.17664v1 Announce Type: new  Abstract: 4D panoptic segmentation in a streaming setting is critical for highly dynamic environments, such as evacuating dense crowds and autonomous driving in complex scenarios, where real-time, fine-grained perception within a constrained time budget is essential. In this paper, we introduce 4DSegStreamer, a novel framework that employs a Dual-Thread System to efficiently process streaming frames. The framework is general and can be seamlessly integrated into existing 3D and 4D segmentation methods to enable real-time capability. It also demonstrates superior robustness compared to existing streaming perception approaches, particularly under high FPS conditions. The system consists of a predictive thread and an inference thread. The predictive thread leverages historical motion and geometric information to extract features and forecast future dynamics. The inference thread ensures timely prediction for incoming frames by aligning with the latest memory and compensating for ego-motion and dynamic object movements. We evaluate 4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of our approach, particularly in accurately predicting dynamic objects in complex scenes.",
        "arxiv_id": "2510.17664",
        "ARXIVID": "2510.17664",
        "COMMENT": "Matches criterion 6 as it focuses on 4D panoptic segmentation in streaming settings, advancing video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17137": {
        "authors": [
            "WenBo Xu",
            "Liu Liu",
            "Li Zhang",
            "Ran Zhang",
            "Hao Wu",
            "Dan Guo",
            "Meng Wang"
        ],
        "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
        "abstract": "arXiv:2510.17137v1 Announce Type: new  Abstract: Articulated objects, such as laptops and drawers, exhibit significant challenges for 3D reconstruction and pose estimation due to their multi-part geometries and variable joint configurations, which introduce structural diversity across different states. To address these challenges, we propose KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation, a unified framework for reconstructing diverse articulated instances and pose estimation from single view input. Specifically, we first encode complete geometry (SDFs), joint angles, and part segmentation into a structured latent space via a novel Kinematic-Aware VAE (KA-VAE). In addition, we employ two conditional diffusion models: one for regressing global pose (SE(3)) and joint parameters, and another for generating the kinematic-aware latent code from partial observations. Finally, we produce an iterative optimization module that bidirectionally refines reconstruction accuracy and kinematic parameters via Chamfer-distance minimization while preserving articulation constraints. Experimental results on synthetic, semi-synthetic, and real-world datasets demonstrate the effectiveness of our approach in accurately reconstructing articulated objects and estimating their kinematic properties.",
        "arxiv_id": "2510.17137",
        "ARXIVID": "2510.17137",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for articulated object reconstruction and pose estimation, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16769": {
        "authors": [
            "Shuo Han",
            "Yukun Cao",
            "Zezhong Ding",
            "Zengyi Gao",
            "S Kevin Zhou",
            "Xike Xie"
        ],
        "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models",
        "abstract": "arXiv:2510.16769v1 Announce Type: new  Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but remain limited by input-token constraints, facing scalability bottlenecks and lacking effective mechanisms to coordinate textual and visual modalities. To address these challenges, we propose GraphVista, a unified framework that enhances both scalability and modality coordination in graph understanding. For scalability, GraphVista organizes graph information hierarchically into a lightweight GraphRAG base, which retrieves only task-relevant textual descriptions and high-resolution visual subgraphs, compressing redundant context while preserving key reasoning elements. For modality coordination, GraphVista introduces a planning agent that routes tasks to the most suitable modality-using the text modality for simple property reasoning and the visual modality for local and structurally complex reasoning grounded in explicit topology. Extensive experiments demonstrate that GraphVista scales to large graphs, up to $200\\times$ larger than those used in existing benchmarks, and consistently outperforms existing textual, visual, and fusion-based methods, achieving up to $4.4\\times$ quality improvement over the state-of-the-art baselines by fully exploiting the complementary strengths of both modalities.",
        "arxiv_id": "2510.16769",
        "ARXIVID": "2510.16769",
        "COMMENT": "Matches criterion 2 as it explores vision-language models for graph understanding, focusing on scalability and modality coordination.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17568": {
        "authors": [
            "Kaichen Zhou",
            "Yuhan Wang",
            "Grace Chen",
            "Xinhai Chang",
            "Gaspard Beaudouin",
            "Fangneng Zhan",
            "Paul Pu Liang",
            "Mengyu Wang"
        ],
        "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
        "abstract": "arXiv:2510.17568v1 Announce Type: new  Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.",
        "arxiv_id": "2510.17568",
        "ARXIVID": "2510.17568",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks like depth estimation and point cloud reconstruction in dynamic scenes.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16870": {
        "authors": [
            "Yudan Ren",
            "Xinlong Wang",
            "Kexin Wang",
            "Tian Xia",
            "Zihan Ma",
            "Zhaowei Li",
            "Xiangrong Bi",
            "Xiao Li",
            "Xiaowei He"
        ],
        "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding",
        "abstract": "arXiv:2510.16870v1 Announce Type: new  Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising results, current understanding of the parallels between artificial neural networks (ANNs) and human brain processing remains limited: (1) unimodal ANN studies fail to capture the brain's inherent multimodal processing capabilities, and (2) multimodal ANN research primarily focuses on high-level model outputs, neglecting the crucial role of individual neurons. To address these limitations, we propose a novel neuron-level analysis framework that investigates the multimodal information processing mechanisms in vision-language models (VLMs) through the lens of human brain activity. Our approach uniquely combines fine-grained artificial neuron (AN) analysis with fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP and METER. Our analysis reveals four key findings: (1) ANs successfully predict biological neurons (BNs) activities across multiple functional networks (including language, vision, attention, and default mode), demonstrating shared representational mechanisms; (2) Both ANs and BNs demonstrate functional redundancy through overlapping neural representations, mirroring the brain's fault-tolerant and collaborative information processing mechanisms; (3) ANs exhibit polarity patterns that parallel the BNs, with oppositely activated BNs showing mirrored activation trends across VLM layers, reflecting the complexity and bidirectional nature of neural information processing; (4) The architectures of CLIP and METER drive distinct BNs: CLIP's independent branches show modality-specific specialization, whereas METER's cross-modal design yields unified cross-modal activation, highlighting the architecture's influence on ANN brain-like properties. These results provide compelling evidence for brain-like hierarchical processing in VLMs at the neuronal level.",
        "arxiv_id": "2510.16870",
        "ARXIVID": "2510.16870",
        "COMMENT": "Matches criterion 2 as it explores vision-language models (VLMs) and their multimodal processing mechanisms.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17197": {
        "authors": [
            "Pu Zhang",
            "Yuwei Li",
            "Xingyuan Xian",
            "Guoming Tang"
        ],
        "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models",
        "abstract": "arXiv:2510.17197v1 Announce Type: new  Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can process increasingly large inputs, which, unlike in LLMs, generates significant visual token redundancy and leads to prohibitive inference costs. While many methods aim to reduce these costs by pruning visual tokens, existing approaches, whether based on attention or diversity, typically neglect the guidance of the text prompt and thus fail to prioritize task relevance. In this work, we propose a novel, zero-shot method that reframes the problem by introducing a prompt-aware perspective, explicitly modeling visual token pruning as a balance between task relevance and information diversity. Our hierarchical approach first selects a core set of task-relevant visual tokens and then supplements them with diversity tokens to preserve broader context. Experiments across multiple models and benchmarks show that our method achieves performance that matches or surpasses the state-of-the-art with only minimal accuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these gains are accompanied by significant reductions in GPU memory footprint and inference latency.",
        "arxiv_id": "2510.17197",
        "ARXIVID": "2510.17197",
        "COMMENT": "Matches criterion 5 as it introduces a prompt-aware token pruning method for vision-language models, focusing on task relevance and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16416": {
        "authors": [
            "Xiaojun Guo",
            "Runyu Zhou",
            "Yifei Wang",
            "Qi Zhang",
            "Chenheng Zhang",
            "Stefanie Jegelka",
            "Xiaohan Wang",
            "Jiajun Chai",
            "Guojun Yin",
            "Wei Lin",
            "Yisen Wang"
        ],
        "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning",
        "abstract": "arXiv:2510.16416v1 Announce Type: new  Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.",
        "arxiv_id": "2510.16416",
        "ARXIVID": "2510.16416",
        "COMMENT": "Matches criterion 2 as it explores vision-language models and introduces a novel reinforcement learning framework for vision-language reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17450": {
        "authors": [
            "Johan Schubert",
            "Farzad Kamrani",
            "Tove Gustavi"
        ],
        "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions",
        "abstract": "arXiv:2510.17450v1 Announce Type: new  Abstract: We develop an active inference route-planning method for the autonomous control of intelligent agents. The aim is to reconnoiter a geographical area to maintain a common operational picture. To achieve this, we construct an evidence map that reflects our current understanding of the situation, incorporating both positive and \"negative\" sensor observations of possible target objects collected over time, and diffusing the evidence across the map as time progresses. The generative model of active inference uses Dempster-Shafer theory and a Gaussian sensor model, which provides input to the agent. The generative process employs a Bayesian approach to update a posterior probability distribution. We calculate the variational free energy for all positions within the area by assessing the divergence between a pignistic probability distribution of the evidence map and a posterior probability distribution of a target object based on the observations, including the level of surprise associated with receiving new observations. Using the free energy, we direct the agents' movements in a simulation by taking an incremental step toward a position that minimizes the free energy. This approach addresses the challenge of exploration and exploitation, allowing agents to balance searching extensive areas of the geographical map while tracking identified target objects.",
        "arxiv_id": "2510.17450",
        "ARXIVID": "2510.17450",
        "COMMENT": "Matches criterion 1 as it presents a novel methodological improvement in spatial reasoning for embodied agents using active inference.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16732": {
        "authors": [
            "Xinqing Li",
            "Xin He",
            "Le Zhang",
            "Yun Liu"
        ],
        "title": "A Comprehensive Survey on World Models for Embodied AI",
        "abstract": "arXiv:2510.16732v1 Announce Type: new  Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.",
        "arxiv_id": "2510.16732",
        "ARXIVID": "2510.16732",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive survey on world models for embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2510.16753": {
        "authors": [
            "Wei Huang",
            "Peining Li",
            "Meiyu Liang",
            "Xu Hou",
            "Junping Du",
            "Yingxia Shao",
            "Guanhua Ye",
            "Wu Liu",
            "Kangkang Lu",
            "Yang Yu"
        ],
        "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion",
        "abstract": "arXiv:2510.16753v1 Announce Type: new  Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by incorporating visual and textual modalities, enabling richer and more expressive entity representations. However, existing MKGs often suffer from incompleteness, which hinder their effectiveness in downstream tasks. Therefore, multimodal knowledge graph completion (MKGC) task is receiving increasing attention. While large language models (LLMs) have shown promise for knowledge graph completion (KGC), their application to the multimodal setting remains underexplored. Moreover, applying Multimodal Large Language Models (MLLMs) to the task of MKGC introduces significant challenges: (1) the large number of image tokens per entity leads to semantic noise and modality conflicts, and (2) the high computational cost of processing large token inputs. To address these issues, we propose Efficient Lightweight Multimodal Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token Compressor (MVTC) based on multi-head attention mechanism, which adaptively compresses image tokens from both textual and visual views, thereby effectively reducing redundancy while retaining necessary information and avoiding modality conflicts. Additionally, we design an attention pruning strategy to remove redundant attention layers from MLLMs, thereby significantly reducing the inference cost. We further introduce a linear projection to compensate for the performance degradation caused by pruning. Extensive experiments on benchmark FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art performance while substantially improving computational efficiency, establishing a new paradigm for multimodal knowledge graph completion.",
        "arxiv_id": "2510.16753",
        "ARXIVID": "2510.16753",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a method for multimodal knowledge graph completion using MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.16989": {
        "authors": [
            "Luca Zanella",
            "Massimiliano Mancini",
            "Yiming Wang",
            "Alessio Tonioni",
            "Elisa Ricci"
        ],
        "title": "Training-free Online Video Step Grounding",
        "abstract": "arXiv:2510.16989v1 Announce Type: new  Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims to detect which steps are performed in a video. Standard approaches for this task require a labeled training set (e.g., with step-level annotations or narrations), which may be costly to collect. Moreover, they process the full video offline, limiting their applications for scenarios requiring online decisions. Thus, in this work, we explore how to perform VSG online and without training. We achieve this by exploiting the zero-shot capabilities of recent Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step associated with a restricted set of frames, without access to the whole video. We show that this online strategy without task-specific tuning outperforms offline and training-based models. Motivated by this finding, we develop Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting knowledge of past frames into the LMM-based predictions. BaGLM exploits Bayesian filtering principles, modeling step transitions via (i) a dependency matrix extracted through large language models and (ii) an estimation of step progress. Experiments on three datasets show superior performance of BaGLM over state-of-the-art training-based offline methods.",
        "arxiv_id": "2510.16989",
        "ARXIVID": "2510.16989",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel method for video step grounding using large multimodal models and Bayesian filtering.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17418": {
        "authors": [
            "Mustafa F. Abdelwahed",
            "Alice Toniolo",
            "Joan Espasa",
            "Ian P. Gent"
        ],
        "title": "Diverse Planning with Simulators via Linear Temporal Logic",
        "abstract": "arXiv:2510.17418v1 Announce Type: new  Abstract: Autonomous agents rely on automated planning algorithms to achieve their objectives. Simulation-based planning offers a significant advantage over declarative models in modelling complex environments. However, relying solely on a planner that produces a single plan may not be practical, as the generated plans may not always satisfy the agent's preferences. To address this limitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner explicitly designed for simulation-based planning problems. $\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define semantic diversity criteria, enabling agents to specify what constitutes meaningfully different plans. By integrating these LTL-based diversity models directly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the generation of semantically diverse plans, addressing a critical limitation of existing diverse planning approaches that may produce syntactically different but semantically identical solutions. Extensive evaluations on various benchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates more diverse plans compared to a baseline approach. This work establishes the feasibility of semantically-guided diverse planning in simulation-based environments, paving the way for innovative approaches in realistic, non-symbolic domains where traditional model-based approaches fail.",
        "arxiv_id": "2510.17418",
        "ARXIVID": "2510.17418",
        "COMMENT": "Matches criteria 3 as it proposes a novel planning approach using Linear Temporal Logic for simulation-based environments, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17382": {
        "authors": [
            "Rishabh Jain",
            "Keisuke Okumura",
            "Michael Amir",
            "Amanda Prorok"
        ],
        "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding",
        "abstract": "arXiv:2510.17382v1 Announce Type: new  Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF) problems in real-time remains challenging even for state-of-the-art planners. To this end, we develop a hybrid framework that integrates a learned heuristic derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a leading search-based algorithm, LaCAM. While prior work has explored learning-guided search in MAPF, such methods have historically underperformed. In contrast, our approach, termed LaGAT, outperforms both purely search-based and purely learning-based methods in dense scenarios. This is achieved through an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of interest, and a deadlock detection scheme to account for imperfect neural guidance. Our results demonstrate that, when carefully designed, hybrid search offers a powerful solution for tightly coupled, challenging multi-agent coordination problems.",
        "arxiv_id": "2510.17382",
        "ARXIVID": "2510.17382",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for multi-agent pathfinding, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17274": {
        "authors": [
            "Katie Luo",
            "Jingwei Ji",
            "Tong He",
            "Runsheng Xu",
            "Yichen Xie",
            "Dragomir Anguelov",
            "Mingxing Tan"
        ],
        "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
        "abstract": "arXiv:2510.17274v1 Announce Type: new  Abstract: Current autonomous driving systems rely on specialized models for perceiving and predicting motion, which demonstrate reliable performance in standard conditions. However, generalizing cost-effectively to diverse real-world scenarios remains a significant challenge. To address this, we propose Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion forecasting models with multimodal large language models (MLLMs). PnF builds on the insight that natural language provides a more effective way to describe and handle complex scenarios, enabling quick adaptation to targeted behaviors. We design prompts to extract structured scene understanding from MLLMs and distill this information into learnable embeddings to augment existing behavior prediction models. Our method leverages the zero-shot reasoning capabilities of MLLMs to achieve significant improvements in motion prediction performance, while requiring no fine-tuning -- making it practical to adopt. We validate our approach on two state-of-the-art motion forecasting models using the Waymo Open Motion Dataset and the nuScenes Dataset, demonstrating consistent performance improvements across both benchmarks.",
        "arxiv_id": "2510.17274",
        "ARXIVID": "2510.17274",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) and their application in motion forecasting.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.17681": {
        "authors": [
            "Yuandong Pu",
            "Le Zhuo",
            "Songhao Han",
            "Jinbo Xing",
            "Kaiwen Zhu",
            "Shuo Cao",
            "Bin Fu",
            "Si Liu",
            "Hongsheng Li",
            "Yu Qiao",
            "Wenlong Zhang",
            "Xi Chen",
            "Yihao Liu"
        ],
        "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
        "abstract": "arXiv:2510.17681v1 Announce Type: new  Abstract: Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc). We further propose the PICAEval, a reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct a training dataset PICA-100K. After evaluating most of the mainstream models, we observe that physical realism remains a challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as a foundation for future work moving from naive content editing toward physically consistent realism.",
        "arxiv_id": "2510.17681",
        "ARXIVID": "2510.17681",
        "COMMENT": "Matches criteria 7 (Vision-Focused Survey Papers) as it systematically evaluates physical realism in image editing.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.16444": {
        "authors": [
            "Kunyu Peng",
            "Di Wen",
            "Jia Fu",
            "Jiamin Wu",
            "Kailun Yang",
            "Junwei Zheng",
            "Ruiping Liu",
            "Yufan Chen",
            "Yuqian Fu",
            "Danda Pani Paudel",
            "Luc Van Gool",
            "Rainer Stiefelhagen"
        ],
        "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba",
        "abstract": "arXiv:2510.16444v1 Announce Type: new  Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize fine-grained, atomic-level actions of a specific person of interest conditioned on natural language descriptions. Distinct from conventional action recognition and detection tasks, RAVAR emphasizes precise language-guided action understanding, which is particularly critical for interactive human action analysis in complex multi-person scenarios. In this work, we extend our previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million frames and >75.1k annotated persons in total. We benchmark this dataset using baselines from multiple related domains, including atomic action localization, video question answering, and text-video retrieval, as well as our earlier model, RefAtomNet. Although RefAtomNet surpasses other baselines by incorporating agent attention to highlight salient features, its ability to align and retrieve cross-modal information remains limited, leading to suboptimal performance in localizing the target person and predicting fine-grained actions. To overcome the aforementioned limitations, we introduce RefAtomNet++, a novel framework that advances cross-modal token aggregation through a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at the partial-keyword, scene-attribute, and holistic-sentence levels. In particular, scanning trajectories are constructed by dynamically selecting the nearest visual spatial tokens at each timestep for both partial-keyword and scene-attribute levels. Moreover, we design a multi-hierarchical semantic-aligned cross-attention strategy, enabling more effective aggregation of spatial and temporal tokens across different semantic hierarchies. Experiments show that RefAtomNet++ establishes new state-of-the-art results. The dataset and code are released at https://github.com/KPeng9510/refAVA2.",
        "arxiv_id": "2510.16444",
        "ARXIVID": "2510.16444",
        "COMMENT": "Matches criteria 6 (Video Understanding) due to its focus on video-based tasks like action recognition and localization.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.17685": {
        "authors": [
            "Min Cao",
            "Xinyu Zhou",
            "Ding Jiang",
            "Bo Du",
            "Mang Ye",
            "Min Zhang"
        ],
        "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning",
        "abstract": "arXiv:2510.17685v1 Announce Type: new  Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person using textual descriptions, facing challenge in modality heterogeneity. Prior works have attempted to address it by developing cross-modal global or local alignment strategies. However, global methods typically overlook fine-grained cross-modal differences, whereas local methods require prior information to explore explicit part alignments. Additionally, current methods are English-centric, restricting their application in multilingual contexts. To alleviate these issues, we pioneer a multilingual TIPR task by developing a multilingual TIPR benchmark, for which we leverage large language models for initial translations and refine them by integrating domain-specific knowledge. Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation Reasoning and Aligning framework to learn alignment across languages and modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module enables bidirectional prediction of masked image and text, implicitly enhancing the modeling of local relations across languages and modalities, a multi-dimensional global alignment module is integrated to bridge the modality heterogeneity. The proposed method achieves new state-of-the-art results on all multilingual TIPR datasets. Data and code are presented in https://github.com/Flame-Chasers/Bi-IRRA.",
        "arxiv_id": "2510.17685",
        "ARXIVID": "2510.17685",
        "COMMENT": "Matches criterion 5 as it focuses on multilingual text-to-image person retrieval, combining image understanding and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.16290": {
        "authors": [
            "Yue Zheng",
            "Xiufang Shi",
            "Jiming Chen",
            "Yuanchao Shu"
        ],
        "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models",
        "abstract": "arXiv:2510.16290v1 Announce Type: new  Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of Vision-Language Models (VLMs). While these models offer superior zero-shot detection capabilities, their immense computational cost and unstable visual grounding performance hinder real-time deployment. To overcome these challenges, we introduce Cerberus, a two-stage cascaded system designed for efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules offline, and combines lightweight filtering with fine-grained VLM reasoning during online inference. The performance gains of Cerberus come from two key innovations: motion mask prompting and rule-based deviation detection. The former directs the VLM's attention to regions relevant to motion, while the latter identifies anomalies as deviations from learned norms rather than enumerating possible anomalies. Extensive evaluations on four datasets show that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a 151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art VLM-based VAD methods, establishing it as a practical solution for real-time video analytics.",
        "arxiv_id": "2510.16290",
        "ARXIVID": "2510.16290",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a real-time video anomaly detection system using cascaded vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.16596": {
        "authors": [
            "Yiyang Huang",
            "Liang Shi",
            "Yitian Zhang",
            "Yi Xu",
            "Yun Fu"
        ],
        "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense",
        "abstract": "arXiv:2510.16596v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks. However, object hallucination, where models produce plausible but inaccurate object descriptions, remains a significant challenge. In contrast to previous work focusing on LLM components, this paper is the first to trace LVLM hallucinations to visual encoders and identifies three key issues: statistical bias, inherent bias, and vulnerability. To address these challenges, we propose SHIELD, a training-free framework that mitigates hallucinations through three strategies: re-weighting visual tokens to reduce statistical bias, introducing noise-derived tokens to counter inherent bias, and applying adversarial attacks with contrastive decoding to address vulnerability. Experiments demonstrate that SHIELD effectively mitigates object hallucinations across diverse benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on the general LVLM benchmark, highlighting its broad applicability. Code will be released.",
        "arxiv_id": "2510.16596",
        "ARXIVID": "2510.16596",
        "COMMENT": "Matches criteria 2 as it addresses hallucination issues in LVLMs by improving visual encoders, focusing on vision-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.16614": {
        "authors": [
            "Xuan Zhang",
            "Ruixiao Li",
            "Zhijian Zhou",
            "Long Li",
            "Yulei Qin",
            "Ke Li",
            "Xing Sun",
            "Xiaoyu Tan",
            "Chao Qu",
            "Yuan Qi"
        ],
        "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards",
        "abstract": "arXiv:2510.16614v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.",
        "arxiv_id": "2510.16614",
        "ARXIVID": "2510.16614",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores reinforcement learning to improve reasoning in LLMs, which could be extended to multimodal tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.16822": {
        "authors": [
            "Yahia Battach",
            "Abdulwahab Felemban",
            "Faizan Farooq Khan",
            "Yousef A. Radwan",
            "Xiang Li",
            "Fabio Marchese",
            "Sara Beery",
            "Burton H. Jones",
            "Francesca Benzoni",
            "Mohamed Elhoseiny"
        ],
        "title": "ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification",
        "abstract": "arXiv:2510.16822v1 Announce Type: new  Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as climate change, underscoring the urgent need for scalable, automated monitoring. We introduce ReefNet, a large public coral reef image dataset with point-label annotations mapped to the World Register of Marine Species (WoRMS). ReefNet aggregates imagery from 76 curated CoralNet sources and an additional site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level hard coral annotations with expert-verified labels. Unlike prior datasets, which are often limited by size, geography, or coarse labels and are not ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global scale to WoRMS. We propose two evaluation settings: (i) a within-source benchmark that partitions each source's images for localized evaluation, and (ii) a cross-source benchmark that withholds entire sources to test domain generalization. We analyze both supervised and zero-shot classification performance on ReefNet and find that while supervised within-source performance is promising, supervised performance drops sharply across domains, and performance is low across the board for zero-shot models, especially for rare and visually similar genera. This provides a challenging benchmark intended to catalyze advances in domain generalization and fine-grained coral classification. We will release our dataset, benchmarking code, and pretrained models to advance robust, domain-adaptive, global coral reef monitoring and conservation.",
        "arxiv_id": "2510.16822",
        "ARXIVID": "2510.16822",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to the introduction of a new benchmark dataset for coral classification, which addresses domain generalization challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.17626": {
        "authors": [
            "Fr\\'ed\\'eric LIN (Universit\\'e Paris-Saclay",
            "CEA",
            "List",
            "F-91120",
            "Palaiseau",
            "France)",
            "Biruk Abere Ambaw (Universit\\'e Paris-Saclay",
            "CEA",
            "List",
            "F-91120",
            "Palaiseau",
            "France)",
            "Adrian Popescu (Universit\\'e Paris-Saclay",
            "CEA",
            "List",
            "F-91120",
            "Palaiseau",
            "France)",
            "Hejer Ammar (Universit\\'e Paris-Saclay",
            "CEA",
            "List",
            "F-91120",
            "Palaiseau",
            "France)",
            "Romaric Audigier (Universit\\'e Paris-Saclay",
            "CEA",
            "List",
            "F-91120",
            "Palaiseau",
            "France)",
            "Herv\\'e Le Borgne (Universit\\'e Paris-Saclay",
            "CEA",
            "List",
            "F-91120",
            "Palaiseau",
            "France)"
        ],
        "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation",
        "abstract": "arXiv:2510.17626v1 Announce Type: new  Abstract: AI systems must adapt to evolving visual environments, especially in domains where object appearances change over time. We introduce Car Models in Time (CaMiT), a fine-grained dataset capturing the temporal evolution of car models, a representative class of technological artifacts. CaMiT includes 787K labeled samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023), supporting both supervised and self-supervised learning. Static pretraining on in-domain data achieves competitive performance with large-scale generalist models while being more resource-efficient, yet accuracy declines when models are tested across years. To address this, we propose a time-incremental classification setting, a realistic continual learning scenario with emerging, evolving, and disappearing classes. We evaluate two strategies: time-incremental pretraining, which updates the backbone, and time-incremental classifier learning, which updates only the final layer, both improving temporal robustness. Finally, we explore time-aware image generation that leverages temporal metadata during training, yielding more realistic outputs. CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained visual recognition and generation.",
        "arxiv_id": "2510.17626",
        "ARXIVID": "2510.17626",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) due to its focus on a dataset and methods for fine-grained visual recognition and generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.16776": {
        "authors": [
            "Mingzheng Zhang",
            "Jinfeng Gao",
            "Dan Xu",
            "Jiangrui Yu",
            "Yuhan Qiao",
            "Lan Chen",
            "Jin Tang",
            "Xiao Wang"
        ],
        "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation",
        "abstract": "arXiv:2510.16776v1 Announce Type: new  Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis.",
        "arxiv_id": "2510.16776",
        "ARXIVID": "2510.16776",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it fine-tunes vision foundation models for medical report generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.16160": {
        "authors": [
            "Ahmad Arrabi",
            "Jay Hwasung Jung",
            "Jax Luo",
            "Nathan Franssen",
            "Scott Raymond",
            "Safwan Wshah"
        ],
        "title": "Automated C-Arm Positioning via Conformal Landmark Localization",
        "abstract": "arXiv:2510.16160v1 Announce Type: new  Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided interventions. However, clinical workflows rely on manual alignment that increases radiation exposure and procedural delays. In this work, we present a pipeline that autonomously navigates the C-arm to predefined anatomical landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary starting location on the operating table, the model predicts a 3D displacement vector toward each target landmark along the body. To ensure reliable deployment, we capture both aleatoric and epistemic uncertainties in the model's predictions and further calibrate them using conformal prediction. The derived prediction regions are interpreted as 3D confidence regions around the predicted landmark locations. The training framework combines a probabilistic loss with skeletal pose regularization to encourage anatomically plausible outputs. We validate our approach on a synthetic X-ray dataset generated from DeepDRR. Results show not only strong localization accuracy across multiple architectures but also well-calibrated prediction bounds. These findings highlight the pipeline's potential as a component in safe and reliable autonomous C-arm systems. Code is available at https://github.com/AhmadArrabi/C_arm_guidance_APAH",
        "arxiv_id": "2510.16160",
        "ARXIVID": "2510.16160",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for embodied AI in the context of autonomous C-arm positioning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.16371": {
        "authors": [
            "Mohammad Javad Ahmadi",
            "Iman Gandomi",
            "Parisa Abdi",
            "Seyed-Farzad Mohammadi",
            "Amirhossein Taslimi",
            "Mehdi Khodaparast",
            "Hassan Hashemi",
            "Mahdi Tavakoli",
            "Hamid D. Taghirad"
        ],
        "title": "Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis",
        "abstract": "arXiv:2510.16371v1 Announce Type: new  Abstract: The development of computer-assisted surgery systems depends on large-scale, annotated datasets. Current resources for cataract surgery often lack the diversity and annotation depth needed to train generalizable deep-learning models. To address this gap, we present a dataset of 3,000 phacoemulsification cataract surgery videos from two surgical centers, performed by surgeons with a range of experience levels. This resource is enriched with four annotation layers: temporal surgical phases, instance segmentation of instruments and anatomical structures, instrument-tissue interaction tracking, and quantitative skill scores based on the established competency rubrics like the ICO-OSCAR. The technical quality of the dataset is supported by a series of benchmarking experiments for key surgical AI tasks, including workflow recognition, scene segmentation, and automated skill assessment. Furthermore, we establish a domain adaptation baseline for the phase recognition task by training a model on a subset of surgical centers and evaluating its performance on a held-out center. The dataset and annotations are available in Google Form (https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).",
        "arxiv_id": "2510.16371",
        "ARXIVID": "2510.16371",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new large-scale benchmark for surgical video analysis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.16196": {
        "authors": [
            "Zheng Huang",
            "Enpei Zhang",
            "Yinghao Cai",
            "Weikang Qiu",
            "Carl Yang",
            "Elynn Chen",
            "Xiang Zhang",
            "Rex Ying",
            "Dawei Zhou",
            "Yujun Yan"
        ],
        "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI",
        "abstract": "arXiv:2510.16196v1 Announce Type: new  Abstract: Understanding how the brain encodes visual information is a central challenge in neuroscience and machine learning. A promising approach is to reconstruct visual stimuli, essentially images, from functional Magnetic Resonance Imaging (fMRI) signals. This involves two stages: transforming fMRI signals into a latent space and then using a pretrained generative model to reconstruct images. The reconstruction quality depends on how similar the latent space is to the structure of neural activity and how well the generative model produces images from that space. Yet, it remains unclear which type of latent space best supports this transformation and how it should be organized to represent visual stimuli effectively. We present two key findings. First, fMRI signals are more similar to the text space of a language model than to either a vision based space or a joint text image space. Second, text representations and the generative model should be adapted to capture the compositional nature of visual stimuli, including objects, their detailed attributes, and relationships. Building on these insights, we propose PRISM, a model that Projects fMRI sIgnals into a Structured text space as an interMediate representation for visual stimuli reconstruction. It includes an object centric diffusion module that generates images by composing individual objects to reduce object detection errors, and an attribute relationship search module that automatically identifies key attributes and relationships that best align with the neural activity. Extensive experiments on real world datasets demonstrate that our framework outperforms existing methods, achieving up to an 8% reduction in perceptual loss. These results highlight the importance of using structured text as the intermediate space to bridge fMRI signals and image reconstruction.",
        "arxiv_id": "2510.16196",
        "ARXIVID": "2510.16196",
        "COMMENT": "Partially matches criteria 5 as it integrates fMRI signals with structured text and generative models, but the focus is more on neuroscience applications than general vision-language tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.17007": {
        "authors": [
            "Ignacio M. De la Jara",
            "Cristian Rodriguez-Opazo",
            "Edison Marrese-Taylor",
            "Felipe Bravo-Marquez"
        ],
        "title": "An empirical study of the effect of video encoders on Temporal Video Grounding",
        "abstract": "arXiv:2510.17007v1 Announce Type: new  Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to localize a natural language query in a long, untrimmed video. It has a key role in the scientific community, in part due to the large amount of video generated every day. Although we find extensive work in this task, we note that research remains focused on a small selection of video representations, which may lead to architectural overfitting in the long run. To address this issue, we propose an empirical study to investigate the impact of different video features on a classical architecture. We extract features for three well-known benchmarks, Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on CNNs, temporal reasoning and transformers. Our results show significant differences in the performance of our model by simply changing the video encoder, while also revealing clear patterns and errors derived from the use of certain features, ultimately indicating potential feature complementarity.",
        "arxiv_id": "2510.17007",
        "ARXIVID": "2510.17007",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding tasks, specifically temporal video grounding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.16973": {
        "authors": [
            "Praveenbalaji Rajendran",
            "Mojtaba Safari",
            "Wenfeng He",
            "Mingzhe Hu",
            "Shansong Wang",
            "Jun Zhou",
            "Xiaofeng Yang"
        ],
        "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis",
        "abstract": "arXiv:2510.16973v1 Announce Type: new  Abstract: Recent advancements in artificial intelligence (AI), particularly foundation models (FMs), have revolutionized medical image analysis, demonstrating strong zero- and few-shot performance across diverse medical imaging tasks, from segmentation to report generation. Unlike traditional task-specific AI models, FMs leverage large corpora of labeled and unlabeled multimodal datasets to learn generalized representations that can be adapted to various downstream clinical applications with minimal fine-tuning. However, despite the rapid proliferation of FM research in medical imaging, the field remains fragmented, lacking a unified synthesis that systematically maps the evolution of architectures, training paradigms, and clinical applications across modalities. To address this gap, this review article provides a comprehensive and structured analysis of FMs in medical image analysis. We systematically categorize studies into vision-only and vision-language FMs based on their architectural foundations, training strategies, and downstream clinical tasks. Additionally, a quantitative meta-analysis of the studies was conducted to characterize temporal trends in dataset utilization and application domains. We also critically discuss persistent challenges, including domain adaptation, efficient fine-tuning, computational constraints, and interpretability along with emerging solutions such as federated learning, knowledge distillation, and advanced prompting. Finally, we identify key future research directions aimed at enhancing the robustness, explainability, and clinical integration of FMs, thereby accelerating their translation into real-world medical practice.",
        "arxiv_id": "2510.16973",
        "ARXIVID": "2510.16973",
        "COMMENT": "Matches criterion 7 as it is a comprehensive survey on foundation models in medical image analysis.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2510.16720": {
        "authors": [
            "Jitao Sang",
            "Jinlin Xiao",
            "Jiarun Han",
            "Jilin Chen",
            "Xiaoyi Chen",
            "Shuyu Wei",
            "Yongjie Sun",
            "Yuhang Wang"
        ],
        "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI",
        "abstract": "arXiv:2510.16720v1 Announce Type: new  Abstract: The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience.",
        "arxiv_id": "2510.16720",
        "ARXIVID": "2510.16720",
        "COMMENT": "Matches criterion 7 as it is a survey paper discussing agentic AI, which includes vision and embodied domains.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2510.16724": {
        "authors": [
            "Minhua Lin",
            "Zongyu Wu",
            "Zhichao Xu",
            "Hui Liu",
            "Xianfeng Tang",
            "Qi He",
            "Charu Aggarwal",
            "Hui Liu",
            "Xiang Zhang",
            "Suhang Wang"
        ],
        "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications",
        "abstract": "arXiv:2510.16724v1 Announce Type: new  Abstract: The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \\emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.",
        "arxiv_id": "2510.16724",
        "ARXIVID": "2510.16724",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on RL-based agentic search.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.15952": {
        "authors": [
            "Myung Ho Kim"
        ],
        "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding",
        "abstract": "arXiv:2510.15952v1 Announce Type: new  Abstract: Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking \"what is intelligence?\" (ontological), SCL asks \"under what conditions does cognition emerge?\" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling \"executable epistemology\" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.",
        "arxiv_id": "2510.15952",
        "ARXIVID": "2510.15952",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17171": {
        "authors": [
            "Feihong Yan",
            "Peiru Wang",
            "Yao Zhu",
            "Kaiyu Pang",
            "Qingyan Wei",
            "Huiqi Li",
            "Linfeng Zhang"
        ],
        "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
        "abstract": "arXiv:2510.17171v1 Announce Type: new  Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual generation than autoregressive (AR) models for the ability of parallel generation, yet their acceleration potential remains constrained by the modeling complexity of spatially correlated visual tokens in a single step. To address this limitation, we introduce Generation then Reconstruction (GtR), a training-free hierarchical sampling strategy that decomposes generation into two stages: structure generation establishing global semantic scaffolding, followed by detail reconstruction efficiently completing remaining tokens. Assuming that it is more difficult to create an image from scratch than to complement images based on a basic image framework, GtR is designed to achieve acceleration by computing the reconstruction stage quickly while maintaining the generation quality by computing the generation stage slowly. Moreover, observing that tokens on the details of an image often carry more semantic information than tokens in the salient regions, we further propose Frequency-Weighted Token Selection (FTS) to offer more computation budget to tokens on image details, which are localized based on the energy of high frequency information. Extensive experiments on ImageNet class-conditional and text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1), substantially outperforming existing acceleration methods across various model scales and generation tasks. Our codes will be released in https://github.com/feihongyan1/GtR.",
        "arxiv_id": "2510.17171",
        "ARXIVID": "2510.17171",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in visual tasks, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.16582": {
        "authors": [
            "Junchi Yu",
            "Yujie Liu",
            "Jindong Gu",
            "Philip Torr",
            "Dongzhan Zhou"
        ],
        "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?",
        "abstract": "arXiv:2510.16582v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances large language models (LLMs) by providing structured and interpretable external knowledge. However, existing KG-based RAG methods struggle to retrieve accurate and diverse information from text-rich KGs for complex real-world queries. Process Reward Models (PRMs) offer a way to align the retrieval process of KG-based RAG with query-specific knowledge requirements, but they heavily rely on process-level supervision signals that are expensive and hard to obtain on KGs. To address this challenge, we propose GraphFlow, a framework that efficiently retrieves accurate and diverse knowledge required for real-world queries from text-rich KGs. GraphFlow employs a transition-based flow matching objective to jointly optimize a retrieval policy and a flow estimator. The flow estimator factorizes the reward of the retrieval outcome into the intermediate retrieval states. Such reward factorization guides the retrieval policy to retrieve candidates from KGs in proportion to their reward. This allows GraphFlow to explore high-quality regions of KGs that yield diverse and relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. GraphFlow outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit rate and recall. It also shows strong generalization to unseen KGs, demonstrating its effectiveness and robustness.",
        "arxiv_id": "2510.16582",
        "ARXIVID": "2510.16582",
        "COMMENT": "Does not match any specific criteria but is related to retrieval-augmented generation, which is tangentially relevant to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17699": {
        "authors": [
            "Aleksandr Oganov",
            "Ilya Bykov",
            "Eva Neudachina",
            "Mishan Aliev",
            "Alexander Tolmachev",
            "Alexander Sidorov",
            "Aleksandr Zuev",
            "Andrey Okhotin",
            "Denis Rakitin",
            "Aibek Alanov"
        ],
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
        "abstract": "arXiv:2510.17699v1 Announce Type: new  Abstract: While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS.",
        "arxiv_id": "2510.17699",
        "ARXIVID": "2510.17699",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and computational efficiency in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17484": {
        "authors": [
            "Muhammad Umer Ramzan",
            "Ali Zia",
            "Abdelwahed Khamis",
            "Noman Ali",
            "Usman Ali",
            "Wei Xiang"
        ],
        "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment",
        "abstract": "arXiv:2510.17484v1 Announce Type: new  Abstract: Salient object detection (SOD) aims to segment visually prominent regions in images and serves as a foundational task for various computer vision applications. We posit that SOD can now reach near-supervised accuracy without a single pixel-level label, but only when reliable pseudo-masks are available. We revisit the prototype-based line of work and make two key observations. First, boundary pixels and interior pixels obey markedly different geometry; second, the global consistency enforced by optimal transport (OT) is underutilized if prototype quality is weak. To address this, we introduce POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's single k-means step with an entropy-guided dual-clustering head: high-entropy pixels are organized by spectral clustering, low-entropy pixels by k-means, and the two prototype sets are subsequently aligned by OT. This split-fuse-transport design yields sharper, part-aware pseudo-masks in a single forward pass, without handcrafted priors. Those masks supervise a standard MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end unsupervised SOD pipeline that eliminates SelfMask's offline voting yet improves both accuracy and training efficiency. Extensive experiments on five benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and weakly supervised methods by up to 36% in F-measure, further narrowing the gap to fully supervised models.",
        "arxiv_id": "2510.17484",
        "ARXIVID": "2510.17484",
        "COMMENT": "Does not closely match any specific criterion but is relevant to unsupervised salient object detection and segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.16302": {
        "authors": [
            "Changhao Wang",
            "Yanfang Liu",
            "Xinxin Fan",
            "Anzhi Zhou",
            "Lao Tian",
            "Yunfeng Lu"
        ],
        "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA",
        "abstract": "arXiv:2510.16302v1 Announce Type: new  Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in retrieval-augmented generation (RAG) for modern large language models (LLMs). The accurate answer can be obtained through retrieving relational structure of entities from knowledge graph (KG). Regarding the inherent relation-dependency and reasoning pattern, multi-hop reasoning can be in general classified into two categories: i) parallel fact-verification multi-hop reasoning question, i.e., requiring simultaneous verifications of multiple independent sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding sequential multi-step inference with intermediate conclusions serving as essential premises for subsequent reasoning. Currently, the multi-hop reasoning approaches singly employ one of two techniques: LLM response-based fact verification and KG path-based chain construction. Nevertheless, the former excels at parallel fact-verification but underperforms on chained reasoning tasks, while the latter demonstrates proficiency in chained multi-hop reasoning but suffers from redundant path retrieval when handling parallel fact-verification reasoning. These limitations deteriorate the efficiency and accuracy for multi-hop QA tasks. To address this challenge, we propose a novel dual-track KG verification and reasoning framework DTKG, which is inspired by the Dual Process Theory in cognitive science. Specifically, DTKG comprises two main stages: the Classification Stage and the Branch Processing Stage.",
        "arxiv_id": "2510.16302",
        "ARXIVID": "2510.16302",
        "COMMENT": "Does not closely match any specific criterion but is relevant to multi-hop reasoning and knowledge graphs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.16382": {
        "authors": [
            "Ze Tao",
            "Jian Zhang",
            "Haowei Li",
            "Xianshuai Li",
            "Yifei Peng",
            "Xiyao Liu",
            "Senzhang Wang",
            "Chao Liu",
            "Sheng Ren",
            "Shichao Zhang"
        ],
        "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization",
        "abstract": "arXiv:2510.16382v1 Announce Type: new  Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a novel causal framework inspired by human intelligence, designed to overcome the limitations of conventional domain generalization models. Unlike approaches that rely on statistics to capture data-label dependencies and learn distortion-invariant representations, HSCM replicates the hierarchical processing and multi-level learning of human vision systems, focusing on modeling fine-grained causal mechanisms. By disentangling and reweighting key image attributes such as color, texture, and shape, HSCM enhances generalization across diverse domains, ensuring robust performance and interpretability. Leveraging the flexibility and adaptability of human intelligence, our approach enables more effective transfer and learning in dynamic, complex environments. Through both theoretical and empirical evaluations, we demonstrate that HSCM outperforms existing domain generalization models, providing a more principled method for capturing causal relationships and improving model robustness. The code is available at https://github.com/lambett/HSCM.",
        "arxiv_id": "2510.16382",
        "ARXIVID": "2510.16382",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to causal representation learning and domain generalization.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17131": {
        "authors": [
            "Xin Gao",
            "Jiyao Liu",
            "Guanghao Li",
            "Yueming Lyu",
            "Jianxiong Gao",
            "Weichen Yu",
            "Ningsheng Xu",
            "Liang Wang",
            "Caifeng Shan",
            "Ziwei Liu",
            "Chenyang Si"
        ],
        "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection",
        "abstract": "arXiv:2510.17131v1 Announce Type: new  Abstract: Recent advancements have explored text-to-image diffusion models for synthesizing out-of-distribution (OOD) samples, substantially enhancing the performance of OOD detection. However, existing approaches typically rely on perturbing text-conditioned embeddings, resulting in semantic instability and insufficient shift diversity, which limit generalization to realistic OOD. To address these challenges, we propose GOOD, a novel and flexible framework that directly guides diffusion sampling trajectories towards OOD regions using off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level guidance: (1) Image-level guidance based on the gradient of log partition to reduce input likelihood, drives samples toward low-density regions in pixel space. (2) Feature-level guidance, derived from k-NN distance in the classifier's latent space, promotes sampling in feature-sparse regions. Hence, this dual-guidance design enables more controllable and diverse OOD sample generation. Additionally, we introduce a unified OOD score that adaptively combines image and feature discrepancies, enhancing detection robustness. We perform thorough quantitative and qualitative analyses to evaluate the effectiveness of GOOD, demonstrating that training with samples generated by GOOD can notably enhance OOD detection performance.",
        "arxiv_id": "2510.17131",
        "ARXIVID": "2510.17131",
        "COMMENT": "Does not match any specific criteria. Focuses on out-of-distribution detection using diffusion models, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.16833": {
        "authors": [
            "Xiangyu Mu",
            "Dongliang Zhou",
            "Jie Hou",
            "Haijun Zhang",
            "Weili Guan"
        ],
        "title": "From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display",
        "abstract": "arXiv:2510.16833v1 Announce Type: new  Abstract: Mannequin-based clothing displays offer a cost-effective alternative to real-model showcases for online fashion presentation, but lack realism and expressive detail. To overcome this limitation, we introduce a new task called mannequin-to-human (M2H) video generation, which aims to synthesize identity-controllable, photorealistic human videos from footage of mannequins. We propose M2HVideo, a pose-aware and identity-preserving video generation framework that addresses two key challenges: the misalignment between head and body motion, and identity drift caused by temporal modeling. In particular, M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial semantics with body pose to produce consistent identity embeddings across frames. To address the loss of fine facial details due to latent space compression, we introduce a mirror loss applied in pixel space through a denoising diffusion implicit model (DDIM)-based one-step denoising. Additionally, we design a distribution-aware adapter that aligns statistical distributions of identity and clothing features to enhance temporal coherence. Extensive experiments on the UBC fashion dataset, our self-constructed ASOS dataset, and the newly collected MannequinVideos dataset captured on-site demonstrate that M2HVideo achieves superior performance in terms of clothing consistency, identity preservation, and video fidelity in comparison to state-of-the-art methods.",
        "arxiv_id": "2510.16833",
        "ARXIVID": "2510.16833",
        "COMMENT": "Does not match any specific criteria. Focuses on mannequin-to-human video generation for fashion, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.15981": {
        "authors": [
            "Rafael Cabral",
            "Tuan Manh Do",
            "Xuejun Yu",
            "Wai Ming Tai",
            "Zijin Feng",
            "Xin Shen"
        ],
        "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization",
        "abstract": "arXiv:2510.15981v1 Announce Type: new  Abstract: Proof autoformalization, the task of translating natural language theorems and proofs into machine-verifiable code, is a critical step for integrating large language models into rigorous mathematical workflows. Current approaches focus on producing executable code, but they frequently fail to preserve the semantic meaning and logical structure of the original human-written argument. To address this, we introduce ProofFlow, a novel pipeline that treats structural fidelity as a primary objective. ProofFlow first constructs a directed acyclic graph (DAG) to map the logical dependencies between proof steps. Then, it employs a novel lemma-based approach to systematically formalize each step as an intermediate lemma, preserving the logical structure of the original argument. To facilitate evaluation, we present a new benchmark of 184 undergraduate-level problems, manually annotated with step-by-step solutions and logical dependency graphs, and introduce ProofScore, a new composite metric to evaluate syntactic correctness, semantic faithfulness, and structural fidelity. Experimental results show our pipeline sets a new state-of-the-art for autoformalization, achieving a ProofScore of 0.545, substantially exceeding baselines like full-proof formalization (0.123), which processes the entire proof at once, and step-proof formalization (0.072), which handles each step independently. Our pipeline, benchmark, and score metric are open-sourced to encourage further progress at https://github.com/Huawei-AI4Math/ProofFlow.",
        "arxiv_id": "2510.15981",
        "ARXIVID": "2510.15981",
        "COMMENT": "Does not match any specific criteria but discusses reasoning and logical structure in proofs, which is tangentially related to reasoning in embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.16476": {
        "authors": [
            "Xiaozhe Li",
            "Xinyu Fang",
            "Shengyuan Ding",
            "Linyang Li",
            "Haodong Duan",
            "Qingwen Liu",
            "Kai Chen"
        ],
        "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems",
        "abstract": "arXiv:2510.16476v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as mathematics, coding, logic, and puzzles through Reinforcement Learning with Verifiable Rewards (RLVR). However, their ability to solve more complex optimization problems - particularly NP-hard tasks - remains underexplored. To bridge this gap, we propose NP-ENGINE, the first comprehensive framework for training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks across five domains, each equipped with (i) a controllable instance generator, (ii) a rule-based verifier, and (iii) a heuristic solver that provides approximate optimal solutions as ground truth. This generator-verifier-heuristic pipeline enables scalable and verifiable RLVR training under hierarchical difficulties. We also introduce NP-BENCH, a benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs' ability to tackle NP-hard level reasoning problems, focusing not only on feasibility but also on solution quality. Additionally, we present QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and achieves SOTA performance with the same model size. Beyond in-domain tasks, we demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain (OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge), as well as non-reasoning tasks such as instruction following. We also observe a scaling trend: increasing task diversity improves OOD generalization. These findings suggest that task-rich RLVR training is a promising direction for advancing LLM's reasoning ability, revealing new insights into the scaling laws of RLVR.",
        "arxiv_id": "2510.16476",
        "ARXIVID": "2510.16476",
        "COMMENT": "Does not match any specific criteria but discusses reasoning capabilities in LLMs, which is tangentially related to spatial intelligence and embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.17440": {
        "authors": [
            "Qiyuan Guan",
            "Xiang Chen",
            "Guiyue Jin",
            "Jiyu Jin",
            "Shumin Fan",
            "Tianyu Song",
            "Jinshan Pan"
        ],
        "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation",
        "abstract": "arXiv:2510.17440v1 Announce Type: new  Abstract: Compared to daytime image deraining, nighttime image deraining poses significant challenges due to inherent complexities of nighttime scenarios and the lack of high-quality datasets that accurately represent the coupling effect between rain and illumination. In this paper, we rethink the task of nighttime image deraining and contribute a new high-quality benchmark, HQ-NightRain, which offers higher harmony and realism compared to existing datasets. In addition, we develop an effective Color Space Transformation Network (CST-Net) for better removing complex rain from nighttime scenes. Specifically, we propose a learnable color space converter (CSC) to better facilitate rain removal in the Y channel, as nighttime rain is more pronounced in the Y channel compared to the RGB color space. To capture illumination information for guiding nighttime deraining, implicit illumination guidance is introduced enabling the learned features to improve the model's robustness in complex scenarios. Extensive experiments show the value of our dataset and the effectiveness of our method. The source code and datasets are available at https://github.com/guanqiyuan/CST-Net.",
        "arxiv_id": "2510.17440",
        "ARXIVID": "2510.17440",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and machine learning, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17614": {
        "authors": [
            "Praphul Singh",
            "Corey Barrett",
            "Sumana Srivasta",
            "Irfan Bulu",
            "Sri Gadde",
            "Krishnaram Kenthapadi"
        ],
        "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration",
        "abstract": "arXiv:2510.17614v1 Announce Type: new  Abstract: Clinicians need ranking systems that work in real time and still justify their choices. Motivated by the need for a low-latency, decoder-based reranker, we present OG-Rank, a single-decoder approach that pairs a pooled first-token scoring signal with an uncertainty-gated explanation step. The model scores all candidates in one pass and generates a brief, structured rationale only when the list is genuinely ambiguous, keeping latency predictable. Trained with a curriculum that concentrates effort on hard cases, OG-Rank delivers strong effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45, nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56, nDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains under the same policy. Encoder baselines trail in both effectiveness and flexibility. The result is a practical recipe: rank fast by default and explain when it helps, a pattern that applies broadly to decision tasks where selective generation buys accuracy at acceptable cost. The single-policy design simplifies deployment and budget planning, and the curriculum principle (spend more on the hard cases, less on the easy ones) readily transfers beyond clinical order selection.",
        "arxiv_id": "2510.17614",
        "ARXIVID": "2510.17614",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17686": {
        "authors": [
            "Taichi Liu",
            "Zhenyu Wang",
            "Ruofeng Liu",
            "Guang Wang",
            "Desheng Zhang"
        ],
        "title": "Towards 3D Objectness Learning in an Open World",
        "abstract": "arXiv:2510.17686v1 Announce Type: new  Abstract: Recent advancements in 3D object detection and novel category detection have made significant progress, yet research on learning generalized 3D objectness remains insufficient. In this paper, we delve into learning open-world 3D objectness, which focuses on detecting all objects in a 3D scene, including novel objects unseen during training. Traditional closed-set 3D detectors struggle to generalize to open-world scenarios, while directly incorporating 3D open-vocabulary models for open-world ability struggles with vocabulary expansion and semantic overlap. To achieve generalized 3D object discovery, We propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect any objects within 3D scenes without relying on hand-crafted text prompts. We introduce the strong generalization and zero-shot capabilities of 2D foundation models, utilizing both 2D semantic priors and 3D geometric priors for class-agnostic proposals to broaden 3D object discovery. Then, by integrating complementary information from point cloud and RGB image in the cross-modal mixture of experts, OP3Det dynamically routes uni-modal and multi-modal features to learn generalized 3D objectness. Extensive experiments demonstrate the extraordinary performance of OP3Det, which significantly surpasses existing open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement compared to closed-world 3D detectors.",
        "arxiv_id": "2510.17686",
        "ARXIVID": "2510.17686",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16704": {
        "authors": [
            "Tianxin Wei",
            "Yifan Chen",
            "Xinrui He",
            "Wenxuan Bao",
            "Jingrui He"
        ],
        "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization",
        "abstract": "arXiv:2510.16704v1 Announce Type: new  Abstract: Distribution shifts between training and testing samples frequently occur in practice and impede model generalization performance. This crucial challenge thereby motivates studies on domain generalization (DG), which aim to predict the label on unseen target domain data by solely using data from source domains. It is intuitive to conceive the class-separated representations learned in contrastive learning (CL) are able to improve DG, while the reality is quite the opposite: users observe directly applying CL deteriorates the performance. We analyze the phenomenon with the insights from CL theory and discover lack of intra-class connectivity in the DG setting causes the deficiency. We thus propose a new paradigm, domain-connecting contrastive learning (DCCL), to enhance the conceptual connectivity across domains and obtain generalizable representations for DG. On the data side, more aggressive data augmentation and cross-domain positive samples are introduced to improve intra-class connectivity. On the model side, to better embed the unseen test domains, we propose model anchoring to exploit the intra-class connectivity in pre-trained representations and complement the anchoring with generative transformation loss. Extensive experiments on five standard DG benchmarks are performed. The results verify that DCCL outperforms state-of-the-art baselines even without domain supervision. The detailed model implementation and the code are provided through https://github.com/weitianxin/DCCL",
        "arxiv_id": "2510.16704",
        "ARXIVID": "2510.16704",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16118": {
        "authors": [
            "Nishad Sahu (Raj)",
            "Shounak Sural (Raj)",
            "Aditya Satish Patil (Raj)",
            "Ragunathan (Raj)",
            "Rajkumar"
        ],
        "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles",
        "abstract": "arXiv:2510.16118v1 Announce Type: new  Abstract: Reliable perception is fundamental for safety critical decision making in autonomous driving. Yet, vision based object detector neural networks remain vulnerable to uncertainty arising from issues such as data bias and distributional shifts. In this paper, we introduce ObjectTransforms, a technique for quantifying and reducing uncertainty in vision based object detection through object specific transformations at both training and inference times. At training time, ObjectTransforms perform color space perturbations on individual objects, improving robustness to lighting and color variations. ObjectTransforms also uses diffusion models to generate realistic, diverse pedestrian instances. At inference time, object perturbations are applied to detected objects and the variance of detection scores are used to quantify predictive uncertainty in real time. This uncertainty signal is then used to filter out false positives and also recover false negatives, improving the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K dataset demonstrate that our method yields notable accuracy improvements and uncertainty reduction across all object classes during training, while predicting desirably higher uncertainty values for false positives as compared to true positives during inference. Our results highlight the potential of ObjectTransforms as a lightweight yet effective mechanism for reducing and quantifying uncertainty in vision-based perception during training and inference respectively.",
        "arxiv_id": "2510.16118",
        "ARXIVID": "2510.16118",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16791": {
        "authors": [
            "Chengxuan Zhu",
            "Shuchen Weng",
            "Jiacong Fang",
            "Peixuan Zhang",
            "Si Li",
            "Chao Xu",
            "Boxin Shi"
        ],
        "title": "Personalized Image Filter: Mastering Your Photographic Style",
        "abstract": "arXiv:2510.16791v1 Announce Type: new  Abstract: Photographic style, as a composition of certain photographic concepts, is the charm behind renowned photographers. But learning and transferring photographic style need a profound understanding of how the photo is edited from the unknown original appearance. Previous works either fail to learn meaningful photographic concepts from reference images, or cannot preserve the content of the content image. To tackle these issues, we proposed a Personalized Image Filter (PIF). Based on a pretrained text-to-image diffusion model, the generative prior enables PIF to learn the average appearance of photographic concepts, as well as how to adjust them according to text prompts. PIF then learns the photographic style of reference images with the textual inversion technique, by optimizing the prompts for the photographic concepts. PIF shows outstanding performance in extracting and transferring various kinds of photographic style. Project page: https://pif.pages.dev/",
        "arxiv_id": "2510.16791",
        "ARXIVID": "2510.16791",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and photographic style transfer, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16701": {
        "authors": [
            "Ni Zhang",
            "Zhiguang Cao",
            "Jianan Zhou",
            "Cong Zhang",
            "Yew-Soon Ong"
        ],
        "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems",
        "abstract": "arXiv:2510.16701v1 Announce Type: new  Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge, demanding substantial expert effort for intent interpretation and algorithm design. While large language models (LLMs) offer a promising path toward automation, current approaches still rely on external intervention, which restrict autonomy and often lead to execution errors and low solution feasibility. To address these challenges, we propose an Agentic Framework with LLMs (AFL) for solving complex vehicle routing problems, achieving full automation from problem instance to solution. AFL directly extracts knowledge from raw inputs and enables self-contained code generation without handcrafted modules or external solvers. To improve trustworthiness, AFL decomposes the overall pipeline into three manageable subtasks and employs four specialized agents whose coordinated interactions enforce cross-functional consistency and logical soundness. Extensive experiments on 60 complex VRPs, ranging from standard benchmarks to practical variants, validate the effectiveness and generality of our framework, showing comparable performance against meticulously designed algorithms. Notably, it substantially outperforms existing LLM-based baselines in both code reliability and solution feasibility, achieving rates close to 100% on the evaluated benchmarks.",
        "arxiv_id": "2510.16701",
        "ARXIVID": "2510.16701",
        "COMMENT": "Does not match any specific criteria but is relevant to AI problem-solving frameworks, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17105": {
        "authors": [
            "Xiaogang Xu",
            "Jian Wang",
            "Yunfan Lu",
            "Ruihang Chu",
            "Ruixing Wang",
            "Jiafei Wu",
            "Bei Yu",
            "Liang Lin"
        ],
        "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
        "abstract": "arXiv:2510.17105v1 Announce Type: new  Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable Diffusion via ControlNet, have achieved remarkable performance in several low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods often sacrifice content fidelity to attain higher perceptual realism. This issue is exacerbated in low-light scenarios, where severely degraded information caused by the darkness limits effective control. We identify two primary causes of fidelity loss: the absence of suitable conditional latent modeling and the lack of bidirectional interaction between the conditional latent and noisy latent in the diffusion process. To address this, we propose a novel optimization strategy for conditioning in pre-trained diffusion models, enhancing fidelity while preserving realism and aesthetics. Our method introduces a mechanism to recover spatial details lost during VAE encoding, i.e., a latent refinement pipeline incorporating generative priors. Additionally, the refined latent condition interacts dynamically with the noisy latent, leading to improved restoration performance. Our approach is plug-and-play, seamlessly integrating into existing diffusion networks to provide more effective control. Extensive experiments demonstrate significant fidelity improvements in PTDB methods.",
        "arxiv_id": "2510.17105",
        "ARXIVID": "2510.17105",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and generative modeling in low-light image enhancement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16556": {
        "authors": [
            "Guangyu Lin",
            "Li Lin",
            "Christina P. Walker",
            "Daniel S. Schiff",
            "Shu Hu"
        ],
        "title": "Fit for Purpose? Deepfake Detection in the Real World",
        "abstract": "arXiv:2510.16556v1 Announce Type: new  Abstract: The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.",
        "arxiv_id": "2510.16556",
        "ARXIVID": "2510.16556",
        "COMMENT": "Does not match any specific criteria but involves deepfake detection in real-world political contexts.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17052": {
        "authors": [
            "Hassan Hamad",
            "Yingru Xu",
            "Liang Zhao",
            "Wenbo Yan",
            "Narendra Gyanchandani"
        ],
        "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems",
        "abstract": "arXiv:2510.17052v1 Announce Type: new  Abstract: Tool-augmented large language models (LLMs) are increasingly employed in real-world applications, but tool usage errors still hinder their reliability. We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight distinct error types specific to tool-calling (e.g., premature invocation, argument misalignment, and misinterpretation of tool outputs) and provides targeted feedback to the main LLM. The main LLM, assumed to have strong reasoning, task understanding and orchestration capabilities, then revises its response based on ToolCritic's feedback. We systematically define these error categories and construct a synthetic dataset to train ToolCritic. Experimental results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic improves tool-calling accuracy by up to 13% over baselines, including zero-shot prompting and self-correction techniques. This represents a promising step toward more robust LLM integration with external tools in real-world dialogue applications.",
        "arxiv_id": "2510.17052",
        "ARXIVID": "2510.17052",
        "COMMENT": "Does not match any specific criteria but involves tool-use error correction in dialogue systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16392": {
        "authors": [
            "Ao Tian",
            "Yunfeng Lu",
            "Xinxin Fan",
            "Changhao Wang",
            "Lanzhi Zhou",
            "Yeyao Zhang",
            "Yanfang Liu"
        ],
        "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile",
        "abstract": "arXiv:2510.16392v1 Announce Type: new  Abstract: Personalized and continuous interactions are the key to enhancing user experience in today's large language model (LLM)-based conversational systems, however, the finite context windows and static parametric memory make it difficult to model the cross-session long-term user states and behavioral consistency. Currently, the existing solutions to this predicament, such as retrieval-augmented generation (RAG) and explicit memory systems, primarily focus on fact-level storage and retrieval, lacking the capability to distill latent preferences and deep traits from the multi-turn dialogues, which limits the long-term and effective user modeling, directly leading to the personalized interactions remaining shallow, and hindering the cross-session continuity. To realize the long-term memory and behavioral consistency for Language Agents in LLM era, we propose a self-evolving memory framework RGMem, inspired by the ideology of classic renormalization group (RG) in physics, this framework enables to organize the dialogue history in multiple scales: it first extracts semantics and user insights from episodic fragments, then through hierarchical coarse-graining and rescaling operations, progressively forms a dynamically-evolved user profile. The core innovation of our work lies in modeling memory evolution as a multi-scale process of information compression and emergence, which accomplishes the high-level and accurate user profiles from noisy and microscopic-level interactions.",
        "arxiv_id": "2510.16392",
        "ARXIVID": "2510.16392",
        "COMMENT": "Does not match any specific criteria but involves memory evolution for language agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16865": {
        "authors": [
            "Yuyang Yu",
            "Zhengwei Chen",
            "Xuemiao Xu",
            "Lei Zhang",
            "Haoxin Yang",
            "Yongwei Nie",
            "Shengfeng He"
        ],
        "title": "Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection",
        "abstract": "arXiv:2510.16865v1 Announce Type: new  Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality control, aiming to identify structural defects with high reliability. However, current memory bank-based methods often suffer from inconsistent feature transformations and limited discriminative capacity, particularly in capturing local geometric details and achieving rotation invariance. These limitations become more pronounced when registration fails, leading to unreliable detection results. We argue that point-cloud registration plays an essential role not only in aligning geometric structures but also in guiding feature extraction toward rotation-invariant and locally discriminative representations. To this end, we propose a registration-induced, rotation-invariant feature extraction framework that integrates the objectives of point-cloud registration and memory-based anomaly detection. Our key insight is that both tasks rely on modeling local geometric structures and leveraging feature similarity across samples. By embedding feature extraction into the registration learning process, our framework jointly optimizes alignment and representation learning. This integration enables the network to acquire features that are both robust to rotations and highly effective for anomaly detection. Extensive experiments on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method consistently outperforms existing approaches in effectiveness and generalizability.",
        "arxiv_id": "2510.16865",
        "ARXIVID": "2510.16865",
        "COMMENT": "Does not match any specific criteria but involves 3D anomaly detection and geometric reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17611": {
        "authors": [
            "Jia Guo",
            "Shuai Lu",
            "Lei Fan",
            "Zelin Li",
            "Donglin Di",
            "Yang Song",
            "Weihang Zhang",
            "Wenbing Zhu",
            "Hong Yan",
            "Fang Chen",
            "Huiqi Li",
            "Hongen Liao"
        ],
        "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection",
        "abstract": "arXiv:2510.17611v1 Announce Type: new  Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized single-class models to unified multi-class models, yet existing multi-class models significantly underperform the most advanced one-for-one counterparts. Moreover, the field has fragmented into specialized methods tailored to specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment barriers and highlighting the need for a unified solution. In this paper, we present Dinomaly2, the first unified framework for full-spectrum image UAD, which bridges the performance gap in multi-class models while seamlessly extending across diverse data modalities and task settings. Guided by the \"less is more\" philosophy, we demonstrate that the orchestration of five simple element achieves superior performance in a standard reconstruction-based framework. This methodological minimalism enables natural extension across diverse tasks without modification, establishing that simplicity is the foundation of true universality. Extensive experiments on 12 UAD benchmarks demonstrate Dinomaly2's full-spectrum superiority across multiple modalities (2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class, inference-unified multi-class, few-shot) and application domains (industrial, biological, outdoor). For example, our multi-class model achieves unprecedented 99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art performance with minimum adaptations. Moreover, using only 8 normal examples per class, our method surpasses previous full-shot models, achieving 98.7% and 97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design, computational scalability, and universal applicability positions Dinomaly2 as a unified solution for the full spectrum of real-world anomaly detection applications.",
        "arxiv_id": "2510.17611",
        "ARXIVID": "2510.17611",
        "COMMENT": "Does not match any specific criteria but is related to anomaly detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17338": {
        "authors": [
            "Jiahao Huo",
            "Mufhumudzi Muthivhi",
            "Terence L. van Zyl",
            "Fredrik Gustafsson"
        ],
        "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition",
        "abstract": "arXiv:2510.17338v1 Announce Type: new  Abstract: Current state-of-the-art Wildlife classification models are trained under the closed world setting. When exposed to unknown classes, they remain overconfident in their predictions. Open-set Recognition (OSR) aims to classify known classes while rejecting unknown samples. Several OSR methods have been proposed to model the closed-set distribution by observing the feature, logit, or softmax probability space. A significant drawback of many existing approaches is the requirement to retrain the pre-trained classification model with the OSR-specific strategy. This study contributes a post-processing OSR method that measures the agreement between the models' features and predicted logits. We propose a probability distribution based on an input's distance to its Nearest Class Mean (NCM). The NCM-based distribution is then compared with the softmax probabilities from the logit space to measure agreement between the NCM and the classification head. Our proposed strategy ranks within the top three on two evaluated datasets, showing consistent performance across the two datasets. In contrast, current state-of-the-art methods excel on a single dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish animals. The code can be found https://github.com/Applied-Representation-Learning-Lab/OSR.",
        "arxiv_id": "2510.17338",
        "ARXIVID": "2510.17338",
        "COMMENT": "Does not closely match any specific criterion but is relevant to open-set recognition in wildlife classification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17609": {
        "authors": [
            "Siqi Chen",
            "Shanyue Guan"
        ],
        "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation",
        "abstract": "arXiv:2510.17609v1 Announce Type: new  Abstract: The advancement of UAV technology has enabled efficient, non-contact structural health monitoring. Combined with photogrammetry, UAVs can capture high-resolution scans and reconstruct detailed 3D models of infrastructure. However, a key challenge remains in segmenting specific structural components from these models-a process traditionally reliant on time-consuming and error-prone manual labeling. To address this issue, we propose a machine learning-based framework for automated segmentation of 3D point clouds. Our approach uses the complementary strengths of real-world UAV-scanned point clouds and synthetic data generated from Building Information Modeling (BIM) to overcome the limitations associated with manual labeling. Validation on a railroad track dataset demonstrated high accuracy in identifying and segmenting major components such as rails and crossties. Moreover, by using smaller-scale datasets supplemented with BIM data, the framework significantly reduced training time while maintaining reasonable segmentation accuracy. This automated approach improves the precision and efficiency of 3D infrastructure model segmentation and advances the integration of UAV and BIM technologies in structural health monitoring and infrastructure management.",
        "arxiv_id": "2510.17609",
        "ARXIVID": "2510.17609",
        "COMMENT": "Does not closely match any specific criterion but is relevant to UAV-based 3D modeling and segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17598": {
        "authors": [
            "Amir Jalilifard",
            "Anderson de Rezende Rocha",
            "Marcos Medeiros Raimundo"
        ],
        "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation",
        "abstract": "arXiv:2510.17598v1 Announce Type: new  Abstract: Effective code generation with language models hinges on two critical factors: accurately understanding the intent of the prompt and generating code that applies algorithmic reasoning to produce correct solutions capable of passing diverse test cases while adhering to the syntax of the target programming language. Unlike other language tasks, code generation requires more than accurate token prediction; it demands comprehension of solution-level and structural relationships rather than merely generating the most likely tokens. very large language model (VLLM) are capable of generating detailed steps toward the correct solution of complex tasks where reasoning is crucial in solving the problem. Such reasoning capabilities may be absent in smaller language models. Therefore, in this work, we distill the reasoning capabilities of a VLLM into a smaller, more efficient model that is faster and cheaper to deploy. Our approach trains the model to emulate the reasoning and problem-solving abilities of the VLLM by learning to identify correct solution pathways and establishing a structural correspondence between problem definitions and potential solutions through a novel method of structure-aware loss optimization. This enables the model to transcend token-level generation and to deeply grasp the overarching structure of solutions for given problems. Experimental results show that our fine-tuned model, developed through a cheap and simple to implement process, significantly outperforms our baseline model in terms of pass@1, average data flow, and average syntax match metrics across the MBPP, MBPP Plus, and HumanEval benchmarks.",
        "arxiv_id": "2510.17598",
        "ARXIVID": "2510.17598",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in machine learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.17235": {
        "authors": [
            "Chong Chen",
            "Ze Liu",
            "Lingfeng Bao",
            "Yanlin Wang",
            "Ting Chen",
            "Daoyuan Wu",
            "Jiachi Chen"
        ],
        "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis",
        "abstract": "arXiv:2510.17235v1 Announce Type: new  Abstract: The cryptocurrency market offers significant investment opportunities but faces challenges including high volatility and fragmented information. Data integration and analysis are essential for informed investment decisions. Currently, investors use three main approaches: (1) Manual analysis across various sources, which depends heavily on individual experience and is time-consuming and prone to bias; (2) Data aggregation platforms-limited in functionality and depth of analysis; (3) Large language model agents-based on static pretrained models, lacking real-time data integration and multi-step reasoning capabilities. To address these limitations, we present Coinvisor, a reinforcement learning-based chatbot that provides comprehensive analytical support for cryptocurrency investment through a multi-agent framework. Coinvisor integrates diverse analytical capabilities through specialized tools. Its key innovation is a reinforcement learning-based tool selection mechanism that enables multi-step planning and flexible integration of diverse data sources. This design supports real-time interaction and adaptive analysis of dynamic content, delivering accurate and actionable investment insights. We evaluated Coinvisor through automated benchmarks on tool calling accuracy and user studies with 20 cryptocurrency investors using our interface. Results show that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base model in tool orchestration. User studies show high satisfaction (4.64/5), with participants preferring Coinvisor to both general LLMs and existing crypto platforms (4.62/5).",
        "arxiv_id": "2510.17235",
        "ARXIVID": "2510.17235",
        "COMMENT": "Does not match any specific criteria but discusses reinforcement learning and chatbot agents, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16730": {
        "authors": [
            "Tianyang Dou",
            "Ming Li",
            "Jiangying Qin",
            "Xuan Liao",
            "Jiageng Zhong",
            "Armin Gruen",
            "Mengyi Deng"
        ],
        "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid",
        "abstract": "arXiv:2510.16730v1 Announce Type: new  Abstract: Coral reefs are vital yet fragile ecosystems that require accurate large-scale mapping for effective conservation. Although global products such as the Allen Coral Atlas provide unprecedented coverage of global coral reef distri-bution, their predictions are frequently limited in spatial precision and semantic consistency, especially in regions requiring fine-grained boundary delineation. To address these challenges, we propose UKANFormer, a novel se-mantic segmentation model designed to achieve high-precision mapping under noisy supervision derived from Allen Coral Atlas. Building upon the UKAN architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans) block in the decoder, enabling the extraction of both global semantic structures and local boundary details. In experiments, UKANFormer achieved a coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming conventional baselines under the same noisy labels setting. Remarkably, the model produces predictions that are visually and structurally more accurate than the noisy labels used for training. These results challenge the notion that data quality directly limits model performance, showing that architectural design can mitigate label noise and sup-port scalable mapping under imperfect supervision. UKANFormer provides a foundation for ecological monitoring where reliable labels are scarce.",
        "arxiv_id": "2510.16730",
        "ARXIVID": "2510.16730",
        "COMMENT": "Does not match any specific criteria but discusses semantic segmentation, which is tangentially related to computer vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.16095": {
        "authors": [
            "Dou Liu",
            "Ying Long",
            "Sophia Zuoqiu",
            "Di Liu",
            "Kang Li",
            "Yiting Lin",
            "Hanyi Liu",
            "Rong Yin",
            "Tian Tang"
        ],
        "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study",
        "abstract": "arXiv:2510.16095v1 Announce Type: new  Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for explainable medical Artificial Intelligence (AI) while constrained by data scarcity. Although Large Language Models (LLMs) can synthesize medical data, their clinical reliability remains unverified. This study evaluates the reliability of LLM-generated CoTs and investigates prompting strategies to enhance their quality. In a blinded comparative study, senior clinicians in Assisted Reproductive Technology (ART) evaluated CoTs generated via three distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and Selective Few-shot (using diverse, high-quality examples). These expert ratings were compared against evaluations from a state-of-the-art AI model (GPT-4o). The Selective Few-shot strategy significantly outperformed other strategies across all human evaluation metrics (p < .001). Critically, the Random Few-shot strategy offered no significant improvement over the Zero-shot baseline, demonstrating that low-quality examples are as ineffective as no examples. The success of the Selective strategy is attributed to two principles: \"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\" (generalization). Notably, the AI evaluator failed to discern these critical performance differences. The clinical reliability of synthetic CoTs is dictated by strategic prompt curation, not the mere presence of examples. We propose a \"Dual Principles\" framework as a foundational methodology to generate trustworthy data at scale. This work offers a validated solution to the data bottleneck and confirms the indispensable role of human expertise in evaluating high-stakes clinical AI.",
        "arxiv_id": "2510.16095",
        "ARXIVID": "2510.16095",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.17035": {
        "authors": [
            "Syed Konain Abbas",
            "Sandip Purnapatra",
            "M. G. Sarwar Murshed",
            "Conor Miller-Lynch",
            "Lambert Igene",
            "Soumyabrata Dey",
            "Stephanie Schuckers",
            "Faraz Hussain"
        ],
        "title": "Conditional Synthetic Live and Spoof Fingerprint Generation",
        "abstract": "arXiv:2510.17035v1 Announce Type: new  Abstract: Large fingerprint datasets, while important for training and evaluation, are time-consuming and expensive to collect and require strict privacy measures. Researchers are exploring the use of synthetic fingerprint data to address these issues. This paper presents a novel approach for generating synthetic fingerprint images (both spoof and live), addressing concerns related to privacy, cost, and accessibility in biometric data collection. Our approach utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce high-resolution synthetic live fingerprints, conditioned on specific finger identities (thumb through little finger). Additionally, we employ CycleGANs to translate these into realistic spoof fingerprints, simulating a variety of presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof fingerprints are crucial for developing robust spoof detection systems. Through these generative models, we created two synthetic datasets (DB2 and DB3), each containing 1,500 fingerprint images of all ten fingers with multiple impressions per finger, and including corresponding spoofs in eight material types. The results indicate robust performance: our StyleGAN3 model achieves a Fr\\'echet Inception Distance (FID) as low as 5, and the generated fingerprints achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably, matching experiments confirm strong privacy preservation, with no significant evidence of identity leakage, confirming the strong privacy-preserving properties of our synthetic datasets.",
        "arxiv_id": "2510.17035",
        "ARXIVID": "2510.17035",
        "COMMENT": "Does not match any specific criteria but involves generative modeling for synthetic fingerprint data.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}