{
    "2508.18634": {
        "authors": [
            "Chunlin Zhong",
            "Qiuxia Hou",
            "Zhangjun Zhou",
            "Shuang Hao",
            "Haonan Lu",
            "Yanhao Zhang",
            "He Tang",
            "Xiang Bai"
        ],
        "title": "OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward",
        "abstract": "arXiv:2508.18634v1 Announce Type: new  Abstract: Video captioning aims to generate comprehensive and coherent descriptions of the video content, contributing to the advancement of both video understanding and generation. However, existing methods often suffer from motion-detail imbalance, as models tend to overemphasize one aspect while neglecting the other. This imbalance results in incomplete captions, which in turn leads to a lack of consistency in video understanding and generation. To address this issue, we propose solutions from two aspects: 1) Data aspect: We constructed the Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage pipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2) Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO). CSER enhances completeness and accuracy in capturing both motion and details through unit-to-set matching and bidirectional validation. Based on the HMD-270K supervised fine-tuning and GRPO post-training with CSER, we developed OwlCap, a powerful video captioning multi-modal large language model (MLLM) with motion-detail balance. Experimental results demonstrate that OwlCap achieves significant improvements compared to baseline models on two benchmarks: the detail-focused VDC (+4.2 Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap model will be publicly released to facilitate video captioning research community advancements.",
        "arxiv_id": "2508.18634",
        "ARXIVID": "2508.18634",
        "COMMENT": "This paper matches criterion 6 as it focuses on video captioning, a video understanding task, and introduces a new dataset and optimization techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.18788": {
        "authors": [
            "Christian L\\\"owens",
            "Thorben Funke",
            "Jingchao Xie",
            "Alexandru Paul Condurache"
        ],
        "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps",
        "abstract": "arXiv:2508.18788v1 Announce Type: new  Abstract: Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at github.com/boschresearch/PseudoMapTrainer.",
        "arxiv_id": "2508.18788",
        "ARXIVID": "2508.18788",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for online mapping without HD maps, which is relevant to embodied/robotic AI benchmarks and methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19209": {
        "authors": [
            "Jianwen Jiang",
            "Weihong Zeng",
            "Zerong Zheng",
            "Jiaqi Yang",
            "Chao Liang",
            "Wang Liao",
            "Han Liang",
            "Yuan Zhang",
            "Mingyuan Gao"
        ],
        "title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation",
        "abstract": "arXiv:2508.19209v1 Announce Type: new  Abstract: Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, \\textbf{we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive.} Our model, \\textbf{OmniHuman-1.5}, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: \\href{https://omnihuman-lab.github.io/v1_5/}",
        "arxiv_id": "2508.19209",
        "ARXIVID": "2508.19209",
        "COMMENT": "Matches criterion 5 as it integrates multimodal inputs (audio, images, text) for video avatar generation, aligning with vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.18797": {
        "authors": [
            "Qi Chai",
            "Zhang Zheng",
            "Junlong Ren",
            "Deheng Ye",
            "Zichuan Lin",
            "Hao Wang"
        ],
        "title": "CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks",
        "abstract": "arXiv:2508.18797v1 Announce Type: new  Abstract: Minecraft, as an open-world virtual interactive environment, has become a prominent platform for research on agent decision-making and execution. Existing works primarily adopt a single Large Language Model (LLM) agent to complete various in-game tasks. However, for complex tasks requiring lengthy sequences of actions, single-agent approaches often face challenges related to inefficiency and limited fault tolerance. Despite these issues, research on multi-agent collaboration remains scarce. In this paper, we propose CausalMACE, a holistic causality planning framework designed to enhance multi-agent systems, in which we incorporate causality to manage dependencies among subtasks. Technically, our proposed framework introduces two modules: an overarching task graph for global task planning and a causality-based module for dependency management, where inherent rules are adopted to perform causal intervention. Experimental results demonstrate our approach achieves state-of-the-art performance in multi-agent cooperative tasks of Minecraft.",
        "arxiv_id": "2508.18797",
        "ARXIVID": "2508.18797",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for multi-agent collaboration in Minecraft, focusing on causality and task planning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19113": {
        "authors": [
            "Dayoon Ko",
            "Jihyuk Kim",
            "Haeju Park",
            "Sohyeon Kim",
            "Dahyun Lee",
            "Yongrae Jo",
            "Gunhee Kim",
            "Moontae Lee",
            "Kyungjae Lee"
        ],
        "title": "Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning",
        "abstract": "arXiv:2508.19113v1 Announce Type: new  Abstract: Large reasoning models (LRMs) have demonstrated strong performance in complex, multi-step reasoning tasks. Existing methods enhance LRMs by sequentially integrating external knowledge retrieval; models iteratively generate queries, retrieve external information, and progressively reason over this information. However, purely sequential querying increases inference latency and context length, diminishing coherence and potentially reducing accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search QA), a synthetic dataset automatically generated from Natural Questions, explicitly designed to train LRMs to distinguish parallelizable from sequential queries. HDS-QA comprises hybrid-hop questions that combine parallelizable independent subqueries (executable simultaneously) and sequentially dependent subqueries (requiring step-by-step resolution), along with synthetic reasoning-querying-retrieval paths involving parallel queries. We fine-tune an LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms state-of-the-art baselines across multiple benchmarks, notably achieving +15.9 and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both requiring comprehensive and exhaustive search. Experimental results highlight two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer search turns, significantly reducing inference latency, and it effectively scales as more turns are permitted. These results demonstrate the efficiency, scalability, and effectiveness of explicitly training LRMs to leverage hybrid parallel and sequential querying.",
        "arxiv_id": "2508.19113",
        "ARXIVID": "2508.19113",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial reasoning in large reasoning models, focusing on hybrid parallel and sequential querying.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.18621": {
        "authors": [
            "Xin Gao",
            "Li Hu",
            "Siqi Hu",
            "Mingyang Huang",
            "Chaonan Ji",
            "Dechao Meng",
            "Jinwei Qi",
            "Penchong Qiao",
            "Zhen Shen",
            "Yafei Song",
            "Ke Sun",
            "Linrui Tian",
            "Guangyuan Wang",
            "Qi Wang",
            "Zhongjian Wang",
            "Jiayu Xiao",
            "Sheng Xu",
            "Bang Zhang",
            "Peng Zhang",
            "Xindi Zhang",
            "Zhe Zhang",
            "Jingren Zhou",
            "Lian Zhuo"
        ],
        "title": "Wan-S2V: Audio-Driven Cinematic Video Generation",
        "abstract": "arXiv:2508.18621v1 Announce Type: new  Abstract: Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing.",
        "arxiv_id": "2508.18621",
        "ARXIVID": "2508.18621",
        "COMMENT": "Matches criterion 5 as it focuses on combining audio (a modality) with video generation tasks, which aligns with integrating multimodal tasks and LLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.18539": {
        "authors": [
            "Kaijie Xu",
            "Clark Verbrugge"
        ],
        "title": "Adaptive Visual Navigation Assistant in 3D RPGs",
        "abstract": "arXiv:2508.18539v1 Announce Type: new  Abstract: In complex 3D game environments, players rely on visual affordances to spot map transition points. Efficient identification of such points is important to client-side auto-mapping, and provides an objective basis for evaluating map cue presentation. In this work, we formalize the task of detecting traversable Spatial Transition Points (STPs)-connectors between two sub regions-and selecting the singular Main STP (MSTP), the unique STP that lies on the designer-intended critical path toward the player's current macro-objective, from a single game frame, proposing this as a new research focus. We introduce a two-stage deep-learning pipeline that first detects potential STPs using Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses local and global visual features. Both stages benefit from parameter-efficient adapters, and we further introduce an optional retrieval-augmented fusion step. Our primary goal is to establish the feasibility of this problem and set baseline performance metrics. We validate our approach on a custom-built, diverse dataset collected from five Action RPG titles. Our experiments reveal a key trade-off: while full-network fine-tuning produces superior STP detection with sufficient data, adapter-only transfer is significantly more robust and effective in low-data scenarios and for the MSTP selection task. By defining this novel problem, providing a baseline pipeline and dataset, and offering initial insights into efficient model adaptation, we aim to contribute to future AI-driven navigation aids and data-informed level-design tools.",
        "arxiv_id": "2508.18539",
        "ARXIVID": "2508.18539",
        "COMMENT": "Matches criterion 1 as it focuses on spatial intelligence and navigation in 3D environments, relevant to embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.18322": {
        "authors": [
            "Jiangfeng Sun",
            "Sihao He",
            "Zhonghong Ou",
            "Meina Song"
        ],
        "title": "Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning",
        "abstract": "arXiv:2508.18322v1 Announce Type: new  Abstract: Multimodal sentiment analysis (MSA) aims to infer emotional states by effectively integrating textual, acoustic, and visual modalities. Despite notable progress, existing multimodal fusion methods often neglect modality-specific structural dependencies and semantic misalignment, limiting their quality, interpretability, and robustness. To address these challenges, we propose a novel framework called the Structural-Semantic Unifier (SSU), which systematically integrates modality-specific structural information and cross-modal semantic grounding for enhanced multimodal representations. Specifically, SSU dynamically constructs modality-specific graphs by leveraging linguistic syntax for text and a lightweight, text-guided attention mechanism for acoustic and visual modalities, thus capturing detailed intra-modal relationships and semantic interactions. We further introduce a semantic anchor, derived from global textual semantics, that serves as a cross-modal alignment hub, effectively harmonizing heterogeneous semantic spaces across modalities. Additionally, we develop a multiview contrastive learning objective that promotes discriminability, semantic consistency, and structural coherence across intra- and inter-modal views. Extensive evaluations on two widely used benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently achieves state-of-the-art performance while significantly reducing computational overhead compared to prior methods. Comprehensive qualitative analyses further validate SSU's interpretability and its ability to capture nuanced emotional patterns through semantically grounded interactions.",
        "arxiv_id": "2508.18322",
        "ARXIVID": "2508.18322",
        "COMMENT": "This paper matches criterion 2 as it explores multimodal fusion techniques, which are relevant to vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.19244": {
        "authors": [
            "Oishi Deb",
            "Anjun Hu",
            "Ashkan Khakzar",
            "Philip Torr",
            "Christian Rupprecht"
        ],
        "title": "Articulate3D: Zero-Shot Text-Driven 3D Object Posing",
        "abstract": "arXiv:2508.19244v1 Announce Type: new  Abstract: We propose a training-free method, Articulate3D, to pose a 3D asset through language control. Despite advances in vision and language models, this task remains surprisingly challenging. To achieve this goal, we decompose the problem into two steps. We modify a powerful image-generator to create target images conditioned on the input image and a text instruction. We then align the mesh to the target images through a multi-view pose optimisation step. In detail, we introduce a self-attention rewiring mechanism (RSActrl) that decouples the source structure from pose within an image generative model, allowing it to maintain a consistent structure across varying poses. We observed that differentiable rendering is an unreliable signal for articulation optimisation; instead, we use keypoints to establish correspondences between input and target images. The effectiveness of Articulate3D is demonstrated across a diverse range of 3D objects and free-form text prompts, successfully manipulating poses while maintaining the original identity of the mesh. Quantitative evaluations and a comparative user study, in which our method was preferred over 85\\% of the time, confirm its superiority over existing approaches. Project page:https://odeb1.github.io/articulate3d_page_deb/",
        "arxiv_id": "2508.19244",
        "ARXIVID": "2508.19244",
        "COMMENT": "This paper matches criterion 5 as it combines image understanding tasks with language models for 3D object posing.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.18904": {
        "authors": [
            "Thien-Phuc Tran",
            "Minh-Quang Nguyen",
            "Minh-Triet Tran",
            "Tam V. Nguyen",
            "Trong-Le Do",
            "Duy-Nam Ly",
            "Viet-Tham Huynh",
            "Khanh-Duy Le",
            "Mai-Khiem Tran",
            "Trung-Nghia Le"
        ],
        "title": "Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025",
        "abstract": "arXiv:2508.18904v1 Announce Type: new  Abstract: The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM Multimedia 2025, introduces the first large-scale benchmark for event-level multimodal understanding. Traditional captioning and retrieval tasks largely focus on surface-level recognition of people, objects, and scenes, often overlooking the contextual and semantic dimensions that define real-world events. EVENTA addresses this gap by integrating contextual, temporal, and semantic information to capture the who, when, where, what, and why behind an image. Built upon the OpenEvents V1 dataset, the challenge features two tracks: Event-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval. A total of 45 teams from six countries participated, with evaluation conducted through Public and Private Test phases to ensure fairness and reproducibility. The top three teams were invited to present their solutions at ACM Multimedia 2025. EVENTA establishes a foundation for context-aware, narrative-driven multimedia AI, with applications in journalism, media analysis, cultural archiving, and accessibility. Further details about the challenge are available at the official homepage: https://ltnghia.github.io/eventa/eventa-2025.",
        "arxiv_id": "2508.18904",
        "ARXIVID": "2508.18904",
        "COMMENT": "Matches criterion 6 as it introduces a new benchmark for event-enriched image analysis, focusing on contextual and semantic dimensions.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.18772": {
        "authors": [
            "Wanqiang Wang",
            "Longzhu He",
            "Wei Zheng"
        ],
        "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs",
        "abstract": "arXiv:2508.18772v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep thinking and knowledge integration in education. However, previous research has primarily focused on generating MCQs with textual options, but it largely overlooks the visual options. Moreover, generating high-quality distractors remains a major challenge due to the high cost and limited scalability of manual authoring. To tackle these problems, we propose a Cross-modal Options Synthesis (CmOS), a novel framework for generating educational MCQs with visual options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning process and Retrieval-Augmented Generation (RAG) to produce semantically plausible and visually similar answer and distractors. It also includes a discrimination module to identify content suitable for visual options. Experimental results on test tasks demonstrate the superiority of CmOS in content discrimination, question generation and visual option generation over existing methods across various subjects and educational levels.",
        "arxiv_id": "2508.18772",
        "ARXIVID": "2508.18772",
        "COMMENT": "Matches criterion 5 as it integrates image understanding tasks with multimodal reasoning for generating visual options in MCQs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.18989": {
        "authors": [
            "Shaivi Malik",
            "Hasnat Md Abdullah",
            "Sriparna Saha",
            "Amit Sheth"
        ],
        "title": "Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone",
        "abstract": "arXiv:2508.18989v1 Announce Type: new  Abstract: As Vision Language Models (VLMs) become integral to real-world applications, understanding their demographic biases is critical. We introduce GRAS, a benchmark for uncovering demographic biases in VLMs across gender, race, age, and skin tone, offering the most diverse coverage to date. We further propose the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark five state-of-the-art VLMs and reveal concerning bias levels, with the least biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings also reveal a methodological insight: evaluating bias in VLMs with visual question answering (VQA) requires considering multiple formulations of a question. Our code, data, and evaluation results are publicly available.",
        "arxiv_id": "2508.18989",
        "ARXIVID": "2508.18989",
        "COMMENT": "Matches criterion 2 as it benchmarks and evaluates bias in Vision Language Models, which is relevant to vision\u2013language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.18633": {
        "authors": [
            "Chenxuan Miao",
            "Yutong Feng",
            "Jianshu Zeng",
            "Zixiang Gao",
            "Hantang Liu",
            "Yunfeng Yan",
            "Donglian Qi",
            "Xi Chen",
            "Bin Wang",
            "Hengshuang Zhao"
        ],
        "title": "ROSE: Remove Objects with Side Effects in Videos",
        "abstract": "arXiv:2508.18633v1 Announce Type: new  Abstract: Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/.",
        "arxiv_id": "2508.18633",
        "ARXIVID": "2508.18633",
        "COMMENT": "Matches criterion 6 as it focuses on video object removal and introduces a benchmark for evaluating video-based tasks, which is relevant to video understanding.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.18297": {
        "authors": [
            "Dhananjay Ashok",
            "Ashutosh Chaubey",
            "Hirona J. Arai",
            "Jonathan May",
            "Jesse Thomason"
        ],
        "title": "Can VLMs Recall Factual Associations From Visual References?",
        "abstract": "arXiv:2508.18297v1 Announce Type: new  Abstract: Through a controlled study, we identify a systematic deficiency in the multimodal grounding of Vision Language Models (VLMs). While VLMs can recall factual associations when provided a textual reference to an entity; their ability to do so is significantly diminished when the reference is visual instead. Forcing VLMs to rely on image representations of an entity halves their ability to recall factual knowledge, suggesting that VLMs struggle to link their internal knowledge of an entity with its image representation. We show that such linking failures are correlated with the expression of distinct patterns in model internal states, and that probes on these internal states achieve over 92% accuracy at flagging cases where the VLM response is unreliable. These probes can be applied, without retraining, to identify when a VLM will fail to correctly answer a question that requires an understanding of multimodal input. When used to facilitate selective prediction on a visual question answering task, the probes increase coverage by 7.87% (absolute) while also reducing the risk of error by 0.9% (absolute). Addressing the systematic, detectable deficiency is an important avenue in language grounding, and we provide informed recommendations for future directions.",
        "arxiv_id": "2508.18297",
        "ARXIVID": "2508.18297",
        "COMMENT": "Matches criterion 2 as it investigates deficiencies in multimodal grounding of Vision Language Models (VLMs), which is directly relevant to VLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.18506": {
        "authors": [
            "Ajinkya Khoche",
            "Qingwen Zhang",
            "Yixi Cai",
            "Sina Sharif Mansouri",
            "Patric Jensfelt"
        ],
        "title": "DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance",
        "abstract": "arXiv:2508.18506v1 Announce Type: new  Abstract: Accurate 3D scene flow estimation is critical for autonomous systems to navigate dynamic environments safely, but creating the necessary large-scale, manually annotated datasets remains a significant bottleneck for developing robust perception models. Current self-supervised methods struggle to match the performance of fully supervised approaches, especially in challenging long-range and adverse weather scenarios, while supervised methods are not scalable due to their reliance on expensive human labeling. We introduce DoGFlow, a novel self-supervised framework that recovers full 3D object motions for LiDAR scene flow estimation without requiring any manual ground truth annotations. This paper presents our cross-modal label transfer approach, where DoGFlow computes motion pseudo-labels in real-time directly from 4D radar Doppler measurements and transfers them to the LiDAR domain using dynamic-aware association and ambiguity-resolved propagation. On the challenging MAN TruckScenes dataset, DoGFlow substantially outperforms existing self-supervised methods and improves label efficiency by enabling LiDAR backbones to achieve over 90% of fully supervised performance with only 10% of the ground truth data. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/",
        "arxiv_id": "2508.18506",
        "ARXIVID": "2508.18506",
        "COMMENT": "Matches criterion 3 as it introduces a novel self-supervised framework for LiDAR scene flow estimation, which is relevant to embodied/robotic AI methods.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.18527": {
        "authors": [
            "Kaijie Xu",
            "Clark Verbrugge"
        ],
        "title": "Generic Guard AI in Stealth Game with Composite Potential Fields",
        "abstract": "arXiv:2508.18527v1 Announce Type: new  Abstract: Guard patrol behavior is central to the immersion and strategic depth of stealth games, while most existing systems rely on hand-crafted routes or specialized logic that struggle to balance coverage efficiency and responsive pursuit with believable naturalness. We propose a generic, fully explainable, training-free framework that integrates global knowledge and local information via Composite Potential Fields, combining three interpretable maps-Information, Confidence, and Connectivity-into a single kernel-filtered decision criterion. Our parametric, designer-driven approach requires only a handful of decay and weight parameters-no retraining-to smoothly adapt across both occupancy-grid and NavMesh-partition abstractions. We evaluate on five representative game maps, two player-control policies, and five guard modes, confirming that our method outperforms classical baseline methods in both capture efficiency and patrol naturalness. Finally, we show how common stealth mechanics-distractions and environmental elements-integrate naturally into our framework as sub modules, enabling rapid prototyping of rich, dynamic, and responsive guard behaviors.",
        "arxiv_id": "2508.18527",
        "ARXIVID": "2508.18527",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial reasoning in guard patrol behavior in stealth games.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2508.19035": {
        "authors": [
            "Congchi Yin",
            "Tianyi Wu",
            "Yankai Shu",
            "Alex Gu",
            "Yunhan Wang",
            "Jun Shao",
            "Xun Jiang",
            "Piji Li"
        ],
        "title": "Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction",
        "abstract": "arXiv:2508.19035v1 Announce Type: new  Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language Models (LLMs) in an interactive, unknown environment. This deficiency leads to the isolated assessment of deductive, inductive, and abductive reasoning, neglecting the integrated reasoning process that is indispensable for humans discovery of real world. We introduce a novel evaluation paradigm, \\textit{black-box interaction}, to tackle this challenge. A black-box is defined by a hidden function that maps a specific set of inputs to outputs. LLMs are required to unravel the hidden function behind the black-box by interacting with it in given exploration turns, and reasoning over observed input-output pairs. Leveraging this idea, we build the \\textsc{Oracle} benchmark which comprises 6 types of black-box task and 96 black-boxes. 19 modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over 70\\% accuracy on most easy black-boxes. But it still struggles with some hard black-box tasks, where its average performance drops below 40\\%. Further analysis indicates a universal difficulty among LLMs: They lack the high-level planning capability to develop efficient and adaptive exploration strategies for hypothesis refinement.",
        "arxiv_id": "2508.19035",
        "ARXIVID": "2508.19035",
        "COMMENT": "Matches criterion 1 as it introduces a novel evaluation paradigm for reasoning in unknown environments, which could be relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.18886": {
        "authors": [
            "Yuexuan Xia",
            "Benteng Ma",
            "Jiang He",
            "Zhiyong Wang",
            "Qi Dou",
            "Yong Xia"
        ],
        "title": "Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models",
        "abstract": "arXiv:2508.18886v1 Announce Type: new  Abstract: Ensuring fairness across demographic groups in medical diagnosis is essential for equitable healthcare, particularly under distribution shifts caused by variations in imaging equipment and clinical practice. Vision-language models (VLMs) exhibit strong generalization, and text prompts encode identity attributes, enabling explicit identification and removal of sensitive directions. However, existing debiasing approaches typically address vision and text modalities independently, leaving residual cross-modal misalignment and fairness gaps. To address this challenge, we propose DualFairVL, a multimodal prompt-learning framework that jointly debiases and aligns cross-modal representations. DualFairVL employs a parallel dual-branch architecture that separates sensitive and target attributes, enabling disentangled yet aligned representations across modalities. Approximately orthogonal text anchors are constructed via linear projections, guiding cross-attention mechanisms to produce fused features. A hypernetwork further disentangles attribute-related information and generates instance-aware visual prompts, which encode dual-modal cues for fairness and robustness. Prototype-based regularization is applied in the visual branch to enforce separation of sensitive features and strengthen alignment with textual anchors. Extensive experiments on eight medical imaging datasets across four modalities show that DualFairVL achieves state-of-the-art fairness and accuracy under both in- and out-of-distribution settings, outperforming full fine-tuning and parameter-efficient baselines with only 3.6M trainable parameters. Code will be released upon publication.",
        "arxiv_id": "2508.18886",
        "ARXIVID": "2508.18886",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on debiasing and aligning vision-language models for medical fairness.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.18696": {
        "authors": [
            "Qun Ji",
            "Peng Li",
            "Mingqiang Wei"
        ],
        "title": "ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting",
        "abstract": "arXiv:2508.18696v1 Announce Type: new  Abstract: High-fidelity reconstruction of deformable tissues from endoscopic videos remains challenging due to the limitations of existing methods in capturing subtle color variations and modeling global deformations. While 3D Gaussian Splatting (3DGS) enables efficient dynamic reconstruction, its fixed per-Gaussian color assignment struggles with intricate textures, and linear deformation modeling fails to model consistent global deformation. To address these issues, we propose ColorGS, a novel framework that integrates spatially adaptive color encoding and enhanced deformation modeling for surgical scene reconstruction. First, we introduce Colored Gaussian Primitives, which employ dynamic anchors with learnable color parameters to adaptively encode spatially varying textures, significantly improving color expressiveness under complex lighting and tissue similarity. Second, we design an Enhanced Deformation Model (EDM) that combines time-aware Gaussian basis functions with learnable time-independent deformations, enabling precise capture of both localized tissue deformations and global motion consistency caused by surgical interactions. Extensive experiments on DaVinci robotic surgery videos and benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior 3DGS-based methods) and superior SSIM (97.25\\%) while maintaining real-time rendering efficiency. Our work advances surgical scene reconstruction by balancing high fidelity with computational practicality, critical for intraoperative guidance and AR/VR applications.",
        "arxiv_id": "2508.18696",
        "ARXIVID": "2508.18696",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on high-fidelity surgical scene reconstruction using advanced vision techniques.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.18984": {
        "authors": [
            "Eric L\\'opez",
            "Artemis Llabr\\'es",
            "Ernest Valveny"
        ],
        "title": "Enhancing Document VQA Models via Retrieval-Augmented Generation",
        "abstract": "arXiv:2508.18984v1 Announce Type: new  Abstract: Document Visual Question Answering (Document VQA) must cope with documents that span dozens of pages, yet leading systems still concatenate every page or rely on very large vision-language models, both of which are memory-hungry. Retrieval-Augmented Generation (RAG) offers an attractive alternative, first retrieving a concise set of relevant segments before generating answers from this selected evidence. In this paper, we systematically evaluate the impact of incorporating RAG into Document VQA through different retrieval variants - text-based retrieval using OCR tokens and purely visual retrieval without OCR - across multiple models and benchmarks. Evaluated on the multi-page datasets MP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the \"concatenate-all-pages\" baseline by up to +22.5 ANLS, while the visual variant achieves +5.0 ANLS improvement without requiring any text extraction. An ablation confirms that retrieval and reranking components drive most of the gain, whereas the layout-guided chunking strategy - proposed in several recent works to leverage page structure - fails to help on these datasets. Our experiments demonstrate that careful evidence selection consistently boosts accuracy across multiple model sizes and multi-page benchmarks, underscoring its practical value for real-world Document VQA.",
        "arxiv_id": "2508.18984",
        "ARXIVID": "2508.18984",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores retrieval-augmented generation for Document VQA.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.18859": {
        "authors": [
            "Muhammad Kashif Ali",
            "Eun Woo Im",
            "Dongjin Kim",
            "Tae Hyun Kim",
            "Vivek Gupta",
            "Haonan Luo",
            "Tianrui Li"
        ],
        "title": "Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization",
        "abstract": "arXiv:2508.18859v1 Announce Type: new  Abstract: Video stabilization remains a fundamental problem in computer vision, particularly pixel-level synthesis solutions for video stabilization, which synthesize full-frame outputs, add to the complexity of this task. These methods aim to enhance stability while synthesizing full-frame videos, but the inherent diversity in motion profiles and visual content present in each video sequence makes robust generalization with fixed parameters difficult. To address this, we present a novel method that improves pixel-level synthesis video stabilization methods by rapidly adapting models to each input video at test time. The proposed approach takes advantage of low-level visual cues available during inference to improve both the stability and visual quality of the output. Notably, the proposed rapid adaptation achieves significant performance gains even with a single adaptation pass. We further propose a jerk localization module and a targeted adaptation strategy, which focuses the adaptation on high-jerk segments for maximizing stability with fewer adaptation steps. The proposed methodology enables modern stabilizers to overcome the longstanding SOTA approaches while maintaining the full frame nature of the modern methods, while offering users with control mechanisms akin to classical approaches. Extensive experiments on diverse real-world datasets demonstrate the versatility of the proposed method. Our approach consistently improves the performance of various full-frame synthesis models in both qualitative and quantitative terms, including results on downstream applications.",
        "arxiv_id": "2508.18859",
        "ARXIVID": "2508.18859",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video stabilization with novel methodologies.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.18753": {
        "authors": [
            "Qinqian Lei",
            "Bo Wang",
            "Robby T. Tan"
        ],
        "title": "Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods",
        "abstract": "arXiv:2508.18753v1 Announce Type: new  Abstract: Prior human-object interaction (HOI) detection methods have integrated early vision-language models (VLMs) such as CLIP, but only as supporting components within their frameworks. In contrast, recent advances in large, generative VLMs suggest that these models may already possess strong ability to understand images involving HOI. This naturally raises an important question: can general-purpose standalone VLMs effectively solve HOI detection, and how do they compare with specialized HOI methods? Answering this requires a benchmark that can accommodate both paradigms. However, existing HOI benchmarks such as HICO-DET were developed before the emergence of modern VLMs, and their evaluation protocols require exact matches to annotated HOI classes. This is poorly aligned with the generative nature of VLMs, which often yield multiple valid interpretations in ambiguous cases. For example, a static image may capture a person mid-motion with a frisbee, which can plausibly be interpreted as either \"throwing\" or \"catching\". When only \"catching\" is annotated, the other, though equally plausible for the image, is marked incorrect when exact matching is used. As a result, correct predictions might be penalized, affecting both VLMs and HOI-specific methods. To avoid penalizing valid predictions, we introduce a new benchmark that reformulates HOI detection as a multiple-answer multiple-choice task, where each question includes only ground-truth positive options and a curated set of negatives that are constructed to reduce ambiguity (e.g., when \"catching\" is annotated, \"throwing\" is not selected as a negative to avoid penalizing valid predictions). The proposed evaluation protocol is the first of its kind for both VLMs and HOI methods, enabling direct comparison and offering new insight into the current state of progress in HOI understanding.",
        "arxiv_id": "2508.18753",
        "ARXIVID": "2508.18753",
        "COMMENT": "Matches criterion 2 as it evaluates Vision-Language Models (VLMs) for human-object interaction, which is relevant to vision-language integration.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2508.18533": {
        "authors": [
            "Kaijie Xu",
            "Clark Verbrugge"
        ],
        "title": "A Database-Driven Framework for 3D Level Generation with LLMs",
        "abstract": "arXiv:2508.18533v1 Announce Type: new  Abstract: Procedural Content Generation for 3D game levels faces challenges in balancing spatial coherence, navigational functionality, and adaptable gameplay progression across multi-floor environments. This paper introduces a novel framework for generating such levels, centered on the offline, LLM-assisted construction of reusable databases for architectural components (facilities and room templates) and gameplay mechanic elements. Our multi-phase pipeline assembles levels by: (1) selecting and arranging instances from the Room Database to form a multi-floor global structure with an inherent topological order; (2) optimizing the internal layout of facilities for each room based on predefined constraints from the Facility Database; and (3) integrating progression-based gameplay mechanics by placing components from a Mechanics Database according to their topological and spatial rules. A subsequent two-phase repair system ensures navigability. This approach combines modular, database-driven design with constraint-based optimization, allowing for systematic control over level structure and the adaptable pacing of gameplay elements. Initial experiments validate the framework's ability in generating diverse, navigable 3D environments and its capability to simulate distinct gameplay pacing strategies through simple parameterization. This research advances PCG by presenting a scalable, database-centric foundation for the automated generation of complex 3D levels with configurable gameplay progression.",
        "arxiv_id": "2508.18533",
        "ARXIVID": "2508.18533",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and procedural content generation for 3D environments, which is relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19229": {
        "authors": [
            "Wei Xiong",
            "Wenting Zhao",
            "Weizhe Yuan",
            "Olga Golovneva",
            "Tong Zhang",
            "Jason Weston",
            "Sainbayar Sukhbaatar"
        ],
        "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
        "abstract": "arXiv:2508.19229v1 Announce Type: new  Abstract: As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.",
        "arxiv_id": "2508.19229",
        "ARXIVID": "2508.19229",
        "COMMENT": "Matches criterion 2 as it explores reasoning and generative modeling in multi-step processes, which could be relevant to improving reasoning in VLLMs or MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19005": {
        "authors": [
            "Yuxuan Cai",
            "Yipeng Hao",
            "Jie Zhou",
            "Hang Yan",
            "Zhikai Lei",
            "Rui Zhen",
            "Zhenhua Han",
            "Yutao Yang",
            "Junsong Li",
            "Qianjun Pan",
            "Tianyu Huai",
            "Qin Chen",
            "Xin Li",
            "Kai Chen",
            "Bo Zhang",
            "Xipeng Qiu",
            "Liang He"
        ],
        "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark",
        "abstract": "arXiv:2508.19005v1 Announce Type: new  Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as \"second nature\".   We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm shifts: From Passive to Proactive, From Context to Memory, and From Imitation to Learning. In this dynamic environment, agents must acquire and distill practical skills and maintain persistent memory to make decisions based on evolving state variables. StuLife provides a comprehensive platform for evaluating lifelong learning capabilities, including memory retention, skill transfer, and self-motivated behavior. Beyond evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of context engineering in advancing AGI.",
        "arxiv_id": "2508.19005",
        "ARXIVID": "2508.19005",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a framework and benchmark (StuLife) for lifelong learning in agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19024": {
        "authors": [
            "Yi Pan",
            "Yujia Zhang",
            "Michael Kampffmeyer",
            "Xiaoguang Zhao"
        ],
        "title": "ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval",
        "abstract": "arXiv:2508.19024v1 Announce Type: new  Abstract: Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task that involves retrieving videos based on queries relevant to only specific segments. While existing works follow the paradigm of developing models to process unimodal features, powerful pretrained vision-language models like CLIP remain underexplored in this field. To bridge this gap, we propose ProPy, a model with systematic architectural adaption of CLIP specifically designed for PRVR. Drawing insights from the semantic relevance of multi-granularity events, ProPy introduces two key innovations: (1) A Prompt Pyramid structure that organizes event prompts to capture semantics at multiple granularity levels, and (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that enables dynamic semantic interaction among events. With these designs, ProPy achieves SOTA performance on three public datasets, outperforming previous models by significant margins. Code is available at https://github.com/BUAAPY/ProPy.",
        "arxiv_id": "2508.19024",
        "ARXIVID": "2508.19024",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video retrieval using a novel architecture (ProPy) and explores partially relevant video retrieval tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19094": {
        "authors": [
            "Vincenzo Polizzi",
            "Stephen Yang",
            "Quentin Clark",
            "Jonathan Kelly",
            "Igor Gilitschenski",
            "David B. Lindell"
        ],
        "title": "VibES: Induced Vibration for Persistent Event-Based Sensing",
        "abstract": "arXiv:2508.19094v1 Announce Type: new  Abstract: Event cameras are a bio-inspired class of sensors that asynchronously measure per-pixel intensity changes. Under fixed illumination conditions in static or low-motion scenes, rigidly mounted event cameras are unable to generate any events, becoming unsuitable for most computer vision tasks. To address this limitation, recent work has investigated motion-induced event stimulation that often requires complex hardware or additional optical components. In contrast, we introduce a lightweight approach to sustain persistent event generation by employing a simple rotating unbalanced mass to induce periodic vibrational motion. This is combined with a motion-compensation pipeline that removes the injected motion and yields clean, motion-corrected events for downstream perception tasks. We demonstrate our approach with a hardware prototype and evaluate it on real-world captured datasets. Our method reliably recovers motion parameters and improves both image reconstruction and edge detection over event-based sensing without motion induction.",
        "arxiv_id": "2508.19094",
        "ARXIVID": "2508.19094",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel method for event-based sensing in robotics using induced vibration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19247": {
        "authors": [
            "Lin Li",
            "Zehuan Huang",
            "Haoran Feng",
            "Gengxiong Zhuang",
            "Rui Chen",
            "Chunchao Guo",
            "Lu Sheng"
        ],
        "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space",
        "abstract": "arXiv:2508.19247v1 Announce Type: new  Abstract: 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.",
        "arxiv_id": "2508.19247",
        "ARXIVID": "2508.19247",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new method for precise 3D editing and a benchmark dataset (Edit3D-Bench).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19182": {
        "authors": [
            "Silvio Giancola",
            "Anthony Cioppa",
            "Marc Guti\\'errez-P\\'erez",
            "Jan Held",
            "Carlos Hinojosa",
            "Victor Joos",
            "Arnaud Leduc",
            "Floriane Magera",
            "Karen Sanchez",
            "Vladimir Somers",
            "Artur Xarles",
            "Antonio Agudo",
            "Alexandre Alahi",
            "Olivier Barnich",
            "Albert Clap\\'es",
            "Christophe De Vleeschouwer",
            "Sergio Escalera",
            "Bernard Ghanem",
            "Thomas B. Moeslund",
            "Marc Van Droogenbroeck",
            "Tomoki Abe",
            "Saad Alotaibi",
            "Faisal Altawijri",
            "Steven Araujo",
            "Xiang Bai",
            "Xiaoyang Bi",
            "Jiawang Cao",
            "Vanyi Chao",
            "Kamil Czarnog\\'orski",
            "Fabian Deuser",
            "Mingyang Du",
            "Tianrui Feng",
            "Patrick Frenzel",
            "Mirco Fuchs",
            "Jorge Garc\\'ia",
            "Konrad Habel",
            "Takaya Hashiguchi",
            "Sadao Hirose",
            "Xinting Hu",
            "Yewon Hwang",
            "Ririko Inoue",
            "Riku Itsuji",
            "Kazuto Iwai",
            "Hongwei Ji",
            "Yangguang Ji",
            "Licheng Jiao",
            "Yuto Kageyama",
            "Yuta Kamikawa",
            "Yuuki Kanasugi",
            "Hyungjung Kim",
            "Jinwook Kim",
            "Takuya Kurihara",
            "Bozheng Li",
            "Lingling Li",
            "Xian Li",
            "Youxing Lian",
            "Dingkang Liang",
            "Hongkai Lin",
            "Jiadong Lin",
            "Jian Liu",
            "Liang Liu",
            "Shuaikun Liu",
            "Zhaohong Liu",
            "Yi Lu",
            "Federico M\\'endez",
            "Huadong Ma",
            "Wenping Ma",
            "Jacek Maksymiuk",
            "Henry Mantilla",
            "Ismail Mathkour",
            "Daniel Matthes",
            "Ayaha Motomochi",
            "Amrulloh Robbani Muhammad",
            "Haruto Nakayama",
            "Joohyung Oh",
            "Yin May Oo",
            "Marcelo Ortega",
            "Norbert Oswald",
            "Rintaro Otsubo",
            "Fabian Perez",
            "Mengshi Qi",
            "Cristian Rey",
            "Abel Reyes-Angulo",
            "Oliver Rose",
            "Hoover Rueda-Chac\\'on",
            "Hideo Saito",
            "Jose Sarmiento",
            "Kanta Sawafuji",
            "Atom Scott",
            "Xi Shen",
            "Pragyan Shrestha",
            "Jae-Young Sim",
            "Long Sun",
            "Yuyang Sun",
            "Tomohiro Suzuki",
            "Licheng Tang",
            "Masato Tonouchi",
            "Ikuma Uchida",
            "Henry O. Velesaca",
            "Tiancheng Wang",
            "Rio Watanabe",
            "Jay Wu",
            "Yongliang Wu",
            "Shunzo Yamagishi",
            "Di Yang",
            "Xu Yang",
            "Yuxin Yang",
            "Hao Ye",
            "Xinyu Ye",
            "Calvin Yeung",
            "Xuanlong Yu",
            "Chao Zhang",
            "Dingyuan Zhang",
            "Kexing Zhang",
            "Zhe Zhao",
            "Xin Zhou",
            "Wenbo Zhu",
            "Julian Ziegler"
        ],
        "title": "SoccerNet 2025 Challenges Results",
        "abstract": "arXiv:2508.19182v1 Announce Type: new  Abstract: The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNet open benchmarking effort, dedicated to advancing computer vision research in football video understanding. This year's challenges span four vision-based tasks: (1) Team Ball Action Spotting, focused on detecting ball-related actions in football broadcasts and assigning actions to teams; (2) Monocular Depth Estimation, targeting the recovery of scene geometry from single-camera broadcast clips through relative depth estimation for each pixel; (3) Multi-View Foul Recognition, requiring the analysis of multiple synchronized camera views to classify fouls and their severity; and (4) Game State Reconstruction, aimed at localizing and identifying all players from a broadcast video to reconstruct the game state on a 2D top-view of the field. Across all tasks, participants were provided with large-scale annotated datasets, unified evaluation protocols, and strong baselines as starting points. This report presents the results of each challenge, highlights the top-performing solutions, and provides insights into the progress made by the community. The SoccerNet Challenges continue to serve as a driving force for reproducible, open research at the intersection of computer vision, artificial intelligence, and sports. Detailed information about the tasks, challenges, and leaderboards can be found at https://www.soccer-net.org, with baselines and development kits available at https://github.com/SoccerNet.",
        "arxiv_id": "2508.19182",
        "ARXIVID": "2508.19182",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces new benchmarks and challenges for football video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2508.18763": {
        "authors": [
            "Chao Hao",
            "Zezheng Wang",
            "Yanhua Huang",
            "Ruiwen Xu",
            "Wenzhe Niu",
            "Xin Liu",
            "Zitong Yu"
        ],
        "title": "Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units",
        "abstract": "arXiv:2508.18763v1 Announce Type: new  Abstract: This paper investigates the enhancement of reasoning capabilities in language models through token-level multi-model collaboration. Our approach selects the optimal tokens from the next token distributions provided by multiple models to perform autoregressive reasoning. Contrary to the assumption that more models yield better results, we introduce a distribution distance-based dynamic selection strategy (DDS) to optimize the multi-model collaboration process. To address the critical challenge of vocabulary misalignment in multi-model collaboration, we propose the concept of minimal complete semantic units (MCSU), which is simple yet enables multiple language models to achieve natural alignment within the linguistic space. Experimental results across various benchmarks demonstrate the superiority of our method. The code will be available at https://github.com/Fanye12/DDS.",
        "arxiv_id": "2508.18763",
        "ARXIVID": "2508.18763",
        "COMMENT": "Does not match any specific criteria but discusses multi-model collaboration and reasoning, which is tangentially related to general interest in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.18812": {
        "authors": [
            "Chenghao Wu",
            "Ruiyang Ren",
            "Junjie Zhang",
            "Ruirui Wang",
            "Zhongrui Ma",
            "Qi Ye",
            "Wayne Xin Zhao"
        ],
        "title": "STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning",
        "abstract": "arXiv:2508.18812v1 Announce Type: new  Abstract: While modern recommender systems are instrumental in navigating information abundance, they remain fundamentally limited by static user modeling and reactive decision-making paradigms. Current large language model (LLM)-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in sparse-data scenarios. We introduce STARec, a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping. This hybrid approach scaffolds agents in acquiring foundational capabilities (preference summarization, rationale generation) while enabling dynamic policy adaptation through simulated feedback loops. Experiments on MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.",
        "arxiv_id": "2508.18812",
        "ARXIVID": "2508.18812",
        "COMMENT": "Does not match any specific criteria but introduces a novel framework for recommender systems using reasoning, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.18760": {
        "authors": [
            "Yi Liu",
            "Xiangyu Liu",
            "Zequn Sun",
            "Wei Hu"
        ],
        "title": "Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models",
        "abstract": "arXiv:2508.18760v1 Announce Type: new  Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex reasoning tasks. However, some questions posed to LRMs are inherently unanswerable, such as math problems lacking sufficient conditions. We find that LRMs continually fail to provide appropriate abstentions when confronted with these unanswerable questions. In this paper, we systematically analyze, investigate, and resolve this issue for trustworthy AI. We first conduct a detailed analysis of the distinct response behaviors of LRMs when facing unanswerable questions. Then, we show that LRMs possess sufficient cognitive capabilities to recognize the flaws in these questions. However, they fail to exhibit appropriate abstention behavior, revealing a misalignment between their internal cognition and external response. Finally, to resolve this issue, we propose a lightweight, two-stage method that combines cognitive monitoring with inference-time intervention. Experimental results demonstrate that our method significantly improves the abstention rate while maintaining the overall reasoning performance.",
        "arxiv_id": "2508.18760",
        "ARXIVID": "2508.18760",
        "COMMENT": "Does not match any specific criteria but discusses reasoning and abstention in LLMs, which is tangentially related to general interest in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19004": {
        "authors": [
            "Pontus Strimling",
            "Simon Karlsson",
            "Irina Vartanova",
            "Kimmo Eriksson"
        ],
        "title": "AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms",
        "abstract": "arXiv:2508.19004v1 Announce Type: new  Abstract: A fundamental question in cognitive science concerns how social norms are acquired and represented. While humans typically learn norms through embodied social experience, we investigated whether large language models can achieve sophisticated norm understanding through statistical learning alone. Across two studies, we systematically evaluated multiple AI systems' ability to predict human social appropriateness judgments for 555 everyday scenarios by examining how closely they predicted the average judgment compared to each human participant. In Study 1, GPT-4.5's accuracy in predicting the collective judgment on a continuous scale exceeded that of every human participant (100th percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7% of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive power, all models showed systematic, correlated errors. These findings demonstrate that sophisticated models of social cognition can emerge from statistical learning over linguistic data alone, challenging strong versions of theories emphasizing the exclusive necessity of embodied experience for cultural competence. The systematic nature of AI limitations across different architectures indicates potential boundaries of pattern-based social understanding, while the models' ability to outperform nearly all individual humans in this predictive task suggests that language serves as a remarkably rich repository for cultural knowledge transmission.",
        "arxiv_id": "2508.19004",
        "ARXIVID": "2508.19004",
        "COMMENT": "Does not match any specific criteria but discusses social norm understanding in AI models, which is tangentially related to embodied cognition.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19188": {
        "authors": [
            "Jeonghwan Kim",
            "Yushi Lan",
            "Armando Fortes",
            "Yongwei Chen",
            "Xingang Pan"
        ],
        "title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling",
        "abstract": "arXiv:2508.19188v1 Announce Type: new  Abstract: Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8$\\times$ faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality.",
        "arxiv_id": "2508.19188",
        "ARXIVID": "2508.19188",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling in mesh generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.18726": {
        "authors": [
            "Hiroaki Aizawa",
            "Yoshikazu Hayashi"
        ],
        "title": "Flatness-aware Curriculum Learning via Adversarial Difficulty",
        "abstract": "arXiv:2508.18726v1 Announce Type: new  Abstract: Neural networks trained by empirical risk minimization often suffer from overfitting, especially to specific samples or domains, which leads to poor generalization. Curriculum Learning (CL) addresses this issue by selecting training samples based on the difficulty. From the optimization perspective, methods such as Sharpness-Aware Minimization (SAM) improve robustness and generalization by seeking flat minima. However, combining CL with SAM is not straightforward. In flat regions, both the loss values and the gradient norms tend to become uniformly small, which makes it difficult to evaluate sample difficulty and design an effective curriculum. To overcome this problem, we propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial vulnerability by leveraging the robustness properties of models trained toward flat minima. Unlike loss- or gradient-based measures, which become ineffective as training progresses into flatter regions, ADM remains informative by measuring the normalized loss gap between original and adversarial examples. We incorporate ADM into CL-based training with SAM to dynamically assess sample difficulty. We evaluated our approach on image classification tasks, fine-grained recognition, and domain generalization. The results demonstrate that our method preserves the strengths of both CL and SAM while outperforming existing curriculum-based and flatness-aware training strategies.",
        "arxiv_id": "2508.18726",
        "ARXIVID": "2508.18726",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.18380": {
        "authors": [
            "Hung-Tien Huang",
            "Dzung Dinh",
            "Junier B. Oliva"
        ],
        "title": "Information Templates: A New Paradigm for Intelligent Active Feature Acquisition",
        "abstract": "arXiv:2508.18380v1 Announce Type: new  Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which, at test time, a policy sequentially chooses which features to acquire (at a cost) before predicting. Existing approaches either train reinforcement learning (RL) policies, which deal with a difficult MDP, or greedy policies that cannot account for the joint informativeness of features or require knowledge about the underlying data distribution. To overcome this, we propose Template-based AFA (TAFA), a non-greedy framework that learns a small library of feature templates--a set of features that are jointly informative--and uses this library of templates to guide the next feature acquisitions. Through identifying feature templates, the proposed framework not only significantly reduces the action space considered by the policy but also alleviates the need to estimate the underlying data distribution. Extensive experiments on synthetic and real-world datasets show that TAFA outperforms the existing state-of-the-art baselines while achieving lower overall acquisition cost and computation.",
        "arxiv_id": "2508.18380",
        "ARXIVID": "2508.18380",
        "COMMENT": "Does not match any specific criteria but proposes a novel framework for active feature acquisition.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19042": {
        "authors": [
            "Norihiro Maruyama",
            "Takahide Yoshida",
            "Hiroki Sato",
            "Atsushi Masumori",
            "Johnsmith",
            "Takashi Ikegami"
        ],
        "title": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents",
        "abstract": "arXiv:2508.19042v1 Announce Type: new  Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model (LLM)-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an LLM, inter-module communication, and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent.",
        "arxiv_id": "2508.19042",
        "ARXIVID": "2508.19042",
        "COMMENT": "Does not match any specific criteria but introduces a modular framework for LLM-based agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19200": {
        "authors": [
            "Xinran Zhao",
            "Boyuan Zheng",
            "Chenglei Si",
            "Haofei Yu",
            "Ken Liu",
            "Runlong Zhou",
            "Ruochen Li",
            "Tong Chen",
            "Xiang Li",
            "Yiming Zhang",
            "Tongshuang Wu"
        ],
        "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
        "abstract": "arXiv:2508.19200v1 Announce Type: new  Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.",
        "arxiv_id": "2508.19200",
        "ARXIVID": "2508.19200",
        "COMMENT": "Does not match any specific criteria but discusses a novel framework for ideation using LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.18646": {
        "authors": [
            "Jun Wang",
            "Ninglun Gu",
            "Kailai Zhang",
            "Zijiao Zhang",
            "Yelun Bao",
            "Jin Yang",
            "Xu Yin",
            "Liwei Liu",
            "Yihuan Liu",
            "Pengyong Li",
            "Gary G. Yen",
            "Junchi Yan"
        ],
        "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap",
        "abstract": "arXiv:2508.18646v1 Announce Type: new  Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: https://github.com/onejune2018/Awesome-LLM-Eval.",
        "arxiv_id": "2508.18646",
        "ARXIVID": "2508.18646",
        "COMMENT": "Does not match any specific criteria but provides a survey on LLM evaluation, which is tangentially related to general interest in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18632": {
        "authors": [
            "Huayi Wang",
            "Haochao Ying",
            "Yuyang Xu",
            "Qibo Qiu",
            "Cheng Zhang",
            "Danny Z. Chen",
            "Ying Sun",
            "Jian Wu"
        ],
        "title": "Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction",
        "abstract": "arXiv:2508.18632v1 Announce Type: new  Abstract: Cancer survival analysis commonly integrates information across diverse medical modalities to make survival-time predictions. Existing methods primarily focus on extracting different decoupled features of modalities and performing fusion operations such as concatenation, attention, and MoE-based (Mixture-of-Experts) fusion. However, these methods still face two key challenges: i) Fixed fusion schemes (concatenation and attention) can lead to model over-reliance on predefined feature combinations, limiting the dynamic fusion of decoupled features; ii) in MoE-based fusion methods, each expert network handles separate decoupled features, which limits information interaction among the decoupled features. To address these challenges, we propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which devises a random feature reorganization strategy between modalities decoupling and dynamic MoE fusion modules.Its advantages are: i) it increases the diversity of feature combinations and granularity, enhancing the generalization ability of the subsequent expert networks; ii) it overcomes the problem of information closure and helps expert networks better capture information among decoupled features. Additionally, we incorporate a regional cross-attention network within the modality decoupling module to improve the representation quality of decoupled features. Extensive experimental results on our in-house Liver Cancer (LC) and three widely used TCGA public datasets confirm the effectiveness of our proposed method. The code will be made publicly available.",
        "arxiv_id": "2508.18632",
        "ARXIVID": "2508.18632",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multimodal learning in a specific application (cancer survival prediction).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18836": {
        "authors": [
            "Luyin Hu",
            "Soheil Gholami",
            "George Dindelegan",
            "Torstein R. Meling",
            "Aude Billard"
        ],
        "title": "Quantitative Outcome-Oriented Assessment of Microsurgical Anastomosis",
        "abstract": "arXiv:2508.18836v1 Announce Type: new  Abstract: Microsurgical anastomosis demands exceptional dexterity and visuospatial skills, underscoring the importance of comprehensive training and precise outcome assessment. Currently, methods such as the outcome-oriented anastomosis lapse index are used to evaluate this procedure. However, they often rely on subjective judgment, which can introduce biases that affect the reliability and efficiency of the assessment of competence. Leveraging three datasets from hospitals with participants at various levels, we introduce a quantitative framework that uses image-processing techniques for objective assessment of microsurgical anastomoses. The approach uses geometric modeling of errors along with a detection and scoring mechanism, enhancing the efficiency and reliability of microsurgical proficiency assessment and advancing training protocols. The results show that the geometric metrics effectively replicate expert raters' scoring for the errors considered in this work.",
        "arxiv_id": "2508.18836",
        "ARXIVID": "2508.18836",
        "COMMENT": "Does not match any specific criteria but introduces a quantitative framework for microsurgical assessment.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18751": {
        "authors": [
            "Byung-Joon Lee",
            "Jin-Seop Lee",
            "Jee-Hyong Lee"
        ],
        "title": "Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction",
        "abstract": "arXiv:2508.18751v1 Announce Type: new  Abstract: Deep neural networks demonstrate strong performance under aligned training-test distributions. However, real-world test data often exhibit domain shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the model to test data during inference. While most TTA studies assume that the training and test data share the same class set (closed-set TTA), real-world scenarios often involve open-set data (open-set TTA), which can degrade closed-set accuracy. A recent study showed that identifying open-set data during adaptation and maximizing its entropy is an effective solution. However, the previous method relies on the source model for filtering, resulting in suboptimal filtering accuracy on domain-shifted test data. In contrast, we found that the adapting model, which learns domain knowledge from noisy test streams, tends to be unstable and leads to error accumulation when used for filtering. To address this problem, we propose Primary-Auxiliary Filtering (PAF), which employs an auxiliary filter to validate data filtered by the primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP), which calibrates the outputs of the adapting model, EMA model, and source model to integrate their complementary knowledge for OSTTA. We validate our approach across diverse closed-set and open-set datasets. Our method enhances both closed-set accuracy and open-set discrimination over existing methods. The code is available at https://github.com/powerpowe/PAF-KIP-OSTTA .",
        "arxiv_id": "2508.18751",
        "ARXIVID": "2508.18751",
        "COMMENT": "Does not match any specific criteria but addresses test-time adaptation challenges.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18799": {
        "authors": [
            "Hassan Abid",
            "Khan Muhammad",
            "Muhammad Haris Khan"
        ],
        "title": "Robust and Label-Efficient Deep Waste Detection",
        "abstract": "arXiv:2508.18799v1 Announce Type: new  Abstract: Effective waste sorting is critical for sustainable recycling, yet AI research in this domain continues to lag behind commercial systems due to limited datasets and reliance on legacy object detectors. In this work, we advance AI-driven waste detection by establishing strong baselines and introducing an ensemble-based semi-supervised learning framework. We first benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on the real-world ZeroWaste dataset, demonstrating that while class-only prompts perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy. Next, to address domain-specific limitations, we fine-tune modern transformer-based detectors, achieving a new baseline of 51.6 mAP. We then propose a soft pseudo-labeling strategy that fuses ensemble predictions using spatial and consensus-aware weighting, enabling robust semi-supervised training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations achieve performance gains that surpass fully supervised training, underscoring the effectiveness of scalable annotation pipelines. Our work contributes to the research community by establishing rigorous baselines, introducing a robust ensemble-based pseudo-labeling pipeline, generating high-quality annotations for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models under real-world waste sorting conditions. Our code is available at: https://github.com/h-abid97/robust-waste-detection.",
        "arxiv_id": "2508.18799",
        "ARXIVID": "2508.18799",
        "COMMENT": "Does not match any specific criteria but focuses on waste detection using vision-based methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18898": {
        "authors": [
            "Mona Mirzaie",
            "Bodo Rosenhahn"
        ],
        "title": "Interpretable Decision-Making for End-to-End Autonomous Driving",
        "abstract": "arXiv:2508.18898v1 Announce Type: new  Abstract: Trustworthy AI is mandatory for the broad deployment of autonomous vehicles. Although end-to-end approaches derive control commands directly from raw data, interpreting these decisions remains challenging, especially in complex urban scenarios. This is mainly attributed to very deep neural networks with non-linear decision boundaries, making it challenging to grasp the logic behind AI-driven decisions. This paper presents a method to enhance interpretability while optimizing control commands in autonomous driving. To address this, we propose loss functions that promote the interpretability of our model by generating sparse and localized feature maps. The feature activations allow us to explain which image regions contribute to the predicted control command. We conduct comprehensive ablation studies on the feature extraction step and validate our method on the CARLA benchmarks. We also demonstrate that our approach improves interpretability, which correlates with reducing infractions, yielding a safer, high-performance driving model. Notably, our monocular, non-ensemble model surpasses the top-performing approaches from the CARLA Leaderboard by achieving lower infraction scores and the highest route completion rate, all while ensuring interpretability.",
        "arxiv_id": "2508.18898",
        "ARXIVID": "2508.18898",
        "COMMENT": "This paper does not match any specific criteria but discusses interpretability in autonomous driving, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18314": {
        "authors": [
            "Bo Xu",
            "Yuhu Guo",
            "Yuchao Wang",
            "Wenting Wang",
            "Yeung Yam",
            "Charlie C. L. Wang",
            "Xinyi Le"
        ],
        "title": "SERES: Semantic-aware neural reconstruction from sparse views",
        "abstract": "arXiv:2508.18314v1 Announce Type: new  Abstract: We propose a semantic-aware neural reconstruction method to generate 3D high-fidelity models from sparse images. To tackle the challenge of severe radiance ambiguity caused by mismatched features in sparse input, we enrich neural implicit representations by adding patch-based semantic logits that are optimized together with the signed distance field and the radiance field. A novel regularization based on the geometric primitive masks is introduced to mitigate shape ambiguity. The performance of our approach has been verified in experimental evaluation. The average chamfer distances of our reconstruction on the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When working as a plugin for those dense reconstruction baselines such as NeuS and Neuralangelo, the average error on the DTU dataset can be reduced by 69% and 68% respectively.",
        "arxiv_id": "2508.18314",
        "ARXIVID": "2508.18314",
        "COMMENT": "This paper does not match any specific criteria but discusses semantic-aware 3D reconstruction, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18733": {
        "authors": [
            "Feiwei Qin",
            "Shichao Lu",
            "Junhao Hou",
            "Changmiao Wang",
            "Meie Fang",
            "Ligang Liu"
        ],
        "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings",
        "abstract": "arXiv:2508.18733v1 Announce Type: new  Abstract: Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD.",
        "arxiv_id": "2508.18733",
        "ARXIVID": "2508.18733",
        "COMMENT": "This paper does not match any specific criteria but is related to generative modeling in CAD, which is tangentially relevant to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18834": {
        "authors": [
            "Zizheng Guo",
            "Bochao Zou",
            "Yinuo Jia",
            "Xiangyu Li",
            "Huimin Ma"
        ],
        "title": "Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression",
        "abstract": "arXiv:2508.18834v1 Announce Type: new  Abstract: Micro-expressions (MEs) are involuntary, low-intensity, and short-duration facial expressions that often reveal an individual's genuine thoughts and emotions. Most existing ME analysis methods rely on window-level classification with fixed window sizes and hard decisions, which limits their ability to capture the complex temporal dynamics of MEs. Although recent approaches have adopted video-level regression frameworks to address some of these challenges, interval decoding still depends on manually predefined, window-based methods, leaving the issue only partially mitigated. In this paper, we propose a prior-guided video-level regression method for ME analysis. We introduce a scalable interval selection strategy that comprehensively considers the temporal evolution, duration, and class distribution characteristics of MEs, enabling precise spotting of the onset, apex, and offset phases. In addition, we introduce a synergistic optimization framework, in which the spotting and recognition tasks share parameters except for the classification heads. This fully exploits complementary information, makes more efficient use of limited data, and enhances the model's capability. Extensive experiments on multiple benchmark datasets demonstrate the state-of-the-art performance of our method, with an STRS of 0.0562 on CAS(ME)$^3$ and 0.2000 on SAMMLV. The code is available at https://github.com/zizheng-guo/BoostingVRME.",
        "arxiv_id": "2508.18834",
        "ARXIVID": "2508.18834",
        "COMMENT": "This paper focuses on micro-expression analysis and video-level regression, which does not match any specific criteria but is tangentially related to video understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}