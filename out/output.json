{
    "2506.14356": {
        "authors": [
            "Xiaoqi Wang",
            "Yi Wang",
            "Lap-Pui Chau"
        ],
        "title": "EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization",
        "abstract": "arXiv:2506.14356v1 Announce Type: new  Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .",
        "arxiv_id": "2506.14356",
        "ARXIVID": "2506.14356",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on egocentric video-language understanding with novel spatial-temporal modeling techniques.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.14404": {
        "authors": [
            "Nikos Spyrou",
            "Athanasios Vlontzos",
            "Paraskevas Pegios",
            "Thomas Melistas",
            "Nefeli Gkouti",
            "Yannis Panagakis",
            "Giorgos Papanastasiou",
            "Sotirios A. Tsaftaris"
        ],
        "title": "Causally Steered Diffusion for Automated Video Counterfactual Generation",
        "abstract": "arXiv:2506.14404v1 Announce Type: new  Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic \"what-if\" video scenarios in diverse areas such as healthcare and digital media.",
        "arxiv_id": "2506.14404",
        "ARXIVID": "2506.14404",
        "COMMENT": "Matches criterion 5 as it combines video understanding tasks with large language models for counterfactual video generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.14142": {
        "authors": [
            "Wenting Chen",
            "Yi Dong",
            "Zhaojun Ding",
            "Yucheng Shi",
            "Yifan Zhou",
            "Fang Zeng",
            "Yijun Luo",
            "Tianyu Lin",
            "Yihang Su",
            "Yichen Wu",
            "Kai Zhang",
            "Zhen Xiang",
            "Tianming Liu",
            "Ninghao Liu",
            "Lichao Sun",
            "Yixuan Yuan",
            "Xiang Li"
        ],
        "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology",
        "abstract": "arXiv:2506.14142v1 Announce Type: new  Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.",
        "arxiv_id": "2506.14142",
        "ARXIVID": "2506.14142",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates visual and textual reasoning for radiology analysis.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.14168": {
        "authors": [
            "Hu Yu",
            "Biao Gong",
            "Hangjie Yuan",
            "DanDan Zheng",
            "Weilong Chai",
            "Jingdong Chen",
            "Kecheng Zheng",
            "Feng Zhao"
        ],
        "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
        "abstract": "arXiv:2506.14168v1 Announce Type: new  Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \\textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\\%$), training data ($0.5\\%$), and GPU resources ($0.2\\%$).",
        "arxiv_id": "2506.14168",
        "ARXIVID": "2506.14168",
        "COMMENT": "Matches criteria 6 as it focuses on video-based tasks, specifically video generation, with novel methodologies like temporal short-to-long curriculum learning.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2506.14435": {
        "authors": [
            "Hongyu Wang",
            "Jiayu Xu",
            "Ruiping Wang",
            "Yan Feng",
            "Yitao Zhai",
            "Peng Pei",
            "Xunliang Cai",
            "Xilin Chen"
        ],
        "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models",
        "abstract": "arXiv:2506.14435v1 Announce Type: new  Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.",
        "arxiv_id": "2506.14435",
        "ARXIVID": "2506.14435",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel approach to training memory-efficient large multimodal models (VLLMs) and integrates image understanding with LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.14189": {
        "authors": [
            "Kunyuan Deng",
            "Yi Wang",
            "Lap-Pui Chau"
        ],
        "title": "Egocentric Human-Object Interaction Detection: A New Benchmark and Method",
        "abstract": "arXiv:2506.14189v1 Announce Type: new  Abstract: Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/",
        "arxiv_id": "2506.14189",
        "ARXIVID": "2506.14189",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and method for egocentric human-object interaction detection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.14224": {
        "authors": [
            "Xinyang Li",
            "Siqi Liu",
            "Bochao Zou",
            "Jiansheng Chen",
            "Huimin Ma"
        ],
        "title": "From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models",
        "abstract": "arXiv:2506.14224v1 Announce Type: new  Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.",
        "arxiv_id": "2506.14224",
        "ARXIVID": "2506.14224",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) and their Theory of Mind capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.14511": {
        "authors": [
            "Zhiwen Shao",
            "Yifan Cheng",
            "Feiran Li",
            "Yong Zhou",
            "Xuequan Lu",
            "Yuan Xie",
            "Lizhuang Ma"
        ],
        "title": "MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution",
        "abstract": "arXiv:2506.14511v1 Announce Type: new  Abstract: Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.",
        "arxiv_id": "2506.14511",
        "ARXIVID": "2506.14511",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for micro-expression recognition and integrates multiple tasks, which is relevant to embodied AI challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.14015": {
        "authors": [
            "Nick Yiwen Huang",
            "Akin Caliskan",
            "Berkay Kicanaoglu",
            "James Tompkin",
            "Hyeongwoo Kim"
        ],
        "title": "Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation",
        "abstract": "arXiv:2506.14015v1 Announce Type: new  Abstract: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.",
        "arxiv_id": "2506.14015",
        "ARXIVID": "2506.14015",
        "COMMENT": "Matches criterion 5 as it discusses disentangling 3D generation from vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.13841": {
        "authors": [
            "Miho Koda",
            "Yu Zheng",
            "Ruixian Ma",
            "Mingyang Sun",
            "Devesh Pansare",
            "Fabio Duarte",
            "Paolo Santi"
        ],
        "title": "LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning",
        "abstract": "arXiv:2506.13841v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.",
        "arxiv_id": "2506.13841",
        "ARXIVID": "2506.13841",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it evaluates LLMs on real-world site selection reasoning, which involves spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.14362": {
        "authors": [
            "Daniele Rege Cambrin",
            "Eleonora Poeta",
            "Eliana Pastor",
            "Isaac Corley",
            "Tania Cerquitelli",
            "Elena Baralis",
            "Paolo Garza"
        ],
        "title": "HydroChronos: Forecasting Decades of Surface Water Change",
        "abstract": "arXiv:2506.14362v1 Announce Type: new  Abstract: Forecasting surface water dynamics is crucial for water resource management and climate change adaptation. However, the field lacks comprehensive datasets and standardized benchmarks. In this paper, we introduce HydroChronos, a large-scale, multi-modal spatiotemporal dataset for surface water dynamics forecasting designed to address this gap. We couple the dataset with three forecasting tasks. The dataset includes over three decades of aligned Landsat 5 and Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse lakes and rivers across Europe, North America, and South America. We also propose AquaClimaTempo UNet, a novel spatiotemporal architecture with a dedicated climate data branch, as a strong benchmark baseline. Our model significantly outperforms a Persistence baseline for forecasting future water dynamics by +14% and +11% F1 across change detection and direction of change classification tasks, and by +0.1 MAE on the magnitude of change regression. Finally, we conduct an Explainable AI analysis to identify the key climate variables and input channels that influence surface water change, providing insights to inform and guide future modeling efforts.",
        "arxiv_id": "2506.14362",
        "ARXIVID": "2506.14362",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for spatiotemporal forecasting in surface water dynamics.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.14674": {
        "authors": [
            "Ling Li",
            "Yao Zhou",
            "Yuxuan Liang",
            "Fugee Tsung",
            "Jiaheng Wei"
        ],
        "title": "Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models",
        "abstract": "arXiv:2506.14674v1 Announce Type: new  Abstract: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories.",
        "arxiv_id": "2506.14674",
        "ARXIVID": "2506.14674",
        "COMMENT": "Matches criterion 2 as it explores large vision-language models (LVLMs) for geo-localization tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.14243": {
        "authors": [
            "Xiaohui Jiang",
            "Haijiang Zhu",
            "Chadei Li",
            "Fulin Tang",
            "Ning An"
        ],
        "title": "Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition",
        "abstract": "arXiv:2506.14243v1 Announce Type: new  Abstract: LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.",
        "arxiv_id": "2506.14243",
        "ARXIVID": "2506.14243",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for 3D place recognition in robotics, addressing underexplored challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.13925": {
        "authors": [
            "Numair Nadeem",
            "Saeed Anwar",
            "Muhammad Hamza Asad",
            "Abdul Bais"
        ],
        "title": "HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment",
        "abstract": "arXiv:2506.13925v1 Announce Type: new  Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.",
        "arxiv_id": "2506.13925",
        "ARXIVID": "2506.13925",
        "COMMENT": "Matches criterion 2 as it integrates vision-language models for semi-supervised segmentation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2506.14728": {
        "authors": [
            "Jiahao Qiu",
            "Xinzhe Juan",
            "Yimin Wang",
            "Ling Yang",
            "Xuan Qi",
            "Tongcheng Zhang",
            "Jiacheng Guo",
            "Yifu Lu",
            "Zixin Yao",
            "Hongru Wang",
            "Shilong Liu",
            "Xun Jiang",
            "Liu Leqi",
            "Mengdi Wang"
        ],
        "title": "AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes",
        "abstract": "arXiv:2506.14728v1 Announce Type: new  Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.",
        "arxiv_id": "2506.14728",
        "ARXIVID": "2506.14728",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for agent distillation in embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2506.14765": {
        "authors": [
            "Nikolaos Dionelis",
            "Jente Bosmans",
            "Riccardo Musto",
            "Giancarlo Paoletti",
            "Simone Sarti",
            "Giacomo Cascarano",
            "Casper Fibaek",
            "Luke Camilleri",
            "Bertrand Le Saux",
            "Nicolas Long\\'ep\\'e"
        ],
        "title": "Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset",
        "abstract": "arXiv:2506.14765v1 Announce Type: new  Abstract: Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).",
        "arxiv_id": "2506.14765",
        "ARXIVID": "2506.14765",
        "COMMENT": "Matches criteria 4 as it discusses scaling Earth Observation foundation models, which are a type of vision foundation model, and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.14451": {
        "authors": [
            "Aditya Shourya",
            "Michel Dumontier",
            "Chang Sun"
        ],
        "title": "Adapting Lightweight Vision Language Models for Radiological Visual Question Answering",
        "abstract": "arXiv:2506.14451v1 Announce Type: new  Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.",
        "arxiv_id": "2506.14451",
        "ARXIVID": "2506.14451",
        "COMMENT": "Matches criterion 2 as it explores a vision-language model fine-tuned for radiological VQA.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.14045": {
        "authors": [
            "Martin Klissarov",
            "Akhil Bagaria",
            "Ziyan Luo",
            "George Konidaris",
            "Doina Precup",
            "Marlos C. Machado"
        ],
        "title": "Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning",
        "abstract": "arXiv:2506.14045v1 Announce Type: new  Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.",
        "arxiv_id": "2506.14045",
        "ARXIVID": "2506.14045",
        "COMMENT": "Matches criterion 7 as it is a survey paper on hierarchical reinforcement learning.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.14667": {
        "authors": [
            "Matt Poyser",
            "Toby P. Breckon"
        ],
        "title": "DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification",
        "abstract": "arXiv:2506.14667v1 Announce Type: new  Abstract: In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.",
        "arxiv_id": "2506.14667",
        "ARXIVID": "2506.14667",
        "COMMENT": "Does not match any specific criteria but discusses a novel NAS framework for image classification.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.14596": {
        "authors": [
            "Ming Xu",
            "Xu Zhang"
        ],
        "title": "PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation",
        "abstract": "arXiv:2506.14596v1 Announce Type: new  Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.",
        "arxiv_id": "2506.14596",
        "ARXIVID": "2506.14596",
        "COMMENT": "Does not match any specific criteria. Focuses on monocular 3D human pose estimation, which is tangentially related to computer vision but not directly relevant to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.14742": {
        "authors": [
            "Ziqiao Peng",
            "Wentao Hu",
            "Junyuan Ma",
            "Xiangyu Zhu",
            "Xiaomei Zhang",
            "Hao Zhao",
            "Hui Tian",
            "Jun He",
            "Hongyan Liu",
            "Zhaoxin Fan"
        ],
        "title": "SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting",
        "abstract": "arXiv:2506.14742v1 Announce Type: new  Abstract: Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.",
        "arxiv_id": "2506.14742",
        "ARXIVID": "2506.14742",
        "COMMENT": "Does not match any specific criteria. Focuses on talking head synthesis, which is tangentially related to computer vision but not directly relevant to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.14373": {
        "authors": [
            "Junyeob Baek",
            "Hosung Lee",
            "Christopher Hoang",
            "Mengye Ren",
            "Sungjin Ahn"
        ],
        "title": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction",
        "abstract": "arXiv:2506.14373v1 Announce Type: new  Abstract: The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.",
        "arxiv_id": "2506.14373",
        "ARXIVID": "2506.14373",
        "COMMENT": "Does not match any specific criterion but is related to symbolic reasoning and tokenization, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.14549": {
        "authors": [
            "Yong Liu",
            "Wenpeng Xiao",
            "Qianqian Wang",
            "Junlin Chen",
            "Shiyin Wang",
            "Yitong Wang",
            "Xinglong Wu",
            "Yansong Tang"
        ],
        "title": "DreamLight: Towards Harmonious and Consistent Image Relighting",
        "abstract": "arXiv:2506.14549v1 Announce Type: new  Abstract: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.",
        "arxiv_id": "2506.14549",
        "ARXIVID": "2506.14549",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and generative modeling, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.14231": {
        "authors": [
            "Omri Haller",
            "Yair Meidan",
            "Dudu Mimran",
            "Yuval Elovici",
            "Asaf Shabtai"
        ],
        "title": "ImpReSS: Implicit Recommender System for Support Conversations",
        "abstract": "arXiv:2506.14231v1 Announce Type: new  Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.",
        "arxiv_id": "2506.14231",
        "ARXIVID": "2506.14231",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to multimodal learning and LLMs in customer support applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14418": {
        "authors": [
            "Jiayi Chen",
            "Yanbiao Ma",
            "Andi Zhang",
            "Weidong Tang",
            "Wei Dai",
            "Bowei Liu"
        ],
        "title": "Compositional Attribute Imbalance in Vision Datasets",
        "abstract": "arXiv:2506.14418v1 Announce Type: new  Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.",
        "arxiv_id": "2506.14418",
        "ARXIVID": "2506.14418",
        "COMMENT": "Does not match any specific criteria but discusses compositional attribute imbalance in vision datasets.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.13980": {
        "authors": [
            "Shahaf David",
            "Yair Meidan",
            "Ido Hersko",
            "Daniel Varnovitzky",
            "Dudu Mimran",
            "Yuval Elovici",
            "Asaf Shabtai"
        ],
        "title": "ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users",
        "abstract": "arXiv:2506.13980v1 Announce Type: new  Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.",
        "arxiv_id": "2506.13980",
        "ARXIVID": "2506.13980",
        "COMMENT": "Does not match any specific criteria but discusses user profiling in chatbot interactions.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14382": {
        "authors": [
            "Ning Zhou",
            "Shanxiong Chen",
            "Mingting Zhou",
            "Haigang Sui",
            "Lieyun Hu",
            "Han Li",
            "Li Hua",
            "Qiming Zhou"
        ],
        "title": "DepthSeg: Depth prompting in remote sensing semantic segmentation",
        "abstract": "arXiv:2506.14382v1 Announce Type: new  Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.",
        "arxiv_id": "2506.14382",
        "ARXIVID": "2506.14382",
        "COMMENT": "Does not match any specific criteria but focuses on remote sensing semantic segmentation with depth prompting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14387": {
        "authors": [
            "William F. Shen",
            "Xinchi Qiu",
            "Nicola Cancedda",
            "Nicholas D. Lane"
        ],
        "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
        "abstract": "arXiv:2506.14387v1 Announce Type: new  Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.",
        "arxiv_id": "2506.14387",
        "ARXIVID": "2506.14387",
        "COMMENT": "Does not match any specific criteria but discusses fine-tuning techniques for LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14706": {
        "authors": [
            "Ni Ou",
            "Zhuo Chen",
            "Xinru Zhang",
            "Junzheng Wang"
        ],
        "title": "Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion",
        "abstract": "arXiv:2506.14706v1 Announce Type: new  Abstract: Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.",
        "arxiv_id": "2506.14706",
        "ARXIVID": "2506.14706",
        "COMMENT": "Does not match any specific criteria. Focuses on camera-LiDAR extrinsic optimization, which is tangentially related to computer vision but not directly relevant to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14696": {
        "authors": [
            "Dahang Wan",
            "Rongsheng Lu",
            "Yang Fang",
            "Xianli Lang",
            "Shuangbao Shu",
            "Jingjing Chen",
            "Siyuan Shen",
            "Ting Xu",
            "Zecong Ye"
        ],
        "title": "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework",
        "abstract": "arXiv:2506.14696v1 Announce Type: new  Abstract: Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: https://github.com/wandahangFY/YOLOv11-RGBT.",
        "arxiv_id": "2506.14696",
        "ARXIVID": "2506.14696",
        "COMMENT": "Does not match any specific criteria but is related to multispectral object detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.13846": {
        "authors": [
            "Runtao Liu",
            "Jiahao Zhan",
            "Yingqing He",
            "Chen Wei",
            "Alan Yuille",
            "Qifeng Chen"
        ],
        "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction",
        "abstract": "arXiv:2506.13846v1 Announce Type: new  Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).",
        "arxiv_id": "2506.13846",
        "ARXIVID": "2506.13846",
        "COMMENT": "Does not match any specific criterion but is related to reinforcement learning and generative modeling, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14008": {
        "authors": [
            "Daniel Montoya",
            "Aymen Bouguerra",
            "Alexandra Gomez-Villa",
            "Fabio Arnez"
        ],
        "title": "FindMeIfYouCan: Bringing Open Set metrics to $\\textit{near} $, $ \\textit{far} $ and $\\textit{farther}$ Out-of-Distribution Object Detection",
        "abstract": "arXiv:2506.14008v1 Announce Type: new  Abstract: State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\\textit{Far}$ and $\\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object.",
        "arxiv_id": "2506.14008",
        "ARXIVID": "2506.14008",
        "COMMENT": "Does not match any specific criterion but is related to object detection and OOD detection, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}