{
    "2507.10548": {
        "authors": [
            "Mingxian Lin",
            "Wei Huang",
            "Yitang Li",
            "Chengjie Jiang",
            "Kui Wu",
            "Fangwei Zhong",
            "Shengju Qian",
            "Xin Wang",
            "Xiaojuan Qi"
        ],
        "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
        "abstract": "arXiv:2507.10548v1 Announce Type: new  Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.",
        "arxiv_id": "2507.10548",
        "ARXIVID": "2507.10548",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark dataset (EmRACE-3K) for embodied reasoning and action in complex environments.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.10034": {
        "authors": [
            "Xianghong Zou",
            "Jianping Li",
            "Zhe Chen",
            "Zhen Cao",
            "Zhen Dong",
            "Qiegen Liu",
            "Bisheng Yang"
        ],
        "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning",
        "abstract": "arXiv:2507.10034v1 Announce Type: new  Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry and robotics applications such as autonomous driving, intelligent transportation, and augmented reality. In real-world large-scale deployments of a positioning system, PCPR models must continuously acquire, update, and accumulate knowledge to adapt to diverse and dynamic environments, i.e., the ability known as continual learning (CL). However, existing PCPR models often suffer from catastrophic forgetting, leading to significant performance degradation in previously learned scenes when adapting to new environments or sensor types. This results in poor model scalability, increased maintenance costs, and system deployment difficulties, undermining the practicality of PCPR. To address these issues, we propose LifelongPR, a novel continual learning framework for PCPR, which effectively extracts and fuses knowledge from sequential point cloud data. First, to alleviate the knowledge loss, we propose a replay sample selection method that dynamically allocates sample sizes according to each dataset's information quantity and selects spatially diverse samples for maximal representativeness. Second, to handle domain shifts, we design a prompt learning-based CL framework with a lightweight prompt module and a two-stage training strategy, enabling domain-specific feature adaptation while minimizing forgetting. Comprehensive experiments on large-scale public and self-collected datasets are conducted to validate the effectiveness of the proposed method. Compared with state-of-the-art (SOTA) methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly available at https://github.com/zouxianghong/LifelongPR.",
        "arxiv_id": "2507.10034",
        "ARXIVID": "2507.10034",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for continual learning in point cloud place recognition, relevant to embodied/robotic AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.09184": {
        "authors": [
            "Qiyan Zhao",
            "Xiaofeng Zhang",
            "Yiheng Li",
            "Yun Xing",
            "Xiaosong Yuan",
            "Feilong Tang",
            "Sinan Fan",
            "Xuhang Chen",
            "Xuyao Zhang",
            "Dahan Wang"
        ],
        "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models",
        "abstract": "arXiv:2507.09184v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge in Large Vision Language Models (LVLMs), with misalignment between multimodal features identified as a key contributing factor. This paper reveals the negative impact of the long-term decay in Rotary Position Encoding (RoPE), used for positional modeling in LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction tokens exhibit uneven perception of image tokens located at different positions within the two-dimensional space: prioritizing image tokens from the bottom-right region since in the one-dimensional sequence, these tokens are positionally closer to the instruction tokens. This biased perception leads to insufficient image-instruction interaction and suboptimal multimodal alignment. We refer to this phenomenon as image alignment bias. To enhance instruction's perception of image tokens at different spatial locations, we propose MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the one-dimensional sequence order and two-dimensional spatial position of image tokens for positional modeling, mitigating hallucinations by alleviating image alignment bias. Experimental results of MCA-LLaVA across various hallucination and general benchmarks demonstrate its effectiveness and generality. The code can be accessed in https://github.com/ErikZ719/MCA-LLaVA.",
        "arxiv_id": "2507.09184",
        "ARXIVID": "2507.09184",
        "COMMENT": "Matches criterion 2 as it addresses hallucination in Large Vision-Language Models (LVLMs) and proposes a novel positional modeling technique.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.10015": {
        "authors": [
            "Jaisidh Singh",
            "Diganta Misra",
            "Boris Knyazev",
            "Antonio Orvieto"
        ],
        "title": "(Almost) Free Modality Stitching of Foundation Models",
        "abstract": "arXiv:2507.10015v1 Announce Type: new  Abstract: Foundation multi-modal models are often designed by stitching of multiple existing pretrained uni-modal models: for example, an image classifier with an autoregressive text model. This stitching process is performed by training a connector module that aims to align the representation-representation or representation-input spaces of these uni-modal models. However, given the complexity of training such connectors on large scale web-based datasets coupled with the ever-increasing number of available pretrained uni-modal models, the task of uni-modal models selection and subsequent connector module training becomes computationally demanding. To address this under-studied critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal uni-modal model selection and connector training by leveraging hypernetworks. Specifically, our framework utilizes the parameter prediction capability of a hypernetwork to obtain jointly trained connector modules for $N \\times M$ combinations of uni-modal models. In our experiments, Hyma reduces the optimal uni-modal model pair search cost by $10\\times$ (averaged across all experiments), while matching the ranking and trained connector performance obtained via grid search across a suite of diverse multi-modal benchmarks.",
        "arxiv_id": "2507.10015",
        "ARXIVID": "2507.10015",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a novel method for stitching uni-modal models into multi-modal models using hypernetworks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.10302": {
        "authors": [
            "Jiahe Zhao",
            "Rongkun Zheng",
            "Yi Wang",
            "Helin Wang",
            "Hengshuang Zhao"
        ],
        "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs",
        "abstract": "arXiv:2507.10302v1 Announce Type: new  Abstract: In video Multimodal Large Language Models (video MLLMs), the visual encapsulation process plays a pivotal role in converting video contents into representative tokens for LLM input. While linear projectors are widely employed for encapsulation, they introduce semantic indistinctness and temporal incoherence when applied to videos. Conversely, the structure of resamplers shows promise in tackling these challenges, but an effective solution remains unexplored. Drawing inspiration from resampler structures, we introduce DisCo, a novel visual encapsulation method designed to yield semantically distinct and temporally coherent visual tokens for video MLLMs. DisCo integrates two key components: (1) A Visual Concept Discriminator (VCD) module, assigning unique semantics for visual tokens by associating them in pair with discriminative concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring consistent temporal focus of visual tokens to video elements across every video frame. Through extensive experiments on multiple video MLLM frameworks, we demonstrate that DisCo remarkably outperforms previous state-of-the-art methods across a variety of video understanding benchmarks, while also achieving higher token efficiency thanks to the reduction of semantic indistinctness. The code: https://github.com/ZJHTerry18/DisCo.",
        "arxiv_id": "2507.10302",
        "ARXIVID": "2507.10302",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel visual encapsulation method for video MLLMs, focusing on vision-language integration and token efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.09305": {
        "authors": [
            "Zhiwei Xu"
        ],
        "title": "DAA*: Deep Angular A Star for Image-based Path Planning",
        "abstract": "arXiv:2507.09305v1 Announce Type: new  Abstract: Path smoothness is often overlooked in path imitation learning from expert demonstrations. In this paper, we introduce a novel learning method, termed deep angular A* (DAA*), by incorporating the proposed path angular freedom (PAF) into A* to improve path similarity through adaptive path smoothness. The PAF aims to explore the effect of move angles on path node expansion by finding the trade-off between their minimum and maximum values, allowing for high adaptiveness for imitation learning. DAA* improves path optimality by closely aligning with the reference path through joint optimization of path shortening and smoothing, which correspond to heuristic distance and PAF, respectively. Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets, 2 video-game datasets, and a real-world drone-view dataset containing 2 scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in path similarity between the predicted and reference paths with a shorter path length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM, and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path loss and path probability map loss, DAA* significantly outperforms the state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also discuss the minor trade-off between path optimality and search efficiency where applicable.",
        "arxiv_id": "2507.09305",
        "ARXIVID": "2507.09305",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial reasoning in path planning, which is relevant to embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.09313": {
        "authors": [
            "Yueqian Wang",
            "Xiaojun Meng",
            "Yifan Wang",
            "Huishuai Zhang",
            "Dongyan Zhao"
        ],
        "title": "ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models",
        "abstract": "arXiv:2507.09313v1 Announce Type: new  Abstract: With the growing research focus on multimodal dialogue systems, the capability for proactive interaction is gradually gaining recognition. As an alternative to conventional turn-by-turn dialogue, users increasingly expect multimodal systems to be more initiative, for example, by autonomously determining the timing of multi-turn responses in real time during video playback. To facilitate progress in this emerging area, we introduce ProactiveBench, the first comprehensive benchmark to evaluate a system's ability to engage in proactive interaction. Since model responses are generated at varying timestamps, we further propose PAUC, the first metric that accounts for the temporal dynamics of model responses. This enables a more accurate evaluation of systems operating in proactive settings. Through extensive benchmarking of various baseline systems on ProactiveBench and a user study of human preferences, we show that PAUC is in better agreement with human preferences than traditional evaluation metrics, which typically only consider the textual content of responses. These findings demonstrate that PAUC provides a more faithful assessment of user experience in proactive interaction scenarios. Project homepage: https://github.com/yellow-binary-tree/ProactiveBench",
        "arxiv_id": "2507.09313",
        "ARXIVID": "2507.09313",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark for video-based tasks and evaluates video understanding in proactive interaction scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.10449": {
        "authors": [
            "Hongyong Han",
            "Wei Wang",
            "Gaowei Zhang",
            "Mingjie Li",
            "Yi Wang"
        ],
        "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding",
        "abstract": "arXiv:2507.10449v1 Announce Type: new  Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous monitoring to support conservation. While coral reef images provide essential information in coral monitoring, interpreting such images remains challenging due to the need for domain expertise. Visual Question Answering (VQA), powered by Large Vision-Language Models (LVLMs), has great potential in user-friendly interaction with coral reef images. However, applying VQA to coral imagery demands a dedicated dataset that addresses two key challenges: domain-specific annotations and multidimensional questions. In this work, we introduce CoralVQA, the first large-scale VQA dataset for coral reef analysis. It contains 12,805 real-world coral images from 67 coral genera collected from 3 oceans, along with 277,653 question-answer pairs that comprehensively assess ecological and health-related conditions. To construct this dataset, we develop a semi-automatic data construction pipeline in collaboration with marine biologists to ensure both scalability and professional-grade data quality. CoralVQA presents novel challenges and provides a comprehensive benchmark for studying vision-language reasoning in the context of coral reef images. By evaluating several state-of-the-art LVLMs, we reveal key limitations and opportunities. These insights form a foundation for future LVLM development, with a particular emphasis on supporting coral conservation efforts.",
        "arxiv_id": "2507.10449",
        "ARXIVID": "2507.10449",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a large-scale VQA dataset for coral reef image understanding, which involves vision-language reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.09111": {
        "authors": [
            "Di Wen",
            "Kunyu Peng",
            "Kailun Yang",
            "Yufan Chen",
            "Ruiping Liu",
            "Junwei Zheng",
            "Alina Roitberg",
            "Rainer Stiefelhagen"
        ],
        "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection",
        "abstract": "arXiv:2507.09111v1 Announce Type: new  Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human assistance, enabling context-aware support. However, models trained on clean datasets degrade in real-world conditions due to unforeseen corruptions, leading to inaccurate prediction. To address this, we introduce the first robustness benchmark for HOI detection, evaluating model resilience under diverse challenges. Despite advances, current models struggle with environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes 20 corruption types based on HICO-DET and V-COCO datasets and a new robustness-focused metric. We systematically analyze existing models in the related field, revealing significant performance drops under corruptions. To improve robustness, we propose a Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to guide the model to be optimized based on holistic and partial cues, dynamically adjusting the model's optimization to enhance robust feature learning. Extensive experiments show our approach outperforms state-of-the-art methods, setting a new standard for robust HOI detection. Benchmarks, datasets, and code will be made publicly available at https://github.com/Kratos-Wen/RoHOI.",
        "arxiv_id": "2507.09111",
        "ARXIVID": "2507.09111",
        "COMMENT": "Matches criterion 3 as it introduces a new robustness benchmark (RoHOI) for human-object interaction detection in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.09374": {
        "authors": [
            "Chenglin Zhu",
            "Tao Zhang",
            "Chong Li",
            "Mingan Lin",
            "Zenan Zhou",
            "Jian Xie"
        ],
        "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique",
        "abstract": "arXiv:2507.09374v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific tasks, particularly those requiring multi-step and interpretable reasoning. Their limitations include insufficient scientific reasoning patterns, lack of global coherence in multi-step inference, and the absence of reflective self-correction, making them unreliable in structured scientific contexts. We introduce EduFlow, the first end-to-end framework that covers the full pipeline of educational scientific reasoning, including data selection, MCTS-based trajectory construction, model training, and output optimization. At its core is EduPRM, a process-aware reward model that critiques reasoning steps with tags and justifications. EduPRM is trained via curriculum learning on three complementary supervision sources: MCTS-guided trajectories, error-injected critiques, and teacher-student dialogues, enabling dynamic adaptation to multi-stage problem solving and iterative refinement during inference. We further propose EduMCTS, a domain-adapted search framework that introduces bootstrapping actions specifically designed for educational reasoning, such as a self-reflection mechanism that promotes reflective error correction. It further leverages EduPRM's fine-grained feedback to guide the search toward higher-quality reasoning trajectories. By applying self-consistency and rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of educational reasoning trajectories. Extensive experiments demonstrate that EduFlow enhances reasoning consistency and coherence. Code, data, and models will be released.",
        "arxiv_id": "2507.09374",
        "ARXIVID": "2507.09374",
        "COMMENT": "Matches criteria 2 as it focuses on enhancing MLLMs' problem-solving proficiency through a novel framework, relevant to vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.10213": {
        "authors": [
            "Shicai Wei",
            "Chunbo Luo",
            "Yang Luo"
        ],
        "title": "Boosting Multimodal Learning via Disentangled Gradient Learning",
        "abstract": "arXiv:2507.10213v1 Announce Type: new  Abstract: Multimodal learning often encounters the under-optimized problem and may have worse performance than unimodal learning. Existing methods attribute this problem to the imbalanced learning between modalities and rebalance them through gradient modulation. However, they fail to explain why the dominant modality in multimodal models also underperforms that in unimodal learning. In this work, we reveal the optimization conflict between the modality encoder and modality fusion module in multimodal models. Specifically, we prove that the cross-modal fusion in multimodal models decreases the gradient passed back to each modality encoder compared with unimodal models. Consequently, the performance of each modality in the multimodal model is inferior to that in the unimodal model. To this end, we propose a disentangled gradient learning (DGL) framework to decouple the optimization of the modality encoder and modality fusion module in the multimodal model. DGL truncates the gradient back-propagated from the multimodal loss to the modality encoder and replaces it with the gradient from unimodal loss. Besides, DGL removes the gradient back-propagated from the unimodal loss to the modality fusion module. This helps eliminate the gradient interference between the modality encoder and modality fusion module while ensuring their respective optimization processes. Finally, extensive experiments on multiple types of modalities, tasks, and frameworks with dense cross-modal interaction demonstrate the effectiveness and versatility of the proposed DGL. Code is available at \\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}",
        "arxiv_id": "2507.10213",
        "ARXIVID": "2507.10213",
        "COMMENT": "Matches criteria 2 as it proposes a novel disentangled gradient learning framework for multimodal learning, addressing vision-language integration challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.09097": {
        "authors": [
            "Yunsoo Kim",
            "Jinge Wu",
            "Honghan Wu"
        ],
        "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze",
        "abstract": "arXiv:2507.09097v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists' eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 24.6% in the report generation task and on average 15.2% for both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks. RadEyeVideo is a step toward a scalable human-centered approach of utilizing LVLMs in medical image analytics.",
        "arxiv_id": "2507.09097",
        "ARXIVID": "2507.09097",
        "COMMENT": "Matches criteria 2 and 5 as it integrates video representations of eye gaze with LVLMs for medical image analysis, showcasing vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.09216": {
        "authors": [
            "Jingguo Liu",
            "Han Yu",
            "Shigang Li",
            "Jianfeng Li"
        ],
        "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models",
        "abstract": "arXiv:2507.09216v1 Announce Type: new  Abstract: Due to the current lack of large-scale datasets at the million-scale level, tasks involving panoramic images predominantly rely on existing two-dimensional pre-trained image benchmark models as backbone networks. However, these networks are not equipped to recognize the distortions and discontinuities inherent in panoramic images, which adversely affects their performance in such tasks. In this paper, we introduce a novel spherical sampling method for panoramic images that enables the direct utilization of existing pre-trained models developed for two-dimensional images. Our method employs spherical discrete sampling based on the weights of the pre-trained models, effectively mitigating distortions while achieving favorable initial training values. Additionally, we apply the proposed sampling method to panoramic image segmentation, utilizing features obtained from the spherical model as masks for specific channel attentions, which yields commendable results on commonly used indoor datasets, Stanford2D3D.",
        "arxiv_id": "2507.09216",
        "ARXIVID": "2507.09216",
        "COMMENT": "Matches criterion 4 as it focuses on adapting foundation models for panoramic image segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.09118": {
        "authors": [
            "Linlan Huang",
            "Xusheng Cao",
            "Haori Lu",
            "Yifan Meng",
            "Fei Yang",
            "Xialei Liu"
        ],
        "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning",
        "abstract": "arXiv:2507.09118v1 Announce Type: new  Abstract: Continual learning aims to enable models to learn sequentially from continuously incoming data while retaining performance on previously learned tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting strong capabilities across various downstream tasks, there has been growing interest in leveraging CLIP for continual learning in such scenarios. Most existing works overlook the inherent modality gap in CLIP, a key factor in its generalization and adaptability. In this paper, we analyze the variations in the modality gap during the fine-tuning of vision-language pre-trained models. Our observations reveal that the modality gap effectively reflects the extent to which pre-trained knowledge is preserved. Based on these insights, we propose a simple yet effective method, MG-CLIP, that improves CLIP's performance in class-incremental learning. Our approach leverages modality gap preservation to mitigate forgetting and modality gap compensation to enhance the capacity for new data, introducing a novel modality-gap-based perspective for continual learning. Extensive experiments on multiple benchmarks demonstrate that our method outperforms existing approaches without requiring additional replay data. Our code is available at https://github.com/linlany/MindtheGap.",
        "arxiv_id": "2507.09118",
        "ARXIVID": "2507.09118",
        "COMMENT": "Matches criterion 5 as it explores continual learning in vision-language models and proposes a method leveraging modality gap preservation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.09850": {
        "authors": [
            "Wei Du",
            "Branislav Kisacanin",
            "George Armstrong",
            "Shubham Toshniwal",
            "Ivan Moshkov",
            "Alexan Ayrapetyan",
            "Sadegh Mahdavi",
            "Dan Zhao",
            "Shizhe Diao",
            "Dragan Masulovic",
            "Marius Stanean",
            "Advaith Avadhanam",
            "Max Wang",
            "Ashmit Dutta",
            "Shitij Govil",
            "Sri Yanamandara",
            "Mihir Tandon",
            "Sriram Ananthakrishnan",
            "Vedant Rathi",
            "David Zhang",
            "Joonseok Kang",
            "Leon Luo",
            "Titu Andreescu",
            "Boris Ginsburg",
            "Igor Gitman"
        ],
        "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation",
        "abstract": "arXiv:2507.09850v1 Announce Type: new  Abstract: Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective.",
        "arxiv_id": "2507.09850",
        "ARXIVID": "2507.09850",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores reasoning capabilities in large language models using chain-of-thought prompting and fine-tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.09500": {
        "authors": [
            "Yiwen Liang",
            "Hui Chen",
            "Yizhe Xiong",
            "Zihan Zhou",
            "Mengyao Lyu",
            "Zijia Lin",
            "Shuaicheng Niu",
            "Sicheng Zhao",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations",
        "abstract": "arXiv:2507.09500v1 Announce Type: new  Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under challenging real-world distribution shifts.",
        "arxiv_id": "2507.09500",
        "ARXIVID": "2507.09500",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on improving vision-language models under distribution shifts using test-time adaptation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.09122": {
        "authors": [
            "Chuan Guo",
            "Inwoo Hwang",
            "Jian Wang",
            "Bing Zhou"
        ],
        "title": "SnapMoGen: Human Motion Generation from Expressive Texts",
        "abstract": "arXiv:2507.09122v1 Announce Type: new  Abstract: Text-to-motion generation has experienced remarkable progress in recent years. However, current approaches remain limited to synthesizing motion from short or general text prompts, primarily due to dataset constraints. This limitation undermines fine-grained controllability and generalization to unseen prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset featuring high-quality motion capture data paired with accurate, expressive textual annotations. The dataset comprises 20K motion clips totaling 44 hours, accompanied by 122K detailed textual descriptions averaging 48 words per description (vs. 12 words of HumanML3D). Importantly, these motion clips preserve original temporal continuity as they were in long sequences, facilitating research in long-term motion generation and blending. We also improve upon previous generative masked modeling approaches. Our model, MoMask++, transforms motion into multi-scale token sequences that better exploit the token capacity, and learns to generate all tokens using a single generative masked transformer. MoMask++ achieves state-of-the-art performance on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the ability to process casual user prompts by employing an LLM to reformat inputs to align with the expressivity and narration style of SnapMoGen. Project webpage: https://snap-research.github.io/SnapMoGen/",
        "arxiv_id": "2507.09122",
        "ARXIVID": "2507.09122",
        "COMMENT": "Matches criterion 6 as it introduces a new dataset and method for text-to-motion generation, advancing video understanding tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.10300": {
        "authors": [
            "Hatef Otroshi Shahreza",
            "S\\'ebastien Marcel"
        ],
        "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding",
        "abstract": "arXiv:2507.10300v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.",
        "arxiv_id": "2507.10300",
        "ARXIVID": "2507.10300",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal large language model (FaceLLM) specifically for facial image understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.09209": {
        "authors": [
            "Xiao Liang",
            "Di Wang",
            "Zhicheng Jiao",
            "Ronghan Li",
            "Pengfei Yang",
            "Quan Wang",
            "Tat-Seng Chua"
        ],
        "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models",
        "abstract": "arXiv:2507.09209v1 Announce Type: new  Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the development of multi-modal medical assistant systems. Despite this progress, current models still have inherent probabilistic uncertainties, often producing erroneous or unverified responses-an issue with serious implications in medical applications. Existing methods aim to enhance the performance of Medical Vision Language Model (MedVLM) by adjusting model structure, fine-tuning with high-quality data, or through preference fine-tuning. However, these training-dependent strategies are costly and still lack sufficient alignment with clinical expertise. To address these issues, we propose an expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance (Expert-CFG) to align MedVLM with clinical expertise without additional training. This framework introduces an uncertainty estimation strategy to identify unreliable outputs. It then retrieves relevant references to assist experts in highlighting key terms and applies classifier-free guidance to refine the token embeddings of MedVLM, ensuring that the adjusted outputs are correct and align with expert highlights. Evaluations across three medical visual question answering benchmarks demonstrate that the proposed Expert-CFG, with 4.2B parameters and limited expert annotations, outperforms state-of-the-art models with 13B parameters. The results demonstrate the feasibility of deploying such a system in resource-limited settings for clinical use.",
        "arxiv_id": "2507.09209",
        "ARXIVID": "2507.09209",
        "COMMENT": "Matches criteria 2 as it proposes a novel framework for aligning MedVLMs with clinical expertise, focusing on vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.09068": {
        "authors": [
            "Dell Zhang",
            "Xiangyu Chen",
            "Jixiang Luo",
            "Mengxi Jia",
            "Changzhi Sun",
            "Ruilong Ren",
            "Jingren Liu",
            "Hao Sun",
            "Xuelong Li"
        ],
        "title": "Infinite Video Understanding",
        "abstract": "arXiv:2507.09068v1 Announce Type: new  Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.",
        "arxiv_id": "2507.09068",
        "ARXIVID": "2507.09068",
        "COMMENT": "Matches criteria 6 as it discusses challenges and future directions for video understanding, particularly for long-duration videos.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.10437": {
        "authors": [
            "Shanshan Zhong",
            "Jiawei Peng",
            "Zehan Zheng",
            "Zhongzhan Huang",
            "Wufei Ma",
            "Guofeng Zhang",
            "Qihao Liu",
            "Alan Yuille",
            "Jieneng Chen"
        ],
        "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos",
        "abstract": "arXiv:2507.10437v1 Announce Type: new  Abstract: Existing methods for reconstructing animatable 3D animals from videos typically rely on sparse semantic keypoints to fit parametric models. However, obtaining such keypoints is labor-intensive, and keypoint detectors trained on limited animal data are often unreliable. To address this, we propose 4D-Animal, a novel framework that reconstructs animatable 3D animals from videos without requiring sparse keypoint annotations. Our approach introduces a dense feature network that maps 2D representations to SMAL parameters, enhancing both the efficiency and stability of the fitting process. Furthermore, we develop a hierarchical alignment strategy that integrates silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D visual models to produce accurate and temporally coherent reconstructions across frames. Extensive experiments demonstrate that 4D-Animal outperforms both model-based and model-free baselines. Moreover, the high-quality 3D assets generated by our method can benefit other 3D tasks, underscoring its potential for large-scale applications. The code is released at https://github.com/zhongshsh/4D-Animal.",
        "arxiv_id": "2507.10437",
        "ARXIVID": "2507.10437",
        "COMMENT": "Matches criteria 6 as it introduces a novel framework for reconstructing animatable 3D animals from videos, focusing on video-based tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.09702": {
        "authors": [
            "Phat Nguyen",
            "Ngai-Man Cheung"
        ],
        "title": "Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI",
        "abstract": "arXiv:2507.09702v1 Announce Type: new  Abstract: Token compression techniques have recently emerged as powerful tools for accelerating Vision Transformer (ViT) inference in computer vision. Due to the quadratic computational complexity with respect to the token sequence length, these methods aim to remove less informative tokens before the attention layers to improve inference throughput. While numerous studies have explored various accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain. First, there is a lack of unified survey that systematically categorizes and compares token compression approaches based on their core strategies (e.g., pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs. plug-in). Second, most benchmarks are limited to standard ViT models (e.g., ViT-B, ViT-L), leaving open the question of whether such methods remain effective when applied to structurally compressed transformers, which are increasingly deployed on resource-constrained edge devices. To address these gaps, we present the first systematic taxonomy and comparative study of token compression methods, and we evaluate representative techniques on both standard and compact ViT architectures. Our experiments reveal that while token compression methods are effective for general-purpose ViTs, they often underperform when directly applied to compact designs. These findings not only provide practical insights but also pave the way for future research on adapting token optimization techniques to compact transformer-based networks for edge AI and AI agent applications.",
        "arxiv_id": "2507.09702",
        "ARXIVID": "2507.09702",
        "COMMENT": "Matches criterion 7 as it provides a comprehensive survey on token compression techniques for Vision Transformers.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2507.09491": {
        "authors": [
            "Yiyang Zhou",
            "Linjie Li",
            "Shi Qiu",
            "Zhengyuan Yang",
            "Yuyang Zhao",
            "Siwei Han",
            "Yangfan He",
            "Kangqi Li",
            "Haonian Ji",
            "Zihao Zhao",
            "Haibo Tong",
            "Lijuan Wang",
            "Huaxiu Yao"
        ],
        "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?",
        "abstract": "arXiv:2507.09491v1 Announce Type: new  Abstract: Existing video benchmarks often resemble image-based benchmarks, with question types like \"What actions does the person perform throughout the video?\" or \"What color is the woman's dress in the video?\" For these, models can often answer by scanning just a few key frames, without deep temporal reasoning. This limits our ability to assess whether large vision-language models (LVLMs) can truly think with videos rather than perform superficial frame-level analysis. To address this, we introduce GLIMPSE, a benchmark specifically designed to evaluate whether LVLMs can genuinely think with videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video understanding beyond static image cues. It consists of 3,269 videos and over 4,342 highly visual-centric questions across 11 categories, including Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions are carefully crafted by human annotators and require watching the entire video and reasoning over full video context-this is what we mean by thinking with video. These questions cannot be answered by scanning selected frames or relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy, but current LVLMs face significant challenges. Even the best-performing model, GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move beyond surface-level reasoning to truly think with videos.",
        "arxiv_id": "2507.09491",
        "ARXIVID": "2507.09491",
        "COMMENT": "Matches criterion 6 as it introduces a new benchmark for video understanding and evaluates large vision-language models on temporal reasoning tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.09574": {
        "authors": [
            "Haozhe Zhao",
            "Zefan Cai",
            "Shuzheng Si",
            "Liang Chen",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Junjie Hu"
        ],
        "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models",
        "abstract": "arXiv:2507.09574v1 Announce Type: new  Abstract: Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: https://github.com/HaozheZhao/MENTOR",
        "arxiv_id": "2507.09574",
        "ARXIVID": "2507.09574",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it proposes a novel multimodal-conditioned tuning framework for autoregressive vision generation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.09082": {
        "authors": [
            "Seungwoo Kim",
            "Khai Loong Aw",
            "Klemen Kotar",
            "Cristobal Eyzaguirre",
            "Wanhee Lee",
            "Yunong Liu",
            "Jared Watrous",
            "Stefan Stojanov",
            "Juan Carlos Niebles",
            "Jiajun Wu",
            "Daniel L. K. Yamins"
        ],
        "title": "Taming generative video models for zero-shot optical flow extraction",
        "abstract": "arXiv:2507.09082v1 Announce Type: new  Abstract: Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow.",
        "arxiv_id": "2507.09082",
        "ARXIVID": "2507.09082",
        "COMMENT": "Matches criteria 6 (Video Understanding) as it focuses on extracting optical flow from videos using generative video models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.09861": {
        "authors": [
            "Yihao Ding",
            "Siwen Luo",
            "Yue Dai",
            "Yanbei Jiang",
            "Zechuan Li",
            "Geoffrey Martin",
            "Yifan Peng"
        ],
        "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends",
        "abstract": "arXiv:2507.09861v1 Announce Type: new  Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field, driven by the need to automatically process documents containing complex visual, textual, and layout information. Recently, Multimodal Large Language Models (MLLMs) have shown remarkable potential in this domain, leveraging both Optical Character Recognition (OCR)-dependent and OCR-free frameworks to extract and interpret information in document images. This survey reviews recent advancements in MLLM-based VRDU, highlighting three core components: (1) methods for encoding and fusing textual, visual, and layout features; (2) training paradigms, including pretraining strategies, instruction-response tuning, and the trainability of different model modules; and (3) datasets utilized for pretraining, instruction-tuning, and supervised fine-tuning. Finally, we discuss the challenges and opportunities in this evolving field and propose future directions to advance the efficiency, generalizability, and robustness of VRDU systems.",
        "arxiv_id": "2507.09861",
        "ARXIVID": "2507.09861",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on MLLM-based Visually Rich Document Understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2507.08979": {
        "authors": [
            "Mahdiyar Molahasani",
            "Azadeh Motamedi",
            "Michael Greenspan",
            "Il-Min Kim",
            "Ali Etemad"
        ],
        "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection",
        "abstract": "arXiv:2507.08979v1 Announce Type: new  Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text embeddings.Extensive experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: https://github.com/MahdiyarMM/PRISM.",
        "arxiv_id": "2507.08979",
        "ARXIVID": "2507.08979",
        "COMMENT": "Matches criterion 2 as it proposes a method (PRISM) to reduce biases in vision-language models, enhancing their robustness.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.09984": {
        "authors": [
            "Junho Lee",
            "Jeongwoo Shin",
            "Hyungwook Choi",
            "Joonseok Lee"
        ],
        "title": "Latent Diffusion Models with Masked AutoEncoders",
        "abstract": "arXiv:2507.09984v1 Announce Type: new  Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in image generation, the desired properties and optimal design of the autoencoders have been underexplored. In this work, we analyze the role of autoencoders in LDMs and identify three key properties: latent smoothness, perceptual compression quality, and reconstruction quality. We demonstrate that existing autoencoders fail to simultaneously satisfy all three properties, and propose Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM framework, introducing Latent Diffusion Models with Masked AutoEncoders (LDMAEs). Through comprehensive experiments, we demonstrate significantly enhanced image generation quality and computational efficiency.",
        "arxiv_id": "2507.09984",
        "ARXIVID": "2507.09984",
        "COMMENT": "Matches criterion 4 as it focuses on improving latent diffusion models with masked autoencoders, which are foundational in computer vision.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.10355": {
        "authors": [
            "Bo Jiang",
            "Xueyang Ze",
            "Beibei Wang",
            "Xixi Wang",
            "Xixi Wan",
            "Bin Luo"
        ],
        "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter",
        "abstract": "arXiv:2507.10355v1 Announce Type: new  Abstract: Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.",
        "arxiv_id": "2507.10355",
        "ARXIVID": "2507.10355",
        "COMMENT": "Matches criterion 2 as it proposes a novel adapter (VRGAdapter) for vision-language models, enhancing their fine-tuning capabilities.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.09008": {
        "authors": [
            "Xiwei Xuan",
            "Xiaoqi Wang",
            "Wenbin He",
            "Jorge Piazentin Ono",
            "Liang Gou",
            "Kwan-Liu Ma",
            "Liu Ren"
        ],
        "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels",
        "abstract": "arXiv:2507.09008v1 Announce Type: new  Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA) have facilitated the auto-labeling of large-scale datasets, enhancing model performance in challenging downstream tasks such as open-vocabulary object detection and segmentation. However, the quality of FM-generated labels is less studied as existing approaches focus more on data quantity over quality. This is because validating large volumes of data without ground truth presents a considerable challenge in practice. Existing methods typically rely on limited metrics to identify problematic data, lacking a comprehensive perspective, or apply human validation to only a small data fraction, failing to address the full spectrum of potential issues. To overcome these challenges, we introduce VISTA, a visual analytics framework that improves data quality to enhance the performance of multi-modal models. Targeting the complex and demanding domain of open-vocabulary image segmentation, VISTA integrates multi-phased data validation strategies with human expertise, enabling humans to identify, understand, and correct hidden issues within FM-generated labels. Through detailed use cases on two benchmark datasets and expert reviews, we demonstrate VISTA's effectiveness from both quantitative and qualitative perspectives.",
        "arxiv_id": "2507.09008",
        "ARXIVID": "2507.09008",
        "COMMENT": "Matches criterion 4 as it focuses on improving foundation model-generated data labels for vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09180": {
        "authors": [
            "Zichun Xu",
            "Yuntao Li",
            "Zhaomin Wang",
            "Lei Zhuang",
            "Guocai Yang",
            "Jingdong Zhao"
        ],
        "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning",
        "abstract": "arXiv:2507.09180v1 Announce Type: new  Abstract: Depth information is robust to scene appearance variations and inherently carries 3D spatial details. In this paper, a visual backbone based on the vision transformer is proposed to fuse RGB and depth modalities for enhancing generalization. Different modalities are first processed by separate CNN stems, and the combined convolutional features are delivered to the scalable vision transformer to obtain visual representations. Moreover, a contrastive unsupervised learning scheme is designed with masked and unmasked tokens to accelerate the sample efficiency during the reinforcement learning progress. For sim2real transfer, a flexible curriculum learning schedule is developed to deploy domain randomization over training processes.",
        "arxiv_id": "2507.09180",
        "ARXIVID": "2507.09180",
        "COMMENT": "Matches criterion 1 as it explores spatial intelligence and depth information in visual reinforcement learning for embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.10375": {
        "authors": [
            "Utkarsh Singhal",
            "Ryan Feng",
            "Stella X. Yu",
            "Atul Prakash"
        ],
        "title": "Test-Time Canonicalization by Foundation Models for Robust Perception",
        "abstract": "arXiv:2507.10375v1 Announce Type: new  Abstract: Real-world visual perception requires invariance to diverse transformations, yet current methods rely heavily on specialized architectures or training on predefined augmentations, limiting generalization. We propose FOCAL, a test-time, data-driven framework that achieves robust perception by leveraging internet-scale visual priors from foundation models. By generating and optimizing candidate transformations toward visually typical, \"canonical\" views, FOCAL enhances robustness without re-training or architectural changes. Our experiments demonstrate improved robustness of CLIP and SAM across challenging transformations, including 2D/3D rotations, illumination shifts (contrast and color), and day-night variations. We also highlight potential applications in active vision. Our approach challenges the assumption that transform-specific training is necessary, instead offering a scalable path to invariance. Our code is available at: https://github.com/sutkarsh/focal.",
        "arxiv_id": "2507.10375",
        "ARXIVID": "2507.10375",
        "COMMENT": "Matches criterion 4 as it focuses on foundation models in computer vision and their application to robust perception.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.10358": {
        "authors": [
            "Hongxu Ma",
            "Chenbo Zhang",
            "Lu Zhang",
            "Jiaogen Zhou",
            "Jihong Guan",
            "Shuigeng Zhou"
        ],
        "title": "Fine-Grained Zero-Shot Object Detection",
        "abstract": "arXiv:2507.10358v1 Announce Type: new  Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to localize and recognize objects of both seen and unseen classes. Existing ZSD works are mainly coarse-grained object detection, where the classes are visually quite different, thus are relatively easy to distinguish. However, in real life we often have to face fine-grained object detection scenarios, where the classes are too similar to be easily distinguished. For example, detecting different kinds of birds, fishes, and flowers.   In this paper, we propose and solve a new problem called Fine-Grained Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of different classes with minute differences in details under the ZSD paradigm. We develop an effective method called MSHC for the FG-ZSD task, which is based on an improved two-stage detector and employs a multi-level semantics-aware embedding alignment loss, ensuring tight coupling between the visual and semantic spaces. Considering that existing ZSD datasets are not suitable for the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds, which contains 148,820 images falling into 36 orders, 140 families, 579 genera and 1432 species. Extensive experiments on FGZSD-Birds show that our method outperforms existing ZSD models.",
        "arxiv_id": "2507.10358",
        "ARXIVID": "2507.10358",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) as it addresses fine-grained zero-shot object detection with a new benchmark dataset.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09214": {
        "authors": [
            "Shiyi Mu",
            "Zichong Gu",
            "Hanqi Lyu",
            "Yilin Gao",
            "Shugong Xu"
        ],
        "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline",
        "abstract": "arXiv:2507.09214v1 Announce Type: new  Abstract: 3D detection technology is widely used in the field of autonomous driving, with its application scenarios gradually expanding from enclosed highways to open conventional roads. For rare anomaly categories that appear on the road, 3D detection models trained on closed sets often misdetect or fail to detect anomaly objects. To address this risk, it is necessary to enhance the generalization ability of 3D detection models for targets of arbitrary shapes and to possess the capability to filter out anomalies. The generalization of 3D detection is limited by two factors: the coupled training of 2D and 3D, and the insufficient diversity in the scale distribution of training samples. This paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm, which decouples the training strategy of 3D and 2D to release the generalization ability for arbitrary 3D foreground detection, and proposes an anomaly scoring algorithm based on foreground confidence prediction, achieving target-level anomaly scoring. In order to further verify and enhance the generalization of anomaly detection, we use a 3D rendering method to synthesize two augmented reality binocular stereo 3D detection datasets which named KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories as extra training data to address the sparse sample distribution issue. Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not used in training to simulate zero-shot scenarios in real-world settings, solely for evaluating 3D anomaly detection. Finally, the performance of the algorithm and the dataset is verified in the experiments. (Code and dataset can be obtained at https://github.com/xxxx/xxx).",
        "arxiv_id": "2507.09214",
        "ARXIVID": "2507.09214",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new dataset and method for 3D anomaly object detection in autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.10293": {
        "authors": [
            "Wenkang Han",
            "Wang Lin",
            "Yiyun Zhou",
            "Qi Liu",
            "Shulei Wang",
            "Chang Yao",
            "Jingyuan Chen"
        ],
        "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration",
        "abstract": "arXiv:2507.10293v1 Announce Type: new  Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from degraded versions. Traditional methods struggle to preserve fine-grained, identity-specific features when degradation is severe, often producing average-looking faces that lack individual characteristics. To address these challenges, we introduce IP-FVR, a novel method that leverages a high-quality reference face image as a visual prompt to provide identity conditioning during the denoising process. IP-FVR incorporates semantically rich identity information from the reference image using decoupled cross-attention mechanisms, ensuring detailed and identity consistent results. For intra-clip identity drift (within 24 frames), we introduce an identity-preserving feedback learning method that combines cosine similarity-based reward signals with suffix-weighted temporal aggregation. This approach effectively minimizes drift within sequences of frames. For inter-clip identity drift, we develop an exponential blending strategy that aligns identities across clips by iteratively blending frames from previous clips during the denoising process. This method ensures consistent identity representation across different clips. Additionally, we enhance the restoration process with a multi-stream negative prompt, guiding the model's attention to relevant facial attributes and minimizing the generation of low-quality or incorrect features. Extensive experiments on both synthetic and real-world datasets demonstrate that IP-FVR outperforms existing methods in both quality and identity preservation, showcasing its substantial potential for practical applications in face video restoration.",
        "arxiv_id": "2507.10293",
        "ARXIVID": "2507.10293",
        "COMMENT": "Matches criteria 6 (Video Understanding) as it focuses on face video restoration with identity preservation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09256": {
        "authors": [
            "Junyu Chen",
            "Yihua Gao",
            "Mingyuan Ge",
            "Mingyong Li"
        ],
        "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching",
        "abstract": "arXiv:2507.09256v1 Announce Type: new  Abstract: Image-text matching is crucial for bridging the semantic gap between computer vision and natural language processing. However, existing methods still face challenges in handling high-order associations and semantic ambiguities among similar instances. These ambiguities arise from subtle differences between soft positive samples (semantically similar but incorrectly labeled) and soft negative samples (locally matched but globally inconsistent), creating matching uncertainties. Furthermore, current methods fail to fully utilize the neighborhood relationships among semantically similar instances within training batches, limiting the model's ability to learn high-order shared knowledge. This paper proposes the Ambiguity-Aware and High-order Relation learning framework (AAHR) to address these issues. AAHR constructs a unified representation space through dynamic clustering prototype contrastive learning, effectively mitigating the soft positive sample problem. The framework introduces global and local feature extraction mechanisms and an adaptive aggregation network, significantly enhancing full-grained semantic understanding capabilities. Additionally, AAHR employs intra-modal and inter-modal correlation matrices to investigate neighborhood relationships among sample instances thoroughly. It incorporates GNN to enhance semantic interactions between instances. Furthermore, AAHR integrates momentum contrastive learning to expand the negative sample set. These combined strategies significantly improve the model's ability to discriminate between features. Experimental results demonstrate that AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching. The code and model checkpoints for this research are available at https://github.com/Image-Text-Matching/AAHR .",
        "arxiv_id": "2507.09256",
        "ARXIVID": "2507.09256",
        "COMMENT": "Matches criteria 5 (Integration of Image/Video and Large Language Models) as it addresses image-text matching with ambiguity-aware and high-order relation learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09102": {
        "authors": [
            "Yiyang Chen",
            "Shanshan Zhao",
            "Lunhao Duan",
            "Changxing Ding",
            "Dacheng Tao"
        ],
        "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning",
        "abstract": "arXiv:2507.09102v1 Announce Type: new  Abstract: Diffusion-based models, widely used in text-to-image generation, have proven effective in 2D representation learning. Recently, this framework has been extended to 3D self-supervised learning by constructing a conditional point generator for enhancing 3D representations. However, its performance remains constrained by the 3D diffusion model, which is trained on the available 3D datasets with limited size. We hypothesize that the robust capabilities of text-to-image diffusion models, particularly Stable Diffusion (SD), which is trained on large-scale datasets, can help overcome these limitations. To investigate this hypothesis, we propose PointSD, a framework that leverages the SD model for 3D self-supervised learning. By replacing the SD model's text encoder with a 3D encoder, we train a point-to-image diffusion model that allows point clouds to guide the denoising of rendered noisy images. With the trained point-to-image diffusion model, we use noise-free images as the input and point clouds as the condition to extract SD features. Next, we train a 3D backbone by aligning its features with these SD features, thereby facilitating direct semantic learning. Comprehensive experiments on downstream point cloud tasks and ablation studies demonstrate that the SD model can enhance point cloud self-supervised learning. Code is publicly available at https://github.com/wdttt/PointSD.",
        "arxiv_id": "2507.09102",
        "ARXIVID": "2507.09102",
        "COMMENT": "Matches criteria 5 (Integration of Image/Video and Large Language Models) as it leverages text-to-image diffusion models for 3D self-supervised learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.10203": {
        "authors": [
            "Shicai Wei",
            "Chunbo Luo",
            "Yang Luo"
        ],
        "title": "Improving Multimodal Learning via Imbalanced Learning",
        "abstract": "arXiv:2507.10203v1 Announce Type: new  Abstract: Multimodal learning often encounters the under-optimized problem and may perform worse than unimodal learning. Existing approaches attribute this issue to imbalanced learning across modalities and tend to address it through gradient balancing. However, this paper argues that balanced learning is not the optimal setting for multimodal learning. With bias-variance analysis, we prove that imbalanced dependency on each modality obeying the inverse ratio of their variances contributes to optimal performance. To this end, we propose the Asymmetric Representation Learning(ARL) strategy to assist multimodal learning via imbalanced optimization. ARL introduces auxiliary regularizers for each modality encoder to calculate their prediction variance. ARL then calculates coefficients via the unimodal variance to re-weight the optimization of each modality, forcing the modality dependence ratio to be inversely proportional to the modality variance ratio. Moreover, to minimize the generalization error, ARL further introduces the prediction bias of each modality and jointly optimizes them with multimodal loss. Notably, all auxiliary regularizers share parameters with the multimodal model and rely only on the modality representation. Thus the proposed ARL strategy introduces no extra parameters and is independent of the structures and fusion methods of the multimodal model. Finally, extensive experiments on various datasets validate the effectiveness and versatility of ARL. Code is available at \\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}",
        "arxiv_id": "2507.10203",
        "ARXIVID": "2507.10203",
        "COMMENT": "Matches criterion 2 as it explores multimodal learning with a novel optimization strategy for vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.09915": {
        "authors": [
            "Siyue Yao",
            "Mingjie Sun",
            "Eng Gee Lim",
            "Ran Yi",
            "Baojiang Zhong",
            "Moncef Gabbouj"
        ],
        "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios",
        "abstract": "arXiv:2507.09915v1 Announce Type: new  Abstract: The scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide \"crucial information\" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be released after acceptance.",
        "arxiv_id": "2507.09915",
        "ARXIVID": "2507.09915",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and data augmentation in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.09207": {
        "authors": [
            "Alexander C. Ogren",
            "Berthy T. Feng",
            "Jihoon Ahn",
            "Katherine L. Bouman",
            "Chiara Daraio"
        ],
        "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves",
        "abstract": "arXiv:2507.09207v1 Announce Type: new  Abstract: Wave propagation on the surface of a material contains information about physical properties beneath its surface. We propose a method for inferring the thickness and stiffness of a structure from just a video of waves on its surface. Our method works by extracting a dispersion relation from the video and then solving a physics-based optimization problem to find the best-fitting thickness and stiffness parameters. We validate our method on both simulated and real data, in both cases showing strong agreement with ground-truth measurements. Our technique provides a proof-of-concept for at-home health monitoring of medically-informative tissue properties, and it is further applicable to fields such as human-computer interaction.",
        "arxiv_id": "2507.09207",
        "ARXIVID": "2507.09207",
        "COMMENT": "Does not match any specific criterion but is an interesting application of video-based analysis for physical property inference.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.09487": {
        "authors": [
            "Changli Wang",
            "Fang Yin",
            "Jiafeng Liu",
            "Rui Wu"
        ],
        "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space",
        "abstract": "arXiv:2507.09487v1 Announce Type: new  Abstract: Visual and semantic concepts are often structured in a hierarchical manner. For instance, textual concept `cat' entails all images of cats. A recent study, MERU, successfully adapts multimodal learning techniques from Euclidean space to hyperbolic space, effectively capturing the visual-semantic hierarchy. However, a critical question remains: how can we more efficiently train a model to capture and leverage this hierarchy? In this paper, we propose the \\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel and efficient method that integrates Masked Image Modeling (MIM) and knowledge distillation techniques within hyperbolic space. To the best of our knowledge, this is the first approach to leverage MIM and knowledge distillation in hyperbolic space to train highly efficient models. In addition, we introduce a distillation loss function specifically designed to facilitate effective knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM and knowledge distillation techniques in hyperbolic space can achieve the same remarkable success as in Euclidean space. Extensive evaluations show that our method excels across a wide range of downstream tasks, significantly outperforming existing models like MERU and CLIP in both image classification and retrieval.",
        "arxiv_id": "2507.09487",
        "ARXIVID": "2507.09487",
        "COMMENT": "Does not closely match any specific criteria but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.09748": {
        "authors": [
            "Yu Lei",
            "Bingde Liu",
            "Qingsong Xie",
            "Haonan Lu",
            "Zhijie Deng"
        ],
        "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation",
        "abstract": "arXiv:2507.09748v1 Announce Type: new  Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence. In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that there exists a mismatching problem between LoRA and 3D distributions in practical implementation. We can simply adjust their optimization order to improve the generation quality. By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD). $L^2$-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. Extensive experiments validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior score distillation-based methods. We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework.",
        "arxiv_id": "2507.09748",
        "ARXIVID": "2507.09748",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in multi-modal learning (text-to-3D generation).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.09168": {
        "authors": [
            "Haiming Zhu",
            "Yangyang Xu",
            "Chenshu Xu",
            "Tingrui Shen",
            "Wenxi Liu",
            "Yong Du",
            "Jun Yu",
            "Shengfeng He"
        ],
        "title": "Stable Score Distillation",
        "abstract": "arXiv:2507.09168v1 Announce Type: new  Abstract: Text-guided image and 3D editing have advanced with diffusion-based models, yet methods like Delta Denoising Score often struggle with stability, spatial control, and editing strength. These limitations stem from reliance on complex auxiliary structures, which introduce conflicting optimization signals and restrict precise, localized edits. We introduce Stable Score Distillation (SSD), a streamlined framework that enhances stability and alignment in the editing process by anchoring a single classifier to the source prompt. Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves cross-prompt alignment, and introduces a constant term null-text branch to stabilize the optimization process. This approach preserves the original content's structure and ensures that editing trajectories are closely aligned with the source prompt, enabling smooth, prompt-specific modifications while maintaining coherence in surrounding regions. Additionally, SSD incorporates a prompt enhancement branch to boost editing strength, particularly for style transformations. Our method achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and text-driven style edits, with faster convergence and reduced complexity, providing a robust and efficient solution for text-guided editing.",
        "arxiv_id": "2507.09168",
        "ARXIVID": "2507.09168",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and text-guided editing.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.09884": {
        "authors": [
            "Xuzhao Li",
            "Xuchen Li",
            "Shiyu Hu",
            "Yongzhen Guo",
            "Wentao Zhang"
        ],
        "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains",
        "abstract": "arXiv:2507.09884v1 Announce Type: new  Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL) to enhance their reasoning capabilities through feedback. A critical challenge is verifying the consistency of model-generated responses and reference answers, since these responses are often lengthy, diverse, and nuanced. Rule-based verifiers struggle with complexity, prompting the use of model-based verifiers. However, specialized verifiers lack flexibility, while general LLM judges can be inconsistent. Existing research primarily focuses on building better verifiers, yet a systematic evaluation of different types of verifiers' performance across domains remains lacking, severely constraining the reliable development of Reinforcement Learning with Verifiable Reward (RLVR). To address this, we propose VerifyBench--a cross-domain comprehensive benchmark for systematically evaluating verifiers. We construct 4,000 expert-level questions covering mathematics, physics, chemistry, and biology. Each question is equipped with reference answers and diverse responses. The reliability of the evaluation is ensured through a rigorous annotation process conducted by a multidisciplinary expert team. We design a four-dimensional experimental framework to comprehensively compare the performance boundaries of specialized verifiers and general LLMs under combined conditions of extracted answers vs. complete responses, and short vs. long outputs. Our evaluation uncovers fundamental trade-offs in verifiers: while specialized verifiers achieve leading accuracy, they exhibit deficiencies in recall; general models show stronger inclusivity but unstable precision. More importantly, we discover verifiers' high sensitivity to input structure and inherent limitations in cross-domain generalization, providing critical insights into the bottlenecks of current verifier technology.",
        "arxiv_id": "2507.09884",
        "ARXIVID": "2507.09884",
        "COMMENT": "Does not match any specific criterion but is relevant to evaluation methodologies in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09896": {
        "authors": [
            "Xiuyu Wu",
            "Xinhao Wang",
            "Xiubin Zhu",
            "Lan Yang",
            "Jiyuan Liu",
            "Xingchen Hu"
        ],
        "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection",
        "abstract": "arXiv:2507.09896v1 Announce Type: new  Abstract: Due to the arbitrary orientation of objects in aerial images, rotation equivariance is a critical property for aerial object detectors. However, recent studies on rotation-equivariant aerial object detection remain scarce. Most detectors rely on data augmentation to enable models to learn approximately rotation-equivariant features. A few detectors have constructed rotation-equivariant networks, but due to the breaking of strict rotation equivariance by typical downsampling processes, these networks only achieve approximately rotation-equivariant backbones. Whether strict rotation equivariance is necessary for aerial image object detection remains an open question. In this paper, we implement a strictly rotation-equivariant backbone and neck network with a more advanced network structure and compare it with approximately rotation-equivariant networks to quantitatively measure the impact of rotation equivariance on the performance of aerial image detectors. Additionally, leveraging the inherently grouped nature of rotation-equivariant features, we propose a multi-branch head network that reduces the parameter count while improving detection accuracy. Based on the aforementioned improvements, this study proposes the Multi-branch head rotation-equivariant single-stage Detector (MessDet), which achieves state-of-the-art performance on the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an exceptionally low parameter count.",
        "arxiv_id": "2507.09896",
        "ARXIVID": "2507.09896",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning due to its focus on rotation-equivariant aerial object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10461": {
        "authors": [
            "Tao Tang",
            "Chengxu Yang"
        ],
        "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
        "abstract": "arXiv:2507.10461v1 Announce Type: new  Abstract: Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.",
        "arxiv_id": "2507.10461",
        "ARXIVID": "2507.10461",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of adaptive convolutional networks in image processing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10283": {
        "authors": [
            "Muyi Bao",
            "Changyu Zeng",
            "Yifan Wang",
            "Zhengni Yang",
            "Zimu Wang",
            "Guangliang Cheng",
            "Jun Qi",
            "Wei Wang"
        ],
        "title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification",
        "abstract": "arXiv:2507.10283v1 Announce Type: new  Abstract: Transformer-based deep neural networks have achieved remarkable success across various computer vision tasks, largely attributed to their long-range self-attention mechanism and scalability. However, most transformer architectures embed images into uniform, grid-based vision tokens, neglecting the underlying semantic meanings of image regions, resulting in suboptimal feature representations. To address this issue, we propose Fuzzy Token Clustering Transformer (FTCFormer), which incorporates a novel clustering-based downsampling module to dynamically generate vision tokens based on the semantic meanings instead of spatial positions. It allocates fewer tokens to less informative regions and more to represent semantically important regions, regardless of their spatial adjacency or shape irregularity. To further enhance feature extraction and representation, we propose a Density Peak Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center determination, a Spatial Connectivity Score (SCS) for token assignment, and a channel-wise merging (Cmerge) strategy for token merging. Extensive experiments on 32 datasets across diverse domains validate the effectiveness of FTCFormer on image classification, showing consistent improvements over the TCFormer baseline, achieving gains of improving 1.43% on five fine-grained datasets, 1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55% on four remote sensing datasets. The code is available at: https://github.com/BaoBao0926/FTCFormer/tree/main.",
        "arxiv_id": "2507.10283",
        "ARXIVID": "2507.10283",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of transformer-based architectures in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09285": {
        "authors": [
            "Chenhao Ding",
            "Jiangtao Zhang",
            "Zongsheng Yue",
            "Hui Wang",
            "Qian Zhao",
            "Deyu Meng"
        ],
        "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring",
        "abstract": "arXiv:2507.09285v1 Announce Type: new  Abstract: Deep prior-based approaches have demonstrated remarkable success in blind motion deblurring (BMD) recently. These methods, however, are often limited by the high non-convexity of the underlying optimization process in BMD, which leads to extreme sensitivity to the initial blur kernel. To address this issue, we propose a novel framework for BMD that leverages a deep generative model to encode the kernel prior and induce a better initialization for the blur kernel. Specifically, we pre-train a kernel generator based on a generative adversarial network (GAN) to aptly characterize the kernel's prior distribution, as well as a kernel initializer to provide a well-informed and high-quality starting point for kernel estimation. By combining these two components, we constrain the BMD solution within a compact latent kernel manifold, thus alleviating the aforementioned sensitivity for kernel initialization. Notably, the kernel generator and initializer are designed to be easily integrated with existing BMD methods in a plug-and-play manner, enhancing their overall performance. Furthermore, we extend our approach to tackle blind non-uniform motion deblurring without the need for additional priors, achieving state-of-the-art performance on challenging benchmark datasets. The source code is available at https://github.com/dch0319/GLKM-Deblur.",
        "arxiv_id": "2507.09285",
        "ARXIVID": "2507.09285",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09619": {
        "authors": [
            "Yilin Lu",
            "Jianghang Lin",
            "Linhuang Xie",
            "Kai Zhao",
            "Yansong Qu",
            "Shengchuan Zhang",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection",
        "abstract": "arXiv:2507.09619v1 Announce Type: new  Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the scarcity of anomaly samples significantly limits the effectiveness of existing methods in tasks such as localization and classification. While several anomaly synthesis approaches have been introduced for data augmentation, they often struggle with low realism, inaccurate mask alignment, and poor generalization. To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a region-guided, few-shot anomaly image-mask pair generation framework. GAA leverages the strong priors of a pretrained latent diffusion model to generate realistic, diverse, and semantically aligned anomalies using only a small number of samples. The framework first employs Localized Concept Decomposition to jointly model the semantic features and spatial information of anomalies, enabling flexible control over the type and location of anomalies. It then utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained semantic clustering of anomaly concepts, thereby enhancing the consistency of anomaly representations. Subsequently, a region-guided mask generation strategy ensures precise alignment between anomalies and their corresponding masks, while a low-quality sample filtering module is introduced to further improve the overall quality of the generated samples. Extensive experiments on the MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance in both anomaly synthesis quality and downstream tasks such as localization and classification.",
        "arxiv_id": "2507.09619",
        "ARXIVID": "2507.09619",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and anomaly detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09269": {
        "authors": [
            "Shuhan Ye",
            "Yuanbin Qian",
            "Chong Wang",
            "Sunqi Lin",
            "Jiazhen Xu",
            "Jiangbo Qian",
            "Yuqi Li"
        ],
        "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks",
        "abstract": "arXiv:2507.09269v1 Announce Type: new  Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in computer vision domain due to their high biological plausibility, event-driven characteristic and energy-saving efficiency. Still, limited annotated event-based datasets and immature SNN architectures result in their performance inferior to that of Artificial Neural Networks (ANNs). To enhance the performance of SNNs on their optimal data format, DVS data, we explore using RGB data and well-performing ANNs to implement knowledge distillation. In this case, solving cross-modality and cross-architecture challenges is necessary. In this paper, we propose cross knowledge distillation (CKD), which not only leverages semantic similarity and sliding replacement to mitigate the cross-modality challenge, but also uses an indirect phased knowledge distillation to mitigate the cross-architecture challenge. We validated our method on main-stream neuromorphic datasets, including N-Caltech101 and CEP-DVS. The experimental results show that our method outperforms current State-of-the-Art methods. The code will be available at https://github.com/ShawnYE618/CKD",
        "arxiv_id": "2507.09269",
        "ARXIVID": "2507.09269",
        "COMMENT": "Does not closely match any specific criteria but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10265": {
        "authors": [
            "Xinlong Ding",
            "Hongwei Yu",
            "Jiawei Li",
            "Feifan Li",
            "Yu Shang",
            "Bochao Zou",
            "Huimin Ma",
            "Jiansheng Chen"
        ],
        "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures",
        "abstract": "arXiv:2507.10265v1 Announce Type: new  Abstract: Camera pose estimation is a fundamental computer vision task that is essential for applications like visual localization and multi-view stereo reconstruction. In the object-centric scenarios with sparse inputs, the accuracy of pose estimation can be significantly influenced by background textures that occupy major portions of the images across different viewpoints. In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which uses identical segments to form discs with multi-fold radial symmetry. These discs maintain high similarity across different viewpoints, enabling effective attacks on pose estimation models even with natural texture segments. Additionally, a projected orientation consistency loss is proposed to optimize the kaleidoscopic segments, leading to significant enhancement in the attack effectiveness. Experimental results show that optimized adversarial kaleidoscopic backgrounds can effectively attack various camera pose estimation models.",
        "arxiv_id": "2507.10265",
        "ARXIVID": "2507.10265",
        "COMMENT": "Does not closely match any specific criteria but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09524": {
        "authors": [
            "Yunwei Lan",
            "Zhigao Cui",
            "Xin Luo",
            "Chang Liu",
            "Nian Wang",
            "Menglin Zhang",
            "Yanzhao Su",
            "Dong Liu"
        ],
        "title": "When Schr\\\"odinger Bridge Meets Real-World Image Dehazing with Unpaired Training",
        "abstract": "arXiv:2507.09524v1 Announce Type: new  Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show promising performance in processing real-world hazy images. However, these methods tend to face limitations due to the generator's limited transport mapping capability, which hinders the full exploitation of their effectiveness in unpaired training paradigms. To address these challenges, we propose DehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges the distributions between hazy and clear images. This enables optimal transport mappings from hazy to clear images in fewer steps, thereby generating high-quality results. To ensure the consistency of structural information and details in the restored images, we introduce detail-preserving regularization, which enforces pixel-level alignment between hazy inputs and dehazed outputs. Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP models in distinguishing hazy images and clear ones, by learning a haze-aware vision-language alignment. Extensive experiments on multiple real-world datasets demonstrate our method's superiority. Code: https://github.com/ywxjm/DehazeSB.",
        "arxiv_id": "2507.09524",
        "ARXIVID": "2507.09524",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision (image dehazing using optimal transport).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09323": {
        "authors": [
            "Kaixuan Cong",
            "Yifan Wang",
            "Rongkun Xue",
            "Yuyang Jiang",
            "Yiming Feng",
            "Jing Yang"
        ],
        "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition",
        "abstract": "arXiv:2507.09323v1 Announce Type: new  Abstract: Humans do not understand individual events in isolation; rather, they generalize concepts within classes and compare them to others. Existing audio-video pre-training paradigms only focus on the alignment of the overall audio-video modalities, without considering the reinforcement of distinguishing easily confused classes through cognitive induction and contrast during training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder (DICCAE), an encoder that aligns audio-video representations at a fine-grained, category-level. DICCAE addresses category confusion by dynamically adjusting the confusion loss based on inter-class confusion degrees, thereby enhancing the model's ability to distinguish between similar activities. To further extend the application of DICCAE, we also introduce a novel training framework that incorporates both audio and video modalities, as well as their fusion. To mitigate the scarcity of audio-video data in the human activity recognition task, we propose a cluster-guided audio-video self-supervised pre-training strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its feature representation quality through extensive ablation studies, validating the necessity of each module.",
        "arxiv_id": "2507.09323",
        "ARXIVID": "2507.09323",
        "COMMENT": "Does not match any specific criterion but is relevant to multi-modal learning (audio-visual fusion for human activity recognition).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09830": {
        "authors": [
            "Shuhao Fu",
            "Philip J. Kellman",
            "Hongjing Lu"
        ],
        "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models",
        "abstract": "arXiv:2507.09830v1 Announce Type: new  Abstract: Both humans and deep learning models can recognize objects from 3D shapes depicted with sparse visual information, such as a set of points randomly sampled from the surfaces of 3D objects (termed a point cloud). Although deep learning models achieve human-like performance in recognizing objects from 3D shapes, it remains unclear whether these models develop 3D shape representations similar to those used by human vision for object recognition. We hypothesize that training with 3D shapes enables models to form representations of local geometric structures in 3D shapes. However, their representations of global 3D object shapes may be limited. We conducted two human experiments systematically manipulating point density and object orientation (Experiment 1), and local geometric structure (Experiment 2). Humans consistently performed well across all experimental conditions. We compared two types of deep learning models, one based on a convolutional neural network (DGCNN) and the other on visual transformers (point transformer), with human performance. We found that the point transformer model provided a better account of human performance than the convolution-based model. The advantage mainly results from the mechanism in the point transformer model that supports hierarchical abstraction of 3D shapes.",
        "arxiv_id": "2507.09830",
        "ARXIVID": "2507.09830",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and 3D object recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09052": {
        "authors": [
            "Fang Chen",
            "Alex Villa",
            "Gongbo Liang",
            "Xiaoyi Lu",
            "Meng Tang"
        ],
        "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?",
        "abstract": "arXiv:2507.09052v1 Announce Type: new  Abstract: Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity of tail class images without compromising the fidelity and diversity of head class images. We achieve this by introducing two deceptively simple but highly effective contrastive loss functions. Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to increase the distance/dissimilarity among synthetic images, particularly for tail classes. To further enhance the diversity of tail classes, our second loss is an MSE loss that contrasts class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. Conditional-unconditional alignment has been shown to enhance the performance of long-tailed GAN. We are the first to adapt such alignment to diffusion models. We successfully leveraged contrastive learning for class-imbalanced diffusion models. Our contrastive learning framework is easy to implement and outperforms standard DDPM and alternative methods for class-imbalanced diffusion models across various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.",
        "arxiv_id": "2507.09052",
        "ARXIVID": "2507.09052",
        "COMMENT": "Does not closely match any specific criterion but is relevant to generative modeling and class-imbalanced diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09955": {
        "authors": [
            "Luolin Xiong",
            "Haofen Wang",
            "Xi Chen",
            "Lu Sheng",
            "Yun Xiong",
            "Jingping Liu",
            "Yanghua Xiao",
            "Huajun Chen",
            "Qing-Long Han",
            "Yang Tang"
        ],
        "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models",
        "abstract": "arXiv:2507.09955v1 Announce Type: new  Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their V3 and R1 series models, which attracted global attention due to their low cost, high performance, and open-source advantages. This paper begins by reviewing the evolution of large AI models focusing on paradigm shifts, the mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm. Subsequently, the paper highlights novel algorithms introduced by DeepSeek, including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE), Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO). The paper then explores DeepSeek engineering breakthroughs in LLM scaling, training, inference, and system-level optimization architecture. Moreover, the impact of DeepSeek models on the competitive AI landscape is analyzed, comparing them to mainstream LLMs across various fields. Finally, the paper reflects on the insights gained from DeepSeek innovations and discusses future trends in the technical and engineering development of large AI models, particularly in data, training, and reasoning.",
        "arxiv_id": "2507.09955",
        "ARXIVID": "2507.09955",
        "COMMENT": "Does not closely match any specific criterion but discusses advancements in large AI models, which is generally relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.09980": {
        "authors": [
            "Zhipeng Xue",
            "Yan Zhang",
            "Ming Li",
            "Chun Li",
            "Yue Liu",
            "Fei Yu"
        ],
        "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures",
        "abstract": "arXiv:2507.09980v1 Announce Type: new  Abstract: Existing multi-view classification and clustering methods typically improve task accuracy by leveraging and fusing information from different views. However, ensuring the reliability of multi-view integration and final decisions is crucial, particularly when dealing with noisy or corrupted data. Current methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty of network predictions, ignoring domain gaps between different modalities. To address this issue, KPHD-Net, based on H\\\"older divergence, is proposed for multi-view classification and clustering tasks. Generally, our KPHD-Net employs a variational Dirichlet distribution to represent class probability distributions, models evidences from different views, and then integrates it with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation effects. Our theoretical analysis demonstrates that Proper H\\\"older divergence offers a more effective measure of distribution discrepancies, ensuring enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence theory, recognized for its superior performance in multi-view fusion tasks, is introduced and combined with the Kalman filter to provide future state estimations. This integration further enhances the reliability of the final fusion results. Extensive experiments show that the proposed KPHD-Net outperforms the current state-of-the-art methods in both classification and clustering tasks regarding accuracy, robustness, and reliability, with theoretical guarantees.",
        "arxiv_id": "2507.09980",
        "ARXIVID": "2507.09980",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multimodal learning and uncertainty quantification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10470": {
        "authors": [
            "Zhicun Yin",
            "Junjie Chen",
            "Ming Liu",
            "Zhixin Wang",
            "Fan Li",
            "Renjing Pei",
            "Xiaoming Li",
            "Rynson W. H. Lau",
            "Wangmeng Zuo"
        ],
        "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction",
        "abstract": "arXiv:2507.10470v1 Announce Type: new  Abstract: Blind facial image restoration is highly challenging due to unknown complex degradations and the sensitivity of humans to faces. Although existing methods introduce auxiliary information from generative priors or high-quality reference images, they still struggle with identity preservation problems, mainly due to improper feature introduction on detailed textures. In this paper, we focus on effectively incorporating appropriate features from high-quality reference images, presenting a novel blind facial image restoration method that considers reference selection, transfer, and reconstruction (RefSTAR). In terms of selection, we construct a reference selection (RefSel) module. For training the RefSel module, we construct a RefSel-HQ dataset through a mask generation pipeline, which contains annotating masks for 10,000 ground truth-reference pairs. As for the transfer, due to the trivial solution in vanilla cross-attention operations, a feature fusion paradigm is designed to force the features from the reference to be integrated. Finally, we propose a reference image reconstruction mechanism that further ensures the presence of reference image features in the output image. The cycle consistency loss is also redesigned in conjunction with the mask. Extensive experiments on various backbone models demonstrate superior performance, showing better identity preservation ability and reference feature transfer quality. Source code, dataset, and pre-trained models are available at https://github.com/yinzhicun/RefSTAR.",
        "arxiv_id": "2507.10470",
        "ARXIVID": "2507.10470",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision applications in image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10072": {
        "authors": [
            "Meng Yu",
            "Kun Zhan"
        ],
        "title": "Frequency Regulation for Exposure Bias Mitigation in Diffusion Models",
        "abstract": "arXiv:2507.10072v1 Announce Type: new  Abstract: Diffusion models exhibit impressive generative capabilities but are significantly impacted by exposure bias. In this paper, we make a key observation: the energy of the predicted noisy images decreases during the diffusion process. Building on this, we identify two important findings: 1) The reduction in energy follows distinct patterns in the low-frequency and high-frequency subbands; 2) This energy reduction results in amplitude variations between the network-reconstructed clean data and the real clean data. Based on the first finding, we introduce a frequency-domain regulation mechanism utilizing wavelet transforms, which separately adjusts the low- and high-frequency subbands. Leveraging the second insight, we provide a more accurate analysis of exposure bias in the two subbands. Our method is training-free and plug-and-play, significantly improving the generative quality of various diffusion models and providing a robust solution to exposure bias across different model architectures. The source code is available at https://github.com/kunzhan/wpp.",
        "arxiv_id": "2507.10072",
        "ARXIVID": "2507.10072",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.10343": {
        "authors": [
            "Hugo Norrby",
            "Gabriel F\\\"arm",
            "Kevin Hernandez-Diaz",
            "Fernando Alonso-Fernandez"
        ],
        "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans",
        "abstract": "arXiv:2507.10343v1 Announce Type: new  Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic segmentation (FGSS) architecture designed to improve the generalization ability of wall segmentation on floorplans. FGSSNet features a U-Net segmentation backbone with a multi-headed dedicated feature extractor used to extract domain-specific feature maps which are injected into the latent space of U-Net to guide the segmentation process. This dedicated feature extractor is trained as an encoder-decoder with selected wall patches, representative of the walls present in the input floorplan, to produce a compressed latent representation of wall patches while jointly trained to predict the wall width. In doing so, we expect that the feature extractor encodes texture and width features of wall patches that are useful to guide the wall segmentation process. Our experiments show increased performance by the use of such injected features in comparison to the vanilla U-Net, highlighting the validity of the proposed approach.",
        "arxiv_id": "2507.10343",
        "ARXIVID": "2507.10343",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of semantic segmentation in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}