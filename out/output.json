{
    "2507.02713": {
        "authors": [
            "Qin Guo",
            "Ailing Zeng",
            "Dongxu Yue",
            "Ceyuan Yang",
            "Yang Cao",
            "Hanzhong Guo",
            "Fei Shen",
            "Wei Liu",
            "Xihui Liu",
            "Dan Xu"
        ],
        "title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation",
        "abstract": "arXiv:2507.02713v1 Announce Type: new  Abstract: Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans (e.g., animals). Moreover, it is difficult to generate multiple overlapping humans and animals based on keypoint controls solely. These challenges arise from two main aspects: the inherent limitations of existing controllable methods and the lack of suitable datasets. First, we design a DiT-based framework, named UniMC, to explore unifying controllable multi-class image generation. UniMC integrates instance- and keypoint-level conditions into compact tokens, incorporating attributes such as class, bounding box, and keypoint coordinates. This approach overcomes the limitations of previous methods that struggled to distinguish instances and classes due to their reliance on skeleton images as conditions. Second, we propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K images with 2.9M instances. This dataset features extensive annotations such as keypoints, bounding boxes, and fine-grained captions for both humans and animals, along with rigorous manual inspection to ensure annotation accuracy. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly in heavy occlusions and multi-class scenarios.",
        "arxiv_id": "2507.02713",
        "ARXIVID": "2507.02713",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) due to its focus on keypoint-guided image generation and integration of multiple modalities.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.02747": {
        "authors": [
            "Jiawei He",
            "Danshi Li",
            "Xinqiang Yu",
            "Zekun Qi",
            "Wenyao Zhang",
            "Jiayi Chen",
            "Zhaoxiang Zhang",
            "Zhizheng Zhang",
            "Li Yi",
            "He Wang"
        ],
        "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale",
        "abstract": "arXiv:2507.02747v1 Announce Type: new  Abstract: As large models gain traction, vision-language-action (VLA) systems are enabling robots to tackle increasingly complex tasks. However, limited by the difficulty of data collection, progress has mainly focused on controlling simple gripper end-effectors. There is little research on functional grasping with large models for human-like dexterous hands. In this paper, we introduce DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction aligned with language instructions using single-view RGBD input. To accomplish this, we generate a dataset of 170 million dexterous grasp poses mapped to semantic parts across 174,000 objects in simulation, paired with detailed part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used to train a VLM and flow-matching-based pose head capable of producing instruction-aligned grasp poses for tabletop objects. To assess DexVLG's performance, we create benchmarks in physics-based simulations and conduct real-world experiments. Extensive testing demonstrates DexVLG's strong zero-shot generalization capabilities-achieving over 76% zero-shot execution success rate and state-of-the-art part-grasp accuracy in simulation-and successful part-aligned grasps on physical objects in real-world scenarios.",
        "arxiv_id": "2507.02747",
        "ARXIVID": "2507.02747",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a vision-language-grasp model for dexterous grasp pose prediction, addressing challenges in robotic grasping.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.02859": {
        "authors": [
            "Jiaer Xia",
            "Bingkui Tong",
            "Yuhang Zang",
            "Rui Shao",
            "Kaiyang Zhou"
        ],
        "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation",
        "abstract": "arXiv:2507.02859v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation.",
        "arxiv_id": "2507.02859",
        "ARXIVID": "2507.02859",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on adapting multimodal LLMs for specialized vision tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.02271": {
        "authors": [
            "Feizhen Huang",
            "Yu Wu",
            "Yutian Lin",
            "Bo Du"
        ],
        "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation",
        "abstract": "arXiv:2507.02271v1 Announce Type: new  Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a crucial role in film and video post-production. However, current methods overlook the cinematic language, a critical component of artistic expression in filmmaking. As a result, their performance deteriorates in scenarios where Foley targets are only partially visible. To address this challenge, we propose a simple self-distillation approach to extend V2A models to cinematic language scenarios. By simulating the cinematic language variations, the student model learns to align the video features of training pairs with the same audio-visual correspondences, enabling it to effectively capture the associations between sounds and partial visual information. Our method not only achieves impressive improvements under partial visibility across all evaluation metrics, but also enhances performance on the large-scale V2A dataset, VGGSound.",
        "arxiv_id": "2507.02271",
        "ARXIVID": "2507.02271",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on video-to-audio generation and cinematic language.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02652": {
        "authors": [
            "Jiajie Jin",
            "Xiaoxi Li",
            "Guanting Dong",
            "Yuyao Zhang",
            "Yutao Zhu",
            "Yang Zhao",
            "Hongjin Qian",
            "Zhicheng Dou"
        ],
        "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search",
        "abstract": "arXiv:2507.02652v1 Announce Type: new  Abstract: Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.",
        "arxiv_id": "2507.02652",
        "ARXIVID": "2507.02652",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on hierarchical reasoning and multi-step information seeking tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02790": {
        "authors": [
            "Xiangfeng Wang",
            "Xiao Li",
            "Yadong Wei",
            "Xueyu Song",
            "Yang Song",
            "Xiaoqiang Xia",
            "Fangrui Zeng",
            "Zaiyi Chen",
            "Liu Liu",
            "Gu Xu",
            "Tong Xu"
        ],
        "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding",
        "abstract": "arXiv:2507.02790v1 Announce Type: new  Abstract: The rapid growth of online video content, especially on short video platforms, has created a growing demand for efficient video editing techniques that can condense long-form videos into concise and engaging clips. Existing automatic editing methods predominantly rely on textual cues from ASR transcripts and end-to-end segment selection, often neglecting the rich visual context and leading to incoherent outputs. In this paper, we propose a human-inspired automatic video editing framework (HIVE) that leverages multimodal narrative understanding to address these limitations. Our approach incorporates character extraction, dialogue analysis, and narrative summarization through multimodal large language models, enabling a holistic understanding of the video content. To further enhance coherence, we apply scene-level segmentation and decompose the editing process into three subtasks: highlight detection, opening/ending selection, and pruning of irrelevant content. To facilitate research in this area, we introduce DramaAD, a novel benchmark dataset comprising over 800 short drama episodes and 500 professionally edited advertisement clips. Experimental results demonstrate that our framework consistently outperforms existing baselines across both general and advertisement-oriented editing tasks, significantly narrowing the quality gap between automatic and human-edited videos.",
        "arxiv_id": "2507.02790",
        "ARXIVID": "2507.02790",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on multimodal narrative understanding and video editing tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02861": {
        "authors": [
            "Zhening Huang",
            "Xiaoyang Wu",
            "Fangcheng Zhong",
            "Hengshuang Zhao",
            "Matthias Nie{\\ss}ner",
            "Joan Lasenby"
        ],
        "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans",
        "abstract": "arXiv:2507.02861v1 Announce Type: new  Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c",
        "arxiv_id": "2507.02861",
        "ARXIVID": "2507.02861",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on 3D scene reconstruction and structured scene graphs for spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02705": {
        "authors": [
            "Qi Xu",
            "Dongxu Wei",
            "Lingzhe Zhao",
            "Wenpu Li",
            "Zhangchi Huang",
            "Shunping Ji",
            "Peidong Liu"
        ],
        "title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment",
        "abstract": "arXiv:2507.02705v1 Announce Type: new  Abstract: Simultaneous understanding and 3D reconstruction plays an important role in developing end-to-end embodied intelligent systems. To achieve this, recent approaches resort to 2D-to-3D feature alignment paradigm, which leads to limited 3D understanding capability and potential semantic information loss. In light of this, we propose SIU3R, the first alignment-free framework for generalizable simultaneous understanding and 3D reconstruction from unposed images. Specifically, SIU3R bridges reconstruction and understanding tasks via pixel-aligned 3D representation, and unifies multiple understanding tasks into a set of unified learnable queries, enabling native 3D understanding without the need of alignment with 2D models. To encourage collaboration between the two tasks with shared representation, we further conduct in-depth analyses of their mutual benefits, and propose two lightweight modules to facilitate their interaction. Extensive experiments demonstrate that our method achieves state-of-the-art performance not only on the individual tasks of 3D reconstruction and understanding, but also on the task of simultaneous understanding and 3D reconstruction, highlighting the advantages of our alignment-free framework and the effectiveness of the mutual benefit designs.",
        "arxiv_id": "2507.02705",
        "ARXIVID": "2507.02705",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a framework for simultaneous scene understanding and 3D reconstruction, emphasizing spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02664": {
        "authors": [
            "Ziyin Zhou",
            "Yunpeng Luo",
            "Yuanchen Wu",
            "Ke Sun",
            "Jiayi Ji",
            "Ke Yan",
            "Shouhong Ding",
            "Xiaoshuai Sun",
            "Yunsheng Wu",
            "Rongrong Ji"
        ],
        "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models",
        "abstract": "arXiv:2507.02664v1 Announce Type: new  Abstract: The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes.",
        "arxiv_id": "2507.02664",
        "ARXIVID": "2507.02664",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a multimodal large language model (MLLM) for AI-generated image detection with novel training strategies and explanations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02279": {
        "authors": [
            "Juntao Liu",
            "Liqiang Niu",
            "Wenchao Chen",
            "Jie Zhou",
            "Fandong Meng"
        ],
        "title": "LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models",
        "abstract": "arXiv:2507.02279v1 Announce Type: new  Abstract: Existing visual token compression methods for Multimodal Large Language Models (MLLMs) predominantly operate as post-encoder modules, limiting their potential for efficiency gains. To address this limitation, we propose LaCo (Layer-wise Visual Token Compression), a novel framework that enables effective token compression within the intermediate layers of the vision encoder. LaCo introduces two core components: 1) a layer-wise pixel-shuffle mechanism that systematically merges adjacent tokens through space-to-channel transformations, and 2) a residual learning architecture with non-parametric shortcuts that preserves critical visual information during compression. Extensive experiments indicate that our LaCo outperforms all existing methods when compressing tokens in the intermediate layers of the vision encoder, demonstrating superior effectiveness. In addition, compared to external compression, our method improves training efficiency beyond 20% and inference throughput over 15% while maintaining strong performance.",
        "arxiv_id": "2507.02279",
        "ARXIVID": "2507.02279",
        "COMMENT": "Matches criteria 2 as it proposes a novel framework for visual token compression in multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02200": {
        "authors": [
            "Xiao Wang",
            "Jingtao Jiang",
            "Qiang Chen",
            "Lan Chen",
            "Lin Zhu",
            "Yaowei Wang",
            "Yonghong Tian",
            "Jin Tang"
        ],
        "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning",
        "abstract": "arXiv:2507.02200v1 Announce Type: new  Abstract: Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/ESTR-CoT.",
        "arxiv_id": "2507.02200",
        "ARXIVID": "2507.02200",
        "COMMENT": "Matches criteria 2 and 5 as it explores a multimodal large language model (MLLM) with vision-language integration and chain-of-thought reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02288": {
        "authors": [
            "De Cheng",
            "Zhipeng Xu",
            "Xinyang Jiang",
            "Dongsheng Li",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "title": "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization",
        "abstract": "arXiv:2507.02288v1 Announce Type: new  Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of performing effectively on unseen target domains. Notably, recent advances in pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated considerable potential in enhancing the generalization capabilities of deep learning models. Despite the increasing attention toward VFM-based domain prompt tuning within DG, the effective design of prompts capable of disentangling invariant features across diverse domains remains a critical challenge. In this paper, we propose addressing this challenge by leveraging the controllable and flexible language prompt of the VFM. Noting that the text modality of VFMs is naturally easier to disentangle, we introduce a novel framework for text feature-guided visual prompt tuning. This framework first automatically disentangles the text prompt using a large language model (LLM) and then learns domain-invariant visual representation guided by the disentangled text feature. However, relying solely on language to guide visual feature disentanglement has limitations, as visual features can sometimes be too complex or nuanced to be fully captured by descriptive text. To address this, we introduce Worst Explicit Representation Alignment (WERA), which extends text-guided visual prompts by incorporating an additional set of abstract prompts. These prompts enhance source domain diversity through stylized image augmentations, while alignment constraints ensure that visual representations remain consistent across both the original and augmented distributions. Experiments conducted on major DG datasets, including PACS, VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method outperforms state-of-the-art DG methods.",
        "arxiv_id": "2507.02288",
        "ARXIVID": "2507.02288",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) due to its focus on visual foundation models and domain generalization.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.02591": {
        "authors": [
            "Weili Xu",
            "Enxin Song",
            "Wenhao Chai",
            "Xuexiang Wen",
            "Tian Ye",
            "Gaoang Wang"
        ],
        "title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding",
        "abstract": "arXiv:2507.02591v1 Announce Type: new  Abstract: The challenge of long video understanding lies in its high computational complexity and prohibitive memory cost, since the memory and computation required by transformer-based LLMs scale quadratically with input sequence length. We propose AuroraLong to address this challenge by replacing the LLM component in MLLMs with a linear RNN language model that handles input sequence of arbitrary length with constant-size hidden states. To further increase throughput and efficiency, we combine visual token merge with linear RNN models by reordering the visual tokens by their sizes in ascending order. Despite having only 2B parameters and being trained exclusively on public data, AuroraLong achieves performance comparable to Transformer-based models of similar size trained on private datasets across multiple video benchmarks. This demonstrates the potential of efficient, linear RNNs to democratize long video understanding by lowering its computational entry barrier. To our best knowledge, we are the first to use a linear RNN based LLM backbone in a LLaVA-like model for open-ended video understanding.",
        "arxiv_id": "2507.02591",
        "ARXIVID": "2507.02591",
        "COMMENT": "Matches criteria 6 as it focuses on video understanding with a novel RNN-based approach for long video tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.02363": {
        "authors": [
            "Jiahao Wu",
            "Rui Peng",
            "Jianbo Jiao",
            "Jiayu Yang",
            "Luyang Tang",
            "Kaiqiang Xiong",
            "Jie Liang",
            "Jinbo Yan",
            "Runling Liu",
            "Ronggang Wang"
        ],
        "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling",
        "abstract": "arXiv:2507.02363v1 Announce Type: new  Abstract: Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: https://wujh2001.github.io/LocalDyGS/.",
        "arxiv_id": "2507.02363",
        "ARXIVID": "2507.02363",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on dynamic scene modeling and video synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02825": {
        "authors": [
            "Yuxuan Zhu",
            "Tengjun Jin",
            "Yada Pruksachatkun",
            "Andy Zhang",
            "Shu Liu",
            "Sasha Cui",
            "Sayash Kapoor",
            "Shayne Longpre",
            "Kevin Meng",
            "Rebecca Weiss",
            "Fazl Barez",
            "Rahul Gupta",
            "Jwala Dhamala",
            "Jacob Merizian",
            "Mario Giulianelli",
            "Harry Coppock",
            "Cozmin Ududec",
            "Jasjeet Sekhon",
            "Jacob Steinhardt",
            "Antony Kellerman",
            "Sarah Schwettmann",
            "Matei Zaharia",
            "Ion Stoica",
            "Percy Liang",
            "Daniel Kang"
        ],
        "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks",
        "abstract": "arXiv:2507.02825v1 Announce Type: new  Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.",
        "arxiv_id": "2507.02825",
        "ARXIVID": "2507.02825",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces guidelines for rigorous agentic benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02857": {
        "authors": [
            "Ziye Li",
            "Hao Luo",
            "Xincheng Shuai",
            "Henghui Ding"
        ],
        "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
        "abstract": "arXiv:2507.02857v1 Announce Type: new  Abstract: Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.",
        "arxiv_id": "2507.02857",
        "ARXIVID": "2507.02857",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a framework for animating conditional images with motion control, integrating image and motion understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02393": {
        "authors": [
            "Seokyeong Lee",
            "Sithu Aung",
            "Junyong Choi",
            "Seungryong Kim",
            "Ig-Jae Kim",
            "Junghyun Cho"
        ],
        "title": "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection",
        "abstract": "arXiv:2507.02393v1 Announce Type: new  Abstract: Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.",
        "arxiv_id": "2507.02393",
        "ARXIVID": "2507.02393",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel pseudo-labeling framework for monocular 3D object detection, addressing challenges in data scarcity and occlusion.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02691": {
        "authors": [
            "Xiangyang Luo",
            "Ye Zhu",
            "Yunfei Liu",
            "Lijian Lin",
            "Cong Wan",
            "Zijian Cai",
            "Shao-Lun Huang",
            "Yu Li"
        ],
        "title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation",
        "abstract": "arXiv:2507.02691v1 Announce Type: new  Abstract: Video face swapping aims to address two primary challenges: effectively transferring the source identity to the target video and accurately preserving the dynamic attributes of the target face, such as head poses, facial expressions, lip-sync, \\etc. Existing methods mainly focus on achieving high-quality identity transfer but often fall short in maintaining the dynamic attributes of the target face, leading to inconsistent results. We attribute this issue to the inherent coupling of facial appearance and motion in videos. To address this, we propose CanonSwap, a novel video face-swapping framework that decouples motion information from appearance information. Specifically, CanonSwap first eliminates motion-related information, enabling identity modification within a unified canonical space. Subsequently, the swapped feature is reintegrated into the original video space, ensuring the preservation of the target face's dynamic attributes. To further achieve precise identity transfer with minimal artifacts and enhanced realism, we design a Partial Identity Modulation module that adaptively integrates source identity features using a spatial mask to restrict modifications to facial regions. Additionally, we introduce several fine-grained synchronization metrics to comprehensively evaluate the performance of video face swapping methods. Extensive experiments demonstrate that our method significantly outperforms existing approaches in terms of visual quality, temporal consistency, and identity preservation. Our project page are publicly available at https://luoxyhappy.github.io/CanonSwap/.",
        "arxiv_id": "2507.02691",
        "ARXIVID": "2507.02691",
        "COMMENT": "Matches criteria 6 as it focuses on video face swapping, addressing challenges in dynamic attribute preservation and identity transfer.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02860": {
        "authors": [
            "Xin Zhou",
            "Dingkang Liang",
            "Kaijin Chen",
            "Tianrui Feng",
            "Xiwu Chen",
            "Hongkai Lin",
            "Yikang Ding",
            "Feiyang Tan",
            "Hengshuang Zhao",
            "Xiang Bai"
        ],
        "title": "Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching",
        "abstract": "arXiv:2507.02860v1 Announce Type: new  Abstract: Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.",
        "arxiv_id": "2507.02860",
        "ARXIVID": "2507.02860",
        "COMMENT": "Matches criteria 6 as it proposes a novel acceleration framework for video generation models, which is relevant to video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02479": {
        "authors": [
            "Teng Fu",
            "Yuwen Chen",
            "Zhuofan Chen",
            "Mengyang Zhao",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios",
        "abstract": "arXiv:2507.02479v1 Announce Type: new  Abstract: Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .",
        "arxiv_id": "2507.02479",
        "ARXIVID": "2507.02479",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark dataset for multi-pedestrian tracking in complex scenarios.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.02813": {
        "authors": [
            "Fangfu Liu",
            "Hao Li",
            "Jiawei Chi",
            "Hanyang Wang",
            "Minghui Yang",
            "Fudong Wang",
            "Yueqi Duan"
        ],
        "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion",
        "abstract": "arXiv:2507.02813v1 Announce Type: new  Abstract: Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.",
        "arxiv_id": "2507.02813",
        "ARXIVID": "2507.02813",
        "COMMENT": "Matches criterion 5 as it combines 3D reconstruction with language-embedded scene understanding, integrating image/video tasks with language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.02299": {
        "authors": [
            "Yunhan Yang",
            "Shuo Chen",
            "Yukun Huang",
            "Xiaoyang Wu",
            "Yuan-Chen Guo",
            "Edmund Y. Lam",
            "Hengshuang Zhao",
            "Tong He",
            "Xihui Liu"
        ],
        "title": "DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation",
        "abstract": "arXiv:2507.02299v1 Announce Type: new  Abstract: Recent advancements in leveraging pre-trained 2D diffusion models achieve the generation of high-quality novel views from a single in-the-wild image. However, existing works face challenges in producing controllable novel views due to the lack of information from multiple views. In this paper, we present DreamComposer++, a flexible and scalable framework designed to improve current view-aware diffusion models by incorporating multi-view conditions. Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to extract 3D representations of an object from various views. These representations are then aggregated and rendered into the latent features of target view through the multi-view feature fusion module. Finally, the obtained features of target view are integrated into pre-trained image or video diffusion models for novel view synthesis. Experimental results demonstrate that DreamComposer++ seamlessly integrates with cutting-edge view-aware diffusion models and enhances their abilities to generate controllable novel views from multi-view conditions. This advancement facilitates controllable 3D object reconstruction and enables a wide range of applications.",
        "arxiv_id": "2507.02299",
        "ARXIVID": "2507.02299",
        "COMMENT": "Matches criterion 5 as it focuses on integrating multi-view 3D representations with diffusion models, combining image understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.02863": {
        "authors": [
            "Yuqi Wu",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory",
        "abstract": "arXiv:2507.02863v1 Announce Type: new  Abstract: Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code is available at: https://github.com/YkiWu/Point3R.",
        "arxiv_id": "2507.02863",
        "ARXIVID": "2507.02863",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for dense 3D scene reconstruction with explicit spatial pointer memory, relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.02373": {
        "authors": [
            "Xizhe Xue",
            "Yang Zhou",
            "Dawei Yan",
            "Ying Li",
            "Haokui Zhang",
            "Rong Xiao"
        ],
        "title": "UVLM: Benchmarking Video Language Model for Underwater World Understanding",
        "abstract": "arXiv:2507.02373v1 Announce Type: new  Abstract: Recently, the remarkable success of large language models (LLMs) has achieved a profound impact on the field of artificial intelligence. Numerous advanced works based on LLMs have been proposed and applied in various scenarios. Among them, video language models (VidLMs) are particularly widely used. However, existing works primarily focus on terrestrial scenarios, overlooking the highly demanding application needs of underwater observation. To overcome this gap, we introduce UVLM, an under water observation benchmark which is build through a collaborative approach combining human expertise and AI models. To ensure data quality, we have conducted in-depth considerations from multiple perspectives. First, to address the unique challenges of underwater environments, we selected videos that represent typical underwater challenges including light variations, water turbidity, and diverse viewing angles to construct the dataset. Second, to ensure data diversity, the dataset covers a wide range of frame rates, resolutions, 419 classes of marine animals, and various static plants and terrains. Next, for task diversity, we adopted a structured design where observation targets are categorized into two major classes: biological and environmental. Each category includes content observation and change/action observation, totaling 20 distinct task types. Finally, we designed several challenging evaluation metrics to enable quantitative comparison and analysis of different methods. Experiments on two representative VidLMs demonstrate that fine-tuning VidLMs on UVLM significantly improves underwater world understanding while also showing potential for slight improvements on existing in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and prompt engineering will be released publicly.",
        "arxiv_id": "2507.02373",
        "ARXIVID": "2507.02373",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark for video understanding in underwater environments, focusing on video-based tasks and evaluations.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.02321": {
        "authors": [
            "Nina Konovalova",
            "Maxim Nikolaev",
            "Andrey Kuznetsov",
            "Aibek Alanov"
        ],
        "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback",
        "abstract": "arXiv:2507.02321v1 Announce Type: new  Abstract: Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).",
        "arxiv_id": "2507.02321",
        "ARXIVID": "2507.02321",
        "COMMENT": "Matches criterion 1 as it presents a novel methodological improvement in spatial reasoning for text-to-image diffusion models, focusing on spatial control.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.02074": {
        "authors": [
            "Sanjeda Akter",
            "Ibne Farabi Shihab",
            "Anuj Sharma"
        ],
        "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges",
        "abstract": "arXiv:2507.02074v1 Announce Type: new  Abstract: Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision-language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast-growing intersection of video understanding and foundation models.",
        "arxiv_id": "2507.02074",
        "ARXIVID": "2507.02074",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a survey paper on crash detection in video using large language models and vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.02379": {
        "authors": [
            "Mingyu Wu",
            "Zhaoguo Wang",
            "Jiabin Wang",
            "Zhiyuan Dong",
            "Jingkai Yang",
            "Qingting Li",
            "Tianyu Huang",
            "Lei Zhao",
            "Mingqiang Li",
            "Fei Wang",
            "Chunhai Fan",
            "Haibo Chen"
        ],
        "title": "An AI-native experimental laboratory for autonomous biomolecular engineering",
        "abstract": "arXiv:2507.02379v1 Announce Type: new  Abstract: Autonomous scientific research, capable of independently conducting complex experiments and serving non-specialists, represents a long-held aspiration. Achieving it requires a fundamental paradigm shift driven by artificial intelligence (AI). While autonomous experimental systems are emerging, they remain confined to areas featuring singular objectives and well-defined, simple experimental workflows, such as chemical synthesis and catalysis. We present an AI-native autonomous laboratory, targeting highly complex scientific experiments for applications like autonomous biomolecular engineering. This system autonomously manages instrumentation, formulates experiment-specific procedures and optimization heuristics, and concurrently serves multiple user requests. Founded on a co-design philosophy of models, experiments, and instruments, the platform supports the co-evolution of AI models and the automation system. This establishes an end-to-end, multi-user autonomous laboratory that handles complex, multi-objective experiments across diverse instrumentation. Our autonomous laboratory supports fundamental nucleic acid functions-including synthesis, transcription, amplification, and sequencing. It also enables applications in fields such as disease diagnostics, drug development, and information storage. Without human intervention, it autonomously optimizes experimental performance to match state-of-the-art results achieved by human scientists. In multi-user scenarios, the platform significantly improves instrument utilization and experimental efficiency. This platform paves the way for advanced biomaterials research to overcome dependencies on experts and resource barriers, establishing a blueprint for science-as-a-service at scale.",
        "arxiv_id": "2507.02379",
        "ARXIVID": "2507.02379",
        "COMMENT": "Does not match any specific criteria but is related to autonomous systems and AI-driven experimentation, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2507.02546": {
        "authors": [
            "Ruicheng Wang",
            "Sicheng Xu",
            "Yue Dong",
            "Yu Deng",
            "Jianfeng Xiang",
            "Zelong Lv",
            "Guangzhong Sun",
            "Xin Tong",
            "Jiaolong Yang"
        ],
        "title": "MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details",
        "abstract": "arXiv:2507.02546v1 Announce Type: new  Abstract: We propose MoGe-2, an advanced open-domain geometry estimation model that recovers a metric scale 3D point map of a scene from a single image. Our method builds upon the recent monocular geometry estimation approach, MoGe, which predicts affine-invariant point maps with unknown scales. We explore effective strategies to extend MoGe for metric geometry prediction without compromising the relative geometry accuracy provided by the affine-invariant point representation. Additionally, we discover that noise and errors in real data diminish fine-grained detail in the predicted geometry. We address this by developing a unified data refinement approach that filters and completes real data from different sources using sharp synthetic labels, significantly enhancing the granularity of the reconstructed geometry while maintaining the overall accuracy. We train our model on a large corpus of mixed datasets and conducted comprehensive evaluations, demonstrating its superior performance in achieving accurate relative geometry, precise metric scale, and fine-grained detail recovery -- capabilities that no previous methods have simultaneously achieved.",
        "arxiv_id": "2507.02546",
        "ARXIVID": "2507.02546",
        "COMMENT": "Does not match any specific criteria but is related to geometry estimation in computer vision, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02437": {
        "authors": [
            "Wei Li",
            "Jingyang Zhang",
            "Lihao Liu",
            "Guoan Wang",
            "Junjun He",
            "Yang Chen",
            "Lixu Gu"
        ],
        "title": "F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning",
        "abstract": "arXiv:2507.02437v1 Announce Type: new  Abstract: Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a source model to unseen medical sites using unlabeled test data, due to the high cost of data annotation. Existing TTA methods consider scenarios where data from one or multiple domains arrives in complete domain units. However, in clinical practice, data usually arrives in domain fragments of arbitrary lengths and in random arrival orders, due to resource constraints and patient variability. This paper investigates a practical Free-Form Test-Time Adaptation (F$^{2}$TTA) task, where a source model is adapted to such free-form domain fragments, with shifts occurring between fragments unpredictably. In this setting, these shifts could distort the adaptation process. To address this problem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT) framework. I-DiPT employs an image-invariant prompt to explore domain-invariant representations for mitigating the unpredictable shifts, and an image-specific prompt to adapt the source model to each test image from the incoming fragments. The prompts may suffer from insufficient knowledge representation since only one image is available for training. To overcome this limitation, we first introduce Uncertainty-oriented Masking (UoM), which encourages the prompts to extract sufficient information from the incoming image via masked consistency learning driven by the uncertainty of the source model representations. Then, we further propose a Parallel Graph Distillation (PGD) method that reuses knowledge from historical image-specific and image-invariant prompts through parallel graph networks. Experiments on breast cancer and glaucoma classification demonstrate the superiority of our method over existing TTA approaches in F$^{2}$TTA. Code is available at https://github.com/mar-cry/F2TTA.",
        "arxiv_id": "2507.02437",
        "ARXIVID": "2507.02437",
        "COMMENT": "Does not match any specific criteria but is related to domain adaptation in medical imaging, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02660": {
        "authors": [
            "Deepak Narayan Gadde",
            "Keerthan Kopparam Radhakrishna",
            "Vaisakh Naduvodi Viswambharan",
            "Aman Kumar",
            "Djones Lettnin",
            "Wolfgang Kunz",
            "Sebastian Simon"
        ],
        "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification",
        "abstract": "arXiv:2507.02660v1 Announce Type: new  Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.",
        "arxiv_id": "2507.02660",
        "ARXIVID": "2507.02660",
        "COMMENT": "Does not match any specific criteria but discusses LLMs in hardware design, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02576": {
        "authors": [
            "Alina F. Dima",
            "Suprosanna Shit",
            "Huaqi Qiu",
            "Robbie Holland",
            "Tamara T. Mueller",
            "Fabio Antonio Musio",
            "Kaiyuan Yang",
            "Bjoern Menze",
            "Rickmer Braren",
            "Marcus Makowski",
            "Daniel Rueckert"
        ],
        "title": "Parametric shape models for vessels learned from segmentations via differentiable voxelization",
        "abstract": "arXiv:2507.02576v1 Announce Type: new  Abstract: Vessels are complex structures in the body that have been studied extensively in multiple representations. While voxelization is the most common of them, meshes and parametric models are critical in various applications due to their desirable properties. However, these representations are typically extracted through segmentations and used disjointly from each other. We propose a framework that joins the three representations under differentiable transformations. By leveraging differentiable voxelization, we automatically extract a parametric shape model of the vessels through shape-to-segmentation fitting, where we learn shape parameters from segmentations without the explicit need for ground-truth shape parameters. The vessel is parametrized as centerlines and radii using cubic B-splines, ensuring smoothness and continuity by construction. Meshes are differentiably extracted from the learned shape parameters, resulting in high-fidelity meshes that can be manipulated post-fit. Our method can accurately capture the geometry of complex vessels, as demonstrated by the volumetric fits in experiments on aortas, aneurysms, and brain vessels.",
        "arxiv_id": "2507.02576",
        "ARXIVID": "2507.02576",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and modeling, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02841": {
        "authors": [
            "Kaiyi Zhang",
            "Ang Lv",
            "Jinpeng Li",
            "Yongbo Wang",
            "Feng Wang",
            "Haoyuan Hu",
            "Rui Yan"
        ],
        "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason",
        "abstract": "arXiv:2507.02841v1 Announce Type: new  Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks.",
        "arxiv_id": "2507.02841",
        "ARXIVID": "2507.02841",
        "COMMENT": "Does not match any specific criterion but is relevant to reinforcement learning and reasoning, which are tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02687": {
        "authors": [
            "JungWoo Chae",
            "Jiyoon Kim",
            "JaeWoong Choi",
            "Kyungyul Kim",
            "Sangheum Hwang"
        ],
        "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data",
        "abstract": "arXiv:2507.02687v1 Announce Type: new  Abstract: Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and regularizing the model's internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2)Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.",
        "arxiv_id": "2507.02687",
        "ARXIVID": "2507.02687",
        "COMMENT": "Does not match any specific criterion. Focuses on personalizing diffusion models, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02726": {
        "authors": [
            "Matthieu Zimmer",
            "Xiaotong Ji",
            "Rasul Tutunov",
            "Anthony Bordg",
            "Jun Wang",
            "Haitham Bou Ammar"
        ],
        "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
        "abstract": "arXiv:2507.02726v1 Announce Type: new  Abstract: Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.",
        "arxiv_id": "2507.02726",
        "ARXIVID": "2507.02726",
        "COMMENT": "Does not match any specific criteria but is related to reasoning and planning in LLMs, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.02773": {
        "authors": [
            "Yuzhang Xie",
            "Hejie Cui",
            "Ziyang Zhang",
            "Jiaying Lu",
            "Kai Shu",
            "Fadi Nahab",
            "Xiao Hu",
            "Carl Yang"
        ],
        "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs",
        "abstract": "arXiv:2507.02773v1 Announce Type: new  Abstract: Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.",
        "arxiv_id": "2507.02773",
        "ARXIVID": "2507.02773",
        "COMMENT": "Does not closely match any specific criterion but is related to multi-agent systems and medical diagnosis, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02554": {
        "authors": [
            "Edan Toledo",
            "Karen Hambardzumyan",
            "Martin Josifoski",
            "Rishi Hazra",
            "Nicolas Baldwin",
            "Alexis Audran-Reiss",
            "Michael Kuchnik",
            "Despoina Magka",
            "Minqi Jiang",
            "Alisia Maria Lupidi",
            "Andrei Lupu",
            "Roberta Raileanu",
            "Kelvin Niu",
            "Tatiana Shavrina",
            "Jean-Christophe Gagnon-Audet",
            "Michael Shvartsman",
            "Shagun Sodhani",
            "Alexander H. Miller",
            "Abhishek Charnalia",
            "Derek Dunfield",
            "Carole-Jean Wu",
            "Pontus Stenetorp",
            "Nicola Cancedda",
            "Jakob Nicolaus Foerster",
            "Yoram Bachrach"
        ],
        "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench",
        "abstract": "arXiv:2507.02554v1 Announce Type: new  Abstract: AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.",
        "arxiv_id": "2507.02554",
        "ARXIVID": "2507.02554",
        "COMMENT": "Does not closely match any specific criterion but is related to automated machine learning, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02616": {
        "authors": [
            "Tianqi Shang",
            "Weiqing He",
            "Charles Zheng",
            "Lingyao Li",
            "Li Shen",
            "Bingxin Zhao"
        ],
        "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making",
        "abstract": "arXiv:2507.02616v1 Announce Type: new  Abstract: The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.",
        "arxiv_id": "2507.02616",
        "ARXIVID": "2507.02616",
        "COMMENT": "Does not closely match any specific criterion but is related to multi-agent systems and decision-making, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02076": {
        "authors": [
            "Mohammad Ali Alomrani",
            "Yingxue Zhang",
            "Derek Li",
            "Qianyi Sun",
            "Soumyasundar Pal",
            "Zhanguang Zhang",
            "Yaochen Hu",
            "Rohan Deepak Ajwani",
            "Antonios Valkanas",
            "Raika Karimi",
            "Peng Cheng",
            "Yunzhou Wang",
            "Pengyi Liao",
            "Hanrui Huang",
            "Bin Wang",
            "Jianye Hao",
            "Mark Coates"
        ],
        "title": "Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs",
        "abstract": "arXiv:2507.02076v1 Announce Type: new  Abstract: Large language models (LLMs) have rapidly progressed into general-purpose agents capable of solving a broad spectrum of tasks. However, current models remain inefficient at reasoning: they apply fixed inference-time compute regardless of task complexity, often overthinking simple problems while underthinking hard ones. This survey presents a comprehensive review of efficient test-time compute (TTC) strategies, which aim to improve the computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy that distinguishes between L1-controllability, methods that operate under fixed compute budgets, and L2-adaptiveness, methods that dynamically scale inference based on input difficulty or model confidence. We benchmark leading proprietary LLMs across diverse datasets, highlighting critical trade-offs between reasoning performance and token usage. Compared to prior surveys on efficient reasoning, our review emphasizes the practical control, adaptability, and scalability of TTC methods. Finally, we discuss emerging trends such as hybrid thinking models and identify key challenges for future work towards making LLMs more computationally efficient, robust, and responsive to user constraints.",
        "arxiv_id": "2507.02076",
        "ARXIVID": "2507.02076",
        "COMMENT": "Does not match any specific criteria but is a survey on efficient reasoning in LLMs, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02395": {
        "authors": [
            "Byung Hyun Lee",
            "Wongi Jeong",
            "Woojae Han",
            "Kyoungbun Lee",
            "Se Young Chun"
        ],
        "title": "Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis",
        "abstract": "arXiv:2507.02395v1 Announce Type: new  Abstract: Multiple instance learning (MIL) significantly reduced annotation costs via bag-level weak labels for large-scale images, such as histopathological whole slide images (WSIs). However, its adaptability to continual tasks with minimal forgetting has been rarely explored, especially on instance classification for localization. Weakly incremental learning for semantic segmentation has been studied for continual localization, but it focused on natural images, leveraging global relationships among hundreds of small patches (e.g., $16 \\times 16$) using pre-trained models. This approach seems infeasible for MIL localization due to enormous amounts ($\\sim 10^5$) of large patches (e.g., $256 \\times 256$) and no available global relationships such as cancer cells. To address these challenges, we propose Continual Multiple Instance Learning with Enhanced Localization (CoMEL), an MIL framework for both localization and adaptability with minimal forgetting. CoMEL consists of (1) Grouped Double Attention Transformer (GDAT) for efficient instance encoding, (2) Bag Prototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling, and (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting in both bag and instance classification. Extensive experiments on three public WSI datasets demonstrate superior performance of CoMEL, outperforming the prior arts by up to $11.00\\%$ in bag-level accuracy and up to $23.4\\%$ in localization accuracy under the continual MIL setup.",
        "arxiv_id": "2507.02395",
        "ARXIVID": "2507.02395",
        "COMMENT": "Does not match any specific criterion but is relevant to machine learning methods for medical imaging, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02353": {
        "authors": [
            "Bowen Chen",
            "Zhao Wang",
            "Shingo Takamatsu"
        ],
        "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent",
        "abstract": "arXiv:2507.02353v1 Announce Type: new  Abstract: Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.",
        "arxiv_id": "2507.02353",
        "ARXIVID": "2507.02353",
        "COMMENT": "Does not match any specific criterion but is relevant to LLM-based applications, which are tangentially related to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02311": {
        "authors": [
            "Le Xu",
            "Qi Zhang",
            "Qixian Zhang",
            "Hongyun Zhang",
            "Duoqian Miao",
            "Cairong Zhao"
        ],
        "title": "Perception Activator: An intuitive and portable framework for brain cognitive exploration",
        "abstract": "arXiv:2507.02311v1 Announce Type: new  Abstract: Recent advances in brain-vision decoding have driven significant progress, reconstructing with high fidelity perceived visual stimuli from neural activity, e.g., functional magnetic resonance imaging (fMRI), in the human visual cortex. Most existing methods decode the brain signal using a two-level strategy, i.e., pixel-level and semantic-level. However, these methods rely heavily on low-level pixel alignment yet lack sufficient and fine-grained semantic alignment, resulting in obvious reconstruction distortions of multiple semantic objects. To better understand the brain's visual perception patterns and how current decoding models process semantic objects, we have developed an experimental framework that uses fMRI representations as intervention conditions. By injecting these representations into multi-scale image features via cross-attention, we compare both downstream performance and intermediate feature changes on object detection and instance segmentation tasks with and without fMRI information. Our results demonstrate that incorporating fMRI signals enhances the accuracy of downstream detection and segmentation, confirming that fMRI contains rich multi-object semantic cues and coarse spatial localization information-elements that current models have yet to fully exploit or integrate.",
        "arxiv_id": "2507.02311",
        "ARXIVID": "2507.02311",
        "COMMENT": "Does not match any specific criterion but is relevant to brain-vision decoding, which is tangentially related to vision and multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02714": {
        "authors": [
            "Yuxuan Wang",
            "Tianwei Cao",
            "Huayu Zhang",
            "Zhongjiang He",
            "Kongming Liang",
            "Zhanyu Ma"
        ],
        "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models",
        "abstract": "arXiv:2507.02714v1 Announce Type: new  Abstract: Image generation has achieved remarkable progress with the development of large-scale text-to-image models, especially diffusion-based models. However, generating human images with plausible details, such as faces or hands, remains challenging due to insufficient supervision of local regions during training. To address this issue, we propose FairHuman, a multi-objective fine-tuning approach designed to enhance both global and local generation quality fairly. Specifically, we first construct three learning objectives: a global objective derived from the default diffusion objective function and two local objectives for hands and faces based on pre-annotated positional priors. Subsequently, we derive the optimal parameter updating strategy under the guidance of the Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware optimization for this multi-objective problem. Based on this, our proposed method can achieve significant improvements in generating challenging local details while maintaining overall quality. Extensive experiments showcase the effectiveness of our method in improving the performance of human image generation under different scenarios.",
        "arxiv_id": "2507.02714",
        "ARXIVID": "2507.02714",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02197": {
        "authors": [
            "Amogh Mannekote",
            "Adam Davies",
            "Guohao Li",
            "Kristy Elizabeth Boyer",
            "ChengXiang Zhai",
            "Bonnie J Dorr",
            "Francesco Pinto"
        ],
        "title": "Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust",
        "abstract": "arXiv:2507.02197v1 Announce Type: new  Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic data for human behavioral research, ensuring that their outputs remain coherent with their assigned roles has become a critical concern. In this paper, we investigate how consistently LLM-based role-playing agents' stated beliefs about the behavior of the people they are asked to role-play (\"what they say\") correspond to their actual behavior during role-play (\"how they act\"). Specifically, we establish an evaluation framework to rigorously measure how well beliefs obtained by prompting the model can predict simulation outcomes in advance. Using an augmented version of the GenAgents persona bank and the Trust Game (a standard economic game used to quantify players' trust and reciprocity), we introduce a belief-behavior consistency metric to systematically investigate how it is affected by factors such as: (1) the types of beliefs we elicit from LLMs, like expected outcomes of simulations versus task-relevant attributes of individual characters LLMs are asked to simulate; (2) when and how we present LLMs with relevant information about Trust Game; and (3) how far into the future we ask the model to forecast its actions. We also explore how feasible it is to impose a researcher's own theoretical priors in the event that the originally elicited beliefs are misaligned with research objectives. Our results reveal systematic inconsistencies between LLMs' stated (or imposed) beliefs and the outcomes of their role-playing simulation, at both an individual- and population-level. Specifically, we find that, even when models appear to encode plausible beliefs, they may fail to apply them in a consistent way. These findings highlight the need to identify how and when LLMs' stated beliefs align with their simulated behavior, allowing researchers to use LLM-based agents appropriately in behavioral studies.",
        "arxiv_id": "2507.02197",
        "ARXIVID": "2507.02197",
        "COMMENT": "Does not match any specific criteria but is related to LLM-based simulations and behavioral studies, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02222": {
        "authors": [
            "Tian Gao",
            "Zhiyuan Zhang",
            "Kaijie Yin",
            "Xu-Cheng Zhong",
            "Hui Kong"
        ],
        "title": "High-Fidelity Differential-information Driven Binary Vision Transformer",
        "abstract": "arXiv:2507.02222v1 Announce Type: new  Abstract: The binarization of vision transformers (ViTs) offers a promising approach to addressing the trade-off between high computational/storage demands and the constraints of edge-device deployment. However, existing binary ViT methods often suffer from severe performance degradation or rely heavily on full-precision modules. To address these issues, we propose DIDB-ViT, a novel binary ViT that is highly informative while maintaining the original ViT architecture and computational efficiency. Specifically, we design an informative attention module incorporating differential information to mitigate information loss caused by binarization and enhance high-frequency retention. To preserve the fidelity of the similarity calculations between binary Q and K tensors, we apply frequency decomposition using the discrete Haar wavelet and integrate similarities across different frequencies. Additionally, we introduce an improved RPReLU activation function to restructure the activation distribution, expanding the model's representational capacity. Experimental results demonstrate that our DIDB-ViT significantly outperforms state-of-the-art network quantization methods in multiple ViT architectures, achieving superior image classification and segmentation performance.",
        "arxiv_id": "2507.02222",
        "ARXIVID": "2507.02222",
        "COMMENT": "Does not match any specific criteria but is related to vision transformers and quantization, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02253": {
        "authors": [
            "Jungkoo Kang"
        ],
        "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation",
        "abstract": "arXiv:2507.02253v1 Announce Type: new  Abstract: Progress in enhancing large language model (LLM) planning and reasoning capabilities is significantly hampered by the bottleneck of scalable, reliable data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully automated system for parametrically generating planning problems - expressed in natural language, a structured intermediate representation, and formal PDDL - and rigorously evaluating the quality of generated plans. I demonstrate NL2FLOW's capabilities by generating a dataset of 2296 problems in the automated workflow generation domain and evaluating multiple open-sourced, instruct-tuned LLMs. My results reveal that the highest performing models achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible solutions. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Notably, I observed that the highest success rate for translating natural language into a JSON representation of a plan was lower than the highest rate of generating a valid plan directly. This suggests that unnecessarily decomposing the reasoning task - introducing intermediate translation steps - may actually degrade performance, implying a benefit to models capable of reasoning directly from natural language to action. As I scale LLM reasoning to increasingly complex problems, the bottlenecks and sources of error within these systems will inevitably shift. Therefore, a dynamic understanding of these limitations - and the tools to systematically reveal them - will be crucial for unlocking the full potential of LLMs as intelligent problem solvers.",
        "arxiv_id": "2507.02253",
        "ARXIVID": "2507.02253",
        "COMMENT": "Does not match any specific criteria but is related to planning and reasoning in LLMs, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02751": {
        "authors": [
            "Mingxin Liu",
            "Peiyuan Zhang",
            "Yuan Liu",
            "Wei Zhang",
            "Yue Zhou",
            "Ning Liao",
            "Ziyang Gong",
            "Junwei Luo",
            "Zhirui Wang",
            "Yi Yu",
            "Xue Yang"
        ],
        "title": "Partial Weakly-Supervised Oriented Object Detection",
        "abstract": "arXiv:2507.02751v1 Announce Type: new  Abstract: The growing demand for oriented object detection (OOD) across various domains has driven significant research in this area. However, the high cost of dataset annotation remains a major concern. Current mainstream OOD algorithms can be mainly categorized into three types: (1) fully supervised methods using complete oriented bounding box (OBB) annotations, (2) semi-supervised methods using partial OBB annotations, and (3) weakly supervised methods using weak annotations such as horizontal boxes or points. However, these algorithms inevitably increase the cost of models in terms of annotation speed or annotation cost. To address this issue, we propose:(1) the first Partial Weakly-Supervised Oriented Object Detection (PWOOD) framework based on partially weak annotations (horizontal boxes or single points), which can efficiently leverage large amounts of unlabeled data, significantly outperforming weakly supervised algorithms trained with partially weak annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware Student (OS-Student) model capable of learning orientation and scale information with only a small amount of orientation-agnostic or scale-agnostic weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF) to reduce the model's sensitivity to static filtering thresholds. Comprehensive experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD framework performs comparably to, or even surpasses, traditional semi-supervised algorithms.",
        "arxiv_id": "2507.02751",
        "ARXIVID": "2507.02751",
        "COMMENT": "Does not closely match any specific criterion but is related to object detection, which is tangentially relevant to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}