{
    "2508.03404": {
        "authors": [
            "Xinlei Yu",
            "Zhangquan Chen",
            "Yudong Zhang",
            "Shilin Lu",
            "Ruolin Shen",
            "Jiangning Zhang",
            "Xiaobin Hu",
            "Yanwei Fu",
            "Shuicheng Yan"
        ],
        "title": "Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling",
        "abstract": "arXiv:2508.03404v1 Announce Type: new  Abstract: Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on document-based tasks. To address this, we propose MACT, a Multi-Agent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct small-scale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with a smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: https://github.com/YU-deep/MACT.git.",
        "arxiv_id": "2508.03404",
        "ARXIVID": "2508.03404",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a novel multi-agent framework for visual document understanding and VQA, focusing on vision-language integration.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.03643": {
        "authors": [
            "Xiangyu Sun",
            "Haoyi jiang",
            "Liu Liu",
            "Seungtae Nam",
            "Gyeongjin Kang",
            "Xinjie wang",
            "Wei Sui",
            "Zhizhong Su",
            "Wenyu Liu",
            "Xinggang Wang",
            "Eunbyung Park"
        ],
        "title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images",
        "abstract": "arXiv:2508.03643v1 Announce Type: new  Abstract: Reconstructing and semantically interpreting 3D scenes from sparse 2D views remains a fundamental challenge in computer vision. Conventional methods often decouple semantic understanding from reconstruction or necessitate costly per-scene optimization, thereby restricting their scalability and generalizability. In this paper, we introduce Uni3R, a novel feed-forward framework that jointly reconstructs a unified 3D scene representation enriched with open-vocabulary semantics, directly from unposed multi-view images. Our approach leverages a Cross-View Transformer to robustly integrate information across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian primitives endowed with semantic feature fields. This unified representation facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic segmentation, and depth prediction, all within a single, feed-forward pass. Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D scene reconstruction and understanding. The code is available at https://github.com/HorizonRobotics/Uni3R.",
        "arxiv_id": "2508.03643",
        "ARXIVID": "2508.03643",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a unified framework for 3D reconstruction and semantic understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.03669": {
        "authors": [
            "Katherine Liu",
            "Sergey Zakharov",
            "Dian Chen",
            "Takuya Ikeda",
            "Greg Shakhnarovich",
            "Adrien Gaidon",
            "Rares Ambrus"
        ],
        "title": "OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World",
        "abstract": "arXiv:2508.03669v1 Announce Type: new  Abstract: We would like to estimate the pose and full shape of an object from a single observation, without assuming known 3D model or category. In this work, we propose OmniShape, the first method of its kind to enable probabilistic pose and shape estimation. OmniShape is based on the key insight that shape completion can be decoupled into two multi-modal distributions: one capturing how measurements project into a normalized object reference frame defined by the dataset and the other modelling a prior over object geometries represented as triplanar neural fields. By training separate conditional diffusion models for these two distributions, we enable sampling multiple hypotheses from the joint pose and shape distribution. OmniShape demonstrates compelling performance on challenging real world datasets. Project website: https://tri-ml.github.io/omnishape",
        "arxiv_id": "2508.03669",
        "ARXIVID": "2508.03669",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on probabilistic pose and shape estimation, which is a novel methodological improvement in spatial reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.03690": {
        "authors": [
            "Youquan Liu",
            "Lingdong Kong",
            "Weidong Yang",
            "Ao Liang",
            "Jianxiong Gao",
            "Yang Wu",
            "Xiang Xu",
            "Xin Li",
            "Linfeng Li",
            "Runnan Chen",
            "Ben Fei"
        ],
        "title": "Veila: Panoramic LiDAR Generation from a Monocular RGB Image",
        "abstract": "arXiv:2508.03690v1 Announce Type: new  Abstract: Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, which remains an open problem. However, it faces three core challenges: (i) semantic and depth cues from RGB are vary spatially, complicating reliable conditioning generation; (ii) modality gaps between RGB appearance and LiDAR geometry amplify alignment errors under noisy diffusion; and (iii) maintaining structural coherence between monocular RGB and panoramic LiDAR is challenging, particularly in non-overlap regions between images and LiDAR. To address these challenges, we propose Veila, a novel conditional diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism (CACM) that strengthens RGB conditioning by adaptively balancing semantic and depth cues according to their local reliability; a Geometric Cross-Modal Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) for enforcing global structural consistency across monocular RGB and panoramic LiDAR. Additionally, we introduce two metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to evaluate alignment quality across modalities. Experiments on nuScenes, SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila achieves state-of-the-art generation fidelity and cross-modal consistency, while enabling generative data augmentation that improves downstream LiDAR semantic segmentation.",
        "arxiv_id": "2508.03690",
        "ARXIVID": "2508.03690",
        "COMMENT": "This paper closely matches criterion 3 as it introduces a novel method (Veila) for generating panoramic LiDAR data, which is relevant to robotics and embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.02841": {
        "authors": [
            "Ziruo Yi",
            "Jinyu Liu",
            "Ting Xiao",
            "Mark V. Albert"
        ],
        "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering",
        "abstract": "arXiv:2508.02841v1 Announce Type: new  Abstract: Radiology visual question answering (RVQA) provides precise answers to questions about chest X-ray images, alleviating radiologists' workload. While recent methods based on multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have shown promising progress in RVQA, they still face challenges in factual accuracy, hallucinations, and cross-modal misalignment. We introduce a multi-agent system (MAS) designed to support complex reasoning in RVQA, with specialized agents for context understanding, multimodal reasoning, and answer validation. We evaluate our system on a challenging RVQA set curated via model disagreement filtering, comprising consistently hard cases across multiple MLLMs. Extensive experiments demonstrate the superiority and effectiveness of our system over strong MLLM baselines, with a case study illustrating its reliability and interpretability. This work highlights the potential of multi-agent approaches to support explainable and trustworthy clinical AI applications that require complex reasoning.",
        "arxiv_id": "2508.02841",
        "ARXIVID": "2508.02841",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a multi-agent system for radiology visual question answering, focusing on multimodal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.03177": {
        "authors": [
            "Zhaoxu Li",
            "Chenqi Kong",
            "Yi Yu",
            "Qiangqiang Wu",
            "Xinghao Jiang",
            "Ngai-Man Cheung",
            "Bihan Wen",
            "Alex Kot",
            "Xudong Jiang"
        ],
        "title": "SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision",
        "abstract": "arXiv:2508.03177v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) recently achieve significant breakthroughs in understanding complex visual-textual contexts. However, hallucination issues still limit their real-world applicability. Although previous mitigation methods effectively reduce hallucinations in photographic images, they largely overlook the potential risks posed by stylized images, which play crucial roles in critical scenarios such as game scene understanding, art education, and medical analysis. In this work, we first construct a dataset comprising photographic images and their corresponding stylized versions with carefully annotated caption labels. We then conduct head-to-head comparisons on both discriminative and generative tasks by benchmarking 13 advanced LVLMs on the collected datasets. Our findings reveal that stylized images tend to induce significantly more hallucinations than their photographic counterparts. To address this issue, we propose Style-Aware Visual Early Revision SAVER, a novel mechanism that dynamically adjusts LVLMs' final outputs based on the token-level visual attention patterns, leveraging early-layer feedback to mitigate hallucinations caused by stylized images. Extensive experiments demonstrate that SAVER achieves state-of-the-art performance in hallucination mitigation across various models, datasets, and tasks.",
        "arxiv_id": "2508.03177",
        "ARXIVID": "2508.03177",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it addresses hallucination issues in large vision-language models, focusing on style-aware visual early revision.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.02890": {
        "authors": [
            "Rongxin Jiang",
            "Robert Long",
            "Chenghao Gu",
            "Mingrui Yan"
        ],
        "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction",
        "abstract": "arXiv:2508.02890v1 Announce Type: new  Abstract: This paper introduces VisuCraft, a novel framework designed to significantly enhance the capabilities of Large Vision-Language Models (LVLMs) in complex visual-guided creative content generation. Existing LVLMs often exhibit limitations in maintaining high visual fidelity, genuine creativity, and precise adherence to nuanced user instructions when generating long-form texts. VisuCraft addresses these challenges by integrating a multimodal structured information extractor (E) and a dynamic prompt generation module (G). The extractor distills fine-grained visual attributes from input images into a rich, structured representation, which the dynamic prompt module then combines with user instructions to create highly optimized prompts for underlying LVLMs (e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed ImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity, and Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs across tasks like story generation and poetry composition. Our results demonstrate remarkable improvements, particularly in creativity and instruction adherence, validating VisuCraft's effectiveness in producing imaginative, visually grounded, and user-aligned long-form creative text. This work unlocks new potential for LVLMs in sophisticated creative AI applications.",
        "arxiv_id": "2508.02890",
        "ARXIVID": "2508.02890",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it enhances LVLMs for creative content generation with structured information extraction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03102": {
        "authors": [
            "Tianjiao Jiang",
            "Zhen Zhang",
            "Yuhang Liu",
            "Javen Qinfeng Shi"
        ],
        "title": "Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning",
        "abstract": "arXiv:2508.03102v1 Announce Type: new  Abstract: Few-shot learning (FSL) often requires effective adaptation of models using limited labeled data. However, most existing FSL methods rely on entangled representations, requiring the model to implicitly recover the unmixing process to obtain disentangled representations using only limited supervision, which hinders effective adaptation. Recent theoretical studies show that multimodal contrastive learning methods, such as CLIP, can disentangle latent representations up to linear transformations. In light of this, we propose the Causal CLIP Adapter (CCA), a novel framework that explicitly disentangles visual features extracted from CLIP using unsupervised Independent Component Analysis (ICA). This removes the need to learn the unmixing process from the labeled data, thereby reducing the number of trainable parameters and mitigating overfitting. Taking a step further, while ICA can obtain visual disentangled representations, it may also disrupt CLIP's intra- and inter-modal alignment. To counteract this, CCA further leverages CLIP's inherent cross-modal alignment by enhancing it in two ways: unidirectionally, through fine-tuning a CLIP-based text classifier, and bidirectionally, via a cross-attention mechanism that enriches visual and textual representations through mutual interaction. Both unimodal and cross-modal classification outputs can be effectively combined linearly to improve classification accuracy. Extensive experiments on 11 benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches in terms of few-shot performance and robustness to distributional shifts, while maintaining computational efficiency. Code will be available at https://github.com/tianjiao-j/CCA.",
        "arxiv_id": "2508.03102",
        "ARXIVID": "2508.03102",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it explores cross-modal alignment and disentanglement for few-shot learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03018": {
        "authors": [
            "Yutong Wang",
            "Pengliang Ji",
            "Kaixin Li",
            "Baolong Bi",
            "Tao Feng",
            "Guillaume Sartoretti"
        ],
        "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning",
        "abstract": "arXiv:2508.03018v1 Announce Type: new  Abstract: Large Language Reasoning Models have demonstrated remarkable success on static tasks, yet their application to multi-round agentic planning in interactive environments faces two fundamental challenges. First, the intractable credit assignment problem renders conventional reinforcement learning ineffective in sparse-reward settings. Second, the computational overhead of verbose, step-by-step reasoning histories is prohibitive. To address these challenges, we propose BPO, a three-stage framework (bootstrapping, extrapolation, and refinement) that establishes a self-improving data flywheel to develop robust reasoning models for long-horizon, sparse-reward environments. Our framework first bootstraps efficient reasoning using the proposed planning quaternions with long-short chain-of-thought fusion. It then extrapolates to out-of-distribution tasks through complexity-stratified curriculum learning. Finally, the model iteratively refines itself by learning exclusively on experiences selected via reward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and WebShop demonstrate that our approach achieves state-of-the-art with significant token efficiency, providing a new recipe for reasoning models in agentic planning.",
        "arxiv_id": "2508.03018",
        "ARXIVID": "2508.03018",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for reasoning models in long-horizon, sparse-reward environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03485": {
        "authors": [
            "Lianwei Yang",
            "Haokun Lin",
            "Tianchen Zhao",
            "Yichen Wu",
            "Hongyu Zhu",
            "Ruiqi Xie",
            "Zhenan Sun",
            "Yu Wang",
            "Qingyi Gu"
        ],
        "title": "LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation",
        "abstract": "arXiv:2508.03485v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) have achieved impressive performance in text-to-image generation. However, their high computational cost and large parameter sizes pose significant challenges for usage in resource-constrained scenarios. Post-training quantization (PTQ) is a promising solution to reduce memory usage and accelerate inference, but existing PTQ methods suffer from severe performance degradation under extreme low-bit settings. We identify two key obstacles to low-bit post-training quantization for DiT models: (1) model weights follow a Gaussian-like distribution with long tails, causing uniform quantization to poorly allocate intervals and leading to significant errors; (2) two types of activation outliers: (i) Mild Outliers with slightly elevated values, and (ii) Salient Outliers with large magnitudes concentrated in specific channels, which disrupt activation quantization. To address these issues, we propose LRQ-DiT, an efficient and accurate PTQ framework. We introduce Twin-Log Quantization (TLQ), a log-based method that aligns well with the weight distribution and reduces quantization errors. We also propose an Adaptive Rotation Scheme (ARS) that dynamically applies Hadamard or outlier-aware rotations based on activation fluctuation, effectively mitigating the impact of both types of outliers. We evaluate LRQ-DiT on PixArt and FLUX under various bit-width settings, and validate the performance on COCO, MJHQ, and sDCI datasets. LRQ-DiT achieves low-bit quantization of DiT models while preserving image quality, outperforming existing PTQ baselines.",
        "arxiv_id": "2508.03485",
        "ARXIVID": "2508.03485",
        "COMMENT": "Matches criterion 5 as it focuses on quantization techniques for diffusion transformers in text-to-image generation, integrating image and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03341": {
        "authors": [
            "Jiayan Nan",
            "Wenquan Ma",
            "Wenlong Wu",
            "Yize Chen"
        ],
        "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science",
        "abstract": "arXiv:2508.03341v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities, yet their inability to maintain persistent memory in long contexts limits their effectiveness as autonomous agents in long-term interactions. While existing memory systems have made progress, their reliance on arbitrary granularity for defining the basic memory unit and passive, rule-based mechanisms for knowledge extraction limits their capacity for genuine learning and evolution. To address these foundational limitations, we present Nemori, a novel self-organizing memory architecture inspired by human cognitive principles. Nemori's core innovation is twofold: First, its Two-Step Alignment Principle, inspired by Event Segmentation Theory, provides a principled, top-down method for autonomously organizing the raw conversational stream into semantically coherent episodes, solving the critical issue of memory granularity. Second, its Predict-Calibrate Principle, inspired by the Free-energy Principle, enables the agent to proactively learn from prediction gaps, moving beyond pre-defined heuristics to achieve adaptive knowledge evolution. This offers a viable path toward handling the long-term, dynamic workflows of autonomous agents. Extensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that Nemori significantly outperforms prior state-of-the-art systems, with its advantage being particularly pronounced in longer contexts.",
        "arxiv_id": "2508.03341",
        "ARXIVID": "2508.03341",
        "COMMENT": "Matches criterion 1 as it presents a novel memory architecture for embodied agents inspired by cognitive science.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03173": {
        "authors": [
            "Jingxuan Wei",
            "Caijun Jia",
            "Qi Chen",
            "Honghao He",
            "Linzhuang Sun",
            "Conghui He",
            "Lijun Wu",
            "Bihui Yu",
            "Cheng Tan"
        ],
        "title": "Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions",
        "abstract": "arXiv:2508.03173v1 Announce Type: new  Abstract: Mathematical geometric reasoning is essential for scientific discovery and educational development, requiring precise logic and rigorous formal verification. While recent advances in Multimodal Large Language Models (MLLMs) have improved reasoning tasks, existing models typically struggle with formal geometric reasoning, particularly when dynamically constructing and verifying auxiliary geometric elements. To address these challenges, we introduce Geoint-R1, a multimodal reasoning framework designed to generate formally verifiable geometric solutions from textual descriptions and visual diagrams. Geoint-R1 uniquely integrates auxiliary elements construction, formal reasoning represented via Lean4, and interactive visualization. To systematically evaluate and advance formal geometric reasoning, we propose the Geoint benchmark, comprising 1,885 rigorously annotated geometry problems across diverse topics such as plane, spatial, and solid geometry. Each problem includes structured textual annotations, precise Lean4 code for auxiliary constructions, and detailed solution steps verified by experts. Extensive experiments demonstrate that Geoint-R1 significantly surpasses existing multimodal and math-specific reasoning models, particularly on challenging problems requiring explicit auxiliary element constructions.",
        "arxiv_id": "2508.03173",
        "ARXIVID": "2508.03173",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a framework for formal geometric reasoning, which involves spatial intelligence and reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.02807": {
        "authors": [
            "Tongchun Zuo",
            "Zaiyu Huang",
            "Shuliang Ning",
            "Ente Lin",
            "Chao Liang",
            "Zerong Zheng",
            "Jianwen Jiang",
            "Yuan Zhang",
            "Mingyuan Gao",
            "Xin Dong"
        ],
        "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework",
        "abstract": "arXiv:2508.02807v1 Announce Type: new  Abstract: Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. \\textbf{In the second stage}, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/",
        "arxiv_id": "2508.02807",
        "ARXIVID": "2508.02807",
        "COMMENT": "This paper matches criterion 6 as it focuses on video virtual try-on, a video understanding task, and introduces a novel two-stage framework (DreamVVT) leveraging diffusion transformers.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03266": {
        "authors": [
            "Huaihai Lyu",
            "Chaofan Chen",
            "Yuheng Ji",
            "Changsheng Xu"
        ],
        "title": "EgoPrompt: Prompt Pool Learning for Egocentric Action Recognition",
        "abstract": "arXiv:2508.03266v1 Announce Type: new  Abstract: Driven by the increasing demand for applications in augmented and virtual reality, egocentric action recognition has emerged as a prominent research area. It is typically divided into two subtasks: recognizing the performed behavior (i.e., verb component) and identifying the objects being acted upon (i.e., noun component) from the first-person perspective. However, most existing approaches treat these two components as independent classification tasks, focusing on extracting component-specific knowledge while overlooking their inherent semantic and contextual relationships, leading to fragmented representations and sub-optimal generalization capability. To address these challenges, we propose a prompt learning-based framework, EgoPrompt, to conduct the egocentric action recognition task. Building on the existing prompting strategy to capture the component-specific knowledge, we construct a Unified Prompt Pool space to establish interaction between the two types of component representations. Specifically, the component representations (from verbs and nouns) are first decomposed into fine-grained patterns with the prompt pair form. Then, these pattern-level representations are fused through an attention-based mechanism to facilitate cross-component interaction. To ensure the prompt pool is informative, we further introduce a novel training objective, Diverse Pool Criteria. This objective realizes our goals from two perspectives: Prompt Selection Frequency Regularization and Prompt Knowledge Orthogonalization. Extensive experiments are conducted on the Ego4D, EPIC-Kitchens, and EGTEA datasets. The results consistently show that EgoPrompt achieves state-of-the-art performance across within-dataset, cross-dataset, and base-to-novel generalization benchmarks.",
        "arxiv_id": "2508.03266",
        "ARXIVID": "2508.03266",
        "COMMENT": "This paper matches criterion 6 as it focuses on egocentric action recognition, a video understanding task, and introduces a novel prompt learning-based framework (EgoPrompt).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.03449": {
        "authors": [
            "Xuan Dong",
            "Xiangyuan Sun",
            "Xia Wang",
            "Jian Song",
            "Ya Li",
            "Weixin Li"
        ],
        "title": "Video Demoireing using Focused-Defocused Dual-Camera System",
        "abstract": "arXiv:2508.03449v1 Announce Type: new  Abstract: Moire patterns, unwanted color artifacts in images and videos, arise from the interference between spatially high-frequency scene contents and the spatial discrete sampling of digital cameras. Existing demoireing methods primarily rely on single-camera image/video processing, which faces two critical challenges: 1) distinguishing moire patterns from visually similar real textures, and 2) preserving tonal consistency and temporal coherence while removing moire artifacts. To address these issues, we propose a dual-camera framework that captures synchronized videos of the same scene: one in focus (retaining high-quality textures but may exhibit moire patterns) and one defocused (with significantly reduced moire patterns but blurred textures). We use the defocused video to help distinguish moire patterns from real texture, so as to guide the demoireing of the focused video. We propose a frame-wise demoireing pipeline, which begins with an optical flow based alignment step to address any discrepancies in displacement and occlusion between the focused and defocused frames. Then, we leverage the aligned defocused frame to guide the demoireing of the focused frame using a multi-scale CNN and a multi-dimensional training loss. To maintain tonal and temporal consistency, our final step involves a joint bilateral filter to leverage the demoireing result from the CNN as the guide to filter the input focused frame to obtain the final output. Experimental results demonstrate that our proposed framework largely outperforms state-of-the-art image and video demoireing methods.",
        "arxiv_id": "2508.03449",
        "ARXIVID": "2508.03449",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a novel method for video demoireing using a dual-camera system.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.02694": {
        "authors": [
            "Ningning Wang",
            "Xavier Hu",
            "Pai Liu",
            "He Zhu",
            "Yue Hou",
            "Heyuan Huang",
            "Shengyu Zhang",
            "Jian Yang",
            "Jiaheng Liu",
            "Ge Zhang",
            "Changwang Zhang",
            "Jun Wang",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou"
        ],
        "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
        "abstract": "arXiv:2508.02694v1 Announce Type: new  Abstract: The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.",
        "arxiv_id": "2508.02694",
        "ARXIVID": "2508.02694",
        "COMMENT": "Matches criterion 3 as it explores efficient agent frameworks, which is relevant to embodied/robotic AI methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03566": {
        "authors": [
            "Xinyu Xiong",
            "Zihuang Wu",
            "Lei Zhang",
            "Lei Lu",
            "Ming Li",
            "Guanbin Li"
        ],
        "title": "SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks",
        "abstract": "arXiv:2508.03566v1 Announce Type: new  Abstract: Recent studies have highlighted the potential of adapting the Segment Anything Model (SAM) for various downstream tasks. However, constructing a more powerful and generalizable encoder to further enhance performance remains an open challenge. In this work, we propose SAM2-UNeXT, an advanced framework that builds upon the core principles of SAM2-UNet while extending the representational capacity of SAM2 through the integration of an auxiliary DINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue layer, our approach enables more accurate segmentation with a simple architecture, relaxing the need for complex decoder designs. Extensive experiments conducted on four benchmarks, including dichotomous image segmentation, camouflaged object detection, marine animal segmentation, and remote sensing saliency detection, demonstrate the superior performance of our proposed method. The code is available at https://github.com/WZH0120/SAM2-UNeXT.",
        "arxiv_id": "2508.03566",
        "ARXIVID": "2508.03566",
        "COMMENT": "Matches criterion 4 as it focuses on adapting vision foundation models (SAM) to downstream segmentation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03524": {
        "authors": [
            "Stefan Brandst\\\"atter",
            "Maximilian K\\\"oller",
            "Philipp Seeb\\\"ock",
            "Alissa Blessing",
            "Felicitas Oberndorfer",
            "Svitlana Pochepnia",
            "Helmut Prosch",
            "Georg Langs"
        ],
        "title": "Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models",
        "abstract": "arXiv:2508.03524v1 Announce Type: new  Abstract: In histopathology, tissue samples are often larger than a standard microscope slide, making stitching of multiple fragments necessary to process entire structures such as tumors. Automated stitching is a prerequisite for scaling analysis, but is challenging due to possible tissue loss during preparation, inhomogeneous morphological distortion, staining inconsistencies, missing regions due to misalignment on the slide, or frayed tissue edges. This limits state-of-the-art stitching methods using boundary shape matching algorithms to reconstruct artificial whole mount slides (WMS). Here, we introduce SemanticStitcher using latent feature representations derived from a visual histopathology foundation model to identify neighboring areas in different fragments. Robust pose estimation based on a large number of semantic matching candidates derives a mosaic of multiple fragments to form the WMS. Experiments on three different histopathology datasets demonstrate that SemanticStitcher yields robust WMS mosaicing and consistently outperforms the state of the art in correct boundary matches.",
        "arxiv_id": "2508.03524",
        "ARXIVID": "2508.03524",
        "COMMENT": "This paper matches criterion 4 as it applies visual foundation models to histopathology image mosaicing, showcasing their application in a specific domain.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.03039": {
        "authors": [
            "Yiran Meng",
            "Junhong Ye",
            "Wei Zhou",
            "Guanghui Yue",
            "Xudong Mao",
            "Ruomei Wang",
            "Baoquan Zhao"
        ],
        "title": "VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering",
        "abstract": "arXiv:2508.03039v1 Announce Type: new  Abstract: Cross-video question answering presents significant challenges beyond traditional single-video understanding, particularly in establishing meaningful connections across video streams and managing the complexity of multi-source information retrieval. We introduce VideoForest, a novel framework that addresses these challenges through person-anchored hierarchical reasoning. Our approach leverages person-level features as natural bridge points between videos, enabling effective cross-video understanding without requiring end-to-end training. VideoForest integrates three key innovations: 1) a human-anchored feature extraction mechanism that employs ReID and tracking algorithms to establish robust spatiotemporal relationships across multiple video sources; 2) a multi-granularity spanning tree structure that hierarchically organizes visual content around person-level trajectories; and 3) a multi-agent reasoning framework that efficiently traverses this hierarchical structure to answer complex cross-video queries. To evaluate our approach, we develop CrossVideoQA, a comprehensive benchmark dataset specifically designed for person-centric cross-video analysis. Experimental results demonstrate VideoForest's superior performance in cross-video reasoning tasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior analysis, and 51.67% in summarization and reasoning, significantly outperforming existing methods. Our work establishes a new paradigm for cross-video understanding by unifying multiple video streams through person-level features, enabling sophisticated reasoning across distributed visual information while maintaining computational efficiency.",
        "arxiv_id": "2508.03039",
        "ARXIVID": "2508.03039",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel framework for cross-video question answering.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.03164": {
        "authors": [
            "Junyoung Lim",
            "Jaewoo Ahn",
            "Gunhee Kim"
        ],
        "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
        "abstract": "arXiv:2508.03164v1 Announce Type: new  Abstract: Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.",
        "arxiv_id": "2508.03164",
        "ARXIVID": "2508.03164",
        "COMMENT": "Matches criterion 6 as it focuses on improving chart captioning, a specific video/visual understanding task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03680": {
        "authors": [
            "Xufang Luo",
            "Yuge Zhang",
            "Zhiyuan He",
            "Zilong Wang",
            "Siyun Zhao",
            "Dongsheng Li",
            "Luna K. Qiu",
            "Yuqing Yang"
        ],
        "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
        "abstract": "arXiv:2508.03680v1 Announce Type: new  Abstract: We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.",
        "arxiv_id": "2508.03680",
        "ARXIVID": "2508.03680",
        "COMMENT": "Matches criterion 3 as it introduces a framework for training AI agents with reinforcement learning, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03447": {
        "authors": [
            "Qiyu Chen",
            "Zhen Qu",
            "Wei Luo",
            "Haiming Yao",
            "Yunkang Cao",
            "Yuxin Jiang",
            "Yinan Duan",
            "Huiyuan Luo",
            "Chengkan Lv",
            "Zhengtao Zhang"
        ],
        "title": "CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection",
        "abstract": "arXiv:2508.03447v1 Announce Type: new  Abstract: Recently, large pre-trained vision-language models have shown remarkable performance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single auxiliary dataset, the model enables cross-category anomaly detection on diverse datasets covering industrial defects and medical lesions. Compared to manually designed prompts, prompt learning eliminates the need for expert knowledge and trial-and-error. However, it still faces the following challenges: (i) static learnable tokens struggle to capture the continuous and diverse patterns of normal and anomalous states, limiting generalization to unseen categories; (ii) fixed textual labels provide overly sparse category information, making the model prone to overfitting to a specific semantic subspace. To address these issues, we propose Conditional Prompt Synthesis (CoPS), a novel framework that synthesizes dynamic prompts conditioned on visual features to enhance ZSAD performance. Specifically, we extract representative normal and anomaly prototypes from fine-grained patch features and explicitly inject them into prompts, enabling adaptive state modeling. Given the sparsity of class labels, we leverage a variational autoencoder to model semantic image features and implicitly fuse varied class tokens into prompts. Additionally, integrated with our spatially-aware alignment mechanism, extensive experiments demonstrate that CoPS surpasses state-of-the-art methods by 2.5% AUROC in both classification and segmentation across 13 industrial and medical datasets. Code will be available at https://github.com/cqylunlun/CoPS.",
        "arxiv_id": "2508.03447",
        "ARXIVID": "2508.03447",
        "COMMENT": "Matches criterion 4 as it proposes a novel framework for zero-shot anomaly detection using vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03469": {
        "authors": [
            "Jiabing Yang",
            "Chenhang Cui",
            "Yiyang Zhou",
            "Yixiang Chen",
            "Peng Xia",
            "Ying Wei",
            "Tao Yu",
            "Yan Huang",
            "Liang Wang"
        ],
        "title": "IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models",
        "abstract": "arXiv:2508.03469v1 Announce Type: new  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated significant progress across multiple domains. However, these models still face the inherent challenge of integrating vision and language for collaborative inference, which often leads to \"hallucinations\", outputs that are not grounded in the corresponding images. Many efforts have been made to address these issues, but each comes with its own limitations, such as high computational cost or expensive dataset annotation. Recent research shows that LVLMs exhibit a long-term bias where hallucinations increase as the sequence length grows, yet the underlying cause remains poorly understood. Building on extensive research into attention mechanisms in LVLMs, we analyze the relationship between this long-term bias and visual attention. In our research, we identify a consistent phenomenon in current LVLMs: the model's attention to visual input diminishes as the generated sequence grows, which we hypothesize to be a key factor contributing to observed increasing hallucinations. Based on these insights, we propose Image attention-guided Key-value merging cOllaborative Decoding (IKOD), a collaborative decoding strategy generating more image-focused sequences. This method derives logits from shorter sequences with higher image attention through key-value merging and combines them with those from the original decoding, effectively mitigating attention degradation and suppressing hallucinations while not incurring too much inference cost. Extensive experiments on both hallucination and comprehensive benchmarks demonstrate IKOD's superior effectiveness in mitigating hallucinations and improving comprehensive capacities for LVLMs. Importantly, IKOD requires no additional training or external tools, making it a lightweight and efficient framework applicable to various models.",
        "arxiv_id": "2508.03469",
        "ARXIVID": "2508.03469",
        "COMMENT": "Matches criterion 2 as it addresses challenges in vision-language integration for large vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03227": {
        "authors": [
            "Hongyu Shen",
            "Junfeng Ni",
            "Yixin Chen",
            "Weishuo Li",
            "Mingtao Pei",
            "Siyuan Huang"
        ],
        "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing",
        "abstract": "arXiv:2508.03227v1 Announce Type: new  Abstract: We address the challenge of lifting 2D visual segmentation to 3D in Gaussian Splatting. Existing methods often suffer from inconsistent 2D masks across viewpoints and produce noisy segmentation boundaries as they neglect these semantic cues to refine the learned Gaussians. To overcome this, we introduce Gaussian Instance Tracing (GIT), which augments the standard Gaussian representation with an instance weight matrix across input views. Leveraging the inherent consistency of Gaussians in 3D, we use this matrix to identify and correct 2D segmentation inconsistencies. Furthermore, since each Gaussian ideally corresponds to a single object, we propose a GIT-guided adaptive density control mechanism to split and prune ambiguous Gaussians during training, resulting in sharper and more coherent 2D and 3D segmentation boundaries. Experimental results show that our method extracts clean 3D assets and consistently improves 3D segmentation in both online (e.g., self-prompting) and offline (e.g., contrastive lifting) settings, enabling applications such as hierarchical segmentation, object extraction, and scene editing.",
        "arxiv_id": "2508.03227",
        "ARXIVID": "2508.03227",
        "COMMENT": "Matches criterion 4 as it focuses on improving 3D segmentation using Gaussian representations, which is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03542": {
        "authors": [
            "Dmitrii Korzh",
            "Dmitrii Tarasov",
            "Artyom Iudin",
            "Elvir Karimov",
            "Matvey Skripkin",
            "Nikita Kuzmin",
            "Andrey Kuznetsov",
            "Oleg Y. Rogov",
            "Ivan Oseledets"
        ],
        "title": "Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences",
        "abstract": "arXiv:2508.03542v1 Announce Type: new  Abstract: Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.",
        "arxiv_id": "2508.03542",
        "ARXIVID": "2508.03542",
        "COMMENT": "Matches criterion 5 as it explores multimodal AI with audio and text integration for mathematical content recognition.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03277": {
        "authors": [
            "Hang Guo",
            "Qing Zhang",
            "Zixuan Gao",
            "Siyuan Yang",
            "Shulin Peng",
            "Xiang Tao",
            "Ting Yu",
            "Yan Wang",
            "Qingli Li"
        ],
        "title": "Efficient Multi-Slide Visual-Language Feature Fusion for Placental Disease Classification",
        "abstract": "arXiv:2508.03277v1 Announce Type: new  Abstract: Accurate prediction of placental diseases via whole slide images (WSIs) is critical for preventing severe maternal and fetal complications. However, WSI analysis presents significant computational challenges due to the massive data volume. Existing WSI classification methods encounter critical limitations: (1) inadequate patch selection strategies that either compromise performance or fail to sufficiently reduce computational demands, and (2) the loss of global histological context resulting from patch-level processing approaches. To address these challenges, we propose an Efficient multimodal framework for Patient-level placental disease Diagnosis, named EmmPD. Our approach introduces a two-stage patch selection module that combines parameter-free and learnable compression strategies, optimally balancing computational efficiency with critical feature preservation. Additionally, we develop a hybrid multimodal fusion module that leverages adaptive graph learning to enhance pathological feature representation and incorporates textual medical reports to enrich global contextual understanding. Extensive experiments conducted on both a self-constructed patient-level Placental dataset and two public datasets demonstrating that our method achieves state-of-the-art diagnostic performance. The code is available at https://github.com/ECNU-MultiDimLab/EmmPD.",
        "arxiv_id": "2508.03277",
        "ARXIVID": "2508.03277",
        "COMMENT": "Matches criterion 2 and 5 as it involves multimodal fusion of visual and textual features for medical diagnosis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03351": {
        "authors": [
            "Yufei Xue",
            "Yushi Huang",
            "Jiawei Shao",
            "Jun Zhang"
        ],
        "title": "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation",
        "abstract": "arXiv:2508.03351v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as an effective approach for compressing large models and accelerating their inference without retraining. While PTQ has been extensively studied in the context of large language models (LLMs), its applicability to vision-language models (VLMs) remains underexplored. In this paper, we identify a modality discrepancy (\\emph{i.e.}, limited text tokens \\emph{vs.} excessive and redundant vision tokens) of VLMs. However, existing Hessian-based LLM PTQ methods treat all tokens equally during quantization, resulting in severe performance drops when applied to VLMs. Motivated by this observation, we propose a novel importance-aware PTQ framework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token redundancy, VLMQ 1) optimizes an importance-aware objective that yields an enhanced Hessian with token-level importance factors, while retaining compatibility with parallelized weight updates, and 2) ensures efficiency and effectiveness by computing these factors via a single lightweight block-wise backward pass, guided by a theoretical connection to token-level perturbations. Extensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the state-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit settings. For example, it achieves a substantial \\textbf{16.45\\%} improvement on MME-RealWorld under 2-bit quantization.",
        "arxiv_id": "2508.03351",
        "ARXIVID": "2508.03351",
        "COMMENT": "Matches criterion 5 as it explores post-training quantization for vision-language models, focusing on efficient integration of vision and language tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03050": {
        "authors": [
            "Zeyu Zhu",
            "Weijia Wu",
            "Mike Zheng Shou"
        ],
        "title": "Multi-human Interactive Talking Dataset",
        "abstract": "arXiv:2508.03050v1 Announce Type: new  Abstract: Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
        "arxiv_id": "2508.03050",
        "ARXIVID": "2508.03050",
        "COMMENT": "Matches criterion 6 as it introduces a dataset and baseline for multi-human talking video generation, relevant to video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03320": {
        "authors": [
            "Peiyu Wang",
            "Yi Peng",
            "Yimeng Gan",
            "Liang Hu",
            "Tianyidan Xie",
            "Xiaokun Wang",
            "Yichen Wei",
            "Chuanxin Tang",
            "Bo Zhu",
            "Changshi Li",
            "Hongyang Wei",
            "Eric Li",
            "Xuchen Song",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation",
        "abstract": "arXiv:2508.03320v1 Announce Type: new  Abstract: We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
        "arxiv_id": "2508.03320",
        "ARXIVID": "2508.03320",
        "COMMENT": "Matches criterion 2 as it explores a unified autoregressive model for multimodal tasks including image understanding and generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03691": {
        "authors": [
            "Youquan Liu",
            "Lingdong Kong",
            "Weidong Yang",
            "Xin Li",
            "Ao Liang",
            "Runnan Chen",
            "Ben Fei",
            "Tongliang Liu"
        ],
        "title": "La La LiDAR: Large-Scale Layout Generation from LiDAR Data",
        "abstract": "arXiv:2508.03691v1 Announce Type: new  Abstract: Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (\"La La LiDAR\"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.",
        "arxiv_id": "2508.03691",
        "ARXIVID": "2508.03691",
        "COMMENT": "Matches criterion 3 as it introduces new benchmarks (Waymo-SG and nuScenes-SG) and methods for LiDAR-based scene generation, relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03127": {
        "authors": [
            "Sai Ma",
            "Zhuang Li",
            "John A Taylor"
        ],
        "title": "Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery",
        "abstract": "arXiv:2508.03127v1 Announce Type: new  Abstract: Vision language models (VLMs) that enable natural language interaction with satellite imagery can democratize Earth observation by accelerating expert workflows, making data accessible to non-specialists, and enabling planet-scale automation. However, existing datasets focus mainly on short-term, high-resolution imagery from a limited number of satellites, overlooking low-resolution, multi-satellite, long-term archives, such as Landsat, that are essential for affordable and bias-robust global monitoring. We address this gap with Landsat30-AU, a large-scale vision-language dataset built from 30-meter resolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over Australia, spanning more than 36 years. The dataset includes two components: Landsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA, comprising 17,725 human-verified visual question answering (VQA) samples across eight remote sensing domains. Both datasets are curated through a bootstrapped pipeline that leverages generic VLMs with iterative refinement and human verification to ensure quality. Our evaluation of eight VLMs on our benchmark reveals that off-the-shelf models struggle to understand satellite imagery. The open-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in captioning and a VQA accuracy of 0.48, highlighting the limitations of current approaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on Landsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and boosts VQA accuracy from \\textbf{0.74} to 0.87. Code and data are available at https://github.com/papersubmit1/landsat30-au.",
        "arxiv_id": "2508.03127",
        "ARXIVID": "2508.03127",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a vision-language dataset for satellite imagery.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.03411": {
        "authors": [
            "Diana-Nicoleta Grigore",
            "Neelu Madan",
            "Andreas Mogelmose",
            "Thomas B. Moeslund",
            "Radu Tudor Ionescu"
        ],
        "title": "SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation",
        "abstract": "arXiv:2508.03411v1 Announce Type: new  Abstract: Unsupervised video segmentation is a challenging computer vision task, especially due to the lack of supervisory signals coupled with the complexity of visual scenes. To overcome this challenge, state-of-the-art models based on slot attention often have to rely on large and computationally expensive neural architectures. To this end, we propose a simple knowledge distillation framework that effectively transfers object-centric representations to a lightweight student. The proposed framework, called SlotMatch, aligns corresponding teacher and student slots via the cosine similarity, requiring no additional distillation objectives or auxiliary supervision. The simplicity of SlotMatch is confirmed via theoretical and empirical evidence, both indicating that integrating additional losses is redundant. We conduct experiments on two datasets to compare the state-of-the-art teacher model, SlotContrast, with our distilled student. The results show that our student based on SlotMatch matches and even outperforms its teacher, while using 3.6x less parameters and running 1.9x faster. Moreover, our student surpasses previous unsupervised video segmentation models.",
        "arxiv_id": "2508.03411",
        "ARXIVID": "2508.03411",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on unsupervised video segmentation with novel methodologies.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.02789": {
        "authors": [
            "Newman Cheng",
            "Gordon Broadbent",
            "William Chappell"
        ],
        "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science",
        "abstract": "arXiv:2508.02789v1 Announce Type: new  Abstract: The capacity for artificial intelligence (AI) to formulate, evolve, and test altered thought patterns under dynamic conditions indicates advanced cognition that is crucial for scientific discovery. The existing AI development landscape falls into two categories: 1) frameworks over non-reasoning models that natively incorporate opinions on how humans think, and 2) reasoning models that abstract precise control of the reasoning intuition away from end users. While powerful, for scientists to maximize utility of AI in scientific discovery, they not only require accuracy and transparency in reasoning, but also steerability. Hence, we introduce an alternative approach that enables deep and precise control over the reasoning process called: a cognitive loop via in-situ optimization (CLIO). CLIO enables large language models (LLMs) to self-formulate ways of approaching a problem, adapt behavior when self-confidence is low, and ultimately provide scientists with a final belief or answer. Through CLIO's open design, scientists can observe uncertainty levels, understand how final belief states are formulated using graph structures, and interject corrections. Without any further post-training, OpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology and medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net or 161.64\\% relative increase when compared to the base GPT-4.1 model and surpasses OpenAI's o3 performance in high and low reasoning effort modes. We further discovered that oscillations within internal uncertainty measures are key in determining the accuracy of CLIO's results, revealing how its open design and internal mechanisms can provide insight and control into scientific decision-making processes.",
        "arxiv_id": "2508.02789",
        "ARXIVID": "2508.02789",
        "COMMENT": "This paper does not directly match any specific criterion but discusses a novel reasoning framework (CLIO) for scientific discovery, which could be tangentially related to embodied AI (criterion 3) due to its focus on reasoning and decision-making.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.03031": {
        "authors": [
            "Ziyang Ma",
            "Baojian Zhou",
            "Deqing Yang",
            "Yanghua Xiao"
        ],
        "title": "From Text to Trajectories: GPT-2 as an ODE Solver via In-Context",
        "abstract": "arXiv:2508.03031v1 Announce Type: new  Abstract: In-Context Learning (ICL) has emerged as a new paradigm in large language models (LLMs), enabling them to perform novel tasks by conditioning on a few examples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for NLP tasks remains poorly understood. To shed light on its underlying mechanisms, this paper investigates whether LLMs can solve ordinary differential equations (ODEs) under the ICL setting. We formulate standard ODE problems and their solutions as sequential prompts and evaluate GPT-2 models on these tasks. Experiments on two types of ODEs show that GPT-2 can effectively learn a meta-ODE algorithm, with convergence behavior comparable to, or better than, the Euler method, and achieve exponential accuracy gains with increasing numbers of demonstrations. Moreover, the model generalizes to out-of-distribution (OOD) problems, demonstrating robust extrapolation capabilities. These empirical findings provide new insights into the mechanisms of ICL in NLP and its potential for solving nonlinear numerical problems.",
        "arxiv_id": "2508.03031",
        "ARXIVID": "2508.03031",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of LLMs and their capabilities in numerical problem-solving.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03402": {
        "authors": [
            "Pingchuan Ma",
            "Xiaopei Yang",
            "Yusong Li",
            "Ming Gui",
            "Felix Krause",
            "Johannes Schusterbauer",
            "Bj\\\"orn Ommer"
        ],
        "title": "SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models",
        "abstract": "arXiv:2508.03402v1 Announce Type: new  Abstract: Explicitly disentangling style and content in vision models remains challenging due to their semantic overlap and the subjectivity of human perception. Existing methods propose separation through generative or discriminative objectives, but they still face the inherent ambiguity of disentangling intertwined concepts. Instead, we ask: Can we bypass explicit disentanglement by learning to merge style and content invertibly, allowing separation to emerge naturally? We propose SCFlow, a flow-matching framework that learns bidirectional mappings between entangled and disentangled representations. Our approach is built upon three key insights: 1) Training solely to merge style and content, a well-defined task, enables invertible disentanglement without explicit supervision; 2) flow matching bridges on arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion models and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51 styles $\\times$ 10,000 content samples) was curated to simulate disentanglement through systematic style-content pairing. Beyond controllable generation tasks, we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot settings and achieves competitive performance, highlighting that disentanglement naturally emerges from the invertible merging process.",
        "arxiv_id": "2508.03402",
        "ARXIVID": "2508.03402",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and disentanglement in vision models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03346": {
        "authors": [
            "Zeju Li",
            "Jianyuan Zhong",
            "Ziyang Zheng",
            "Xiangyu Wen",
            "Zhijian Xu",
            "Yingying Cheng",
            "Fan Zhang",
            "Qiang Xu"
        ],
        "title": "Compressing Chain-of-Thought in LLMs via Step Entropy",
        "abstract": "arXiv:2508.03346v1 Announce Type: new  Abstract: Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances LLM inference efficiency while rigorously preserving accuracy, offering profound implications for practical LLM deployment and a deeper understanding of reasoning structures.",
        "arxiv_id": "2508.03346",
        "ARXIVID": "2508.03346",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of reasoning and efficiency in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03117": {
        "authors": [
            "Vinicius Lima",
            "Dzung T. Phan",
            "Jayant Kalagnanam",
            "Dhaval Patel",
            "Nianjun Zhou"
        ],
        "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation",
        "abstract": "arXiv:2508.03117v1 Announce Type: new  Abstract: We present a framework for training trustworthy large language model (LLM) agents for optimization modeling via a verifiable synthetic data generation pipeline. Focusing on linear and mixed-integer linear programming, our approach begins with structured symbolic representations and systematically produces natural language descriptions, mathematical formulations, and solver-executable code. By programmatically constructing each instance with known optimal solutions, the pipeline ensures full verifiability and enables automatic filtering of low-quality demonstrations generated by teacher models. Each dataset instance includes a structured representation of the optimization problem, a corresponding natural language description, the verified optimal solution, and step-by-step demonstrations - generated by a teacher model - that show how to model and solve the problem across multiple optimization modeling languages. This enables supervised fine-tuning of open-source LLMs specifically tailored to optimization tasks. To operationalize this pipeline, we introduce OptiTrust, a modular LLM agent that performs multi-stage translation from natural language to solver-ready code, leveraging stepwise demonstrations, multi-language inference, and majority-vote cross-validation. Our agent achieves state-of-the-art performance on standard benchmarks. Out of 7 datasets, it achieves the highest accuracy on six and outperforms the next-best algorithm by at least 8 percentage on three of them. Our approach provides a scalable, verifiable, and principled path toward building reliable LLM agents for real-world optimization applications.",
        "arxiv_id": "2508.03117",
        "ARXIVID": "2508.03117",
        "COMMENT": "Does not match any specific criterion but is related to optimization modeling and trustworthy LLM agents, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.02961": {
        "authors": [
            "Boshi Huang",
            "Fabio Nonato de Paula"
        ],
        "title": "Defend LLMs Through Self-Consciousness",
        "abstract": "arXiv:2508.02961v1 Announce Type: new  Abstract: This paper introduces a novel self-consciousness defense mechanism for Large Language Models (LLMs) to combat prompt injection attacks. Unlike traditional approaches that rely on external classifiers, our method leverages the LLM's inherent reasoning capabilities to perform self-protection. We propose a framework that incorporates Meta-Cognitive and Arbitration Modules, enabling LLMs to evaluate and regulate their own outputs autonomously. Our approach is evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate significant improvements in defense success rates across models and datasets, with some achieving perfect and near-perfect defense in Enhanced Mode. We also analyze the trade-off between defense success rate improvement and computational overhead. This self-consciousness method offers a lightweight, cost-effective solution for enhancing LLM ethics, particularly beneficial for GenAI use cases across various platforms.",
        "arxiv_id": "2508.02961",
        "ARXIVID": "2508.02961",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of improving LLMs through self-consciousness mechanisms.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03186": {
        "authors": [
            "Heng Wu",
            "Qian Zhang",
            "Guixu Zhang"
        ],
        "title": "Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling",
        "abstract": "arXiv:2508.03186v1 Announce Type: new  Abstract: Accurate monocular depth estimation remains a challenging problem due to the inherent ambiguity that stems from the ill-posed nature of recovering 3D structure from a single view, where multiple plausible depth configurations can produce identical 2D projections. In this paper, we present a novel depth estimation method that combines both local and global cues to improve prediction accuracy. Specifically, we propose the Gated Large Kernel Attention Module (GLKAM) to effectively capture multi-scale local structural information by leveraging large kernel convolutions with a gated mechanism. To further enhance the global perception of the network, we introduce the Global Bin Prediction Module (GBPM), which estimates the global distribution of depth bins and provides structural guidance for depth regression. Extensive experiments on the NYU-V2 and KITTI dataset demonstrate that our method achieves competitive performance and outperforms existing approaches, validating the effectiveness of each proposed component.",
        "arxiv_id": "2508.03186",
        "ARXIVID": "2508.03186",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and depth estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03415": {
        "authors": [
            "Shivangi Nigam",
            "Adarsh Prasad Behera",
            "Shekhar Verma",
            "P. Nagabhushan"
        ],
        "title": "Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN",
        "abstract": "arXiv:2508.03415v1 Announce Type: new  Abstract: This paper presents Fd-CycleGAN, an image-to-image (I2I) translation framework that enhances latent representation learning to approximate real data distributions. Building upon the foundation of CycleGAN, our approach integrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to capture fine-grained local pixel semantics while preserving structural coherence from the source domain. We employ distribution-based loss metrics, including KL/JS divergence and log-based similarity measures, to explicitly quantify the alignment between real and generated image distributions in both spatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we conduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a synthetically augmented Strike-off dataset. Compared to baseline CycleGAN and other state-of-the-art methods, our approach demonstrates superior perceptual quality, faster convergence, and improved mode diversity, particularly in low-data regimes. By effectively capturing local and global distribution characteristics, Fd-CycleGAN achieves more visually coherent and semantically consistent translations. Our results suggest that frequency-guided latent learning significantly improves generalization in image translation tasks, with promising applications in document restoration, artistic style transfer, and medical image synthesis. We also provide comparative insights with diffusion-based generative models, highlighting the advantages of our lightweight adversarial approach in terms of training efficiency and qualitative output.",
        "arxiv_id": "2508.03415",
        "ARXIVID": "2508.03415",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and image-to-image translation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03516": {
        "authors": [
            "Shiben Liu",
            "Mingyue Xu",
            "Huijie Fan",
            "Qiang Wang",
            "Yandong Tang",
            "Zhi Han"
        ],
        "title": "Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification",
        "abstract": "arXiv:2508.03516v1 Announce Type: new  Abstract: Lifelong person re-identification (LReID) encounters a key challenge: balancing the preservation of old knowledge with adaptation to new information. Existing LReID methods typically employ knowledge distillation to enforce representation alignment. However, these approaches ignore two crucial aspects: specific distribution awareness and cross-domain unified knowledge learning, both of which are essential for addressing this challenge. To overcome these limitations, we propose a novel distribution-aware knowledge unification and association (DKUA) framework where domain-style modeling is performed for each instance to propagate domain-specific representations, enhancing anti-forgetting and generalization capacity. Specifically, we design a distribution-aware model to transfer instance-level representations of the current domain into the domain-specific representations with the different domain styles, preserving learned knowledge without storing old samples. Next, we propose adaptive knowledge consolidation (AKC) to dynamically generate the unified representation as a cross-domain representation center. To further mitigate forgetting, we develop a unified knowledge association (UKA) mechanism, which explores the unified representation as a bridge to explicitly model inter-domain associations, reducing inter-domain gaps. Finally, distribution-based knowledge transfer (DKT) is proposed to prevent the current domain distribution from deviating from the cross-domain distribution center, improving adaptation capacity. Experimental results show our DKUA outperforms the existing methods by 7.6%/5.3% average mAP/R@1 improvement on anti-forgetting and generalization capacity, respectively. Our code will be publicly released.",
        "arxiv_id": "2508.03516",
        "ARXIVID": "2508.03516",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of lifelong learning and person re-identification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03132": {
        "authors": [
            "Arion Zimmermann",
            "Soon-Jo Chung",
            "Fred Hadaegh"
        ],
        "title": "COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks",
        "abstract": "arXiv:2508.03132v1 Announce Type: new  Abstract: The accurate state estimation of unknown bodies in space is a critical challenge with applications ranging from the tracking of space debris to the shape estimation of small bodies. A necessary enabler to this capability is to find and track features on a continuous stream of images. Existing methods, such as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates, whereas modern deep learning methods yield higher quality features at the cost of more demanding computational resources which might not be available on space-qualified hardware. Additionally, both classical and data-driven methods are not robust to the highly opaque self-cast shadows on the object of interest. We show that, as the target body rotates, these shadows may lead to large biases in the resulting pose estimates. For these objects, a bias in the real-time pose estimation algorithm may mislead the spacecraft's state estimator and cause a mission failure, especially if the body undergoes a chaotic tumbling motion. We present COFFEE, the Celestial Occlusion Fast FEature Extractor, a real-time pose estimation framework for asteroids designed to leverage prior information on the sun phase angle given by sun-tracking sensors commonly available onboard spacecraft. By associating salient contours to their projected shadows, a sparse set of features are detected, invariant to the motion of the shadows. A Sparse Neural Network followed by an attention-based Graph Neural Network feature matching model are then jointly trained to provide a set of correspondences between successive frames. The resulting pose estimation pipeline is found to be bias-free, more accurate than classical pose estimation pipelines and an order of magnitude faster than other state-of-the-art deep learning pipelines on synthetic data as well as on renderings of the tumbling asteroid Apophis.",
        "arxiv_id": "2508.03132",
        "ARXIVID": "2508.03132",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and pose estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03625": {
        "authors": [
            "Daniel DeAlcala",
            "Aythami Morales",
            "Julian Fierrez",
            "Ruben Tolosana"
        ],
        "title": "AttZoom: Attention Zoom for Better Visual Features",
        "abstract": "arXiv:2508.03625v1 Announce Type: new  Abstract: We present Attention Zoom, a modular and model-agnostic spatial attention mechanism designed to improve feature extraction in convolutional neural networks (CNNs). Unlike traditional attention approaches that require architecture-specific integration, our method introduces a standalone layer that spatially emphasizes high-importance regions in the input. We evaluated Attention Zoom on multiple CNN backbones using CIFAR-100 and TinyImageNet, showing consistent improvements in Top-1 and Top-5 classification accuracy. Visual analyses using Grad-CAM and spatial warping reveal that our method encourages fine-grained and diverse attention patterns. Our results confirm the effectiveness and generality of the proposed layer for improving CCNs with minimal architectural overhead.",
        "arxiv_id": "2508.03625",
        "ARXIVID": "2508.03625",
        "COMMENT": "Does not match any specific criterion but is related to improving CNNs with spatial attention, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03006": {
        "authors": [
            "Fan Yang",
            "Yihao Huang",
            "Jiayi Zhu",
            "Ling Shi",
            "Geguang Pu",
            "Jin Song Dong",
            "Kailong Wang"
        ],
        "title": "Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models",
        "abstract": "arXiv:2508.03006v1 Announce Type: new  Abstract: Diffusion-based text-to-image (T2I) models enable high-quality image generation but also pose significant risks of misuse, particularly in producing not-safe-for-work (NSFW) content. While prior detection methods have focused on filtering prompts before generation or moderating images afterward, the in-generation phase of diffusion models remains largely unexplored for NSFW detection. In this paper, we introduce In-Generation Detection (IGD), a simple yet effective approach that leverages the predicted noise during the diffusion process as an internal signal to identify NSFW content. This approach is motivated by preliminary findings suggesting that the predicted noise may capture semantic cues that differentiate NSFW from benign prompts, even when the prompts are adversarially crafted. Experiments conducted on seven NSFW categories show that IGD achieves an average detection accuracy of 91.32% over naive and adversarial NSFW prompts, outperforming seven baseline methods.",
        "arxiv_id": "2508.03006",
        "ARXIVID": "2508.03006",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in generative modeling and safety in text-to-image models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03030": {
        "authors": [
            "Siyuan Li",
            "Yifan Yu",
            "Yanchen Deng",
            "Zhihao Zhang",
            "Mengjing Chen",
            "Fangzhou Zhu",
            "Tao Zhong",
            "Jianye Hao",
            "Peng Liu",
            "Bo An"
        ],
        "title": "Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming",
        "abstract": "arXiv:2508.03030v1 Announce Type: new  Abstract: Mixed-integer linear programming (MILP) has been a fundamental problem in combinatorial optimization. Previous works have designed a plethora of hard-coded heuristics to accomplish challenging MILP solving with domain knowledge. Driven by the high capability of neural networks, recent research is devoted to replacing manually designed heuristics with learned policies. Although learning-based MILP methods have shown great promise, existing worksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without considering their interdependence, severely hurting the solving speed and quality. To address this issue, we propose a novel multi-agent-based policy learning framework for MILP (Collab-Solver), which can collaboratively optimize the policies for multiple modules. Specifically, we formulate the collaboration of cut selection and branching in MILP solving as a Stackelberg game. Under this formulation, we develop a two-phase learning paradigm to stabilize the collaborative policy learning, where the first phase achieves the data-communicated policy pretraining and the second phase further orchestrates the policy learning for various modules. The jointly learned policy significantly improves the solving performance on both synthetic and large-scale real-world MILP datasets. Moreover, the policies learned by Collab-Solver have also demonstrated excellent generalization abilities across different instance sets.",
        "arxiv_id": "2508.03030",
        "ARXIVID": "2508.03030",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in optimization and machine learning methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03252": {
        "authors": [
            "Wentao Qu",
            "Guofeng Mei",
            "Jing Wang",
            "Yujiao Wu",
            "Xiaoshui Huang",
            "Liang Xiao"
        ],
        "title": "Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion",
        "abstract": "arXiv:2508.03252v1 Announce Type: new  Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust 3D object detection tasks. Existing methods often rely on the score matching from 3D boxes or pre-trained diffusion priors. However, they typically require multi-step iterations in inference, which limits efficiency. To address this, we propose a \\textbf{R}obust single-stage fully \\textbf{S}parse 3D object \\textbf{D}etection \\textbf{Net}work with a Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in latent feature spaces through lightweight denoising networks like multi-level denoising autoencoders (DAEs). This enables RSDNet to effectively understand scene distributions under multi-level perturbations, achieving robust and reliable detection. Meanwhile, we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise samples and targets, enhancing RSDNet robustness to multiple perturbations. Furthermore, a semantic-geometric conditional guidance is introduced to perceive the object boundaries and shapes, alleviating the center feature missing problem in sparse representations, enabling RSDNet to perform in a fully sparse detection pipeline. Moreover, the detachable denoising network design of DLF enables RSDNet to perform single-step detection in inference, further enhancing detection efficiency. Extensive experiments on public benchmarks show that RSDNet can outperform existing methods, achieving state-of-the-art detection.",
        "arxiv_id": "2508.03252",
        "ARXIVID": "2508.03252",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in machine learning and robust detection methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03077": {
        "authors": [
            "Anran Wu",
            "Long Peng",
            "Xin Di",
            "Xueyuan Dai",
            "Chen Wu",
            "Yang Wang",
            "Xueyang Fu",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions",
        "abstract": "arXiv:2508.03077v1 Announce Type: new  Abstract: Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of optimization-based 3DGS by enabling fast and high-quality reconstruction without the need for per-scene optimization. However, existing feedforward approaches typically assume that input multi-view images are clean and high-quality. In real-world scenarios, images are often captured under challenging conditions such as noise, low light, or rain, resulting in inaccurate geometry and degraded 3D reconstruction. To address these challenges, we propose a general and efficient multi-view feature enhancement module, RobustGS, which substantially improves the robustness of feedforward 3DGS methods under various adverse imaging conditions, enabling high-quality 3D reconstruction. The RobustGS module can be seamlessly integrated into existing pretrained pipelines in a plug-and-play manner to enhance reconstruction robustness. Specifically, we introduce a novel component, Generalized Degradation Learner, designed to extract generic representations and distributions of multiple degradations from multi-view inputs, thereby enhancing degradation-awareness and improving the overall quality of 3D reconstruction. In addition, we propose a novel semantic-aware state-space model. It first leverages the extracted degradation representations to enhance corrupted inputs in the feature space. Then, it employs a semantic-aware strategy to aggregate semantically similar information across different views, enabling the extraction of fine-grained cross-view correspondences and further improving the quality of 3D representations. Extensive experiments demonstrate that our approach, when integrated into existing methods in a plug-and-play manner, consistently achieves state-of-the-art reconstruction quality across various types of degradations.",
        "arxiv_id": "2508.03077",
        "ARXIVID": "2508.03077",
        "COMMENT": "Does not match any specific criteria but involves improving 3D reconstruction under challenging conditions, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03539": {
        "authors": [
            "Long Qian",
            "Bingke Zhu",
            "Yingying Chen",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection",
        "abstract": "arXiv:2508.03539v1 Announce Type: new  Abstract: Despite substantial progress in anomaly synthesis methods, existing diffusion-based and coarse inpainting pipelines commonly suffer from structural deficiencies such as micro-structural discontinuities, limited semantic controllability, and inefficient generation. To overcome these limitations, we introduce ARAS, a language-conditioned, auto-regressive anomaly synthesis approach that precisely injects local, text-specified defects into normal images via token-anchored latent editing. Leveraging a hard-gated auto-regressive operator and a training-free, context-preserving masked sampling kernel, ARAS significantly enhances defect realism, preserves fine-grained material textures, and provides continuous semantic control over synthesized anomalies. Integrated within our Quality-Aware Re-weighted Anomaly Detection (QARAD) framework, we further propose a dynamic weighting strategy that emphasizes high-quality synthetic samples by computing an image-text similarity score with a dual-encoder model. Extensive experiments across three benchmark datasets-MVTec AD, VisA, and BTAD, demonstrate that our QARAD outperforms SOTA methods in both image- and pixel-level anomaly detection tasks, achieving improved accuracy, robustness, and a 5 times synthesis speedup compared to diffusion-based alternatives. Our complete code and synthesized dataset will be publicly available.",
        "arxiv_id": "2508.03539",
        "ARXIVID": "2508.03539",
        "COMMENT": "Does not match any specific criteria but involves anomaly synthesis and detection, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03388": {
        "authors": [
            "Yizhe Xiong",
            "Zihan Zhou",
            "Yiwen Liang",
            "Hui Chen",
            "Zijia Lin",
            "Tianxiang Hao",
            "Fan Zhang",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "title": "Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation",
        "abstract": "arXiv:2508.03388v1 Announce Type: new  Abstract: Test-Time Adaptation (TTA) has emerged as an effective solution for adapting Vision Transformers (ViT) to distribution shifts without additional training data. However, existing TTA methods often incur substantial computational overhead, limiting their applicability in resource-constrained real-world scenarios. To reduce inference cost, plug-and-play token aggregation methods merge redundant tokens in ViTs to reduce total processed tokens. Albeit efficient, it suffers from significant performance degradation when directly integrated with existing TTA methods. We formalize this problem as Efficient Test-Time Adaptation (ETTA), seeking to preserve the adaptation capability of TTA while reducing inference latency. In this paper, we first provide a theoretical analysis from a novel mutual information perspective, showing that token aggregation inherently leads to information loss, which cannot be fully mitigated by conventional norm-tuning-based TTA methods. Guided by this insight, we propose to \\textbf{N}eutralize Token \\textbf{A}ggregation \\textbf{v}ia \\textbf{I}nformation \\textbf{A}ugmentation (\\textbf{NAVIA}). Specifically, we directly augment the [CLS] token embedding and incorporate adaptive biases into the [CLS] token in shallow layers of ViTs. We theoretically demonstrate that these augmentations, when optimized via entropy minimization, recover the information lost due to token aggregation. Extensive experiments across various out-of-distribution benchmarks demonstrate that NAVIA significantly outperforms state-of-the-art methods by over 2.5\\%, while achieving an inference latency reduction of more than 20\\%, effectively addressing the ETTA challenge.",
        "arxiv_id": "2508.03388",
        "ARXIVID": "2508.03388",
        "COMMENT": "Does not match any specific criteria but involves efficient test-time adaptation for vision transformers, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03038": {
        "authors": [
            "Qi Peng",
            "Jialin Cui",
            "Jiayuan Xie",
            "Yi Cai",
            "Qing Li"
        ],
        "title": "Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree",
        "abstract": "arXiv:2508.03038v1 Announce Type: new  Abstract: Large language models (LLMs) have shown great potential in the medical domain. However, existing models still fall short when faced with complex medical diagnosis task in the real world. This is mainly because they lack sufficient reasoning depth, which leads to information loss or logical jumps when processing a large amount of specialized medical data, leading to diagnostic errors. To address these challenges, we propose Tree-of-Reasoning (ToR), a novel multi-agent framework designed to handle complex scenarios. Specifically, ToR introduces a tree structure that can clearly record the reasoning path of LLMs and the corresponding clinical evidence. At the same time, we propose a cross-validation mechanism to ensure the consistency of multi-agent decision-making, thereby improving the clinical reasoning ability of multi-agents in complex medical scenarios. Experimental results on real-world medical data show that our framework can achieve better performance than existing baseline methods.",
        "arxiv_id": "2508.03038",
        "ARXIVID": "2508.03038",
        "COMMENT": "Does not match any specific criteria but involves multi-agent reasoning and clinical reasoning, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03483": {
        "authors": [
            "Dasol Choi Jihwan Lee",
            "Minjae Lee",
            "Minsuk Kahng"
        ],
        "title": "When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models",
        "abstract": "arXiv:2508.03483v1 Announce Type: new  Abstract: While prior research on text-to-image generation has predominantly focused on biases in human depictions, we investigate a more subtle yet pervasive phenomenon: demographic bias in generated objects (e.g., cars). We introduce SODA (Stereotyped Object Diagnostic Audit), a novel framework for systematically measuring such biases. Our approach compares visual attributes of objects generated with demographic cues (e.g., \"for young people'') to those from neutral prompts, across 2,700 images produced by three state-of-the-art models (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories. Through a comprehensive analysis, we uncover strong associations between specific demographic groups and visual attributes, such as recurring color patterns prompted by gender or ethnicity cues. These patterns reflect and reinforce not only well-known stereotypes but also more subtle and unintuitive biases. We also observe that some models generate less diverse outputs, which in turn amplifies the visual disparities compared to neutral prompts. Our proposed auditing framework offers a practical approach for testing, revealing how stereotypes still remain embedded in today's generative models. We see this as an essential step toward more systematic and responsible AI development.",
        "arxiv_id": "2508.03483",
        "ARXIVID": "2508.03483",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of bias in generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.02999": {
        "authors": [
            "Xinjie Zhao",
            "Moritz Blum",
            "Fan Gao",
            "Yingjian Chen",
            "Boming Yang",
            "Luis Marquez-Carpintero",
            "M\\'onica Pina-Navarro",
            "Yanran Fu",
            "So Morikawa",
            "Yusuke Iwasawa",
            "Yutaka Matsuo",
            "Chanjun Park",
            "Irene Li"
        ],
        "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots",
        "abstract": "arXiv:2508.02999v1 Announce Type: new  Abstract: AGENTiGraph is a user-friendly, agent-driven system that enables intuitive interaction and management of domain-specific data through the manipulation of knowledge graphs in natural language. It gives non-technical users a complete, visual solution to incrementally build and refine their knowledge bases, allowing multi-round dialogues and dynamic updates without specialized query languages. The flexible design of AGENTiGraph, including intent classification, task planning, and automatic knowledge integration, ensures seamless reasoning between diverse tasks. Evaluated on a 3,500-query benchmark within an educational scenario, the system outperforms strong zero-shot baselines (achieving 95.12% classification accuracy, 90.45% execution success), indicating potential scalability to compliance-critical or multi-step queries in legal and medical domains, e.g., incorporating new statutes or research on the fly. Our open-source demo offers a powerful new paradigm for multi-turn enterprise knowledge management that bridges LLMs and structured graphs.",
        "arxiv_id": "2508.02999",
        "ARXIVID": "2508.02999",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of multi-modal learning and knowledge graph integration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03324": {
        "authors": [
            "Satyapreet Singh Yadav",
            "Chandra Sekhar Seelamantula",
            "Chetan Singh Thakur"
        ],
        "title": "Live Demonstration: Neuromorphic Radar for Gesture Recognition",
        "abstract": "arXiv:2508.03324v1 Announce Type: new  Abstract: We present a neuromorphic radar framework for real-time, low-power hand gesture recognition (HGR) using an event-driven architecture inspired by biological sensing. Our system comprises a 24 GHz Doppler radar front-end and a custom neuromorphic sampler that converts intermediate-frequency (IF) signals into sparse spike-based representations via asynchronous sigma-delta encoding. These events are directly processed by a lightweight neural network deployed on a Cortex-M0 microcontroller, enabling low-latency inference without requiring spectrogram reconstruction. Unlike conventional radar HGR pipelines that continuously sample and process data, our architecture activates only when meaningful motion is detected, significantly reducing memory, power, and computation overhead. Evaluated on a dataset of five gestures collected from seven users, our system achieves > 85% real-time accuracy. To the best of our knowledge, this is the first work that employs bio-inspired asynchronous sigma-delta encoding and an event-driven processing framework for radar-based HGR.",
        "arxiv_id": "2508.03324",
        "ARXIVID": "2508.03324",
        "COMMENT": "Does not match any specific criteria but is related to neuromorphic radar for gesture recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.02923": {
        "authors": [
            "Minh-Hai Nguyen",
            "Edouard Pauwels",
            "Pierre Weiss"
        ],
        "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution",
        "abstract": "arXiv:2508.02923v1 Announce Type: new  Abstract: The Maximum A Posteriori (MAP) estimation is a widely used framework in blind deconvolution to recover sharp images from blurred observations. The estimated image and blur filter are defined as the maximizer of the posterior distribution. However, when paired with sparsity-promoting image priors, MAP estimation has been shown to favors blurry solutions, limiting its effectiveness. In this paper, we revisit this result using diffusion-based priors, a class of models that capture realistic image distributions. Through an empirical examination of the prior's likelihood landscape, we uncover two key properties: first, blurry images tend to have higher likelihoods; second, the landscape contains numerous local minimizers that correspond to natural images. Building on these insights, we provide a theoretical analysis of the blind deblurring posterior. This reveals that the MAP estimator tends to produce sharp filters (close to the Dirac delta function) and blurry solutions. However local minimizers of the posterior, which can be obtained with gradient descent, correspond to realistic, natural images, effectively solving the blind deconvolution problem. Our findings suggest that overcoming MAP's limitations requires good local initialization to local minima in the posterior landscape. We validate our analysis with numerical experiments, demonstrating the practical implications of our insights for designing improved priors and optimization techniques.",
        "arxiv_id": "2508.02923",
        "ARXIVID": "2508.02923",
        "COMMENT": "Does not match any specific criteria but is related to diffusion priors and blind deconvolution.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03017": {
        "authors": [
            "Liheng Zhang",
            "Weihao Yu",
            "Zubo Lu",
            "Haozhi Gu",
            "Jin Huang"
        ],
        "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting",
        "abstract": "arXiv:2508.03017v1 Announce Type: new  Abstract: Recent advancements in 3D Gaussian Splatting have enhanced efficient and high-quality novel view synthesis. However, representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance. To address this issue, we propose SA-3DGS, a method that significantly reduces storage costs while maintaining rendering quality. SA-3DGS learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, thereby enabling effective pruning and redundancy reduction. Next, the importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, improving the codebook's expressive capability while reducing model size. Finally, the codebook repair module leverages contextual scene information to repair the codebook, thereby recovering the original Gaussian point attributes and mitigating the degradation in rendering quality caused by information loss. Experimental results on several benchmark datasets show that our method achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods (e.g., LightGaussian), showcasing excellent performance and strong generalization ability.",
        "arxiv_id": "2508.03017",
        "ARXIVID": "2508.03017",
        "COMMENT": "Does not match any specific criteria but is related to 3D Gaussian splatting and compression.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03397": {
        "authors": [
            "Xinzhu Li",
            "Juepeng Zheng",
            "Yikun Chen",
            "Xudong Mao",
            "Guanghui Yue",
            "Wei Zhou",
            "Chenlei Lv",
            "Ruomei Wang",
            "Fan Zhou",
            "Baoquan Zhao"
        ],
        "title": "DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition",
        "abstract": "arXiv:2508.03397v1 Announce Type: new  Abstract: Robust gait recognition requires highly discriminative representations, which are closely tied to input modalities. While binary silhouettes and skeletons have dominated recent literature, these 2D representations fall short of capturing sufficient cues that can be exploited to handle viewpoint variations, and capture finer and meaningful details of gait. In this paper, we introduce a novel framework, termed DepthGait, that incorporates RGB-derived depth maps and silhouettes for enhanced gait recognition. Specifically, apart from the 2D silhouette representation of the human body, the proposed pipeline explicitly estimates depth maps from a given RGB image sequence and uses them as a new modality to capture discriminative features inherent in human locomotion. In addition, a novel multi-scale and cross-level fusion scheme has also been developed to bridge the modality gap between depth maps and silhouettes. Extensive experiments on standard benchmarks demonstrate that the proposed DepthGait achieves state-of-the-art performance compared to peer methods and attains an impressive mean rank-1 accuracy on the challenging datasets.",
        "arxiv_id": "2508.03397",
        "ARXIVID": "2508.03397",
        "COMMENT": "Does not match any specific criteria but is related to gait recognition using depth and silhouette sequences.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03189": {
        "authors": [
            "Tianshuo Zhang",
            "Siran Peng",
            "Li Gao",
            "Haoyuan Zhang",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "title": "Unifying Locality of KANs and Feature Drift Compensation for Data-free Continual Face Forgery Detection",
        "abstract": "arXiv:2508.03189v1 Announce Type: new  Abstract: The rapid advancements in face forgery techniques necessitate that detectors continuously adapt to new forgery methods, thus situating face forgery detection within a continual learning paradigm. However, when detectors learn new forgery types, their performance on previous types often degrades rapidly, a phenomenon known as catastrophic forgetting. Kolmogorov-Arnold Networks (KANs) utilize locally plastic splines as their activation functions, enabling them to learn new tasks by modifying only local regions of the functions while leaving other areas unaffected. Therefore, they are naturally suitable for addressing catastrophic forgetting. However, KANs have two significant limitations: 1) the splines are ineffective for modeling high-dimensional images, while alternative activation functions that are suitable for images lack the essential property of locality; 2) in continual learning, when features from different domains overlap, the mapping of different domains to distinct curve regions always collapses due to repeated modifications of the same regions. In this paper, we propose a KAN-based Continual Face Forgery Detection (KAN-CFD) framework, which includes a Domain-Group KAN Detector (DG-KD) and a data-free replay Feature Separation strategy via KAN Drift Compensation Projection (FS-KDCP). DG-KD enables KANs to fit high-dimensional image inputs while preserving locality and local plasticity. FS-KDCP avoids the overlap of the KAN input spaces without using data from prior tasks. Experimental results demonstrate that the proposed method achieves superior performance while notably reducing forgetting.",
        "arxiv_id": "2508.03189",
        "ARXIVID": "2508.03189",
        "COMMENT": "Does not match any specific criteria but is related to continual learning and face forgery detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03256": {
        "authors": [
            "Gang Dai",
            "Yifan Zhang",
            "Yutao Qin",
            "Qiangya Guo",
            "Shuangping Huang",
            "Shuicheng Yan"
        ],
        "title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation",
        "abstract": "arXiv:2508.03256v1 Announce Type: new  Abstract: Existing handwritten text generation methods primarily focus on isolated words. However, realistic handwritten text demands attention not only to individual words but also to the relationships between them, such as vertical alignment and horizontal spacing. Therefore, generating entire text lines emerges as a more promising and comprehensive task. However, this task poses significant challenges, including the accurate modeling of complex style patterns encompassing both intra- and inter-word relationships, and maintaining content accuracy across numerous characters. To address these challenges, we propose DiffBrush, a novel diffusion-based model for handwritten text-line generation. Unlike existing methods, DiffBrush excels in both style imitation and content accuracy through two key strategies: (1) content-decoupled style learning, which disentangles style from content to better capture intra-word and inter-word style patterns by using column- and row-wise masking; and (2) multi-scale content learning, which employs line and word discriminators to ensure global coherence and local accuracy of textual content. Extensive experiments show that DiffBrush excels in generating high-quality text lines, particularly in style reproduction and content preservation. Code is available at https://github.com/dailenson/DiffBrush.",
        "arxiv_id": "2508.03256",
        "ARXIVID": "2508.03256",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling for handwritten text.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03373": {
        "authors": [
            "Ni Tang",
            "Xiaotong Luo",
            "Zihan Cheng",
            "Liangtai Zhou",
            "Dongxiao Zhang",
            "Yanyun Qu"
        ],
        "title": "Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration",
        "abstract": "arXiv:2508.03373v1 Announce Type: new  Abstract: Diffusion models have revealed powerful potential in all-in-one image restoration (AiOIR), which is talented in generating abundant texture details. The existing AiOIR methods either retrain a diffusion model or fine-tune the pretrained diffusion model with extra conditional guidance. However, they often suffer from high inference costs and limited adaptability to diverse degradation types. In this paper, we propose an efficient AiOIR method, Diffusion Once and Done (DOD), which aims to achieve superior restoration performance with only one-step sampling of Stable Diffusion (SD) models. Specifically, multi-degradation feature modulation is first introduced to capture different degradation prompts with a pretrained diffusion model. Then, parameter-efficient conditional low-rank adaptation integrates the prompts to enable the fine-tuning of the SD model for adapting to different degradation types. Besides, a high-fidelity detail enhancement module is integrated into the decoder of SD to improve structural and textural details. Experiments demonstrate that our method outperforms existing diffusion-based restoration approaches in both visual quality and inference efficiency.",
        "arxiv_id": "2508.03373",
        "ARXIVID": "2508.03373",
        "COMMENT": "Does not match any specific criteria but is related to image restoration using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03484": {
        "authors": [
            "Zhiyao Xu",
            "Dan Zhao",
            "Qingsong Zou",
            "Qing Li",
            "Yong Jiang",
            "Yuhang Wang",
            "Jingyu Xiao"
        ],
        "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes",
        "abstract": "arXiv:2508.03484v1 Announce Type: new  Abstract: As smart homes become increasingly prevalent, intelligent models are widely used for tasks such as anomaly detection and behavior prediction. These models are typically trained on static datasets, making them brittle to behavioral drift caused by seasonal changes, lifestyle shifts, or evolving routines. However, collecting new behavior data for retraining is often impractical due to its slow pace, high cost, and privacy concerns. In this paper, we propose SmartGen, an LLM-based framework that synthesizes context-aware user behavior data to support continual adaptation of downstream smart home models. SmartGen consists of four key components. First, we design a Time and Semantic-aware Split module to divide long behavior sequences into manageable, semantically coherent subsequences under dual time-span constraints. Second, we propose Semantic-aware Sequence Compression to reduce input length while preserving representative semantics by clustering behavior mapping in latent space. Third, we introduce Graph-guided Sequence Synthesis, which constructs a behavior relationship graph and encodes frequent transitions into prompts, guiding the LLM to generate data aligned with contextual changes while retaining core behavior patterns. Finally, we design a Two-stage Outlier Filter to identify and remove implausible or semantically inconsistent outputs, aiming to improve the factual coherence and behavioral validity of the generated sequences. Experiments on three real-world datasets demonstrate that SmartGen significantly enhances model performance on anomaly detection and behavior prediction tasks under behavioral drift, with anomaly detection improving by 85.43% and behavior prediction by 70.51% on average. The code is available at https://github.com/horizonsinzqs/SmartGen.",
        "arxiv_id": "2508.03484",
        "ARXIVID": "2508.03484",
        "COMMENT": "Does not match any specific criteria but involves behavior sequence generation for smart homes, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.03441": {
        "authors": [
            "Ning Zhu",
            "Xiaochuan Ma",
            "Shaoting Zhang",
            "Guotai Wang"
        ],
        "title": "MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis",
        "abstract": "arXiv:2508.03441v1 Announce Type: new  Abstract: Cold-Start Active Learning (CSAL) aims to select informative samples for annotation without prior knowledge, which is important for improving annotation efficiency and model performance under a limited annotation budget in medical image analysis. Most existing CSAL methods rely on Self-Supervised Learning (SSL) on the target dataset for feature extraction, which is inefficient and limited by insufficient feature representation. Recently, pre-trained Foundation Models (FMs) have shown powerful feature extraction ability with a potential for better CSAL. However, this paradigm has been rarely investigated, with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, we propose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medical image analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasets under different annotation budgets, covering classification and segmentation tasks from diverse medical modalities. It is also the first CSAL benchmark that evaluates both the feature extraction and sample selection stages. Our experimental results reveal that: 1) Most FMs are effective feature extractors for CSAL, with DINO family performing the best in segmentation; 2) The performance differences of these FMs are large in segmentation tasks, while small for classification; 3) Different sample selection strategies should be considered in CSAL on different datasets, with Active Learning by Processing Surprisal (ALPS) performing the best in segmentation while RepDiv leading for classification. The code is available at https://github.com/HiLab-git/MedCAL-Bench.",
        "arxiv_id": "2508.03441",
        "ARXIVID": "2508.03441",
        "COMMENT": "Does not match any specific criteria but involves benchmarks for medical image analysis, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}