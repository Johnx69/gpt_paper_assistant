{
    "2506.15298": {
        "authors": [
            "Xinqi Fan",
            "Jingting Li",
            "John See",
            "Moi Hoon Yap",
            "Wen-Huang Cheng",
            "Xiaobai Li",
            "Xiaopeng Hong",
            "Su-Jing Wang",
            "Adrian K. Davision"
        ],
        "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering",
        "abstract": "arXiv:2506.15298v1 Announce Type: new  Abstract: Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environment. In recent years, substantial advancements have been made in the areas of ME recognition, spotting, and generation. However, conventional approaches that treat spotting and recognition as separate tasks are suboptimal, particularly for analyzing long-duration videos in realistic settings. Concurrently, the emergence of multimodal large language models (MLLMs) and large vision-language models (LVLMs) offers promising new avenues for enhancing ME analysis through their powerful multimodal reasoning capabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that reflect these evolving research directions: (1) ME spot-then-recognize (ME-STR), which integrates ME spotting and subsequent recognition in a unified sequential pipeline; and (2) ME visual question answering (ME-VQA), which explores ME understanding through visual question answering, leveraging MLLMs or LVLMs to address diverse question types related to MEs. All participating algorithms are required to run on this test set and submit their results on a leaderboard. More details are available at https://megc2025.github.io.",
        "arxiv_id": "2506.15298",
        "ARXIVID": "2506.15298",
        "COMMENT": "Matches criteria 2 and 6 as it introduces tasks involving multimodal large language models (MLLMs) for micro-expression analysis and visual question answering, which are video-based tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.15677": {
        "authors": [
            "Yining Hong",
            "Rui Sun",
            "Bingxuan Li",
            "Xingcheng Yao",
            "Maxine Wu",
            "Alexander Chien",
            "Da Yin",
            "Ying Nian Wu",
            "Zhecan James Wang",
            "Kai-Wei Chang"
        ],
        "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence",
        "abstract": "arXiv:2506.15677v1 Announce Type: new  Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.",
        "arxiv_id": "2506.15677",
        "ARXIVID": "2506.15677",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for embodied web agents integrating physical and digital intelligence.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.15220": {
        "authors": [
            "Changli Tang",
            "Yixuan Li",
            "Yudong Yang",
            "Jimin Zhuang",
            "Guangzhi Sun",
            "Wei Li",
            "Zejun Ma",
            "Chao Zhang"
        ],
        "title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models",
        "abstract": "arXiv:2506.15220v1 Announce Type: new  Abstract: Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with paired audio) captioning through directed preference optimisation (DPO). We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimised using DPO. To further improve training, we propose a novel multi-round DPO (MrDPO) approach, which involves periodically updating the DPO reference model, merging and re-initialising the LoRA module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilise the process. Experimental results show that MrDPO significantly enhances video-SALMONN 2's captioning accuracy, reducing the captioning error rates by 28\\%. The final video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining highly competitive performance to the state-of-the-art on widely used video question-answering benchmarks among models of similar size. Codes are available at \\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.",
        "arxiv_id": "2506.15220",
        "ARXIVID": "2506.15220",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 6 (Video Understanding) as it focuses on video captioning using an advanced audio-visual large language model.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.15564": {
        "authors": [
            "Jinheng Xie",
            "Zhenheng Yang",
            "Mike Zheng Shou"
        ],
        "title": "Show-o2: Improved Native Unified Multimodal Models",
        "abstract": "arXiv:2506.15564v1 Announce Type: new  Abstract: This paper presents improved native unified multimodal models, \\emph{i.e.,} Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o.",
        "arxiv_id": "2506.15564",
        "ARXIVID": "2506.15564",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a unified multimodal model for text, image, and video understanding and generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.15218": {
        "authors": [
            "Dan He",
            "Weisheng Li",
            "Guofen Wang",
            "Yuping Huang",
            "Shiqiang Liu"
        ],
        "title": "DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder",
        "abstract": "arXiv:2506.15218v1 Announce Type: new  Abstract: Multimodal medical image fusion (MMIF) extracts the most meaningful information from multiple source images, enabling a more comprehensive and accurate diagnosis. Achieving high-quality fusion results requires a careful balance of brightness, color, contrast, and detail; this ensures that the fused images effectively display relevant anatomical structures and reflect the functional status of the tissues. However, existing MMIF methods have limited capacity to capture detailed features during conventional training and suffer from insufficient cross-modal feature interaction, leading to suboptimal fused image quality. To address these issues, this study proposes a two-stage diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In Stage I, a diffusion process trains UNet for image reconstruction. UNet captures detailed information through progressive denoising and represents multilevel data, providing a rich set of feature representations for the subsequent fusion network. In Stage II, noisy images at various steps are input into the fusion network to enhance the model's feature recognition capability. Three key fusion modules are also integrated to process medical images from different modalities adaptively. Ultimately, the robust network structure and a hybrid loss function are integrated to harmonize the fused image's brightness, color, contrast, and detail, enhancing its quality and information density. The experimental results across various medical image types demonstrate that the proposed method performs exceptionally well regarding objective evaluation metrics. The fused image preserves appropriate brightness, a comprehensive distribution of radioactive tracers, rich textures, and clear edges. The code is available at https://github.com/HeDan-11/DM-FNet.",
        "arxiv_id": "2506.15218",
        "ARXIVID": "2506.15218",
        "COMMENT": "Matches criterion 5 as it proposes a novel diffusion model-based framework for multimodal medical image fusion, integrating image understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.15675": {
        "authors": [
            "Zhen Li",
            "Chuanhao Li",
            "Xiaofeng Mao",
            "Shaoheng Lin",
            "Ming Li",
            "Shitian Zhao",
            "Zhaopan Xu",
            "Xinyue Li",
            "Yukang Feng",
            "Jianwen Sun",
            "Zizhen Li",
            "Fanrui Zhang",
            "Jiaxin Ai",
            "Zhixiang Wang",
            "Yuwei Wu",
            "Tong He",
            "Jiangmiao Pang",
            "Yu Qiao",
            "Yunde Jia",
            "Kaipeng Zhang"
        ],
        "title": "Sekai: A Video Dataset towards World Exploration",
        "abstract": "arXiv:2506.15675v1 Announce Type: new  Abstract: Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.",
        "arxiv_id": "2506.15675",
        "ARXIVID": "2506.15675",
        "COMMENT": "Matches criterion 6 as it introduces a large-scale video dataset for world exploration and demonstrates its use in training an interactive video exploration model.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2506.15649": {
        "authors": [
            "Ankan Deria",
            "Adinath Madhavrao Dukre",
            "Feilong Tang",
            "Sara Atito",
            "Sudipta Roy",
            "Muhammad Awais",
            "Muhammad Haris Khan",
            "Imran Razzak"
        ],
        "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning",
        "abstract": "arXiv:2506.15649v1 Announce Type: new  Abstract: Despite significant advances in inference-time search for vision-language models (VLMs), existing approaches remain both computationally expensive and prone to unpenalized, low-confidence generations which often lead to persistent hallucinations. We introduce \\textbf{Value-guided Inference with Margin-based Reward (ViMaR)}, a two-stage inference framework that improves both efficiency and output fidelity by combining a temporal-difference value model with a margin-aware reward adjustment. In the first stage, we perform a single pass to identify the highest-value caption among diverse candidates. In the second stage, we selectively refine only those segments that were overlooked or exhibit weak visual grounding, thereby eliminating frequently rewarded evaluations. A calibrated margin-based penalty discourages low-confidence continuations while preserving descriptive richness. Extensive experiments across multiple VLM architectures demonstrate that ViMaR generates captions that are significantly more reliable, factually accurate, detailed, and explanatory, while achieving over 4$\\times$ speedup compared to existing value-guided methods. Specifically, we show that ViMaR trained solely on LLaVA Mistral-7B, \\textit{generalizes effectively to guide decoding in a stronger unseen model}. To further validate this, we adapt the ViMaR to steer generation in LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption quality and demonstrating robust cross-model guidance. This cross-model generalization highlights ViMaR's flexibility and modularity, positioning it as a scalable and transferable inference-time decoding strategy. Furthermore, when ViMaR-generated captions are used for self-training, the underlying models achieve substantial gains across a broad suite of visual comprehension benchmarks, underscoring the potential of fast, accurate, and self-improving VLM pipelines.",
        "arxiv_id": "2506.15649",
        "ARXIVID": "2506.15649",
        "COMMENT": "Matches criterion 2 as it proposes a novel inference framework for vision-language models (VLMs) to improve captioning efficiency and fidelity.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.15477": {
        "authors": [
            "Chunlei Li",
            "Jingyang Hou",
            "Yilei Shi",
            "Jingliang Hu",
            "Xiao Xiang Zhu",
            "Lichao Mou"
        ],
        "title": "Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning",
        "abstract": "arXiv:2506.15477v1 Announce Type: new  Abstract: Medical report generation from imaging data remains a challenging task in clinical practice. While large language models (LLMs) show great promise in addressing this challenge, their effective integration with medical imaging data still deserves in-depth exploration. In this paper, we present MRG-LLM, a novel multimodal large language model (MLLM) that combines a frozen LLM with a learnable visual encoder and introduces a dynamic prompt customization mechanism. Our key innovation lies in generating instance-specific prompts tailored to individual medical images through conditional affine transformations derived from visual features. We propose two implementations: prompt-wise and promptbook-wise customization, enabling precise and targeted report generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets demonstrate that MRG-LLM achieves state-of-the-art performance in medical report generation. Our code will be made publicly available.",
        "arxiv_id": "2506.15477",
        "ARXIVID": "2506.15477",
        "COMMENT": "Matches criterion 2 as it explores a novel multimodal large language model (MLLM) for medical report generation with a dynamic prompt customization mechanism.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.15442": {
        "authors": [
            "Team Hunyuan3D",
            "Shuhui Yang",
            "Mingxin Yang",
            "Yifei Feng",
            "Xin Huang",
            "Sheng Zhang",
            "Zebin He",
            "Di Luo",
            "Haolin Liu",
            "Yunfei Zhao",
            "Qingxiang Lin",
            "Zeqiang Lai",
            "Xianghui Yang",
            "Huiwen Shi",
            "Zibo Zhao",
            "Bowen Zhang",
            "Hongyu Yan",
            "Lifu Wang",
            "Sicong Liu",
            "Jihong Zhang",
            "Meng Chen",
            "Liang Dong",
            "Yiwen Jia",
            "Yulin Cai",
            "Jiaao Yu",
            "Yixuan Tang",
            "Dongyuan Guo",
            "Junlin Yu",
            "Hao Zhang",
            "Zheng Ye",
            "Peng He",
            "Runzhou Wu",
            "Shida Wei",
            "Chao Zhang",
            "Yonghao Tan",
            "Yifu Sun",
            "Lin Niu",
            "Shirui Huang",
            "Bojian Zheng",
            "Shu Liu",
            "Shilin Chen",
            "Xiang Yuan",
            "Xiaofeng Yang",
            "Kai Liu",
            "Jianchen Zhu",
            "Peng Chen",
            "Tian Liu",
            "Di Wang",
            "Yuhong Liu",
            "Linus",
            "Jie Jiang",
            "Jingwei Huang",
            "Chunchao Guo"
        ],
        "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material",
        "abstract": "arXiv:2506.15442v1 Announce Type: new  Abstract: 3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.",
        "arxiv_id": "2506.15442",
        "ARXIVID": "2506.15442",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on a system for generating high-fidelity 3D assets using advanced 3D generative models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.15377": {
        "authors": [
            "Ruoyu Wang",
            "Xinshu Li",
            "Chen Wang",
            "Lina Yao"
        ],
        "title": "Efficient and Generalizable Environmental Understanding for Visual Navigation",
        "abstract": "arXiv:2506.15377v1 Announce Type: new  Abstract: Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.",
        "arxiv_id": "2506.15377",
        "ARXIVID": "2506.15377",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on improving environmental understanding for visual navigation in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.14907": {
        "authors": [
            "Yizhen Zhang",
            "Yang Ding",
            "Shuoshuo Zhang",
            "Xinchen Zhang",
            "Haoling Li",
            "Zhong-zhi Li",
            "Peijie Wang",
            "Jie Wu",
            "Lei Ji",
            "Yelong Shen",
            "Yujiu Yang",
            "Yeyun Gong"
        ],
        "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning",
        "abstract": "arXiv:2506.14907v1 Announce Type: new  Abstract: Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1, recent emerging research has begun exploring the use of reinforcement learning (RL) to enhance vision-language models (VLMs) for multimodal reasoning tasks. However, most existing multimodal reinforcement learning approaches remain limited to spatial reasoning within single-image contexts, yet still struggle to generalize to more complex and real-world scenarios involving multi-image positional reasoning, where understanding the relationships across images is crucial. To address this challenge, we propose a general reinforcement learning approach PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy designed to enhance the exploration-exploitation trade-off, thereby improving learning efficiency and task performance. Specifically, we introduce permutation of image sequences to simulate varied positional relationships to explore more spatial and positional diversity. Furthermore, we design a rollout filtering mechanism for resampling to focus on trajectories that contribute most to learning optimal behaviors to exploit learned policies effectively. We evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image benchmarks. Our experiments confirm that PeRL trained model consistently surpasses R1-related and interleaved VLM baselines by a large margin, achieving state-of-the-art performance on multi-image benchmarks, while preserving comparable performance on single-image tasks.",
        "arxiv_id": "2506.14907",
        "ARXIVID": "2506.14907",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on reinforcement learning for interleaved vision-language reasoning tasks involving spatial and positional relationships.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.14903": {
        "authors": [
            "Renjith Prasad",
            "Abhilekh Borah",
            "Hasnat Md Abdullah",
            "Chathurangi Shyalika",
            "Gurpreet Singh",
            "Ritvik Garimella",
            "Rajarshi Roy",
            "Harshul Surana",
            "Nasrin Imanpour",
            "Suranjana Trivedy",
            "Amit Sheth",
            "Amitava Das"
        ],
        "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization",
        "abstract": "arXiv:2506.14903v1 Announce Type: new  Abstract: Alignment is crucial for text-to-image (T2I) models to ensure that generated images faithfully capture user intent while maintaining safety and fairness. Direct Preference Optimization (DPO), prominent in large language models (LLMs), is extending its influence to T2I systems. This paper introduces DPO-Kernels for T2I models, a novel extension enhancing alignment across three dimensions: (i) Hybrid Loss, integrating embedding-based objectives with traditional probability-based loss for improved optimization; (ii) Kernelized Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet kernels for richer feature transformations and better separation between safe and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and R'enyi divergences for enhanced stability and robustness. We introduce DETONATE, the first large-scale benchmark of its kind, comprising approximately 100K curated image pairs categorized as chosen and rejected. DETONATE encapsulates three axes of social bias and discrimination: Race, Gender, and Disability. Prompts are sourced from hate speech datasets, with images generated by leading T2I models including Stable Diffusion 3.5 Large, Stable Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality Index (AQI), a novel geometric measure quantifying latent-space separability of safe/unsafe image activations, revealing hidden vulnerabilities. Empirically, we demonstrate that DPO-Kernels maintain strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are publicly released.",
        "arxiv_id": "2506.14903",
        "ARXIVID": "2506.14903",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a benchmark and optimization techniques for text-to-image alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.15313": {
        "authors": [
            "Leonid Ivanov",
            "Vasily Yuryev",
            "Dmitry Yudin"
        ],
        "title": "MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning",
        "abstract": "arXiv:2506.15313v1 Announce Type: new  Abstract: In autonomous driving, high-definition (HD) maps and semantic maps in bird's-eye view (BEV) are essential for accurate localization, planning, and decision-making. This paper introduces an enhanced End-to-End model named MapFM for online vectorized HD map generation. We show significantly boost feature representation quality by incorporating powerful foundation model for encoding camera images. To further enrich the model's understanding of the environment and improve prediction quality, we integrate auxiliary prediction heads for semantic segmentation in the BEV representation. This multi-task learning approach provides richer contextual supervision, leading to a more comprehensive scene representation and ultimately resulting in higher accuracy and improved quality of the predicted vectorized HD maps. The source code is available at https://github.com/LIvanoff/MapFM.",
        "arxiv_id": "2506.15313",
        "ARXIVID": "2506.15313",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model-driven approach for HD map generation in autonomous driving, leveraging multi-task learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.15285": {
        "authors": [
            "Mattia Nardon",
            "Stefano Messelodi",
            "Antonio Granata",
            "Fabio Poiesi",
            "Alberto Danese",
            "Davide Boscaini"
        ],
        "title": "AI-driven visual monitoring of industrial assembly tasks",
        "abstract": "arXiv:2506.15285v1 Announce Type: new  Abstract: Visual monitoring of industrial assembly tasks is critical for preventing equipment damage due to procedural errors and ensuring worker safety. Although commercial solutions exist, they typically require rigid workspace setups or the application of visual markers to simplify the problem. We introduce ViMAT, a novel AI-driven system for real-time visual monitoring of assembly tasks that operates without these constraints. ViMAT combines a perception module that extracts visual observations from multi-view video streams with a reasoning module that infers the most likely action being performed based on the observed assembly state and prior task knowledge. We validate ViMAT on two assembly tasks, involving the replacement of LEGO components and the reconfiguration of hydraulic press molds, demonstrating its effectiveness through quantitative and qualitative analysis in challenging real-world scenarios characterized by partial and uncertain visual observations. Project page: https://tev-fbk.github.io/ViMAT",
        "arxiv_id": "2506.15285",
        "ARXIVID": "2506.15285",
        "COMMENT": "Matches criterion 3 as it introduces a novel AI-driven system for real-time visual monitoring of industrial assembly tasks, addressing challenges in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.14827": {
        "authors": [
            "Yifeng Gao",
            "Yifan Ding",
            "Hongyu Su",
            "Juncheng Li",
            "Yunhan Zhao",
            "Lin Luo",
            "Zixing Chen",
            "Li Wang",
            "Xin Wang",
            "Yixu Wang",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "title": "DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning",
        "abstract": "arXiv:2506.14827v1 Announce Type: new  Abstract: As AI-generated video becomes increasingly pervasive across media platforms, the ability to reliably distinguish synthetic content from authentic footage has become both urgent and essential. Existing approaches have primarily treated this challenge as a binary classification task, offering limited insight into where or why a model identifies a video as AI-generated. However, the core challenge extends beyond simply detecting subtle artifacts; it requires providing fine-grained, persuasive evidence that can convince auditors and end-users alike. To address this critical gap, we introduce DAVID-X, the first dataset to pair AI-generated videos with detailed defect-level, temporal-spatial annotations and written rationales. Leveraging these rich annotations, we present DAVID-XR1, a video-language model designed to deliver an interpretable chain of visual reasoning-including defect categorization, temporal-spatial localization, and natural language explanations. This approach fundamentally transforms AI-generated video detection from an opaque black-box decision into a transparent and verifiable diagnostic process. We demonstrate that a general-purpose backbone, fine-tuned on our compact dataset and enhanced with chain-of-thought distillation, achieves strong generalization across a variety of generators and generation modes. Our results highlight the promise of explainable detection methods for trustworthy identification of AI-generated video content.",
        "arxiv_id": "2506.14827",
        "ARXIVID": "2506.14827",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on detecting AI-generated videos with explainable reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.15673": {
        "authors": [
            "Kai He",
            "Ruofan Liang",
            "Jacob Munkberg",
            "Jon Hasselgren",
            "Nandita Vijaykumar",
            "Alexander Keller",
            "Sanja Fidler",
            "Igor Gilitschenski",
            "Zan Gojcic",
            "Zian Wang"
        ],
        "title": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting",
        "abstract": "arXiv:2506.15673v1 Announce Type: new  Abstract: We address the challenge of relighting a single image or video, a task that demands precise scene intrinsic understanding and high-quality light transport synthesis. Existing end-to-end relighting models are often limited by the scarcity of paired multi-illumination data, restricting their ability to generalize across diverse scenes. Conversely, two-stage pipelines that combine inverse and forward rendering can mitigate data requirements but are susceptible to error accumulation and often fail to produce realistic outputs under complex lighting conditions or with sophisticated materials. In this work, we introduce a general-purpose approach that jointly estimates albedo and synthesizes relit outputs in a single pass, harnessing the generative capabilities of video diffusion models. This joint formulation enhances implicit scene comprehension and facilitates the creation of realistic lighting effects and intricate material interactions, such as shadows, reflections, and transparency. Trained on synthetic multi-illumination data and extensive automatically labeled real-world videos, our model demonstrates strong generalization across diverse domains and surpasses previous methods in both visual fidelity and temporal consistency.",
        "arxiv_id": "2506.15673",
        "ARXIVID": "2506.15673",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it addresses video relighting with a novel generative approach using video diffusion models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.14831": {
        "authors": [
            "C\\'eline Finet",
            "Stephane Da Silva Martins",
            "Jean-Bernard Hayet",
            "Ioannis Karamouzas",
            "Javad Amirian",
            "Sylvie Le H\\'egarat-Mascle",
            "Julien Pettr\\'e",
            "Emanuel Aldea"
        ],
        "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review",
        "abstract": "arXiv:2506.14831v1 Announce Type: new  Abstract: With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as autonomous navigation and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2024. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.",
        "arxiv_id": "2506.14831",
        "ARXIVID": "2506.14831",
        "COMMENT": "Matches criterion 7 as it is a comprehensive survey on multi-agent human trajectory prediction, synthesizing recent advancements and challenges.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.14854": {
        "authors": [
            "Varun Mannam",
            "Zhenyu Shi"
        ],
        "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis",
        "abstract": "arXiv:2506.14854v1 Announce Type: new  Abstract: Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.",
        "arxiv_id": "2506.14854",
        "ARXIVID": "2506.14854",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video annotation and key-frame generation for retail video analysis.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.15672": {
        "authors": [
            "Yao Zhang",
            "Chenyang Lin",
            "Shijie Tang",
            "Haokun Chen",
            "Shijie Zhou",
            "Yunpu Ma",
            "Volker Tresp"
        ],
        "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence",
        "abstract": "arXiv:2506.15672v1 Announce Type: new  Abstract: The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.",
        "arxiv_id": "2506.15672",
        "ARXIVID": "2506.15672",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of multi-agent systems and AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.15369": {
        "authors": [
            "Aleksandr Algasov",
            "Ekaterina Nepovinnykh",
            "Fedor Zolotarev",
            "Tuomas Eerola",
            "Heikki K\\\"alvi\\\"ainen",
            "Pavel Zem\\v{c}\\'ik",
            "Charles V. Stewart"
        ],
        "title": "Unsupervised Pelage Pattern Unwrapping for Animal Re-identification",
        "abstract": "arXiv:2506.15369v1 Announce Type: new  Abstract: Existing individual re-identification methods often struggle with the deformable nature of animal fur or skin patterns which undergo geometric distortions due to body movement and posture changes. In this paper, we propose a geometry-aware texture mapping approach that unwarps pelage patterns, the unique markings found on an animal's skin or fur, into a canonical UV space, enabling more robust feature matching. Our method uses surface normal estimation to guide the unwrapping process while preserving the geometric consistency between the 3D surface and the 2D texture space. We focus on two challenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards (Panthera pardus). Both species have distinctive yet highly deformable fur patterns. By integrating our pattern-preserving UV mapping with existing re-identification techniques, we demonstrate improved accuracy across diverse poses and viewing angles. Our framework does not require ground truth UV annotations and can be trained in a self-supervised manner. Experiments on seal and leopard datasets show up to a 5.4% improvement in re-identification accuracy.",
        "arxiv_id": "2506.15369",
        "ARXIVID": "2506.15369",
        "COMMENT": "Does not match any specific criteria but is related to animal re-identification using geometry-aware texture mapping.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15050": {
        "authors": [
            "Tiantian Fan",
            "Lingjun Liu",
            "Yu Yue",
            "Jiaze Chen",
            "Chengyi Wang",
            "Qiying Yu",
            "Chi Zhang",
            "Zhiqi Lin",
            "Ruofei Zhu",
            "Yufeng Yuan",
            "Xiaochen Zuo",
            "Bole Ma",
            "Mofan Zhang",
            "Gaohong Liu",
            "Ru Zhang",
            "Haotian Zhou",
            "Cong Xie",
            "Ruidong Zhu",
            "Zhi Zhang",
            "Xin Liu",
            "Mingxuan Wang",
            "Lin Yan",
            "Yonghui Wu"
        ],
        "title": "Truncated Proximal Policy Optimization",
        "abstract": "arXiv:2506.15050v1 Announce Type: new  Abstract: Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.",
        "arxiv_id": "2506.15050",
        "ARXIVID": "2506.15050",
        "COMMENT": "Does not match any specific criteria but is related to reinforcement learning and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15577": {
        "authors": [
            "Di Wang",
            "Shi Li"
        ],
        "title": "A Unified Graph-based Framework for Scalable 3D Tree Reconstruction and Non-Destructive Biomass Estimation from Point Clouds",
        "abstract": "arXiv:2506.15577v1 Announce Type: new  Abstract: Estimating forest above-ground biomass (AGB) is crucial for assessing carbon storage and supporting sustainable forest management. Quantitative Structural Model (QSM) offers a non-destructive approach to AGB estimation through 3D tree structural reconstruction. However, current QSM methods face significant limitations, as they are primarily designed for individual trees,depend on high-quality point cloud data from terrestrial laser scanning (TLS), and also require multiple pre-processing steps that hinder scalability and practical deployment. This study presents a novel unified framework that enables end-to-end processing of large-scale point clouds using an innovative graph-based pipeline. The proposed approach seamlessly integrates tree segmentation,leaf-wood separation and 3D skeletal reconstruction through dedicated graph operations including pathing and abstracting for tree topology reasoning. Comprehensive validation was conducted on datasets with varying leaf conditions (leaf-on and leaf-off), spatial scales (tree- and plot-level), and data sources (TLS and UAV-based laser scanning, ULS). Experimental results demonstrate strong performance under challenging conditions, particularly in leaf-on scenarios (~20% relative error) and low-density ULS datasets with partial coverage (~30% relative error). These findings indicate that the proposed framework provides a robust and scalable solution for large-scale, non-destructive AGB estimation. It significantly reduces dependency on specialized pre-processing tools and establishes ULS as a viable alternative to TLS. To our knowledge, this is the first method capable of enabling seamless, end-to-end 3D tree reconstruction at operational scales. This advancement substantially improves the feasibility of QSM-based AGB estimation, paving the way for broader applications in forest inventory and climate change research.",
        "arxiv_id": "2506.15577",
        "ARXIVID": "2506.15577",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15318": {
        "authors": [
            "Lanfeng Zhong",
            "Xin Liao",
            "Shichuan Zhang",
            "Shaoting Zhang",
            "Guotai Wang"
        ],
        "title": "OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models",
        "abstract": "arXiv:2506.15318v1 Announce Type: new  Abstract: Pathology image classification plays a crucial role in accurate medical diagnosis and treatment planning. Training high-performance models for this task typically requires large-scale annotated datasets, which are both expensive and time-consuming to acquire. Active Learning (AL) offers a solution by iteratively selecting the most informative samples for annotation, thereby reducing the labeling effort. However, most AL methods are designed under the assumption of a closed-set scenario, where all the unannotated images belong to target classes. In real-world clinical environments, the unlabeled pool often contains a substantial amount of Out-Of-Distribution (OOD) data, leading to low efficiency of annotation in traditional AL methods. Furthermore, most existing AL methods start with random selection in the first query round, leading to a significant waste of labeling costs in open-set scenarios. To address these challenges, we propose OpenPath, a novel open-set active learning approach for pathological image classification leveraging a pre-trained Vision-Language Model (VLM). In the first query, we propose task-specific prompts that combine target and relevant non-target class prompts to effectively select In-Distribution (ID) and informative samples from the unlabeled pool. In subsequent queries, Diverse Informative ID Sampling (DIS) that includes Prototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic Sampling (EGSS) is proposed to ensure both purity and informativeness in a query, avoiding the selection of OOD samples. Experiments on two public pathology image datasets show that OpenPath significantly enhances the model's performance due to its high purity of selected samples, and outperforms several state-of-the-art open-set AL methods. The code is available at \\href{https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}..",
        "arxiv_id": "2506.15318",
        "ARXIVID": "2506.15318",
        "COMMENT": "Does not match any specific criteria but is relevant to pathology image classification using vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15560": {
        "authors": [
            "Xingrui Qin",
            "Wentao Zhao",
            "Chuan Cao",
            "Yihe Niu",
            "Houcheng Jiang",
            "Jingchuan Wang"
        ],
        "title": "RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation",
        "abstract": "arXiv:2506.15560v1 Announce Type: new  Abstract: Dense metric depth estimation using millimeter-wave radar typically requires dense LiDAR supervision, generated via multi-frame projection and interpolation, to guide the learning of accurate depth from sparse radar measurements and RGB images. However, this paradigm is both costly and data-intensive. To address this, we propose RaCalNet, a novel framework that eliminates the need for dense supervision by using sparse LiDAR to supervise the learning of refined radar measurements, resulting in a supervision density of merely around 1% compared to dense-supervised methods. Unlike previous approaches that associate radar points with broad image regions and rely heavily on dense labels, RaCalNet first recalibrates and refines sparse radar points to construct accurate depth priors. These priors then serve as reliable anchors to guide monocular depth prediction, enabling metric-scale estimation without resorting to dense supervision. This design improves structural consistency and preserves fine details. Despite relying solely on sparse supervision, RaCalNet surpasses state-of-the-art dense-supervised methods, producing depth maps with clear object contours and fine-grained textures. Extensive experiments on the ZJU-4DRadarCam dataset and real-world deployment scenarios demonstrate its effectiveness, reducing RMSE by 35.30% and 34.89%, respectively.",
        "arxiv_id": "2506.15560",
        "ARXIVID": "2506.15560",
        "COMMENT": "Does not match any specific criteria but is relevant to depth estimation and radar calibration in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15207": {
        "authors": [
            "Mohamad A. Hady",
            "Siyi Hu",
            "Mahardhika Pratama",
            "Jimmy Cao",
            "Ryszard Kowalczyk"
        ],
        "title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study",
        "abstract": "arXiv:2506.15207v1 Announce Type: new  Abstract: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.",
        "arxiv_id": "2506.15207",
        "ARXIVID": "2506.15207",
        "COMMENT": "Does not match any specific criteria but is relevant to multi-agent reinforcement learning and satellite coordination.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15153": {
        "authors": [
            "Yufei Liu",
            "Haoke Xiao",
            "Jiaxing Chai",
            "Yongcun Zhang",
            "Rong Wang",
            "Zijie Meng",
            "Zhiming Luo"
        ],
        "title": "SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts",
        "abstract": "arXiv:2506.15153v1 Announce Type: new  Abstract: The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on low-contrast medical images. To address this issue, we propose SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choose the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based few-shot methods.",
        "arxiv_id": "2506.15153",
        "ARXIVID": "2506.15153",
        "COMMENT": "Does not match any specific criteria but explores the use of large vision models in medical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.15404": {
        "authors": [
            "Anju Chhetri",
            "Jari Korhonen",
            "Prashnna Gyawali",
            "Binod Bhattarai"
        ],
        "title": "NERO: Explainable Out-of-Distribution Detection with Neuron-level Relevance",
        "abstract": "arXiv:2506.15404v1 Announce Type: new  Abstract: Ensuring reliability is paramount in deep learning, particularly within the domain of medical imaging, where diagnostic decisions often hinge on model outputs. The capacity to separate out-of-distribution (OOD) samples has proven to be a valuable indicator of a model's reliability in research. In medical imaging, this is especially critical, as identifying OOD inputs can help flag potential anomalies that might otherwise go undetected. While many OOD detection methods rely on feature or logit space representations, recent works suggest these approaches may not fully capture OOD diversity. To address this, we propose a novel OOD scoring mechanism, called NERO, that leverages neuron-level relevance at the feature layer. Specifically, we cluster neuron-level relevance for each in-distribution (ID) class to form representative centroids and introduce a relevance distance metric to quantify a new sample's deviation from these centroids, enhancing OOD separability. Additionally, we refine performance by incorporating scaled relevance in the bias term and combining feature norms. Our framework also enables explainable OOD detection. We validate its effectiveness across multiple deep learning architectures on the gastrointestinal imaging benchmarks Kvasir and GastroVision, achieving improvements over state-of-the-art OOD detection methods.",
        "arxiv_id": "2506.15404",
        "ARXIVID": "2506.15404",
        "COMMENT": "Does not match any specific criteria but is related to out-of-distribution detection in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.15260": {
        "authors": [
            "Adrian Poniatowski",
            "Natalie Gentner",
            "Manuel Barusco",
            "Davide Dalle Pezze",
            "Samuele Salti",
            "Gian Antonio Susto"
        ],
        "title": "Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing",
        "abstract": "arXiv:2506.15260v1 Announce Type: new  Abstract: In the semiconductor sector, due to high demand but also strong and increasing competition, time to market and quality are key factors in securing significant market share in various application areas. Thanks to the success of deep learning methods in recent years in the computer vision domain, Industry 4.0 and 5.0 applications, such as defect classification, have achieved remarkable success. In particular, Domain Adaptation (DA) has proven highly effective since it focuses on using the knowledge learned on a (source) domain to adapt and perform effectively on a different but related (target) domain. By improving robustness and scalability, DA minimizes the need for extensive manual re-labeling or re-training of models. This not only reduces computational and resource costs but also allows human experts to focus on high-value tasks. Therefore, we tested the efficacy of DA techniques in semi-supervised and unsupervised settings within the context of the semiconductor field. Moreover, we propose the DBACS approach, a CycleGAN-inspired model enhanced with additional loss terms to improve performance. All the approaches are studied and validated on real-world Electron Microscope images considering the unsupervised and semi-supervised settings, proving the usefulness of our method in advancing DA techniques for the semiconductor field.",
        "arxiv_id": "2506.15260",
        "ARXIVID": "2506.15260",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision applications in industry.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}