{
    "2510.05560": {
        "authors": [
            "Hongchi Xia",
            "Chih-Hao Lin",
            "Hao-Yu Hsu",
            "Quentin Leboutet",
            "Katelyn Gao",
            "Michael Paulitsch",
            "Benjamin Ummenhofer",
            "Shenlong Wang"
        ],
        "title": "HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video",
        "abstract": "arXiv:2510.05560v1 Announce Type: new  Abstract: Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.",
        "arxiv_id": "2510.05560",
        "ARXIVID": "2510.05560",
        "COMMENT": "Matches criterion 3 as it introduces HoloScene, a framework for creating simulation-ready 3D worlds from videos, relevant to embodied AI and robotics.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.05684": {
        "authors": [
            "Suwhan Choi",
            "Jaeyoon Jung",
            "Haebin Seong",
            "Minchan Kim",
            "Minyeong Kim",
            "Yongjun Cho",
            "Yoonshik Kim",
            "Yubeen Park",
            "Youngjae Yu",
            "Yunsung Lee"
        ],
        "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
        "abstract": "arXiv:2510.05684v1 Announce Type: new  Abstract: Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/",
        "arxiv_id": "2510.05684",
        "ARXIVID": "2510.05684",
        "COMMENT": "Matches criterion 3 as it introduces a new framework (D2E) for embodied AI using desktop data for pretraining, addressing challenges in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.05580": {
        "authors": [
            "Chen Li",
            "Zhantao Yang",
            "Han Zhang",
            "Fangyi Chen",
            "Chenchen Zhu",
            "Anudeepsekhar Bolimera",
            "Marios Savvides"
        ],
        "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
        "abstract": "arXiv:2510.05580v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.",
        "arxiv_id": "2510.05580",
        "ARXIVID": "2510.05580",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for embodied agents, focusing on efficient adaptation and generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.06131": {
        "authors": [
            "Jiawei Mao",
            "Yuhan Wang",
            "Lifeng Chen",
            "Can Zhao",
            "Yucheng Tang",
            "Dong Yang",
            "Liangqiong Qu",
            "Daguang Xu",
            "Yuyin Zhou"
        ],
        "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation",
        "abstract": "arXiv:2510.06131v1 Announce Type: new  Abstract: Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.",
        "arxiv_id": "2510.06131",
        "ARXIVID": "2510.06131",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a multimodal large language model for unified medical multimodal generation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.06209": {
        "authors": [
            "Jiahao Wang",
            "Zhenpei Yang",
            "Yijing Bai",
            "Yingwei Li",
            "Yuliang Zou",
            "Bo Sun",
            "Abhijit Kundu",
            "Jose Lezama",
            "Luna Yue Huang",
            "Zehao Zhu",
            "Jyh-Jing Hwang",
            "Dragomir Anguelov",
            "Mingxing Tan",
            "Chiyu Max Jiang"
        ],
        "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
        "abstract": "arXiv:2510.06209v1 Announce Type: new  Abstract: Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.",
        "arxiv_id": "2510.06209",
        "ARXIVID": "2510.06209",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel evaluation framework for end-to-end driving and video generation models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.05610": {
        "authors": [
            "Jiaqi Liu",
            "Tao Huang",
            "Chang Xu"
        ],
        "title": "Efficient Conditional Generation on Scale-based Visual Autoregressive Models",
        "abstract": "arXiv:2510.05610v1 Announce Type: new  Abstract: Recent advances in autoregressive (AR) models have demonstrated their potential to rival diffusion models in image synthesis. However, for complex spatially-conditioned generation, current AR approaches rely on fine-tuning the pre-trained model, leading to significant training costs. In this paper, we propose the Efficient Control Model (ECM), a plug-and-play framework featuring a lightweight control module that introduces control signals via a distributed architecture. This architecture consists of context-aware attention layers that refine conditional features using real-time generated tokens, and a shared gated feed-forward network (FFN) designed to maximize the utilization of its limited capacity and ensure coherent control feature learning. Furthermore, recognizing the critical role of early-stage generation in determining semantic structure, we introduce an early-centric sampling strategy that prioritizes learning early control sequences. This approach reduces computational cost by lowering the number of training tokens per iteration, while a complementary temperature scheduling during inference compensates for the resulting insufficient training of late-stage tokens. Extensive experiments on scale-based AR models validate that our method achieves high-fidelity and diverse control over image generation, surpassing existing baselines while significantly improving both training and inference efficiency.",
        "arxiv_id": "2510.05610",
        "ARXIVID": "2510.05610",
        "COMMENT": "Matches criterion 5 as it focuses on conditional generation in visual autoregressive models, integrating image understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.05593": {
        "authors": [
            "Zeqi Gu",
            "Markos Georgopoulos",
            "Xiaoliang Dai",
            "Marjan Ghazvininejad",
            "Chu Wang",
            "Felix Juefei-Xu",
            "Kunpeng Li",
            "Yujun Shi",
            "Zecheng He",
            "Zijian He",
            "Jiawei Zhou",
            "Abe Davis",
            "Jialiang Wang"
        ],
        "title": "Improving Chain-of-Thought Efficiency for Autoregressive Image Generation",
        "abstract": "arXiv:2510.05593v1 Announce Type: new  Abstract: Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.",
        "arxiv_id": "2510.05593",
        "ARXIVID": "2510.05593",
        "COMMENT": "Matches criterion 2 as it explores chain-of-thought reasoning in autoregressive multimodal large language models for image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.05903": {
        "authors": [
            "Sebastian H\\\"ofer",
            "Dorian Henning",
            "Artemij Amiranashvili",
            "Douglas Morrison",
            "Mariliza Tzes",
            "Ingmar Posner",
            "Marc Matvienko",
            "Alessandro Rennola",
            "Anton Milan"
        ],
        "title": "Kaputt: A Large-Scale Dataset for Visual Defect Detection",
        "abstract": "arXiv:2510.05903v1 Announce Type: new  Abstract: We present a novel large-scale dataset for defect detection in a logistics setting. Recent work on industrial anomaly detection has primarily focused on manufacturing scenarios with highly controlled poses and a limited number of object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC scores. In contrast to manufacturing, anomaly detection in retail logistics faces new challenges, particularly in the diversity and variability of object pose and appearance. Leading anomaly detection methods fall short when applied to this new setting. To bridge this gap, we introduce a new benchmark that overcomes the current limitations of existing datasets. With over 230,000 images (and more than 29,000 defective instances), it is 40 times larger than MVTec-AD and contains more than 48,000 distinct objects. To validate the difficulty of the problem, we conduct an extensive evaluation of multiple state-of-the-art anomaly detection methods, demonstrating that they do not surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that existing methods struggle to leverage normal samples under heavy pose and appearance variation. With our large-scale dataset, we set a new benchmark and encourage future research towards solving this challenging problem in retail logistics anomaly detection. The dataset is available for download under https://www.kaputt-dataset.com.",
        "arxiv_id": "2510.05903",
        "ARXIVID": "2510.05903",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a large-scale dataset for visual defect detection in logistics.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.05722": {
        "authors": [
            "Jiaojiao Ye",
            "Jiaxing Zhong",
            "Qian Xie",
            "Yuzhou Zhou",
            "Niki Trigoni",
            "Andrew Markham"
        ],
        "title": "Data Factory with Minimal Human Effort Using VLMs",
        "abstract": "arXiv:2510.05722v1 Announce Type: new  Abstract: Generating enough and diverse data through augmentation offers an efficient solution to the time-consuming and labour-intensive process of collecting and annotating pixel-wise images. Traditional data augmentation techniques often face challenges in manipulating high-level semantic attributes, such as materials and textures. In contrast, diffusion models offer a robust alternative, by effectively utilizing text-to-image or image-to-image transformation. However, existing diffusion-based methods are either computationally expensive or compromise on performance. To address this issue, we introduce a novel training-free pipeline that integrates pretrained ControlNet and Vision-Language Models (VLMs) to generate synthetic images paired with pixel-level labels. This approach eliminates the need for manual annotations and significantly improves downstream tasks. To improve the fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i present promising performance and outperform concurrent work for one-shot semantic segmentation.",
        "arxiv_id": "2510.05722",
        "ARXIVID": "2510.05722",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates vision-language models with ControlNet for data generation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.06078": {
        "authors": [
            "Tao Zhe",
            "Rui Liu",
            "Fateme Memar",
            "Xiao Luo",
            "Wei Fan",
            "Xinyue Ye",
            "Zhongren Peng",
            "Dongjie Wang"
        ],
        "title": "Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents",
        "abstract": "arXiv:2510.06078v1 Announce Type: new  Abstract: Route recommendation aims to provide users with optimal travel plans that satisfy diverse and complex requirements. Classical routing algorithms (e.g., shortest-path and constraint-aware search) are efficient but assume structured inputs and fixed objectives, limiting adaptability to natural-language queries. Recent LLM-based approaches enhance flexibility but struggle with spatial reasoning and the joint modeling of route-level and POI-level preferences. To address these limitations, we propose RouteLLM, a hierarchical multi-agent framework that grounds natural-language intents into constraint-aware routes. It first parses user queries into structured intents including POIs, paths, and constraints. A manager agent then coordinates specialized sub-agents: a constraint agent that resolves and formally check constraints, a POI agent that retrieves and ranks candidate POIs, and a path refinement agent that refines routes via a routing engine with preference-conditioned costs. A final verifier agent ensures constraint satisfaction and produces the final route with an interpretable rationale. This design bridges linguistic flexibility and spatial structure, enabling reasoning over route feasibility and user preferences. Experiments show that our method reliably grounds textual preferences into constraint-aware routes, improving route quality and preference satisfaction over classical methods.",
        "arxiv_id": "2510.06078",
        "ARXIVID": "2510.06078",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it addresses spatial reasoning and route recommendation using hierarchical LLM agents.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.05976": {
        "authors": [
            "Eashan Adhikarla",
            "Yixin Liu",
            "Brian D. Davison"
        ],
        "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis",
        "abstract": "arXiv:2510.05976v1 Announce Type: new  Abstract: Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.",
        "arxiv_id": "2510.05976",
        "ARXIVID": "2510.05976",
        "COMMENT": "Matches criterion 7 as it is a survey paper on diffusion models for low-light image enhancement, providing a taxonomy and performance analysis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.05891": {
        "authors": [
            "Yanran Zhang",
            "Bingyao Yu",
            "Yu Zheng",
            "Wenzhao Zheng",
            "Yueqi Duan",
            "Lei Chen",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "$\\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection",
        "abstract": "arXiv:2510.05891v1 Announce Type: new  Abstract: The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.",
        "arxiv_id": "2510.05891",
        "ARXIVID": "2510.05891",
        "COMMENT": "Matches criterion 4 as it focuses on detecting images generated by visual autoregressive models, which relates to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.05759": {
        "authors": [
            "Zexin Zheng",
            "Huangyu Dai",
            "Lingtao Mao",
            "Xinyu Sun",
            "Zihan Liang",
            "Ben Chen",
            "Yuqing Ding",
            "Chenyi Lei",
            "Wenwu Ou",
            "Han Li",
            "Kun Gai"
        ],
        "title": "OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search",
        "abstract": "arXiv:2510.05759v1 Announce Type: new  Abstract: Traditional vision search, similar to search and recommendation systems, follows the multi-stage cascading architecture (MCA) paradigm to balance efficiency and conversion. Specifically, the query image undergoes feature extraction, recall, pre-ranking, and ranking stages, ultimately presenting the user with semantically similar products that meet their preferences. This multi-view representation discrepancy of the same object in the query and the optimization objective collide across these stages, making it difficult to achieve Pareto optimality in both user experience and conversion. In this paper, an end-to-end generative framework, OneVision, is proposed to address these problems. OneVision builds on VRQ, a vision-aligned residual quantization encoding, which can align the vastly different representations of an object across multiple viewpoints while preserving the distinctive features of each product as much as possible. Then a multi-stage semantic alignment scheme is adopted to maintain strong visual similarity priors while effectively incorporating user-specific information for personalized preference generation. In offline evaluations, OneVision performs on par with online MCA, while improving inference efficiency by 21% through dynamic pruning. In A/B tests, it achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and +3.12% order volume. These results demonstrate that a semantic ID centric, generative architecture can unify retrieval and personalization while simplifying the serving pathway.",
        "arxiv_id": "2510.05759",
        "ARXIVID": "2510.05759",
        "COMMENT": "Matches criterion 2 as it explores a novel generative framework for multi-view vision search, integrating vision and user-specific information.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.05996": {
        "authors": [
            "Moritz Schneider",
            "Robert Krug",
            "Narunas Vaskevicius",
            "Luigi Palmieri",
            "Michael Volpp",
            "Joschka Boedecker"
        ],
        "title": "Information-Theoretic Policy Pre-Training with Empowerment",
        "abstract": "arXiv:2510.05996v1 Announce Type: new  Abstract: Empowerment, an information-theoretic measure of an agent's potential influence on its environment, has emerged as a powerful intrinsic motivation and exploration framework for reinforcement learning (RL). Besides for unsupervised RL and skill learning algorithms, the specific use of empowerment as a pre-training signal has received limited attention in the literature. We show that empowerment can be used as a pre-training signal for data-efficient downstream task adaptation. For this we extend the traditional notion of empowerment by introducing discounted empowerment, which balances the agent's control over the environment across short- and long-term horizons. Leveraging this formulation, we propose a novel pre-training paradigm that initializes policies to maximize discounted empowerment, enabling agents to acquire a robust understanding of environmental dynamics. We analyze empowerment-based pre-training for various existing RL algorithms and empirically demonstrate its potential as a general-purpose initialization strategy: empowerment-maximizing policies with long horizons are data-efficient and effective, leading to improved adaptability in downstream tasks. Our findings pave the way for future research to scale this framework to high-dimensional and complex tasks, further advancing the field of RL.",
        "arxiv_id": "2510.05996",
        "ARXIVID": "2510.05996",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it explores empowerment-based pre-training for reinforcement learning, which could be relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.06063": {
        "authors": [
            "Austin Feng",
            "Andreas Varvarigos",
            "Ioannis Panitsas",
            "Daniela Fernandez",
            "Jinbiao Wei",
            "Yuwei Guo",
            "Jialin Chen",
            "Ali Maatouk",
            "Leandros Tassiulas",
            "Rex Ying"
        ],
        "title": "TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis",
        "abstract": "arXiv:2510.06063v1 Announce Type: new  Abstract: Modern enterprises generate vast streams of time series metrics when monitoring complex systems, known as observability data. Unlike conventional time series from domains such as weather, observability data are zero-inflated, highly stochastic, and exhibit minimal temporal structure. Despite their importance, observability datasets are underrepresented in public benchmarks due to proprietary restrictions. Existing datasets are often anonymized and normalized, removing scale information and limiting their use for tasks beyond forecasting, such as anomaly detection, root-cause analysis, and multi-modal reasoning. To address this gap, we introduce TelecomTS, a large-scale observability dataset derived from a 5G telecommunications network. TelecomTS features heterogeneous, de-anonymized covariates with explicit scale information and supports a suite of downstream tasks, including anomaly detection, root-cause analysis, and a question-answering benchmark requiring multi-modal reasoning. Benchmarking state-of-the-art time series, language, and reasoning models reveals that existing approaches struggle with the abrupt, noisy, and high-variance dynamics of observability data. Our experiments also underscore the importance of preserving covariates' absolute scale, emphasizing the need for foundation time series models that natively leverage scale information for practical observability applications.",
        "arxiv_id": "2510.06063",
        "ARXIVID": "2510.06063",
        "COMMENT": "Matches criterion 6 as it introduces a multi-modal dataset for time series and language analysis, which could be relevant for video understanding tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.05318": {
        "authors": [
            "Nan Huo",
            "Xiaohan Xu",
            "Jinyang Li",
            "Per Jacobsson",
            "Shipei Lin",
            "Bowen Qin",
            "Binyuan Hui",
            "Xiaolong Li",
            "Ge Qu",
            "Shuzheng Si",
            "Linheng Han",
            "Edward Alexander",
            "Xintong Zhu",
            "Rui Qin",
            "Ruihan Yu",
            "Yiyao Jin",
            "Feige Zhou",
            "Weihao Zhong",
            "Yun Chen",
            "Hongyu Liu",
            "Chenhao Ma",
            "Fatma Ozcan",
            "Yannis Papakonstantinou",
            "Reynold Cheng"
        ],
        "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions",
        "abstract": "arXiv:2510.05318v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.",
        "arxiv_id": "2510.05318",
        "ARXIVID": "2510.05318",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of large language models and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.06052": {
        "authors": [
            "Haiquan Lu",
            "Gongfan Fang",
            "Xinyin Ma",
            "Qi Li",
            "Xinchao Wang"
        ],
        "title": "MixReasoning: Switching Modes to Think",
        "abstract": "arXiv:2510.06052v1 Announce Type: new  Abstract: Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.",
        "arxiv_id": "2510.06052",
        "ARXIVID": "2510.06052",
        "COMMENT": "Does not match any specific criteria but discusses adaptive reasoning models, which is tangentially related to reasoning in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.06098": {
        "authors": [
            "Yinjian Wang",
            "Wei Li",
            "Yuanyuan Gui",
            "Gemine Vivone"
        ],
        "title": "Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution",
        "abstract": "arXiv:2510.06098v1 Announce Type: new  Abstract: Fusing a hyperspectral image with a multispectral image acquired over the same scene, \\textit{i.e.}, hyperspectral image super-resolution, has become a popular computational way to access the latent high-spatial-spectral-resolution image. To date, a variety of fusion methods have been proposed, among which the tensor-based ones have testified that multiple priors, such as multidimensional low-rankness and spatial total variation at multiple levels, effectively drive the fusion process. However, existing tensor-based models can only effectively leverage one or two priors at one or two levels, since simultaneously incorporating multi-level priors inevitably increases model complexity. This introduces challenges in both balancing the weights of different priors and optimizing multi-block structures. Concerning this, we present a novel hyperspectral super-resolution model compactly characterizing these multi-level priors of hyperspectral images within the tensor framework. Firstly, the proposed model decouples the spectral low-rankness and spatial priors by casting the latent high-spatial-spectral-resolution image into spectral subspace and spatial maps via block term decomposition. Secondly, these spatial maps are stacked as the spatial tensor encoding the high-order spatial low-rankness and smoothness priors, which are co-modeled via the proposed non-convex mode-shuffled tensor correlated total variation. Finally, we draw inspiration from the linearized alternating direction method of multipliers to design an efficient algorithm to optimize the resulting model, theoretically proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments on multiple datasets demonstrate the effectiveness of the proposed algorithm. The code implementation will be available from https://github.com/WongYinJ.",
        "arxiv_id": "2510.06098",
        "ARXIVID": "2510.06098",
        "COMMENT": "Does not match any specific criteria but discusses tensor representation for hyperspectral image super-resolution, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.06123": {
        "authors": [
            "Mosong Ma",
            "Tania Stathaki",
            "Michalis Lazarou"
        ],
        "title": "Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework",
        "abstract": "arXiv:2510.06123v1 Announce Type: new  Abstract: Deep learning in medical imaging is often limited by scarce and imbalanced annotated data. We present SSGNet, a unified framework that combines class specific generative modeling with iterative semisupervised pseudo labeling to enhance both classification and segmentation. Rather than functioning as a standalone model, SSGNet augments existing baselines by expanding training data with StyleGAN3 generated images and refining labels through iterative pseudo labeling. Experiments across multiple medical imaging benchmarks demonstrate consistent gains in classification and segmentation performance, while Frechet Inception Distance analysis confirms the high quality of generated samples. These results highlight SSGNet as a practical strategy to mitigate annotation bottlenecks and improve robustness in medical image analysis.",
        "arxiv_id": "2510.06123",
        "ARXIVID": "2510.06123",
        "COMMENT": "Does not match any specific criteria but discusses generative and semi-supervised frameworks for medical imaging, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.06036": {
        "authors": [
            "Qingyu Yin",
            "Chak Tou Leong",
            "Linyi Yang",
            "Wenxuan Huang",
            "Wenjie Li",
            "Xiting Wang",
            "Jaehong Yoon",
            "YunXing",
            "XingYu",
            "Jinjin Gu"
        ],
        "title": "Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?",
        "abstract": "arXiv:2510.06036v1 Announce Type: new  Abstract: Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as \\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose \\textbf{Cliff-as-a-Judge}, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.",
        "arxiv_id": "2510.06036",
        "ARXIVID": "2510.06036",
        "COMMENT": "Does not match any specific criteria but provides insights into safety alignment in reasoning models, which is tangentially related to large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.05971": {
        "authors": [
            "Ron Keuth",
            "Paul Kaftan",
            "Mattias P. Heinrich"
        ],
        "title": "Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging",
        "abstract": "arXiv:2510.05971v1 Announce Type: new  Abstract: The generalization of the Transformer architecture via MetaFormer has reshaped our understanding of its success in computer vision. By replacing self-attention with simpler token mixers, MetaFormer provides strong baselines for vision tasks. However, while extensively studied on natural image datasets, its use in medical imaging remains scarce, and existing works rarely compare different token mixers, potentially overlooking more suitable designs choices. In this work, we present the first comprehensive study of token mixers for medical imaging. We systematically analyze pooling-, convolution-, and attention-based token mixers within the MetaFormer architecture on image classification (global prediction task) and semantic segmentation (dense prediction task). Our evaluation spans eight datasets covering diverse modalities and common challenges in the medical domain. Given the prevalence of pretraining from natural images to mitigate medical data scarcity, we also examine transferring pretrained weights to new token mixers. Our results show that, for classification, low-complexity token mixers (e.g. grouped convolution or pooling) are sufficient, aligning with findings on natural images. Pretrained weights remain useful despite the domain gap introduced by the new token mixer. For segmentation, we find that the local inductive bias of convolutional token mixers is essential. Grouped convolutions emerge as the preferred choice, as they reduce runtime and parameter count compared to standard convolutions, while the MetaFormer's channel-MLPs already provide the necessary cross-channel interactions. Our code is available on GitHub.",
        "arxiv_id": "2510.05971",
        "ARXIVID": "2510.05971",
        "COMMENT": "Does not match any specific criteria but discusses token mixing in MetaFormer for medical imaging, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.05950": {
        "authors": [
            "Songyuan Sui",
            "Zihang Xu",
            "Yu-Neng Chuang",
            "Kwei-Herng Lai",
            "Xia Hu"
        ],
        "title": "Training-Free Time Series Classification via In-Context Reasoning with LLM Agents",
        "abstract": "arXiv:2510.05950v1 Announce Type: new  Abstract: Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at https://github.com/SongyuanSui/FETATSC.",
        "arxiv_id": "2510.05950",
        "ARXIVID": "2510.05950",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and time series analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.05899": {
        "authors": [
            "Jiesi Hu",
            "Yanwu Yang",
            "Zhiyu Ye",
            "Jinyan Zhou",
            "Jianfeng Cao",
            "Hanyang Peng",
            "Ting Ma"
        ],
        "title": "Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning",
        "abstract": "arXiv:2510.05899v1 Announce Type: new  Abstract: Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at https://github.com/jiesihu/Weak-ICL.",
        "arxiv_id": "2510.05899",
        "ARXIVID": "2510.05899",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.05266": {
        "authors": [
            "Christina Thrainer",
            "Md Meftahul Ferdaus",
            "Mahdi Abdelguerfi",
            "Christian Guetl",
            "Steven Sloan",
            "Kendall N. Niles",
            "Ken Pathak"
        ],
        "title": "Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation",
        "abstract": "arXiv:2510.05266v1 Announce Type: new  Abstract: Few-shot semantic segmentation is vital for deep learning-based infrastructure inspection applications, where labeled training examples are scarce and expensive. Although existing deep learning frameworks perform well, the need for extensive labeled datasets and the inability to learn new defect categories with little data are problematic. We present our Enhanced Feature Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert and sewer defect categories using a prototypical learning framework. Our approach has three main contributions: (1) adaptive E-FPN encoder using InceptionSepConv blocks and depth-wise separable convolutions for efficient multi-scale feature extraction; (2) prototypical learning with masked average pooling for powerful prototype generation from small support examples; and (3) attention-based feature representation through global self-attention, local self-attention and cross-attention. Comprehensive experimentation on challenging infrastructure inspection datasets illustrates that the method achieves excellent few-shot performance, with the best configuration being 8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way classification testing. The self-attention method had the most significant performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over baselines. Our framework addresses the critical need to rapidly respond to new defect types in infrastructure inspection systems with limited new training data that lead to more efficient and economical maintenance plans for critical infrastructure systems.",
        "arxiv_id": "2510.05266",
        "ARXIVID": "2510.05266",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.05760": {
        "authors": [
            "Gianmarco Perantoni",
            "Lorenzo Bruzzone"
        ],
        "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
        "abstract": "arXiv:2510.05760v1 Announce Type: new  Abstract: Deep learning has gained broad interest in remote sensing image scene classification thanks to the effectiveness of deep neural networks in extracting the semantics from complex data. However, deep networks require large amounts of training samples to obtain good generalization capabilities and are sensitive to errors in the training labels. This is a problem in remote sensing since highly reliable labels can be obtained at high costs and in limited amount. However, many sources of less reliable labeled data are available, e.g., obsolete digital maps. In order to train deep networks with larger datasets, we propose both the combination of single or multiple weak sources of labeled data with a small but reliable dataset to generate multisource labeled datasets and a novel training strategy where the reliability of each source is taken in consideration. This is done by exploiting the transition matrices describing the statistics of the errors of each source. The transition matrices are embedded into the labels and used during the training process to weigh each label according to the related source. The proposed method acts as a weighting scheme at gradient level, where each instance contributes with different weights to the optimization of different classes. The effectiveness of the proposed method is validated by experiments on different datasets. The results proved the robustness and capability of leveraging on unreliable source of labels of the proposed method.",
        "arxiv_id": "2510.05760",
        "ARXIVID": "2510.05760",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and remote sensing.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}