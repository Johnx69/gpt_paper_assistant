{
    "2508.10572": {
        "authors": [
            "Tuyen Tran",
            "Thao Minh Le",
            "Truyen Tran"
        ],
        "title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation",
        "abstract": "arXiv:2508.10572v1 Announce Type: new  Abstract: Referring-based Video Object Segmentation is a multimodal problem that requires producing fine-grained segmentation results guided by external cues. Traditional approaches to this task typically involve training specialized models, which come with high computational complexity and manual annotation effort. Recent advances in vision-language foundation models open a promising direction toward training-free approaches. Several studies have explored leveraging these general-purpose models for fine-grained segmentation, achieving performance comparable to that of fully supervised, task-specific models. However, existing methods rely on fixed pipelines that lack the flexibility needed to adapt to the dynamic nature of the task. To address this limitation, we propose Multi-Modal Agent, a novel agentic system designed to solve this task in a more flexible and adaptive manner. Specifically, our method leverages the reasoning capabilities of large language models (LLMs) to generate dynamic workflows tailored to each input. This adaptive procedure iteratively interacts with a set of specialized tools designed for low-level tasks across different modalities to identify the target object described by the multimodal cues. Our agentic approach demonstrates clear improvements over prior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.",
        "arxiv_id": "2508.10572",
        "ARXIVID": "2508.10572",
        "COMMENT": "Matches criteria 2 and 6. Proposes a novel agentic system for multimodal-guided video object segmentation, relevant to video understanding and multimodal LLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.10104": {
        "authors": [
            "Oriane Sim\\'eoni",
            "Huy V. Vo",
            "Maximilian Seitzer",
            "Federico Baldassarre",
            "Maxime Oquab",
            "Cijo Jose",
            "Vasil Khalidov",
            "Marc Szafraniec",
            "Seungeun Yi",
            "Micha\\\"el Ramamonjisoa",
            "Francisco Massa",
            "Daniel Haziza",
            "Luca Wehrstedt",
            "Jianyuan Wang",
            "Timoth\\'ee Darcet",
            "Th\\'eo Moutakanni",
            "Leonel Sentana",
            "Claire Roberts",
            "Andrea Vedaldi",
            "Jamie Tolan",
            "John Brandt",
            "Camille Couprie",
            "Julien Mairal",
            "Herv\\'e J\\'egou",
            "Patrick Labatut",
            "Piotr Bojanowski"
        ],
        "title": "DINOv3",
        "abstract": "arXiv:2508.10104v1 Announce Type: new  Abstract: Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.",
        "arxiv_id": "2508.10104",
        "ARXIVID": "2508.10104",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces DINOv3, a versatile vision foundation model with significant improvements in self-supervised learning.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.10635": {
        "authors": [
            "Hosam Elgendy",
            "Ahmed Sharshar",
            "Ahmed Aboeitta",
            "Mohsen Guizani"
        ],
        "title": "ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation",
        "abstract": "arXiv:2508.10635v1 Announce Type: new  Abstract: Understanding environmental changes from aerial imagery is vital for climate resilience, urban planning, and ecosystem monitoring. Yet, current vision language models (VLMs) overlook causal signals from environmental sensors, rely on single-source captions prone to stylistic bias, and lack interactive scenario-based reasoning. We present ChatENV, the first interactive VLM that jointly reasons over satellite image pairs and real-world sensor data. Our framework: (i) creates a 177k-image dataset forming 152k temporal pairs across 62 land-use classes in 197 countries with rich sensor metadata (e.g., temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for stylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using efficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV achieves strong performance in temporal and \"what-if\" reasoning (e.g., BERT-F1 0.903) and rivals or outperforms state-of-the-art temporal models, while supporting interactive scenario-based analysis. This positions ChatENV as a powerful tool for grounded, sensor-aware environmental monitoring.",
        "arxiv_id": "2508.10635",
        "ARXIVID": "2508.10635",
        "COMMENT": "Matches criteria 2 and 5 as it introduces an interactive vision-language model (VLM) for environmental monitoring, integrating satellite imagery and sensor data for reasoning tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2508.10576": {
        "authors": [
            "Zheng Qin",
            "Ruobing Zheng",
            "Yabing Wang",
            "Tianqi Li",
            "Yi Yuan",
            "Jingdong Chen",
            "Le Wang"
        ],
        "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs",
        "abstract": "arXiv:2508.10576v1 Announce Type: new  Abstract: While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/",
        "arxiv_id": "2508.10576",
        "ARXIVID": "2508.10576",
        "COMMENT": "Matches criteria 2 and 5 closely as it explores Multimodal Large Language Models (MLLMs) with a focus on reasoning and integration of visual, audio, and text modalities.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.10427": {
        "authors": [
            "Keishi Ishihara",
            "Kento Sasaki",
            "Tsubasa Takahashi",
            "Daiki Shiono",
            "Yu Yamaguchi"
        ],
        "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes",
        "abstract": "arXiv:2508.10427v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have been applied to autonomous driving to support decision-making in complex real-world scenarios. However, their training on static, web-sourced image-text pairs fundamentally limits the precise spatiotemporal reasoning required to understand and predict dynamic traffic scenes. We address this critical gap with STRIDE-QA, a large-scale visual question answering (VQA) dataset for physically grounded reasoning from an ego-centric perspective. Constructed from 100 hours of multi-sensor driving data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16 million QA pairs over 285K frames. Grounded by dense, automatically generated annotations including 3D bounding boxes, segmentation masks, and multi-object tracks, the dataset uniquely supports both object-centric and ego-centric reasoning through three novel QA tasks that require spatial localization and temporal prediction. Our benchmarks demonstrate that existing VLMs struggle significantly, achieving near-zero scores on prediction consistency. In contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains, achieving 55% success in spatial localization and 28% consistency in future motion prediction, compared to near-zero scores from general-purpose VLMs. Therefore, STRIDE-QA establishes a comprehensive foundation for developing more reliable VLMs for safety-critical autonomous systems.",
        "arxiv_id": "2508.10427",
        "ARXIVID": "2508.10427",
        "COMMENT": "Matches criterion 6. Introduces a large-scale VQA dataset for spatiotemporal reasoning in urban driving scenes, relevant to video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.10528": {
        "authors": [
            "Ziye Deng",
            "Ruihan He",
            "Jiaxiang Liu",
            "Yuan Wang",
            "Zijie Meng",
            "Songtao Jiang",
            "Yong Xie",
            "Zuozhu Liu"
        ],
        "title": "Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset",
        "abstract": "arXiv:2508.10528v1 Announce Type: new  Abstract: Medical image grounding aims to align natural language phrases with specific regions in medical images, serving as a foundational task for intelligent diagnosis, visual question answering (VQA), and automated report generation (MRG). However, existing research is constrained by limited modality coverage, coarse-grained annotations, and the absence of a unified, generalizable grounding framework. To address these challenges, we construct a large-scale medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level annotations across seven imaging modalities, covering diverse anatomical structures and pathological findings. The dataset supports both segmentation and grounding tasks with hierarchical region labels, ranging from organ-level boundaries to fine-grained lesions. Based on this foundation, we propose Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather than relying on explicitly designed expert modules, Med-GLIP implicitly acquires hierarchical semantic understanding from diverse training data -- enabling it to recognize multi-granularity structures, such as distinguishing lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP consistently outperforms state-of-the-art baselines across multiple grounding benchmarks. Furthermore, integrating its spatial outputs into downstream tasks, including medical VQA and report generation, leads to substantial performance gains. Our dataset will be released soon.",
        "arxiv_id": "2508.10528",
        "ARXIVID": "2508.10528",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces Med-GLIP, a framework for medical image grounding integrating language and image understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.10774": {
        "authors": [
            "Youping Gu",
            "Xiaolong Li",
            "Yuhao Hu",
            "Bohan Zhuang"
        ],
        "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation",
        "abstract": "arXiv:2508.10774v1 Announce Type: new  Abstract: Diffusion transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates sparsity into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/.",
        "arxiv_id": "2508.10774",
        "ARXIVID": "2508.10774",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on efficient video generation with novel methods like BLADE.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10432": {
        "authors": [
            "Baichen Liu",
            "Qi Lyu",
            "Xudong Wang",
            "Jiahua Dong",
            "Lianqing Liu",
            "Zhi Han"
        ],
        "title": "CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation",
        "abstract": "arXiv:2508.10432v1 Announce Type: new  Abstract: Continual video instance segmentation demands both the plasticity to absorb new object categories and the stability to retain previously learned ones, all while preserving temporal consistency across frames. In this work, we introduce Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier attempt tailored to address the instance-wise, category-wise, and task-wise confusion in continual video instance segmentation. For instance-wise learning, we model instance tracking and construct instance correlation loss, which emphasizes the correlation with the prior query space while strengthening the specificity of the current task query. For category-wise learning, we build an adaptive residual semantic prompt (ARSP) learning framework, which constructs a learnable semantic residual prompt pool generated by category text and uses an adjustive query-prompt matching mechanism to build a mapping relationship between the query of the current task and the semantic residual prompt. Meanwhile, a semantic consistency loss based on the contrastive learning is introduced to maintain semantic coherence between object queries and residual prompts during incremental training. For task-wise learning, to ensure the correlation at the inter-task level within the query space, we introduce a concise yet powerful initialization strategy for incremental prompts. Extensive experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that CRISP significantly outperforms existing continual segmentation methods in the long-term continual video instance segmentation task, avoiding catastrophic forgetting and effectively improving segmentation and classification performance. The code is available at https://github.com/01upup10/CRISP.",
        "arxiv_id": "2508.10432",
        "ARXIVID": "2508.10432",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on continual video instance segmentation with novel methods like CRISP.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10770": {
        "authors": [
            "Tiancheng Han",
            "Yunfei Gao",
            "Yong Li",
            "Wuzhou Yu",
            "Qiaosheng Zhang",
            "Wenqi Shao"
        ],
        "title": "From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models",
        "abstract": "arXiv:2508.10770v1 Announce Type: new  Abstract: Spatio-physical reasoning, a foundation capability for understanding the real physics world, is a critical step towards building robust world models. While recent vision language models (VLMs) have shown remarkable progress in specialized domains like multimodal mathematics and pure spatial understanding, their capability for spatio-physical reasoning remains largely unexplored. This paper provides a comprehensive diagnostic analysis of mainstream VLMs, revealing that current models perform inadequately on this crucial task. Further detailed analysis shows that this underperformance is largely attributable to biases caused by human-like prior and a lack of deep reasoning. To address these challenges, we apply supervised fine-tuning followed by rule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant improvements in spatio-physical reasoning capabilities and surpassing leading proprietary models. Nevertheless, despite this success, the model's generalization to new physics scenarios remains limited -- underscoring the pressing need for new approaches in spatio-physical reasoning.",
        "arxiv_id": "2508.10770",
        "ARXIVID": "2508.10770",
        "COMMENT": "Matches criterion 1 as it focuses on spatio-physical reasoning in vision-language models, which is a form of spatial intelligence.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2508.10287": {
        "authors": [
            "Simindokht Jahangard",
            "Mehrzad Mohammadi",
            "Yi Shen",
            "Zhixi Cai",
            "Hamid Rezatofighi"
        ],
        "title": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics",
        "abstract": "arXiv:2508.10287v1 Announce Type: new  Abstract: Recent advances in Vision-Language Models (VLMs) and large language models (LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI agents like robots. However, existing visual reasoning benchmarks often suffer from several limitations: they lack a clear definition of reasoning complexity, offer have no control to generate questions over varying difficulty and task customization, and fail to provide structured, step-by-step reasoning annotations (workflows). To bridge these gaps, we formalize reasoning complexity, introduce an adaptive query engine that generates customizable questions of varying complexity with detailed intermediate annotations, and extend the JRDB dataset with human-object interaction and geometric relationship annotations to create JRDB-Reasoning, a benchmark tailored for visual reasoning in human-crowded environments. Our engine and benchmark enable fine-grained evaluation of visual reasoning frameworks and dynamic assessment of visual-language models across reasoning levels.",
        "arxiv_id": "2508.10287",
        "ARXIVID": "2508.10287",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (JRDB-Reasoning) for visual reasoning in robotics, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10655": {
        "authors": [
            "Zhangyong Tang",
            "Tianyang Xu",
            "Xuefeng Zhu",
            "Chunyang Cheng",
            "Tao Zhou",
            "Xiaojun Wu",
            "Josef Kittler"
        ],
        "title": "Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking",
        "abstract": "arXiv:2508.10655v1 Announce Type: new  Abstract: Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \\textit{inconsistency} between training and testing, thus leading to performance \\textit{degradation}. To address these issues, this work advances in two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\\%. \\ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \\textit{https://github.com/Zhangyong-Tang/UniBench300}.",
        "arxiv_id": "2508.10655",
        "ARXIVID": "2508.10655",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a unified benchmark and continual learning approach for multi-modal visual object tracking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10522": {
        "authors": [
            "Quang Nguyen",
            "Nhat Le",
            "Baoru Huang",
            "Minh Nhat Vu",
            "Chengcheng Tang",
            "Van Nguyen",
            "Ngan Le",
            "Thieu Vo",
            "Anh Nguyen"
        ],
        "title": "EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba",
        "abstract": "arXiv:2508.10522v1 Announce Type: new  Abstract: Estimating human dance motion is a challenging task with various industrial applications. Recently, many efforts have focused on predicting human dance motion using either egocentric video or music as input. However, the task of jointly estimating human motion from both egocentric video and music remains largely unexplored. In this paper, we aim to develop a new method that predicts human dance motion from both egocentric video and music. In practice, the egocentric view often obscures much of the body, making accurate full-pose estimation challenging. Additionally, incorporating music requires the generated head and body movements to align well with both visual and musical inputs. We first introduce EgoAIST++, a new large-scale dataset that combines both egocentric views and music with more than 36 hours of dancing motion. Drawing on the success of diffusion models and Mamba on modeling sequences, we develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly captures the skeleton structure of the human body. We illustrate that our approach is theoretically supportive. Intensive experiments show that our method clearly outperforms state-of-the-art approaches and generalizes effectively to real-world data.",
        "arxiv_id": "2508.10522",
        "ARXIVID": "2508.10522",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a method for human dance motion estimation from egocentric video and music, along with a new dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10631": {
        "authors": [
            "Nicola Dall'Asen",
            "Xiaofeng Zhang",
            "Reyhane Askari Hemmat",
            "Melissa Hall",
            "Jakob Verbeek",
            "Adriana Romero-Soriano",
            "Michal Drozdzal"
        ],
        "title": "Increasing the Utility of Synthetic Images through Chamfer Guidance",
        "abstract": "arXiv:2508.10631v1 Announce Type: new  Abstract: Conditional image generative models hold considerable promise to produce infinite amounts of synthetic training data. Yet, recent progress in generation quality has come at the expense of generation diversity, limiting the utility of these models as a source of synthetic training data. Although guidance-based approaches have been introduced to improve the utility of generated data by focusing on quality or diversity, the (implicit or explicit) utility functions oftentimes disregard the potential distribution shift between synthetic and real data. In this work, we introduce Chamfer Guidance: a training-free guidance approach which leverages a handful of real exemplar images to characterize the quality and diversity of synthetic data. We show that by leveraging the proposed Chamfer Guidance, we can boost the diversity of the generations w.r.t. a dataset of real images while maintaining or improving the generation quality on ImageNet-1k and standard geo-diversity benchmarks. Our approach achieves state-of-the-art few-shot performance with as little as 2 exemplar real images, obtaining 96.4\\% in terms of precision, and 86.4\\% in terms of distributional coverage, which increase to 97.5\\% and 92.7\\%, respectively, when using 32 real images. We showcase the benefits of the Chamfer Guidance generation by training downstream image classifiers on synthetic data, achieving accuracy boost of up to 15\\% for in-distribution over the baselines, and up to 16\\% in out-of-distribution. Furthermore, our approach does not require using the unconditional model, and thus obtains a 31\\% reduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling time.",
        "arxiv_id": "2508.10631",
        "ARXIVID": "2508.10631",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it discusses Chamfer Guidance for improving synthetic image generation and its utility for downstream tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10711": {
        "authors": [
            "NextStep Team",
            "Chunrui Han",
            "Guopeng Li",
            "Jingwei Wu",
            "Quan Sun",
            "Yan Cai",
            "Yuang Peng",
            "Zheng Ge",
            "Deyu Zhou",
            "Haomiao Tang",
            "Hongyu Zhou",
            "Kenkun Liu",
            "Ailin Huang",
            "Bin Wang",
            "Changxin Miao",
            "Deshan Sun",
            "En Yu",
            "Fukun Yin",
            "Gang Yu",
            "Hao Nie",
            "Haoran Lv",
            "Hanpeng Hu",
            "Jia Wang",
            "Jian Zhou",
            "Jianjian Sun",
            "Kaijun Tan",
            "Kang An",
            "Kangheng Lin",
            "Liang Zhao",
            "Mei Chen",
            "Peng Xing",
            "Rui Wang",
            "Shiyu Liu",
            "Shutao Xia",
            "Tianhao You",
            "Wei Ji",
            "Xianfang Zeng",
            "Xin Han",
            "Xuelin Zhang",
            "Yana Wei",
            "Yanming Xu",
            "Yimin Jiang",
            "Yingming Wang",
            "Yu Zhou",
            "Yucheng Han",
            "Ziyang Meng",
            "Binxing Jiao",
            "Daxin Jiang",
            "Xiangyu Zhang",
            "Yibo Zhu"
        ],
        "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "abstract": "arXiv:2508.10711v1 Announce Type: new  Abstract: Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.",
        "arxiv_id": "2508.10711",
        "ARXIVID": "2508.10711",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it discusses a novel autoregressive model for text-to-image generation with continuous tokens.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10264": {
        "authors": [
            "Haonan Ge",
            "Yiwei Wang",
            "Ming-Hsuan Yang",
            "Yujun Cai"
        ],
        "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs",
        "abstract": "arXiv:2508.10264v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across multimodal tasks. However, they often produce hallucinations -- text that is inconsistent with visual input, due to the limited ability to verify information in different regions of the image. To address this, we propose Multi-Region Fusion Decoding (MRFD), a training-free decoding method that improves factual grounding by modeling inter-region consistency. MRFD identifies salient regions using cross-attention, generates initial responses for each, and computes reliability weights based on Jensen-Shannon Divergence (JSD) among the responses. These weights guide a consistency-aware fusion of per-region predictions, using region-aware prompts inspired by Chain-of-Thought reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD significantly reduces hallucinations and improves response factuality without requiring model updates.",
        "arxiv_id": "2508.10264",
        "ARXIVID": "2508.10264",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a method to reduce hallucinations in large vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.10667": {
        "authors": [
            "Shixiong Xu",
            "Chenghao Zhang",
            "Lubin Fan",
            "Yuan Zhou",
            "Bin Fan",
            "Shiming Xiang",
            "Gaofeng Meng",
            "Jieping Ye"
        ],
        "title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models",
        "abstract": "arXiv:2508.10667v1 Announce Type: new  Abstract: Large visual language models (LVLMs) have demonstrated impressive performance in coarse-grained geo-localization at the country or city level, but they struggle with fine-grained street-level localization within urban areas. In this paper, we explore integrating city-wide address localization capabilities into LVLMs, facilitating flexible address-related question answering using street-view images. A key challenge is that the street-view visual question-and-answer (VQA) data provides only microscopic visual cues, leading to subpar performance in fine-tuned models. To tackle this issue, we incorporate perspective-invariant satellite images as macro cues and propose cross-view alignment tuning including a satellite-view and street-view image grafting mechanism, along with an automatic label generation mechanism. Then LVLM's global understanding of street distribution is enhanced through cross-view matching. Our proposed model, named AddressVLM, consists of two-stage training protocols: cross-view alignment tuning and address localization tuning. Furthermore, we have constructed two street-view VQA datasets based on image address localization datasets from Pittsburgh and San Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM outperforms counterpart LVLMs by over 9% and 12% in average address localization accuracy on these two datasets, respectively.",
        "arxiv_id": "2508.10667",
        "ARXIVID": "2508.10667",
        "COMMENT": "Matches criterion 2. Explores fine-grained address localization using large vision-language models, which aligns with visual and multimodal LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.10893": {
        "authors": [
            "Yushi Lan",
            "Yihang Luo",
            "Fangzhou Hong",
            "Shangchen Zhou",
            "Honghua Chen",
            "Zhaoyang Lyu",
            "Shuai Yang",
            "Bo Dai",
            "Chen Change Loy",
            "Xingang Pan"
        ],
        "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer",
        "abstract": "arXiv:2508.10893v1 Announce Type: new  Abstract: We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.",
        "arxiv_id": "2508.10893",
        "ARXIVID": "2508.10893",
        "COMMENT": "Matches criterion 3. Introduces a novel method for 3D reconstruction using causal Transformers, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.10337": {
        "authors": [
            "Chenliang Zhang",
            "Lin Wang",
            "Yuanyuan Lu",
            "Yusheng Qi",
            "Kexin Wang",
            "Peixu Hou",
            "Wenshi Chen"
        ],
        "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering",
        "abstract": "arXiv:2508.10337v1 Announce Type: new  Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the META CRAG-MM challenge. The challenge requires building a comprehensive retrieval-augmented generation system capable for multi-modal multi-turn question answering. The competition consists of three tasks: (1) answering questions using structured data retrieved from an image-based mock knowledge graph, (2) synthesizing information from both knowledge graphs and web search results, and (3) handling multi-turn conversations that require context understanding and information aggregation from multiple sources. For Task 1, our solution is based on the vision large language model, enhanced by supervised fine-tuning with knowledge distilled from GPT-4.1. We further applied curriculum learning strategies to guide reinforcement learning, resulting in improved answer accuracy and reduced hallucination. For Task 2 and Task 3, we additionally leveraged web search APIs to incorporate external knowledge, enabling the system to better handle complex queries and multi-turn conversations. Our approach achieved 1st place in Task 1 with a significant lead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of the integration of curriculum learning with reinforcement learning in our training pipeline.",
        "arxiv_id": "2508.10337",
        "ARXIVID": "2508.10337",
        "COMMENT": "Matches criterion 2 as it explores a multimodal large language model (VLLM) for question answering with curriculum learning and reinforcement learning strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.10869": {
        "authors": [
            "Sushant Gautam",
            "Vajira Thambawita",
            "Michael Riegler",
            "P{\\aa}l Halvorsen",
            "Steven Hicks"
        ],
        "title": "Medico 2025: Visual Question Answering for Gastrointestinal Imaging",
        "abstract": "arXiv:2508.10869v1 Announce Type: new  Abstract: The Medico 2025 challenge addresses Visual Question Answering (VQA) for Gastrointestinal (GI) imaging, organized as part of the MediaEval task series. The challenge focuses on developing Explainable Artificial Intelligence (XAI) models that answer clinically relevant questions based on GI endoscopy images while providing interpretable justifications aligned with medical reasoning. It introduces two subtasks: (1) answering diverse types of visual questions using the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500 images and 159,549 complex question-answer (QA) pairs, serves as the benchmark for the challenge. By combining quantitative performance metrics and expert-reviewed explainability assessments, this task aims to advance trustworthy Artificial Intelligence (AI) in medical image analysis. Instructions, data access, and an updated guide for participation are available in the official competition repository: https://github.com/simula/MediaEval-Medico-2025",
        "arxiv_id": "2508.10869",
        "ARXIVID": "2508.10869",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on visual question answering for gastrointestinal imaging, integrating vision and language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.10268": {
        "authors": [
            "Yujie Zhao",
            "Jiabei Zeng",
            "Shiguang Shan"
        ],
        "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones",
        "abstract": "arXiv:2508.10268v1 Announce Type: new  Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the estimators still struggle to generalize across individuals due to personal differences. Therefore, person-specific calibration is required for accurate PoG estimation. However, calibrated PoG estimators are often sensitive to head pose variations. To address this, we investigate the key factors influencing calibrated estimators and explore pose-robust calibration strategies. Specifically, we first construct a benchmark, MobilePoG, which includes facial images from 32 individuals focusing on designated points under either fixed or continuously changing head poses. Using this benchmark, we systematically analyze how the diversity of calibration points and head poses influences estimation accuracy. Our experiments show that introducing a wider range of head poses during calibration improves the estimator's ability to handle pose variation. Building on this insight, we propose a dynamic calibration strategy in which users fixate on calibration points while moving their phones. This strategy naturally introduces head pose variation during a user-friendly and efficient calibration process, ultimately producing a better calibrated PoG estimator that is less sensitive to head pose variations than those using conventional calibration strategies. Codes and datasets are available at our project page.",
        "arxiv_id": "2508.10268",
        "ARXIVID": "2508.10268",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MobilePoG) for gaze estimation and proposes a novel calibration strategy.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10339": {
        "authors": [
            "Andrew Bai",
            "Justin Cui",
            "Ruochen Wang",
            "Cho-Jui Hsieh"
        ],
        "title": "Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models",
        "abstract": "arXiv:2508.10339v1 Announce Type: new  Abstract: Vision-language instruction tuning achieves two main purposes: learning visual concepts and learning visual skills. In this paper, we found that vision-language benchmarks fall into the dichotomy of mainly benefiting from training on instructions with similar skills or visual concepts. Inspired by the discovery, we designed a simple targeted training data selection method to optimize the performance of a given benchmark. We first extract the concepts/skills from the benchmark, determine whether the benchmark predominantly benefits from similar concepts or skills, and finally select instructions with the most matching concepts/skills. Experiments on 10+ benchmarks validate the effectiveness of our targeted data selection method, showing +0.9\\% over the best existing baseline averaged over all benchmarks and +1.5\\% on the skill-focused subset. Our findings underscore the importance of recognizing the inherent trade-off within instruction selection, which requires balancing the acquisition of conceptual knowledge against visual skill.",
        "arxiv_id": "2508.10339",
        "ARXIVID": "2508.10339",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it discusses instruction tuning for vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10316": {
        "authors": [
            "Yuanzhi Liang",
            "Yijie Fang",
            "Rui Li",
            "Ziqi Ni",
            "Ruijie Su",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances",
        "abstract": "arXiv:2508.10316v1 Announce Type: new  Abstract: Generative models have made significant progress in synthesizing visual content, including images, videos, and 3D/4D structures. However, they are typically trained with surrogate objectives such as likelihood or reconstruction loss, which often misalign with perceptual quality, semantic accuracy, or physical realism. Reinforcement learning (RL) offers a principled framework for optimizing non-differentiable, preference-driven, and temporally structured objectives. Recent advances demonstrate its effectiveness in enhancing controllability, consistency, and human alignment across generative tasks. This survey provides a systematic overview of RL-based methods for visual content generation. We review the evolution of RL from classical control to its role as a general-purpose optimization tool, and examine its integration into image, video, and 3D/4D generation. Across these domains, RL serves not only as a fine-tuning mechanism but also as a structural component for aligning generation with complex, high-level goals. We conclude with open challenges and future research directions at the intersection of RL and generative modeling.",
        "arxiv_id": "2508.10316",
        "ARXIVID": "2508.10316",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it discusses integrating reinforcement learning with visual generative models, which aligns with vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10833": {
        "authors": [
            "Zhangxuan Gu",
            "Zhengwen Zeng",
            "Zhenyu Xu",
            "Xingran Zhou",
            "Shuheng Shen",
            "Yunfei Liu",
            "Beitong Zhou",
            "Changhua Meng",
            "Tianyu Xia",
            "Weizhi Chen",
            "Yue Wen",
            "Jingya Dou",
            "Fei Tang",
            "Jinzhen Lin",
            "Yulin Liu",
            "Zhenlin Guo",
            "Yichen Gong",
            "Heng Jia",
            "Changlong Gao",
            "Yuan Guo",
            "Yong Deng",
            "Zhenyu Guo",
            "Liang Chen",
            "Weiqiang Wang"
        ],
        "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
        "abstract": "arXiv:2508.10833v1 Announce Type: new  Abstract: We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \\& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.",
        "arxiv_id": "2508.10833",
        "ARXIVID": "2508.10833",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and methods for UI agents, which are relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10453": {
        "authors": [
            "Qiang Zhu",
            "Xiandong Meng",
            "Yuxian Jiang",
            "Fan Zhang",
            "David Bull",
            "Shuyuan Zhu",
            "Bing Zeng"
        ],
        "title": "Trajectory-aware Shifted State Space Models for Online Video Super-Resolution",
        "abstract": "arXiv:2508.10453v1 Announce Type: new  Abstract: Online video super-resolution (VSR) is an important technique for many real-world video processing applications, which aims to restore the current high-resolution video frame based on temporally previous frames. Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model. Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7\\% complexity reduction (in MACs). The source code for TS-Mamba will be available at https://github.com.",
        "arxiv_id": "2508.10453",
        "ARXIVID": "2508.10453",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel method for online video super-resolution with trajectory-aware modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10897": {
        "authors": [
            "Mengyuan Liu",
            "Xinshun Wang",
            "Zhongbin Fang",
            "Deheng Ye",
            "Xia Li",
            "Tao Tang",
            "Songtao Wu",
            "Xiangtai Li",
            "Ming-Hsuan Yang"
        ],
        "title": "Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning",
        "abstract": "arXiv:2508.10897v1 Announce Type: new  Abstract: This paper aims to model 3D human motion across domains, where a single model is expected to handle multiple modalities, tasks, and datasets. Existing cross-domain models often rely on domain-specific components and multi-stage training, which limits their practicality and scalability. To overcome these challenges, we propose a new setting to train a unified cross-domain model through a single process, eliminating the need for domain-specific components and multi-stage training. We first introduce Pose-in-Context (PiC), which leverages in-context learning to create a pose-centric cross-domain model. While PiC generalizes across multiple pose-based tasks and datasets, it encounters difficulties with modality diversity, prompting strategy, and contextual dependency handling. We thus propose Human-in-Context (HiC), an extension of PiC that broadens generalization across modalities, tasks, and datasets. HiC combines pose and mesh representations within a unified framework, expands task coverage, and incorporates larger-scale datasets. Additionally, HiC introduces a max-min similarity prompt sampling strategy to enhance generalization across diverse domains and a network architecture with dual-branch context injection for improved handling of contextual dependencies. Extensive experimental results show that HiC performs better than PiC in terms of generalization, data scale, and performance across a wide range of domains. These results demonstrate the potential of HiC for building a unified cross-domain 3D human motion model with improved flexibility and scalability. The source codes and models are available at https://github.com/BradleyWang0416/Human-in-Context.",
        "arxiv_id": "2508.10897",
        "ARXIVID": "2508.10897",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a unified cross-domain 3D human motion model with novel methods for generalization across tasks and datasets.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.10133": {
        "authors": [
            "Thanh-Dat Truong",
            "Christophe Bobda",
            "Nitin Agarwal",
            "Khoa Luu"
        ],
        "title": "MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning",
        "abstract": "arXiv:2508.10133v1 Announce Type: new  Abstract: Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach\\footnote{The source code of this work will be publicly available.} to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach.",
        "arxiv_id": "2508.10133",
        "ARXIVID": "2508.10133",
        "COMMENT": "Matches criterion 2 as it introduces a novel multimodal fusion learning approach using attention-based normalizing flows.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.10769": {
        "authors": [
            "Zhiqi Shen",
            "Shaojing Fan",
            "Danni Xu",
            "Terence Sim",
            "Mohan Kankanhalli"
        ],
        "title": "Modeling Human Responses to Multimodal AI Content",
        "abstract": "arXiv:2508.10769v1 Announce Type: new  Abstract: As AI-generated content becomes widespread, so does the risk of misinformation. While prior research has primarily focused on identifying whether content is authentic, much less is known about how such content influences human perception and behavior. In domains like trading or the stock market, predicting how people react (e.g., whether a news post will go viral), can be more critical than verifying its factual accuracy. To address this, we take a human-centered approach and introduce the MhAIM Dataset, which contains 154,552 online posts (111,153 of them AI-generated), enabling large-scale analysis of how people respond to AI-generated content. Our human study reveals that people are better at identifying AI content when posts include both text and visuals, particularly when inconsistencies exist between the two. We propose three new metrics: trustworthiness, impact, and openness, to quantify how users judge and engage with online content. We present T-Lens, an LLM-based agent system designed to answer user queries by incorporating predicted human responses to multimodal information. At its core is HR-MCP (Human Response Model Context Protocol), built on the standardized Model Context Protocol (MCP), enabling seamless integration with any LLM. This integration allows T-Lens to better align with human reactions, enhancing both interpretability and interaction capabilities. Our work provides empirical insights and practical tools to equip LLMs with human-awareness capabilities. By highlighting the complex interplay among AI, human cognition, and information reception, our findings suggest actionable strategies for mitigating the risks of AI-driven misinformation.",
        "arxiv_id": "2508.10769",
        "ARXIVID": "2508.10769",
        "COMMENT": "Matches criterion 2 as it explores multimodal AI content and integrates human response modeling with LLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.10865": {
        "authors": [
            "Mojtaba Safari",
            "Shansong Wang",
            "Mingzhe Hu",
            "Zach Eidex",
            "Qiang Li",
            "Xiaofeng Yang"
        ],
        "title": "Performance of GPT-5 in Brain Tumor MRI Reasoning",
        "abstract": "arXiv:2508.10865v1 Announce Type: new  Abstract: Accurate differentiation of brain tumor types on magnetic resonance imaging (MRI) is critical for guiding treatment planning in neuro-oncology. Recent advances in large language models (LLMs) have enabled visual question answering (VQA) approaches that integrate image interpretation with natural language reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain metastases (MET). Each case included multi-sequence MRI triplanar mosaics and structured clinical features transformed into standardized VQA items. Models were assessed in a zero-shot chain-of-thought setting for accuracy on both visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%), and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single model dominating across all cohorts. These findings suggest that GPT-5 family models can achieve moderate accuracy in structured neuro-oncological VQA tasks, but not at a level acceptable for clinical use.",
        "arxiv_id": "2508.10865",
        "ARXIVID": "2508.10865",
        "COMMENT": "Matches criterion 5 as it evaluates GPT-5 models for visual question answering tasks involving brain tumor MRI reasoning, integrating image and language models.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2508.10429": {
        "authors": [
            "Yi Dong",
            "Yusuke Muraoka",
            "Scott Shi",
            "Yi Zhang"
        ],
        "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance",
        "abstract": "arXiv:2508.10429v1 Announce Type: new  Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence dataset with verifiable provenance. It is a curated approximately 10% open subset of an original 1.2 million, quality-accepted corpus of food images annotated for a wide range of information (such as dish name, region of creation). The corpus was collected over six weeks from over 87,000 contributors using the Codatta contribution model, which combines community sourcing with configurable AI-assisted quality checks; each submission is linked to a wallet address in a secure off-chain ledger for traceability, with a full on-chain protocol on the roadmap. We describe the schema, pipeline, and QA, and validate utility by fine-tuning large vision-language models (ChatGPT 5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning yields consistent gains over out-of-box baselines across standard metrics; we report results primarily on the MM-Food-100K subset. We release MM-Food-100K for publicly free access and retain approximately 90% for potential commercial access with revenue sharing to contributors.",
        "arxiv_id": "2508.10429",
        "ARXIVID": "2508.10429",
        "COMMENT": "Matches criterion 2 as it involves fine-tuning vision-language models on a multimodal dataset.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2508.10838": {
        "authors": [
            "Peng Xu",
            "Zhiyu Xiang",
            "Jingyun Fu",
            "Tianyu Pu",
            "Kai Wang",
            "Chaojie Ji",
            "Tingming Bai",
            "Eryun Liu"
        ],
        "title": "Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning",
        "abstract": "arXiv:2508.10838v1 Announce Type: new  Abstract: Current self-supervised stereo matching relies on the photometric consistency assumption, which breaks down in occluded regions due to ill-posed correspondences. To address this issue, we propose BaCon-Stereo, a simple yet effective contrastive learning framework for self-supervised stereo network training in both non-occluded and occluded regions. We adopt a teacher-student paradigm with multi-baseline inputs, in which the stereo pairs fed into the teacher and student share the same reference view but differ in target views. Geometrically, regions occluded in the student's target view are often visible in the teacher's, making it easier for the teacher to predict in these regions. The teacher's prediction is rescaled to match the student's baseline and then used to supervise the student. We also introduce an occlusion-aware attention map to better guide the student in learning occlusion completion. To support training, we synthesize a multi-baseline dataset BaCon-20k. Extensive experiments demonstrate that BaCon-Stereo improves prediction in both occluded and non-occluded regions, achieves strong generalization and robustness, and outperforms state-of-the-art self-supervised methods on both KITTI 2015 and 2012 benchmarks. Our code and dataset will be released upon paper acceptance.",
        "arxiv_id": "2508.10838",
        "ARXIVID": "2508.10838",
        "COMMENT": "Matches none of the specific criteria but is tangentially related to computer vision and machine learning through its focus on stereo matching and self-supervised learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10900": {
        "authors": [
            "Shuteng Wang",
            "Christian Theobalt",
            "Vladislav Golyanik"
        ],
        "title": "Quantum Visual Fields with Neural Amplitude Encoding",
        "abstract": "arXiv:2508.10900v1 Announce Type: new  Abstract: Quantum Implicit Neural Representations (QINRs) include components for learning and execution on gate-based quantum computers. While QINRs recently emerged as a promising new paradigm, many challenges concerning their architecture and ansatz design, the utility of quantum-mechanical properties, training efficiency and the interplay with classical modules remain. This paper advances the field by introducing a new type of QINR for 2D image and 3D geometric field learning, which we collectively refer to as Quantum Visual Field (QVF). QVF encodes classical data into quantum statevectors using neural amplitude encoding grounded in a learnable energy manifold, ensuring meaningful Hilbert space embeddings. Our ansatz follows a fully entangled design of learnable parametrised quantum circuits, with quantum (unitary) operations performed in the real Hilbert space, resulting in numerically stable training with fast convergence. QVF does not rely on classical post-processing -- in contrast to the previous QINR learning approach -- and directly employs projective measurement to extract learned signals encoded in the ansatz. Experiments on a quantum hardware simulator demonstrate that QVF outperforms the existing quantum approach and widely used classical foundational baselines in terms of visual representation accuracy across various metrics and model characteristics, such as learning of high-frequency details. We also show applications of QVF in 2D and 3D field completion and 3D shape interpolation, highlighting its practical potential.",
        "arxiv_id": "2508.10900",
        "ARXIVID": "2508.10900",
        "COMMENT": "Does not match any specific criterion but is an interesting exploration of quantum visual fields, which may have implications for generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10464": {
        "authors": [
            "Bella Specktor-Fadida",
            "Malte Hoffmann"
        ],
        "title": "SingleStrip: learning skull-stripping from a single labeled example",
        "abstract": "arXiv:2508.10464v1 Announce Type: new  Abstract: Deep learning segmentation relies heavily on labeled data, but manual labeling is laborious and time-consuming, especially for volumetric images such as brain magnetic resonance imaging (MRI). While recent domain-randomization techniques alleviate the dependency on labeled data by synthesizing diverse training images from label maps, they offer limited anatomical variability when very few label maps are available. Semi-supervised self-training addresses label scarcity by iteratively incorporating model predictions into the training set, enabling networks to learn from unlabeled data. In this work, we combine domain randomization with self-training to train three-dimensional skull-stripping networks using as little as a single labeled example. First, we automatically bin voxel intensities, yielding labels we use to synthesize images for training an initial skull-stripping model. Second, we train a convolutional autoencoder (AE) on the labeled example and use its reconstruction error to assess the quality of brain masks predicted for unlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the network, achieving skull-stripping performance on out-of-distribution data that approaches models trained with more labeled images. We compare AE-based ranking to consistency-based ranking under test-time augmentation, finding that the AE approach yields a stronger correlation with segmentation accuracy. Our results highlight the potential of combining domain randomization and AE-based quality control to enable effective semi-supervised segmentation from extremely limited labeled data. This strategy may ease the labeling burden that slows progress in studies involving new anatomical structures or emerging imaging techniques.",
        "arxiv_id": "2508.10464",
        "ARXIVID": "2508.10464",
        "COMMENT": "Does not match any specific criteria. Focuses on semi-supervised segmentation for medical imaging, which is tangential to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10881": {
        "authors": [
            "Lingen Li",
            "Guangzhi Wang",
            "Zhaoyang Zhang",
            "Yaowei Li",
            "Xiaoyu Li",
            "Qi Dou",
            "Jinwei Gu",
            "Tianfan Xue",
            "Ying Shan"
        ],
        "title": "ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing",
        "abstract": "arXiv:2508.10881v1 Announce Type: new  Abstract: Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.",
        "arxiv_id": "2508.10881",
        "ARXIVID": "2508.10881",
        "COMMENT": "Does not match any specific criteria. Focuses on cartoon production using generative models, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10407": {
        "authors": [
            "Eunseo Koh",
            "Seunghoo Hong",
            "Tae-Young Kim",
            "Simon S. Woo",
            "Jae-Pil Heo"
        ],
        "title": "Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models",
        "abstract": "arXiv:2508.10407v1 Announce Type: new  Abstract: Text-to-Image (T2I) diffusion models have made significant progress in generating diverse high-quality images from textual prompts. However, these models still face challenges in suppressing content that is strongly entangled with specific words. For example, when generating an image of ``Charlie Chaplin\", a ``mustache\" consistently appears even if explicitly instructed not to include it, as the concept of ``mustache\" is strongly entangled with ``Charlie Chaplin\". To address this issue, we propose a novel approach to directly suppress such entangled content within the text embedding space of diffusion models. Our method introduces a delta vector that modifies the text embedding to weaken the influence of undesired content in the generated image, and we further demonstrate that this delta vector can be easily obtained through a zero-shot approach. Furthermore, we propose a Selective Suppression with Delta Vector (SSDV) method to adapt delta vector into the cross-attention mechanism, enabling more effective suppression of unwanted content in regions where it would otherwise be generated. Additionally, we enabled more precise suppression in personalized T2I models by optimizing delta vector, which previous baselines were unable to achieve. Extensive experimental results demonstrate that our approach significantly outperforms existing methods, both in terms of quantitative and qualitative metrics.",
        "arxiv_id": "2508.10407",
        "ARXIVID": "2508.10407",
        "COMMENT": "Does not match any specific criteria. Focuses on text-to-image diffusion models and suppression of entangled content, which is tangential to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10164": {
        "authors": [
            "Bin Hong",
            "Jiayu Liu",
            "Zhenya Huang",
            "Kai Zhang",
            "Mengdi Zhang"
        ],
        "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization",
        "abstract": "arXiv:2508.10164v1 Announce Type: new  Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong performance on complex tasks through long Chain-of-Thought (CoT) reasoning. However, their lengthy outputs increase computational costs and may lead to overthinking, raising challenges in balancing reasoning effectiveness and efficiency. Current methods for efficient reasoning often compromise reasoning quality or require extensive resources. This paper investigates efficient methods to reduce the generation length of LRMs. We analyze generation path distributions and filter generated trajectories through difficulty estimation. Subsequently, we analyze the convergence behaviors of the objectives of various preference optimization methods under a Bradley-Terry loss based framework. Based on the analysis, we propose Length Controlled Preference Optimization (LCPO) that directly balances the implicit reward related to NLL loss. LCPO can effectively learn length preference with limited data and training. Extensive experiments demonstrate that our approach significantly reduces the average output length by over 50\\% across multiple benchmarks while maintaining the reasoning performance. Our work highlights the potential for computationally efficient approaches in guiding LRMs toward efficient reasoning.",
        "arxiv_id": "2508.10164",
        "ARXIVID": "2508.10164",
        "COMMENT": "Does not match any specific criteria. Focuses on optimizing reasoning length in LLMs, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10358": {
        "authors": [
            "Mengtao Zhou",
            "Sifan Wu",
            "Huan Zhang",
            "Qi Sima",
            "Bang Liu"
        ],
        "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles",
        "abstract": "arXiv:2508.10358v1 Announce Type: new  Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative reasoning--the proactive construction, testing, and revision of hypotheses in information-sparse environments. Existing benchmarks, often static or focused on social deduction, fail to capture the dynamic, exploratory nature of this reasoning process. To address this gap, we introduce a comprehensive research framework based on the classic \"Turtle Soup\" game, integrating a benchmark, an agent, and an evaluation protocol. We present TurtleSoup-Bench, the first large-scale, bilingual, interactive benchmark for imaginative reasoning, comprising 800 turtle soup puzzles sourced from both the Internet and expert authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs' performance in this setting. To evaluate reasoning quality, we develop a multi-dimensional protocol measuring logical consistency, detail completion, and conclusion alignment. Experiments with leading LLMs reveal clear capability limits, common failure patterns, and a significant performance gap compared to humans. Our work offers new insights into LLMs' imaginative reasoning and establishes a foundation for future research on exploratory agent behavior.",
        "arxiv_id": "2508.10358",
        "ARXIVID": "2508.10358",
        "COMMENT": "Does not match any specific criteria. Focuses on imaginative reasoning in LLMs, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10566": {
        "authors": [
            "Shiyu Liu",
            "Kui Jiang",
            "Xianming Liu",
            "Hongxun Yao",
            "Xiaocheng Feng"
        ],
        "title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis",
        "abstract": "arXiv:2508.10566v1 Announce Type: new  Abstract: Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.",
        "arxiv_id": "2508.10566",
        "ARXIVID": "2508.10566",
        "COMMENT": "Does not match any specific criteria. Focuses on audio-driven talking head synthesis, which is not directly related to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10680": {
        "authors": [
            "Busra Bulut",
            "Maik Dannecker",
            "Thomas Sanchez",
            "Sara Neves Silva",
            "Vladyslav Zalevskyi",
            "Steven Jia",
            "Jean-Baptiste Ledoux",
            "Guillaume Auzias",
            "Fran\\c{c}ois Rousseau",
            "Jana Hutter",
            "Daniel Rueckert",
            "Meritxell Bach Cuadra"
        ],
        "title": "Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping",
        "abstract": "arXiv:2508.10680v1 Announce Type: new  Abstract: T2 mapping in fetal brain MRI has the potential to improve characterization of the developing brain, especially at mid-field (0.55T), where T2 decay is slower. However, this is challenging as fetal MRI acquisition relies on multiple motion-corrupted stacks of thick slices, requiring slice-to-volume reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently, T2 mapping involves repeated acquisitions of these stacks at each echo time (TE), leading to long scan times and high sensitivity to motion. We tackle this challenge with a method that jointly reconstructs data across TEs, addressing severe motion. Our approach combines implicit neural representations with a physics-informed regularization that models T2 decay, enabling information sharing across TEs while preserving anatomical and quantitative T2 fidelity. We demonstrate state-of-the-art performance on simulated fetal brain and in vivo adult datasets with fetal-like motion. We also present the first in vivo fetal T2 mapping results at 0.55T. Our study shows potential for reducing the number of stacks per TE in T2 mapping by leveraging anatomical redundancy.",
        "arxiv_id": "2508.10680",
        "ARXIVID": "2508.10680",
        "COMMENT": "Does not match any specific criterion but is relevant to medical imaging and implicit neural representations.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10801": {
        "authors": [
            "Ziqi Ye",
            "Shuran Ma",
            "Jie Yang",
            "Xiaoyi Yang",
            "Ziyang Gong",
            "Xue Yang",
            "Haipeng Wang"
        ],
        "title": "Object Fidelity Diffusion for Remote Sensing Image Generation",
        "abstract": "arXiv:2508.10801v1 Announce Type: new  Abstract: High-precision controllable remote sensing image generation is both meaningful and challenging. Existing diffusion models often produce low-fidelity images due to their inability to adequately capture morphological details, which may affect the robustness and reliability of object detection models. To enhance the accuracy and fidelity of generated objects in remote sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which effectively improves the fidelity of generated objects. Specifically, we are the first to extract the prior shapes of objects based on the layout for diffusion models in remote sensing. Then, we introduce a dual-branch diffusion model with diffusion consistency loss, which can generate high-fidelity remote sensing images without providing real images during the sampling phase. Furthermore, we introduce DDPO to fine-tune the diffusion process, making the generated remote sensing images more diverse and semantically consistent. Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art methods in the remote sensing across key quality metrics. Notably, the performance of several polymorphic and small object classes shows significant improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for airplanes, ships, and vehicles, respectively.",
        "arxiv_id": "2508.10801",
        "ARXIVID": "2508.10801",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in remote sensing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10719": {
        "authors": [
            "Longxiang Tang",
            "Ruihang Chu",
            "Xiang Wang",
            "Yujin Han",
            "Pingyu Wu",
            "Chunming He",
            "Yingya Zhang",
            "Shiwei Zhang",
            "Jiaya Jia"
        ],
        "title": "Exploiting Discriminative Codebook Prior for Autoregressive Image Generation",
        "abstract": "arXiv:2508.10719v1 Announce Type: new  Abstract: Advanced discrete token-based autoregressive image generation systems first tokenize images into sequences of token indices with a codebook, and then model these sequences in an autoregressive paradigm. While autoregressive generative models are trained only on index values, the prior encoded in the codebook, which contains rich token similarity information, is not exploited. Recent studies have attempted to incorporate this prior by performing naive k-means clustering on the tokens, helping to facilitate the training of generative models with a reduced codebook. However, we reveal that k-means clustering performs poorly in the codebook feature space due to inherent issues, including token space disparity and centroid distance inaccuracy. In this work, we propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to k-means clustering for more effectively mining and utilizing the token similarity information embedded in the codebook. DCPE replaces the commonly used centroid-based distance, which is found to be unsuitable and inaccurate for the token feature space, with a more reasonable instance-based distance. Using an agglomerative merging technique, it further addresses the token space disparity issue by avoiding splitting high-density regions and aggregating low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play and integrates seamlessly with existing codebook prior-based paradigms. With the discriminative prior extracted, DCPE accelerates the training of autoregressive models by 42% on LlamaGen-B and improves final FID and IS performance.",
        "arxiv_id": "2508.10719",
        "ARXIVID": "2508.10719",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10445": {
        "authors": [
            "Hang Jin",
            "Chenqiang Gao",
            "Junjie Guo",
            "Fangcen Liu",
            "Kanghui Tian",
            "Qinyao Chang"
        ],
        "title": "DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations",
        "abstract": "arXiv:2508.10445v1 Announce Type: new  Abstract: Infrared-visible object detection has shown great potential in real-world applications, enabling robust all-day perception by leveraging the complementary information of infrared and visible images. However, existing methods typically require dual-modality annotations to output detection results for both modalities during prediction, which incurs high annotation costs. To address this challenge, we propose a novel infrared-visible Decoupled Object Detection framework with Single-modality Annotations, called DOD-SA. The architecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative Teacher-Student Network (CoSD-TSNet), which consists of a single-modality branch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The teacher model generates pseudo-labels for the unlabeled modality, simultaneously supporting the training of the student model. The collaborative design enables cross-modality knowledge transfer from the labeled modality to the unlabeled modality, and facilitates effective SM-to-DMD branch supervision. To further improve the decoupling ability of the model and the pseudo-label quality, we introduce a Progressive and Self-Tuning Training Strategy (PaST) that trains the model in three stages: (1) pretraining SM-Branch, (2) guiding the learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In addition, we design a Pseudo Label Assigner (PLA) to align and pair labels across modalities, explicitly addressing modality misalignment during training. Extensive experiments on the DroneVehicle dataset demonstrate that our method outperforms state-of-the-art (SOTA).",
        "arxiv_id": "2508.10445",
        "ARXIVID": "2508.10445",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10600": {
        "authors": [
            "Yuxin Cao",
            "Yedi Zhang",
            "Wentao He",
            "Yifan Liao",
            "Yan Xiao",
            "Chang Li",
            "Zhiyong Huang",
            "Jin Song Dong"
        ],
        "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving",
        "abstract": "arXiv:2508.10600v1 Announce Type: new  Abstract: Learning-based autonomous driving systems remain critically vulnerable to adversarial patches, posing serious safety and security risks in their real-world deployment. Black-box attacks, notable for their high attack success rate without model knowledge, are especially concerning, with their transferability extensively studied to reduce computational costs compared to query-based attacks. Previous transferability-based black-box attacks typically adopt mean Average Precision (mAP) as the evaluation metric and design training loss accordingly. However, due to the presence of multiple detected bounding boxes and the relatively lenient Intersection over Union (IoU) thresholds, the attack effectiveness of these approaches is often overestimated, resulting in reduced success rates in practical attacking scenarios. Furthermore, patches trained on low-resolution data often fail to maintain effectiveness on high-resolution images, limiting their transferability to autonomous driving datasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch Attack framework for 2D object detection in autonomous driving, specifically optimized for high-resolution datasets. First, we introduce a novel metric, Practical Attack Success Rate (PASR), to more accurately quantify attack effectiveness with greater relevance for pedestrian safety. Second, we present a tailored Localization-Confidence Suppression Loss (LCSL) to improve attack transferability under PASR. Finally, to maintain the transferability for high-resolution datasets, we further incorporate the Probabilistic Scale-Preserving Padding (PSPP) into the patch attack pipeline as a data preprocessing step. Extensive experiments show that P$^3$A outperforms state-of-the-art attacks on unseen models and unseen high-resolution datasets, both under the proposed practical IoU-based evaluation metric and the previous mAP-based metrics.",
        "arxiv_id": "2508.10600",
        "ARXIVID": "2508.10600",
        "COMMENT": "Does not match any specific criteria. Focuses on adversarial patch attacks for 2D object detection, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10868": {
        "authors": [
            "Yibo Zhang",
            "Li Zhang",
            "Rui Ma",
            "Nan Cao"
        ],
        "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures",
        "abstract": "arXiv:2508.10868v1 Announce Type: new  Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.",
        "arxiv_id": "2508.10868",
        "ARXIVID": "2508.10868",
        "COMMENT": "Does not match any specific criteria. Focuses on a 3D dataset with high-resolution textures, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10507": {
        "authors": [
            "Zheng Zhou",
            "Jia-Chen Zhang",
            "Yu-Jie Xiong",
            "Chun-Ming Xia"
        ],
        "title": "Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting",
        "abstract": "arXiv:2508.10507v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS).",
        "arxiv_id": "2508.10507",
        "ARXIVID": "2508.10507",
        "COMMENT": "Does not match any specific criteria. Focuses on optimization for 3D Gaussian splatting, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10898": {
        "authors": [
            "Chaoyue Song",
            "Xiu Li",
            "Fan Yang",
            "Zhongcong Xu",
            "Jiacheng Wei",
            "Fayao Liu",
            "Jiashi Feng",
            "Guosheng Lin",
            "Jianfeng Zhang"
        ],
        "title": "Puppeteer: Rig and Animate Your 3D Models",
        "abstract": "arXiv:2508.10898v1 Announce Type: new  Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.",
        "arxiv_id": "2508.10898",
        "ARXIVID": "2508.10898",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D model rigging and animation, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10599": {
        "authors": [
            "Xinyan Jiang",
            "Lin Zhang",
            "Jiayi Zhang",
            "Qingsong Yang",
            "Guimin Hu",
            "Di Wang",
            "Lijie Hu"
        ],
        "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models",
        "abstract": "arXiv:2508.10599v1 Announce Type: new  Abstract: Activation steering offers a promising approach to controlling the behavior of Large Language Models by directly manipulating their internal activations. However, most existing methods struggle to jointly steer multiple attributes, often resulting in interference and undesirable trade-offs. To address this challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel framework for effective multi-attribute steering via subspace representation fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal subspaces to each attribute, isolating their influence within the model's representation space. MSRS also incorporates a hybrid subspace composition strategy: it combines attribute-specific subspaces for unique steering directions with a shared subspace for common steering directions. A dynamic weighting function learns to efficiently integrate these components for precise control. During inference, MSRS introduces a token-level steering mechanism that dynamically identifies and intervenes on the most semantically relevant tokens, enabling fine-grained behavioral modulation. Experimental results show that MSRS significantly reduces attribute conflicts, surpasses existing methods across a range of attributes, and generalizes effectively to diverse downstream tasks.",
        "arxiv_id": "2508.10599",
        "ARXIVID": "2508.10599",
        "COMMENT": "Does not match any specific criteria. Focuses on activation steering for multi-attribute control in large language models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10293": {
        "authors": [
            "Chuhuai Yue",
            "Chengqi Dong",
            "Yinan Gao",
            "Hang He",
            "Jiajun Chai",
            "Guojun Yin",
            "Wei Lin"
        ],
        "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward",
        "abstract": "arXiv:2508.10293v1 Announce Type: new  Abstract: Large reasoning models (LRMs) have recently achieved significant progress in complex reasoning tasks, aided by reinforcement learning with verifiable rewards. However, LRMs often suffer from overthinking, expending excessive computation on simple problems and reducing efficiency. Existing efficient reasoning methods typically require accurate task assessment to preset token budgets or select reasoning modes, which limits their flexibility and reliability. In this work, we revisit the essence of overthinking and identify that encouraging effective steps while penalizing ineffective ones is key to its solution. To this end, we propose a novel rule-based verifiable stepwise reward mechanism (VSRM), which assigns rewards based on the performance of intermediate states in the reasoning trajectory. This approach is intuitive and naturally fits the step-by-step nature of reasoning tasks. We conduct extensive experiments on standard mathematical reasoning benchmarks, including AIME24 and AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our method achieves substantial output length reduction while maintaining original reasoning performance, striking an optimal balance between efficiency and accuracy. Further analysis of overthinking frequency and pass@k score before and after training demonstrates that our approach in deed effectively suppresses ineffective steps and encourages effective reasoning, fundamentally alleviating the overthinking problem. All code will be released upon acceptance.",
        "arxiv_id": "2508.10293",
        "ARXIVID": "2508.10293",
        "COMMENT": "Does not match any specific criteria. Focuses on efficient reasoning in large reasoning models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10297": {
        "authors": [
            "Yiyi Ma",
            "Yuanzhi Liang",
            "Xiu Li",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild",
        "abstract": "arXiv:2508.10297v1 Announce Type: new  Abstract: We present Interleaved Learning for Motion Synthesis (InterSyn), a novel framework that targets the generation of realistic interaction motions by learning from integrated motions that consider both solo and multi-person dynamics. Unlike previous methods that treat these components separately, InterSyn employs an interleaved learning strategy to capture the natural, dynamic interactions and nuanced coordination inherent in real-world scenarios. Our framework comprises two key modules: the Interleaved Interaction Synthesis (INS) module, which jointly models solo and interactive behaviors in a unified paradigm from a first-person perspective to support multiple character interactions, and the Relative Coordination Refinement (REC) module, which refines mutual dynamics and ensures synchronized motions among characters. Experimental results show that the motion sequences generated by InterSyn exhibit higher text-to-motion alignment and improved diversity compared with recent methods, setting a new benchmark for robust and natural motion synthesis. Additionally, our code will be open-sourced in the future to promote further research and development in this area.",
        "arxiv_id": "2508.10297",
        "ARXIVID": "2508.10297",
        "COMMENT": "Does not match any specific criteria. Focuses on motion synthesis for dynamic interactions, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10492": {
        "authors": [
            "Shicheng Xu",
            "Xin Huang",
            "Zihao Wei",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model",
        "abstract": "arXiv:2508.10492v1 Announce Type: new  Abstract: Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.",
        "arxiv_id": "2508.10492",
        "ARXIVID": "2508.10492",
        "COMMENT": "Does not match any specific criteria. Focuses on clinical diagnosis using LLMs, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10741": {
        "authors": [
            "Lixin Jia",
            "Zhiqing Guo",
            "Gaobo Yang",
            "Liejun Wang",
            "Keqin Li"
        ],
        "title": "Forgery Guided Learning Strategy with Dual Perception Network for Deepfake Cross-domain Detection",
        "abstract": "arXiv:2508.10741v1 Announce Type: new  Abstract: The emergence of deepfake technology has introduced a range of societal problems, garnering considerable attention. Current deepfake detection methods perform well on specific datasets, but exhibit poor performance when applied to datasets with unknown forgery techniques. Moreover, as the gap between emerging and traditional forgery techniques continues to widen, cross-domain detection methods that rely on common forgery traces are becoming increasingly ineffective. This situation highlights the urgency of developing deepfake detection technology with strong generalization to cope with fast iterative forgery techniques. To address these challenges, we propose a Forgery Guided Learning (FGL) strategy designed to enable detection networks to continuously adapt to unknown forgery techniques. Specifically, the FGL strategy captures the differential information between known and unknown forgery techniques, allowing the model to dynamically adjust its learning process in real time. To further improve the ability to perceive forgery traces, we design a Dual Perception Network (DPNet) that captures both differences and relationships among forgery traces. In the frequency stream, the network dynamically perceives and extracts discriminative features across various forgery techniques, establishing essential detection cues. These features are then integrated with spatial features and projected into the embedding space. In addition, graph convolution is employed to perceive relationships across the entire feature space, facilitating a more comprehensive understanding of forgery trace correlations. Extensive experiments show that our approach generalizes well across different scenarios and effectively handles unknown forgery challenges, providing robust support for deepfake detection. Our code is available on https://github.com/vpsg-research/FGL.",
        "arxiv_id": "2508.10741",
        "ARXIVID": "2508.10741",
        "COMMENT": "Does not match any specific criteria. Focuses on deepfake detection, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10383": {
        "authors": [
            "Yechan Kim",
            "Dongho Yoon",
            "Younkwan Lee",
            "Unse Fatima",
            "Hong Kook Kim",
            "Songjae Lee",
            "Sanga Park",
            "Jeong Ho Park",
            "Seonjong Kang",
            "Moongu Jeon"
        ],
        "title": "Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise",
        "abstract": "arXiv:2508.10383v1 Announce Type: new  Abstract: While previous studies on image segmentation focus on handling severe (or explicit) label noise, real-world datasets also exhibit subtle (or implicit) label imperfections. These arise from inherent challenges, such as ambiguous object boundaries and annotator variability. Although not explicitly present, such mild and latent noise can still impair model performance. Typical data augmentation methods, which apply identical transformations to the image and its label, risk amplifying these subtle imperfections and limiting the model's generalization capacity. In this paper, we introduce NSegment+, a novel augmentation framework that decouples image and label transformations to address such realistic noise for semantic segmentation. By introducing controlled elastic deformations only to segmentation labels while preserving the original images, our method encourages models to focus on learning robust representations of object structures despite minor label inconsistencies. Extensive experiments demonstrate that NSegment+ consistently improves performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even without bells and whistles, highlighting the importance of addressing implicit label noise. These gains can be further amplified when combined with other training tricks, including CutMix and Label Smoothing.",
        "arxiv_id": "2508.10383",
        "ARXIVID": "2508.10383",
        "COMMENT": "Does not match any specific criteria. Focuses on semantic segmentation and label noise, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10669": {
        "authors": [
            "Zhenye Yang",
            "Jinpeng Chen",
            "Huan Li",
            "Xiongnan Jin",
            "Xuanyang Li",
            "Junwei Zhang",
            "Hongbo Gao",
            "Kaimin Wei",
            "Senzhang Wang"
        ],
        "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation",
        "abstract": "arXiv:2508.10669v1 Announce Type: new  Abstract: Conversational recommender systems (CRSs) aim to proactively capture user preferences through natural language dialogue and recommend high-quality items. To achieve this, CRS gathers user preferences via a dialog module and builds user profiles through a recommendation module to generate appropriate recommendations. However, existing CRS faces challenges in capturing the deep semantics of user preferences and dialogue context. In particular, the efficient integration of external knowledge graph (KG) information into dialogue generation and recommendation remains a pressing issue. Traditional approaches typically combine KG information directly with dialogue content, which often struggles with complex semantic relationships, resulting in recommendations that may not align with user expectations.   To address these challenges, we introduce STEP, a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. At its heart, an F-Former progressively aligns the dialogue context with knowledge-graph entities through a three-stage curriculum, thus resolving fine-grained semantic mismatches. The fused representation is then injected into the frozen language model via two minimal yet adaptive prefix prompts: a conversation prefix that steers response generation toward user intent and a recommendation prefix that biases item ranking toward knowledge-consistent candidates. This dual-prompt scheme allows the model to share cross-task semantics while respecting the distinct objectives of dialogue and recommendation. Experimental results show that STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.",
        "arxiv_id": "2508.10669",
        "ARXIVID": "2508.10669",
        "COMMENT": "Does not match any specific criteria. Focuses on conversational recommender systems, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.10530": {
        "authors": [
            "Zetian Sun",
            "Dongfang Li",
            "Baotian Hu"
        ],
        "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment",
        "abstract": "arXiv:2508.10530v1 Announce Type: new  Abstract: The alignment of language models (LMs) with human preferences is critical for building reliable AI systems. The problem is typically framed as optimizing an LM policy to maximize the expected reward that reflects human preferences. Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment method that directly optimize the policy from static preference data, and further improved by incorporating on-policy sampling (i.e., preference candidates generated during the training loop) for better LM alignment. However, we show on-policy data is not always optimal, with systematic effectiveness difference emerging between static and on-policy preference candidates. For example, on-policy data can result in a 3$\\times$ effectiveness compared with static data for Llama-3, and a 0.4$\\times$ effectiveness for Zephyr. To explain the phenomenon, we propose the alignment stage assumption, which divides the alignment process into two distinct stages: the preference injection stage, which benefits from diverse data, and the preference fine-tuning stage, which favors high-quality data. Through theoretical and empirical analysis, we characterize these stages and propose an effective algorithm to identify the boundaries between them. We perform experiments on 5 models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO, SLiC-HF) to show the generalizability of alignment stage assumption and boundary measurement.",
        "arxiv_id": "2508.10530",
        "ARXIVID": "2508.10530",
        "COMMENT": "Does not match any specific criteria. Focuses on alignment stages in language models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}