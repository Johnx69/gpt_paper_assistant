{
    "2508.00823": {
        "authors": [
            "Wenxuan Guo",
            "Xiuwei Xu",
            "Hang Yin",
            "Ziwei Wang",
            "Jianjiang Feng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
        "abstract": "arXiv:2508.00823v1 Announce Type: new  Abstract: Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.",
        "arxiv_id": "2508.00823",
        "ARXIVID": "2508.00823",
        "COMMENT": "Matches criterion 1 and 3 as it presents a novel method for spatial reasoning in embodied agents and addresses image-goal navigation challenges.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.00378": {
        "authors": [
            "Shixin Yi",
            "Lin Shang"
        ],
        "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding",
        "abstract": "arXiv:2508.00378v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in vision-language models (VLMs), but it often produces explanations that are linguistically fluent yet lack grounding in visual content. We observe that such hallucinations arise in part from the absence of an explicit verification mechanism during multi-step reasoning. To address this, we propose \\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with \\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces visual verification into the reasoning process. CoRGI follows a three-stage pipeline: it first generates a textual reasoning chain, then extracts supporting visual evidence for each reasoning step via a dedicated module (VEVM), and finally synthesizes the textual rationale with visual evidence to generate a grounded, verified answer. The framework can be integrated with existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR benchmark and find that it improves reasoning performance on two representative open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm the contribution of each step in the verification module, and human evaluations suggest that CoRGI leads to more factual and helpful explanations. We also examine alternative designs for the visual verification step and discuss potential limitations of post-hoc verification frameworks. These findings highlight the importance of grounding intermediate reasoning steps in visual evidence to enhance the robustness of multimodal reasoning.",
        "arxiv_id": "2508.00378",
        "ARXIVID": "2508.00378",
        "COMMENT": "Matches criterion 2 as it explores reasoning in vision-language models with visual grounding, improving multimodal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.00473": {
        "authors": [
            "Jiaping Cao",
            "Kangkang Zhou",
            "Juan Du"
        ],
        "title": "HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection",
        "abstract": "arXiv:2508.00473v1 Announce Type: new  Abstract: Video anomaly detection is a fundamental task in video surveillance, with broad applications in public safety and intelligent monitoring systems. Although previous methods leverage Euclidean representations in RGB or depth domains, such embeddings are inherently limited in capturing hierarchical event structures and spatio-temporal continuity. To address these limitations, we propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for anomaly detection in 3D point cloud videos. Our approach first extracts per-frame spatial features from point cloud sequences via point cloud extractor, and then embeds them into Lorentzian hyperbolic space, which better captures the latent hierarchical structure of events. To model temporal dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism that leverages Lorentzian inner products and curvature-aware softmax to learn temporal dependencies under non-Euclidean geometry. Our method performs all feature transformations and anomaly scoring directly within full Lorentzian space rather than via tangent space approximation. Extensive experiments demonstrate that HyPCV-Former achieves state-of-the-art performance across multiple anomaly categories, with a 7\\% improvement on the TIMo dataset and a 5.6\\% gain on the DAD dataset compared to benchmarks. The code will be released upon paper acceptance.",
        "arxiv_id": "2508.00473",
        "ARXIVID": "2508.00473",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding, specifically anomaly detection in 3D point cloud videos, with a novel hyperbolic spatio-temporal transformer.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.00568": {
        "authors": [
            "Jingchao Xie",
            "Oussema Dhaouadi",
            "Weirong Chen",
            "Johannes Meier",
            "Jacques Kaiser",
            "Daniel Cremers"
        ],
        "title": "CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry",
        "abstract": "arXiv:2508.00568v1 Announce Type: new  Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and augmented reality, with unsupervised approaches eliminating the need for expensive ground-truth labels. However, these methods struggle when dynamic objects violate the static scene assumption, leading to erroneous pose estimations. We tackle this problem by uncertainty modeling, which is a commonly used technique that creates robust masks to filter out dynamic objects and occlusions without requiring explicit motion segmentation. Traditional uncertainty modeling considers only single-frame information, overlooking the uncertainties across consecutive frames. Our key insight is that uncertainty must be propagated and combined across temporal frames to effectively identify unreliable regions, particularly in dynamic scenes. To address this challenge, we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end approach that combines target frame uncertainty with projected reference frame uncertainty using a principled probabilistic formulation. Built upon vision transformer backbones, our model simultaneously learns depth, uncertainty estimation, and camera poses. Consequently, experiments on the KITTI and nuScenes datasets demonstrate significant improvements over previous unsupervised monocular end-to-end two-frame-based methods and exhibit strong performance in challenging highway scenes where other approaches often fail. Additionally, comprehensive ablation studies validate the effectiveness of cross-frame uncertainty propagation.",
        "arxiv_id": "2508.00568",
        "ARXIVID": "2508.00568",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (CoProU-VO) for embodied AI, specifically in unsupervised monocular visual odometry, addressing dynamic scene challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.00171": {
        "authors": [
            "David Restrepo",
            "Ira Ktena",
            "Maria Vakalopoulou",
            "Stergios Christodoulidis",
            "Enzo Ferrante"
        ],
        "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI",
        "abstract": "arXiv:2508.00171v1 Announce Type: new  Abstract: Clinical decision-making relies on the integrated analysis of medical images and the associated clinical reports. While Vision-Language Models (VLMs) can offer a unified framework for such tasks, they can exhibit strong biases toward one modality, frequently overlooking critical visual cues in favor of textual information. In this work, we introduce Selective Modality Shifting (SMS), a perturbation-based approach to quantify a model's reliance on each modality in binary classification tasks. By systematically swapping images or text between samples with opposing labels, we expose modality-specific biases. We assess six open-source VLMs-four generalist models and two fine-tuned for medical data-on two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray) and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance and the calibration of every model in both unperturbed and perturbed settings, we reveal a marked dependency on text input, which persists despite the presence of complementary visual information. We also perform a qualitative attention-based analysis which further confirms that image content is often overshadowed by text details. Our findings highlight the importance of designing and evaluating multimodal medical models that genuinely integrate visual and textual cues, rather than relying on single-modality signals.",
        "arxiv_id": "2508.00171",
        "ARXIVID": "2508.00171",
        "COMMENT": "Matches criterion 5 as it investigates biases in vision-language models and their integration of image and text modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.00395": {
        "authors": [
            "Fei Zhang",
            "Tianfei Zhou",
            "Jiangchao Yao",
            "Ya Zhang",
            "Ivor W. Tsang",
            "Yanfeng Wang"
        ],
        "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning",
        "abstract": "arXiv:2508.00395v1 Announce Type: new  Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks. Our code will be released at https://github.com/Ferenas/DAPT.",
        "arxiv_id": "2508.00395",
        "ARXIVID": "2508.00395",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a novel prompt tuning framework for vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.00213": {
        "authors": [
            "Shayan Jalilian",
            "Abdul Bais"
        ],
        "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters",
        "abstract": "arXiv:2508.00213v1 Announce Type: new  Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization in prompt-based segmentation. Yet, the potential of semantic text prompts remains underexplored compared to traditional spatial prompts like points and boxes. This paper introduces SAM-PTx, a parameter-efficient approach for adapting SAM using frozen CLIP-derived text embeddings as class-level semantic guidance. Specifically, we propose a lightweight adapter design called Parallel-Text that injects text embeddings into SAM's image encoder, enabling semantics-guided segmentation while keeping most of the original architecture frozen. Our adapter modifies only the MLP-parallel branch of each transformer block, preserving the attention pathway for spatial reasoning. Through supervised experiments and ablations on the COD10K dataset as well as low-data subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as input improves segmentation performance over purely spatial prompt baselines. To our knowledge, this is the first work to use text prompts for segmentation on the COD10K dataset. These results suggest that integrating semantic conditioning into SAM's architecture offers a practical and scalable path for efficient adaptation with minimal computational complexity.",
        "arxiv_id": "2508.00213",
        "ARXIVID": "2508.00213",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it explores text-guided fine-tuning of SAM using semantic text prompts.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.00265": {
        "authors": [
            "Henghui Ding",
            "Song Tang",
            "Shuting He",
            "Chang Liu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "title": "Multimodal Referring Segmentation: A Survey",
        "abstract": "arXiv:2508.00265v1 Announce Type: new  Abstract: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
        "arxiv_id": "2508.00265",
        "ARXIVID": "2508.00265",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive survey on multimodal referring segmentation.",
        "RELEVANCE": 9,
        "NOVELTY": 5
    },
    "2508.00557": {
        "authors": [
            "Qi Chen",
            "Lingxiao Yang",
            "Yun Chen",
            "Nailong Zhao",
            "Jianhuang Lai",
            "Jie Shao",
            "Xiaohua Xie"
        ],
        "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation",
        "abstract": "arXiv:2508.00557v1 Announce Type: new  Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful approach for enhancing open-vocabulary semantic segmentation (OVSS). However, the substantial computational and resource demands associated with training on large datasets have prompted interest in training-free methods for OVSS. Existing training-free approaches primarily focus on modifying model architectures and generating prototypes to improve segmentation performance. However, they often neglect the challenges posed by class redundancy, where multiple categories are not present in the current test image, and visual-language ambiguity, where semantic similarities among categories create confusion in class activation. These issues can lead to suboptimal class activation maps and affinity-refined activation maps. Motivated by these observations, we propose FreeCP, a novel training-free class purification framework designed to address these challenges. FreeCP focuses on purifying semantic categories and rectifying errors caused by redundancy and ambiguity. The purified class representations are then leveraged to produce final segmentation predictions. We conduct extensive experiments across eight benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP, as a plug-and-play module, significantly boosts segmentation performance when combined with other OVSS methods.",
        "arxiv_id": "2508.00557",
        "ARXIVID": "2508.00557",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on open-vocabulary semantic segmentation using vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.00599": {
        "authors": [
            "Junzhe Lu",
            "Jing Lin",
            "Hongkun Dou",
            "Ailing Zeng",
            "Yue Deng",
            "Xian Liu",
            "Zhongang Cai",
            "Lei Yang",
            "Yulun Zhang",
            "Haoqian Wang",
            "Ziwei Liu"
        ],
        "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior",
        "abstract": "arXiv:2508.00599v1 Announce Type: new  Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.",
        "arxiv_id": "2508.00599",
        "ARXIVID": "2508.00599",
        "COMMENT": "Matches criterion 4 as it focuses on a diffusion-based foundation model for 3D whole-body human pose modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.00746": {
        "authors": [
            "Regine Hartwig",
            "Dominik Muhle",
            "Riccardo Marin",
            "Daniel Cremers"
        ],
        "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference",
        "abstract": "arXiv:2508.00746v1 Announce Type: new  Abstract: Recent advances in feature learning have shown that self-supervised vision foundation models can capture semantic correspondences but often lack awareness of underlying 3D geometry. GECO addresses this gap by producing geometrically coherent features that semantically distinguish parts based on geometry (e.g., left/right eyes, front/back legs). We propose a training framework based on optimal transport, enabling supervision beyond keypoints, even under occlusions and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2% faster than prior methods, while achieving state-of-the-art performance on PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively. Finally, we show that PCK alone is insufficient to capture geometric quality and introduce new metrics and insights for more geometry-aware feature learning. Link to project page: https://reginehartwig.github.io/publications/geco/",
        "arxiv_id": "2508.00746",
        "ARXIVID": "2508.00746",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on geometrically consistent embeddings and their applications in vision tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.00592": {
        "authors": [
            "Jiajun Le",
            "Jiayi Ma"
        ],
        "title": "GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry",
        "abstract": "arXiv:2508.00592v1 Announce Type: new  Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing smoothness and global consistency priors when estimating motion fields between pairs of images. However, in complex real-world scenes, characterized by extreme viewpoint and scale changes as well as pronounced depth discontinuities, the motion field often exhibits diverse and heterogeneous motion patterns. Most existing methods lack targeted modeling strategies and fail to explicitly account for this variability, resulting in estimated motion fields that diverge from their true underlying structure and distribution. We observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion patterns. Building on this insight, we re-architect motion field modeling in two-view geometry with GeoMoE, a streamlined framework. Specifically, we first devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier probability signals to perform a structure-aware decomposition of the motion field into heterogeneous sub-fields, sharply curbing outlier-induced bias. Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each sub-field along spatial-context and channel-semantic paths and routes it to a customized expert for targeted modeling, thereby decoupling heterogeneous motion regimes, suppressing cross-sub-field interference and representational entanglement, and yielding fine-grained motion-field rectification. With this minimalist design, GeoMoE outperforms prior state-of-the-art methods in relative pose and homography estimation and shows strong generalization. The source code and pre-trained models are available at https://github.com/JiajunLe/GeoMoE.",
        "arxiv_id": "2508.00592",
        "ARXIVID": "2508.00592",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a novel method for motion field modeling in two-view geometry, which involves spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.00260": {
        "authors": [
            "Hyundong Jin",
            "Hyung Jin Chang",
            "Eunwoo Kim"
        ],
        "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models",
        "abstract": "arXiv:2508.00260v1 Announce Type: new  Abstract: Continual learning enables pre-trained generative vision-language models (VLMs) to incorporate knowledge from new tasks without retraining data from previous ones. Recent methods update a visual projector to translate visual information for new tasks, connecting pre-trained vision encoders with large language models. However, such adjustments may cause the models to prioritize visual inputs over language instructions, particularly learning tasks with repetitive types of textual instructions. To address the neglect of language instructions, we propose a novel framework that grounds the translation of visual information on instructions for language models. We introduce a mixture of visual projectors, each serving as a specialized visual-to-language translation expert based on the given instruction context to adapt to new tasks. To avoid using experts for irrelevant instruction contexts, we propose an expert recommendation strategy that reuses experts for tasks similar to those previously learned. Additionally, we introduce expert pruning to alleviate interference from the use of experts that cumulatively activated in previous tasks. Extensive experiments on diverse vision-language tasks demonstrate that our method outperforms existing continual learning approaches by generating instruction-following responses.",
        "arxiv_id": "2508.00260",
        "ARXIVID": "2508.00260",
        "COMMENT": "Matches criterion 5 as it focuses on integrating vision tasks with large language models using instruction-grounded visual projectors.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.00152": {
        "authors": [
            "Li Mi",
            "Manon Bechaz",
            "Zeming Chen",
            "Antoine Bosselut",
            "Devis Tuia"
        ],
        "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration",
        "abstract": "arXiv:2508.00152v1 Announce Type: new  Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented in various modalities (e.g., aerial images, ground-level images, or text), within a predefined search area. Current methods approach AGL as a goal-reaching reinforcement learning (RL) problem with a distance-based reward. They localize the goal by implicitly learning to minimize the relative distance from it. However, when distance estimation becomes challenging or when encountering unseen targets and environments, the agent exhibits reduced robustness and generalization ability due to the less reliable exploration strategy learned during training. In this paper, we propose GeoExplorer, an AGL agent that incorporates curiosity-driven exploration through intrinsic rewards. Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic, enabling robust, diverse, and contextually relevant exploration based on effective environment modeling. These capabilities have been proven through extensive experiments across four AGL benchmarks, demonstrating the effectiveness and generalization ability of GeoExplorer in diverse settings, particularly in localizing unfamiliar targets and environments.",
        "arxiv_id": "2508.00152",
        "ARXIVID": "2508.00152",
        "COMMENT": "Matches criterion 1 as it involves spatial intelligence and exploration strategies for active geo-localization.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.00471": {
        "authors": [
            "Yiwen Wang",
            "Xinning Chai",
            "Yuhong Zhang",
            "Zhengxue Cheng",
            "Jun Zhao",
            "Rong Xie",
            "Li Song"
        ],
        "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution",
        "abstract": "arXiv:2508.00471v1 Announce Type: new  Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated impressive results in enhancing low-resolution videos. However, due to limitations in adequately controlling the generation process, achieving high fidelity alignment with the low-resolution input while maintaining temporal consistency across frames remains a significant challenge. In this work, we propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel approach that incorporates both semantic and temporal-spatio guidance in the latent diffusion space to address these challenges. By incorporating high-level semantic information and integrating spatial and temporal information, our approach achieves a seamless balance between recovering intricate details and ensuring temporal coherence. Our method not only preserves high-reality visual content but also significantly enhances fidelity. Extensive experiments demonstrate that SeTe-VSR outperforms existing methods in terms of detail recovery and perceptual quality, highlighting its effectiveness for complex video super-resolution tasks.",
        "arxiv_id": "2508.00471",
        "ARXIVID": "2508.00471",
        "COMMENT": "Matches criterion 6 as it focuses on video super-resolution with semantic and temporal integration.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.00359": {
        "authors": [
            "Zongheng Tang",
            "Yi Liu",
            "Yifan Sun",
            "Yulu Gao",
            "Jinyu Chen",
            "Runsheng Xu",
            "Si Liu"
        ],
        "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective",
        "abstract": "arXiv:2508.00359v1 Announce Type: new  Abstract: Collaborative perception shares information among different agents and helps solving problems that individual agents may face, e.g., occlusions and small sensing range. Prior methods usually separate the multi-agent fusion and multi-time fusion into two consecutive steps. In contrast, this paper proposes an efficient collaborative perception that aggregates the observations from different agents (space) and different times into a unified spatio-temporal space simultanesouly. The unified spatio-temporal space brings two benefits, i.e., efficient feature transmission and superior feature fusion. 1) Efficient feature transmission: each static object yields a single observation in the spatial temporal space, and thus only requires transmission only once (whereas prior methods re-transmit all the object features multiple times). 2) superior feature fusion: merging the multi-agent and multi-time fusion into a unified spatial-temporal aggregation enables a more holistic perspective, thereby enhancing perception performance in challenging scenarios. Consequently, our Collaborative perception with Spatio-temporal Transformer (CoST) gains improvement in both efficiency and accuracy. Notably, CoST is not tied to any specific method and is compatible with a majority of previous methods, enhancing their accuracy while reducing the transmission bandwidth.",
        "arxiv_id": "2508.00359",
        "ARXIVID": "2508.00359",
        "COMMENT": "Matches criterion 3 as it introduces a novel spatio-temporal fusion method for collaborative perception in multi-agent systems.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.00477": {
        "authors": [
            "Yuzhuo Chen",
            "Zehua Ma",
            "Jianhua Wang",
            "Kai Kang",
            "Shunyu Yao",
            "Weiming Zhang"
        ],
        "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer",
        "abstract": "arXiv:2508.00477v1 Announce Type: new  Abstract: In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.",
        "arxiv_id": "2508.00477",
        "ARXIVID": "2508.00477",
        "COMMENT": "Matches criterion 5 as it introduces a layout-aware multi-image composition framework combining image understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.00356": {
        "authors": [
            "Angelos Vlachos",
            "Giorgos Filandrianos",
            "Maria Lymperaiou",
            "Nikolaos Spanos",
            "Ilias Mitsouras",
            "Vasileios Karampinis",
            "Athanasios Voulodimos"
        ],
        "title": "Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning",
        "abstract": "arXiv:2508.00356v1 Announce Type: new  Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning. Our approach tackles the challenge of interleaved multimodal reasoning across diverse datasets and task formats by employing a dual-agent system: a language-based PromptEngineer, which generates context-aware, task-specific prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible for final inference. The framework is fully automated, modular, and training-free, enabling generalization across classification, question answering, and free-form generation tasks involving one or multiple input images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE Challenge (Track A), covering a broad spectrum of visual reasoning tasks including document QA, visual comparison, dialogue-based understanding, and scene-level inference. Our results demonstrate that LVLMs can effectively reason over multiple images when guided by informative prompts. Notably, Claude 3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13% accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how design choices-such as model selection, shot count, and input length-influence the reasoning performance of different LVLMs.",
        "arxiv_id": "2508.00356",
        "ARXIVID": "2508.00356",
        "COMMENT": "Matches criterion 2 as it explores a novel framework for multi-image vision-language reasoning using large vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.00391": {
        "authors": [
            "Guanjie Huang",
            "Danny H. K. Tsang",
            "Shan Yang",
            "Guangzhi Lei",
            "Li Liu"
        ],
        "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition",
        "abstract": "arXiv:2508.00391v1 Announce Type: new  Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading with hand coding to facilitate communication for individuals with hearing impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures and lip movements into text via AI-driven methods. Traditionally, the temporal asynchrony between hand and lip movements requires the design of complex modules to facilitate effective multimodal fusion. However, constrained by limited data availability, current methods demonstrate insufficient capacity for adequately training these fusion mechanisms, resulting in suboptimal performance. Recently, multi-agent systems have shown promising capabilities in handling complex tasks with limited data availability. To this end, we propose the first collaborative multi-agent system for ACSR, named Cued-Agent. It integrates four specialized sub-agents: a Multimodal Large Language Model-based Hand Recognition agent that employs keyframe screening and CS expert prompt strategies to decode hand movements, a pretrained Transformer-based Lip Recognition agent that extracts lip features from the input video, a Hand Prompt Decoding agent that dynamically integrates hand prompts with lip features during inference in a training-free manner, and a Self-Correction Phoneme-to-Word agent that enables post-process and end-to-end conversion from phoneme sequences to natural language sentences for the first time through semantic refinement. To support this study, we expand the existing Mandarin CS dataset by collecting data from eight hearing-impaired cuers, establishing a mixed dataset of fourteen subjects. Extensive experiments demonstrate that our Cued-Agent performs superbly in both normal and hearing-impaired scenarios compared with state-of-the-art methods. The implementation is available at https://github.com/DennisHgj/Cued-Agent.",
        "arxiv_id": "2508.00391",
        "ARXIVID": "2508.00391",
        "COMMENT": "Matches criterion 2 as it involves a multimodal large language model for hand and lip recognition in Cued Speech.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00726": {
        "authors": [
            "Jiale Li",
            "Mingrui Wu",
            "Zixiang Jin",
            "Hao Chen",
            "Jiayi Ji",
            "Xiaoshuai Sun",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models",
        "abstract": "arXiv:2508.00726v1 Announce Type: new  Abstract: Despite growing interest in hallucination in Multimodal Large Language Models, existing studies primarily focus on single-image settings, leaving hallucination in multi-image scenarios largely unexplored. To address this gap, we conduct the first systematic study of hallucinations in multi-image MLLMs and propose MIHBench, a benchmark specifically tailored for evaluating object-related hallucinations across multiple images. MIHBench comprises three core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object Count Hallucination, and Object Identity Consistency Hallucination, targeting semantic understanding across object existence, quantity reasoning, and cross-view identity consistency. Through extensive evaluation, we identify key factors associated with the occurrence of multi-image hallucinations, including: a progressive relationship between the number of image inputs and the likelihood of hallucination occurrences; a strong correlation between single-image hallucination tendencies and those observed in multi-image contexts; and the influence of same-object image ratios and the positional placement of negative samples within image sequences on the occurrence of object identity consistency hallucination. To address these challenges, we propose a Dynamic Attention Balancing mechanism that adjusts inter-image attention distributions while preserving the overall visual attention proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that our method effectively reduces hallucination occurrences and enhances semantic integration and reasoning stability in multi-image scenarios.",
        "arxiv_id": "2508.00726",
        "ARXIVID": "2508.00726",
        "COMMENT": "Matches criterion 2 as it benchmarks and mitigates hallucinations in multimodal large language models, specifically in multi-image scenarios.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00728": {
        "authors": [
            "Guanning Zeng",
            "Xiang Zhang",
            "Zirui Wang",
            "Haiyang Xu",
            "Zeyuan Chen",
            "Bingnan Li",
            "Zhuowen Tu"
        ],
        "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation",
        "abstract": "arXiv:2508.00728v1 Announce Type: new  Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model that tackles both general counting challenges and enables precise quantity control for text-to-image (T2I) generation. A core contribution is the 'cardinality' map, a novel regression target that accounts for variations in object size and spatial distribution. Leveraging representation alignment and a hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between open-vocabulary counting and T2I generation control. Its fully differentiable architecture facilitates gradient-based optimization, enabling accurate object count estimation and fine-grained guidance for generative models. Extensive experiments demonstrate that YOLO-Count achieves state-of-the-art counting accuracy while providing robust and effective quantity control for T2I systems.",
        "arxiv_id": "2508.00728",
        "ARXIVID": "2508.00728",
        "COMMENT": "Matches criterion 5 as it introduces a differentiable object counting model for text-to-image generation, combining image understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00399": {
        "authors": [
            "Raiyaan Abdullah",
            "Yogesh Singh Rawat",
            "Shruti Vyas"
        ],
        "title": "iSafetyBench: A video-language benchmark for safety in industrial environment",
        "abstract": "arXiv:2508.00399v1 Announce Type: new  Abstract: Recent advances in vision-language models (VLMs) have enabled impressive generalization across diverse video understanding tasks under zero-shot settings. However, their capabilities in high-stakes industrial domains-where recognizing both routine operations and safety-critical anomalies is essential-remain largely underexplored. To address this gap, we introduce iSafetyBench, a new video-language benchmark specifically designed to evaluate model performance in industrial environments across both normal and hazardous scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world industrial settings, annotated with open-vocabulary, multi-label action tags spanning 98 routine and 67 hazardous action categories. Each clip is paired with multiple-choice questions for both single-label and multi-label evaluation, enabling fine-grained assessment of VLMs in both standard and safety-critical contexts. We evaluate eight state-of-the-art video-language models under zero-shot conditions. Despite their strong performance on existing video benchmarks, these models struggle with iSafetyBench-particularly in recognizing hazardous activities and in multi-label scenarios. Our results reveal significant performance gaps, underscoring the need for more robust, safety-aware multimodal models for industrial applications. iSafetyBench provides a first-of-its-kind testbed to drive progress in this direction. The dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.",
        "arxiv_id": "2508.00399",
        "ARXIVID": "2508.00399",
        "COMMENT": "Matches criterion 6 as it introduces a new video-language benchmark for safety in industrial environments.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00397": {
        "authors": [
            "Xi Xue",
            "Kunio Suzuki",
            "Nabarun Goswami",
            "Takuya Shintate"
        ],
        "title": "Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency",
        "abstract": "arXiv:2508.00397v1 Announce Type: new  Abstract: The rapid advancement of diffusion-based video generation models has led to increasingly realistic synthetic content, presenting new challenges for video forgery detection. Existing methods often struggle to capture fine-grained temporal inconsistencies, particularly in AI-generated videos with high visual fidelity and coherent motion. In this work, we propose a detection framework that leverages spatial-temporal consistency by combining RGB appearance features with optical flow residuals. The model adopts a dual-branch architecture, where one branch analyzes RGB frames to detect appearance-level artifacts, while the other processes flow residuals to reveal subtle motion anomalies caused by imperfect temporal synthesis. By integrating these complementary features, the proposed method effectively detects a wide range of forged videos. Extensive experiments on text-to-video and image-to-video tasks across ten diverse generative models demonstrate the robustness and strong generalization ability of the proposed approach.",
        "arxiv_id": "2508.00397",
        "ARXIVID": "2508.00397",
        "COMMENT": "Matches criterion 6 as it focuses on video forgery detection using spatial-temporal consistency and optical flow residuals.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00406": {
        "authors": [
            "Tao Wu",
            "Jingyuan Ye",
            "Ying Fu"
        ],
        "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos",
        "abstract": "arXiv:2508.00406v1 Announce Type: new  Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade the quality of long-range dynamic scene videos. Existing methods struggle with restoring edge details and eliminating mixed distortions, especially under conditions of strong turbulence and complex dynamics. To address these challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines turbulence intensity, optical flow, and proportions of dynamic regions to accurately quantify video dynamic intensity under varying turbulence conditions and provide a high-dynamic turbulence training dataset. Additionally, we propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework that consists of three stages: \\textbf{de-tilting} for geometric stabilization, \\textbf{motion segmentation enhancement} for dynamic region refinement, and \\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight backbones and stage-wise joint training to ensure both efficiency and high restoration quality. Experimental results demonstrate that the proposed method effectively suppresses motion trailing artifacts, restores edge details and exhibits strong generalization capability, especially in real-world scenarios characterized by high-turbulence and complex dynamics. We will make the code and datasets openly available.",
        "arxiv_id": "2508.00406",
        "ARXIVID": "2508.00406",
        "COMMENT": "Matches criterion 6 as it proposes a novel multi-stage framework for video restoration under turbulent conditions.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00088": {
        "authors": [
            "Mateo de Mayo",
            "Daniel Cremers",
            "Taih\\'u Pire"
        ],
        "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking",
        "abstract": "arXiv:2508.00088v1 Announce Type: new  Abstract: Humanoid robots and mixed reality headsets benefit from the use of head-mounted sensors for tracking. While advancements in visual-inertial odometry (VIO) and simultaneous localization and mapping (SLAM) have produced new and high-quality state-of-the-art tracking systems, we show that these are still unable to gracefully handle many of the challenging settings presented in the head-mounted use cases. Common scenarios like high-intensity motions, dynamic occlusions, long tracking sessions, low-textured areas, adverse lighting conditions, saturation of sensors, to name a few, continue to be covered poorly by existing datasets in the literature. In this way, systems may inadvertently overlook these essential real-world issues. To address this, we present the Monado SLAM dataset, a set of real sequences taken from multiple virtual reality headsets. We release the dataset under a permissive CC BY 4.0 license, to drive advancements in VIO/SLAM research and development.",
        "arxiv_id": "2508.00088",
        "ARXIVID": "2508.00088",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for visual-inertial tracking in challenging scenarios.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00500": {
        "authors": [
            "Haoyu Wang",
            "Chris M. Poskitt",
            "Jun Sun",
            "Jiali Wei"
        ],
        "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking",
        "abstract": "arXiv:2508.00500v1 Announce Type: new  Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities across domains such as robotics, virtual assistants, and web automation. However, their stochastic behavior introduces significant safety risks that are difficult to anticipate. Existing rule-based enforcement systems, such as AgentSpec, focus on developing reactive safety rules, which typically respond only when unsafe behavior is imminent or has already occurred. These systems lack foresight and struggle with long-horizon dependencies and distribution shifts. To address these limitations, we propose Pro2Guard, a proactive runtime enforcement framework grounded in probabilistic reachability analysis. Pro2Guard abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it anticipates future risks by estimating the probability of reaching unsafe states, triggering interventions before violations occur when the predicted risk exceeds a user-defined threshold. By incorporating semantic validity checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability while approximating the underlying ground-truth model. We evaluate Pro2Guard extensively across two safety-critical domains: embodied household agents and autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early on up to 93.6% of unsafe tasks using low thresholds, while configurable modes (e.g., reflect) allow balancing safety with task success, maintaining up to 80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100% prediction of traffic law violations and collisions, anticipating risks up to 38.66 seconds ahead.",
        "arxiv_id": "2508.00500",
        "ARXIVID": "2508.00500",
        "COMMENT": "Matches criterion 3 as it introduces a proactive runtime enforcement framework for embodied agents, addressing safety challenges.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00259": {
        "authors": [
            "Wentao Sun",
            "Hanqing Xu",
            "Quanyun Wu",
            "Dedong Zhang",
            "Yiping Chen",
            "Lingfei Ma",
            "John S. Zelek",
            "Jonathan Li"
        ],
        "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting",
        "abstract": "arXiv:2508.00259v1 Announce Type: new  Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations. Unlike existing methods that suffer from prolonged initialization and limited multi-view consistency, our approach achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline. The key innovation lies in two aspects: (1) a point cloud-based Gaussian primitive decoder that generates 3D instance masks within 1 minute, and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view consistency. Extensive experiments demonstrate significant improvements over previous state-of-the-art methods, achieving performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency. To address the limitations of current benchmarks (single-object focus, inconsistent 3D evaluation, small scale, and partial coverage), we present DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in radiance fields, featuring: (1) complex multi-object scenes, (2) globally consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D masks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.",
        "arxiv_id": "2508.00259",
        "ARXIVID": "2508.00259",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (DesktopObjects-360) for 3D segmentation in radiance fields.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.00271": {
        "authors": [
            "Hongjin Qian",
            "Zheng Liu"
        ],
        "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning",
        "abstract": "arXiv:2508.00271v1 Announce Type: new  Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \\textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.",
        "arxiv_id": "2508.00271",
        "ARXIVID": "2508.00271",
        "COMMENT": "Does not directly match any specific criterion but is relevant to general AI and agentic systems, which may align with your friend's broader interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.00169": {
        "authors": [
            "Bhavya Goyal",
            "Felipe Gutierrez-Barragan",
            "Wei Lin",
            "Andreas Velten",
            "Yin Li",
            "Mohit Gupta"
        ],
        "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs",
        "abstract": "arXiv:2508.00169v1 Announce Type: new  Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation used in various scene understanding tasks. Modern LiDARs face key challenges in several real-world scenarios, such as long-distance or low-albedo objects, producing sparse or erroneous point clouds. These errors, which are rooted in the noisy raw LiDAR measurements, get propagated to downstream perception models, resulting in potentially severe loss of accuracy. This is because conventional 3D processing pipelines do not retain any uncertainty information from the raw measurements when constructing point clouds.   We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation where each point is augmented with a probability attribute that encapsulates the measurement uncertainty (or confidence) in the raw data. We further introduce inference approaches that leverage PPC for robust 3D object detection; these methods are versatile and can be used as computationally lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both simulations and real captures, that PPC-based 3D inference methods outperform several baselines using LiDAR as well as camera-LiDAR fusion models, across challenging indoor and outdoor scenarios involving small, distant, and low-albedo objects, as well as strong ambient light.   Our project webpage is at https://bhavyagoyal.github.io/ppc .",
        "arxiv_id": "2508.00169",
        "ARXIVID": "2508.00169",
        "COMMENT": "Does not match any specific criteria. Focuses on probabilistic point clouds for robust 3D object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.00443": {
        "authors": [
            "Longfei Huang",
            "Yu Liang",
            "Hao Zhang",
            "Jinwei Chen",
            "Wei Dong",
            "Lunde Chen",
            "Wanyu Liu",
            "Bo Li",
            "Pengtao Jiang"
        ],
        "title": "SDMatte: Grafting Diffusion Models for Interactive Matting",
        "abstract": "arXiv:2508.00443v1 Announce Type: new  Abstract: Recent interactive matting methods have shown satisfactory performance in capturing the primary regions of objects, but they fall short in extracting fine-grained details in edge regions. Diffusion models trained on billions of image-text pairs, demonstrate exceptional capability in modeling highly complex data distributions and synthesizing realistic texture details, while exhibiting robust text-driven interaction capabilities, making them an attractive solution for interactive matting. To this end, we propose SDMatte, a diffusion-driven interactive matting model, with three key contributions. First, we exploit the powerful priors of diffusion models and transform the text-driven interaction capability into visual prompt-driven interaction capability to enable interactive matting. Second, we integrate coordinate embeddings of visual prompts and opacity embeddings of target objects into U-Net, enhancing SDMatte's sensitivity to spatial position information and opacity information. Third, we propose a masked self-attention mechanism that enables the model to focus on areas specified by visual prompts, leading to better performance. Extensive experiments on multiple datasets demonstrate the superior performance of our method, validating its effectiveness in interactive matting. Our code and model are available at https://github.com/vivoCameraResearch/SDMatte.",
        "arxiv_id": "2508.00443",
        "ARXIVID": "2508.00443",
        "COMMENT": "Does not match any specific criteria. Focuses on interactive matting using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.00222": {
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Yongding Tao",
            "Huanyu Liu",
            "Kechi Zhang",
            "Lili Mou",
            "Rongyu Cao",
            "Yingwei Ma",
            "Jue Chen",
            "Binhua Li",
            "Zhi Jin",
            "Fei Huang",
            "Yongbin Li",
            "Ge Li"
        ],
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization",
        "abstract": "arXiv:2508.00222v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its inherently on-policy strategy with LLM's immense action space and sparse reward. Further, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel approach that synergizes internal exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components: Multiple Importance Sampling to address for distributional mismatch from external data, and an Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. The results show that RL-PLUS achieves state-of-the-art performance compared with existing RLVR methods on six math reasoning benchmarks and exhibits superior performance on six out-of-distribution reasoning tasks. It also achieves consistent and significant gains across diverse model families, with average relative improvements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across multiple benchmarks indicate that RL-PLUS effectively resolves the capability boundary collapse problem.",
        "arxiv_id": "2508.00222",
        "ARXIVID": "2508.00222",
        "COMMENT": "Does not match any specific criteria. Focuses on reinforcement learning with large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.00205": {
        "authors": [
            "Xiangyu Kong",
            "Hengde Zhu",
            "Haoqin Sun",
            "Zhihao Guo",
            "Jiayan Gu",
            "Xinyi Ni",
            "Wei Zhang",
            "Shizhe Liu",
            "Siyang Song"
        ],
        "title": "Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition",
        "abstract": "arXiv:2508.00205v1 Announce Type: new  Abstract: Automatic real personality recognition (RPR) aims to evaluate human real personality traits from their expressive behaviours. However, most existing solutions generally act as external observers to infer observers' personality impressions based on target individuals' expressive behaviours, which significantly deviate from their real personalities and consistently lead to inferior recognition performance. Inspired by the association between real personality and human internal cognition underlying the generation of expressive behaviours, we propose a novel RPR approach that efficiently simulates personalised internal cognition from easy-accessible external short audio-visual behaviours expressed by the target individual. The simulated personalised cognition, represented as a set of network weights that enforce the personalised network to reproduce the individual-specific facial reactions, is further encoded as a novel graph containing two-dimensional node and edge feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for inferring real personality traits from it. To simulate real personality-related cognition, an end-to-end strategy is designed to jointly train our cognition simulation, 2D graph construction, and personality recognition modules.",
        "arxiv_id": "2508.00205",
        "ARXIVID": "2508.00205",
        "COMMENT": "Does not match any specific criteria. Focuses on real personality recognition using audio-visual behaviors.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.00748": {
        "authors": [
            "Laura Pedrouzo-Rodriguez",
            "Pedro Delgado-DeRobles",
            "Luis F. Gomez",
            "Ruben Tolosana",
            "Ruben Vera-Rodriguez",
            "Aythami Morales",
            "Julian Fierrez"
        ],
        "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos",
        "abstract": "arXiv:2508.00748v1 Announce Type: new  Abstract: Photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. These avatars allow for more immersive communication, but they also introduce serious security risks. One emerging threat is impersonation: an attacker can steal a user's avatar-preserving their appearance and voice-making it nearly impossible to detect its fraudulent usage by sight or sound alone. In this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. Our main question is whether an individual's facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatar's visual appearance is a facsimile of its owner. To answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and impostor avatar videos. We also propose a lightweight, explainable spatio-temporal Graph Convolutional Network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. Experimental results demonstrate that facial motion cues enable meaningful identity verification with AUC values approaching 80%. The proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.",
        "arxiv_id": "2508.00748",
        "ARXIVID": "2508.00748",
        "COMMENT": "Does not directly match any specific criterion but is tangentially related to vision and biometric verification, which may align with your friend's general interest in vision-related tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00366": {
        "authors": [
            "Liang Han",
            "Xu Zhang",
            "Haichuan Song",
            "Kanle Shi",
            "Yu-Shen Liu",
            "Zhizhong Han"
        ],
        "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies",
        "abstract": "arXiv:2508.00366v1 Announce Type: new  Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or scene from few RGB images. The latest methods are either generalization-based or overfitting-based. However, the generalization-based methods do not generalize well on views that were unseen during training, while the reconstruction quality of overfitting-based methods is still limited by the limited geometry clues. To address this issue, we propose SparseRecon, a novel neural implicit reconstruction method for sparse views with volume rendering-based feature consistency and uncertainty-guided depth constraint. Firstly, we introduce a feature consistency loss across views to constrain the neural implicit field. This design alleviates the ambiguity caused by insufficient consistency information of views and ensures completeness and smoothness in the reconstruction results. Secondly, we employ an uncertainty-guided depth constraint to back up the feature consistency loss in areas with occlusion and insignificant features, which recovers geometry details for better reconstruction quality. Experimental results demonstrate that our method outperforms the state-of-the-art methods, which can produce high-quality geometry with sparse-view input, especially in the scenarios with small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.",
        "arxiv_id": "2508.00366",
        "ARXIVID": "2508.00366",
        "COMMENT": "Does not directly match any specific criterion but is relevant to computer vision and 3D reconstruction, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00766": {
        "authors": [
            "Irene Iele",
            "Francesco Di Feola",
            "Valerio Guarrasi",
            "Paolo Soda"
        ],
        "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation",
        "abstract": "arXiv:2508.00766v1 Announce Type: new  Abstract: Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: https://github.com/cosbidev/Sample-Aware_TTA.",
        "arxiv_id": "2508.00766",
        "ARXIVID": "2508.00766",
        "COMMENT": "Does not match any specific criteria. Focuses on medical image-to-image translation with test-time adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00412": {
        "authors": [
            "Hanqi Chen",
            "Xu Zhang",
            "Xiaoliu Guan",
            "Lielin Jiang",
            "Guanzhong Wang",
            "Zeyu Chen",
            "Yi Liu"
        ],
        "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
        "abstract": "arXiv:2508.00412v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.",
        "arxiv_id": "2508.00412",
        "ARXIVID": "2508.00412",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00311": {
        "authors": [
            "Yufeng Zhong",
            "Zhixiong Zeng",
            "Lei Chen",
            "Longrong Yang",
            "Liming Zheng",
            "Jing Huang",
            "Siqi Yang",
            "Lin Ma"
        ],
        "title": "DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios",
        "abstract": "arXiv:2508.00311v1 Announce Type: new  Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for the intelligent analysis of scientific literature. However, both task-specific and general vision-language models often struggle to handle the structural diversity, complexity, and real-world variability inherent in mathematical content. In this work, we present DocTron-Formula, a unified framework built upon general vision-language models, thereby eliminating the need for specialized architectures. Furthermore, we introduce CSFormula, a large-scale and challenging dataset that encompasses multidisciplinary and structurally complex formulas at the line, paragraph, and page levels. Through straightforward supervised fine-tuning, our approach achieves state-of-the-art performance across a variety of styles, scientific domains, and complex layouts. Experimental results demonstrate that our method not only surpasses specialized models in terms of accuracy and robustness, but also establishes a new paradigm for the automated understanding of complex scientific documents.",
        "arxiv_id": "2508.00311",
        "ARXIVID": "2508.00311",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to vision-language models and OCR tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00587": {
        "authors": [
            "Marc H\\\"olle",
            "Walter Kellermann",
            "Vasileios Belagiannis"
        ],
        "title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection",
        "abstract": "arXiv:2508.00587v1 Announce Type: new  Abstract: Semantic segmentation models trained on known object classes often fail in real-world autonomous driving scenarios by confidently misclassifying unknown objects. While pixel-wise out-of-distribution detection can identify unknown objects, existing methods struggle in complex scenes where rare object classes are often confused with truly unknown objects. We introduce an uncertainty-aware likelihood ratio estimation method that addresses these limitations. Our approach uses an evidential classifier within a likelihood ratio test to distinguish between known and unknown pixel features from a semantic segmentation model, while explicitly accounting for uncertainty. Instead of producing point estimates, our method outputs probability distributions that capture uncertainty from both rare training examples and imperfect synthetic outliers. We show that by incorporating uncertainty in this way, outlier exposure can be leveraged more effectively. Evaluated on five standard benchmark datasets, our method achieves the lowest average false positive rate (2.5%) among state-of-the-art while maintaining high average precision (90.91%) and incurring only negligible computational overhead. Code is available at https://github.com/glasbruch/ULRE.",
        "arxiv_id": "2508.00587",
        "ARXIVID": "2508.00587",
        "COMMENT": "Does not match any specific criteria but involves out-of-distribution detection in semantic segmentation, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00324": {
        "authors": [
            "Yeonjun In",
            "Wonjoong Kim",
            "Sangwu Park",
            "Chanyoung Park"
        ],
        "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge",
        "abstract": "arXiv:2508.00324v1 Announce Type: new  Abstract: Although large reasoning models (LRMs) have demonstrated impressive capabilities on complex tasks, recent studies reveal that these models frequently fulfill harmful user instructions, raising significant safety concerns. In this paper, we investigate the underlying cause of LRM safety risks and find that models already possess sufficient safety knowledge but fail to activate it during reasoning. Based on this insight, we propose R1-Act, a simple and efficient post-training method that explicitly triggers safety knowledge through a structured reasoning process. R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods. Notably, it requires only 1,000 training examples and 90 minutes of training on a single RTX A6000 GPU. Extensive experiments across multiple LRM backbones and sizes demonstrate the robustness, scalability, and practical efficiency of our approach.",
        "arxiv_id": "2508.00324",
        "ARXIVID": "2508.00324",
        "COMMENT": "Does not match any specific criteria but focuses on safety alignment in reasoning models, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00581": {
        "authors": [
            "Ruiqing Ding",
            "Qianfang Sun",
            "Yongkang Leng",
            "Hui Yin",
            "Xiaojian Li"
        ],
        "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation",
        "abstract": "arXiv:2508.00581v1 Announce Type: new  Abstract: Pre-consultation is a critical component of effective healthcare delivery. However, generating comprehensive pre-consultation questionnaires from complex, voluminous Electronic Medical Records (EMRs) is a challenging task. Direct Large Language Model (LLM) approaches face difficulties in this task, particularly regarding information completeness, logical order, and disease-level synthesis. To address this issue, we propose a novel multi-stage LLM-driven framework: Stage 1 extracts atomic assertions (key facts with timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes disease knowledge by clustering representative networks from an EMR corpus; Stage 3 generates tailored personal and standardized disease-specific questionnaires based on these structured representations. This framework overcomes limitations of direct methods by building explicit clinical knowledge. Evaluated on a real-world EMR dataset and validated by clinical experts, our method demonstrates superior performance in information coverage, diagnostic relevance, understandability, and generation time, highlighting its practical potential to enhance patient information collection.",
        "arxiv_id": "2508.00581",
        "ARXIVID": "2508.00581",
        "COMMENT": "Does not match any specific criteria but is related to LLM applications in healthcare, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00414": {
        "authors": [
            "Tianqing Fang",
            "Zhisong Zhang",
            "Xiaoyang Wang",
            "Rui Wang",
            "Can Qin",
            "Yuxuan Wan",
            "Jun-Yu Ma",
            "Ce Zhang",
            "Jiaqi Chen",
            "Xiyun Li",
            "Hongming Zhang",
            "Haitao Mi",
            "Dong Yu"
        ],
        "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training",
        "abstract": "arXiv:2508.00414v1 Announce Type: new  Abstract: General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro",
        "arxiv_id": "2508.00414",
        "ARXIVID": "2508.00414",
        "COMMENT": "Does not match any specific criteria. Focuses on general AI agents and foundation models, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00367": {
        "authors": [
            "Joonmyung Choi",
            "Sanghyeok Lee",
            "Byungoh Ko",
            "Eunseo Kim",
            "Jihyung Kil",
            "Hyunwoo J. Kim"
        ],
        "title": "Representation Shift: Unifying Token Compression with FlashAttention",
        "abstract": "arXiv:2508.00367v1 Announce Type: new  Abstract: Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift.",
        "arxiv_id": "2508.00367",
        "ARXIVID": "2508.00367",
        "COMMENT": "Does not match any specific criteria. Focuses on token compression and FlashAttention, which are not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.00665": {
        "authors": [
            "Maryam Mosleh",
            "Marie Devlin",
            "Ellis Solaiman"
        ],
        "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI",
        "abstract": "arXiv:2508.00665v1 Announce Type: new  Abstract: Artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. Yet many of these systems lack transparency, offering limited insight into how decisions are made. Most explainable AI (XAI) techniques focus on technical outputs but neglect user roles and comprehension. This paper proposes a hybrid framework that integrates traditional XAI techniques with generative AI models and user personalisation to generate multimodal, personalised explanations tailored to user needs. We redefine explainability as a dynamic communication process tailored to user roles and learning goals. We outline the framework's design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation. Our aim is to move towards explainable AI that enhances transparency while supporting user-centred experiences.",
        "arxiv_id": "2508.00665",
        "ARXIVID": "2508.00665",
        "COMMENT": "Does not directly match any specific criterion but is relevant to multimodal explainable AI, which may align with your friend's general interest in multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.00493": {
        "authors": [
            "Alfie Roddan",
            "Tobias Czempiel",
            "Chi Xu",
            "Daniel S. Elson",
            "Stamatia Giannarou"
        ],
        "title": "SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation",
        "abstract": "arXiv:2508.00493v1 Announce Type: new  Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral medical imaging that introduces spectral angle prompting to guide the Segment Anything Model (SAM) using spectral similarity alongside spatial cues. This early fusion of spectral information enables more accurate and robust segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0 achieves up to +3.8% higher Dice scores compared to RGB-only models and up to +3.1% over prior spectral fusion methods. Our approach enhances few-shot and zero-shot performance, demonstrating strong generalization in challenging low-data and noisy scenarios common in clinical imaging.",
        "arxiv_id": "2508.00493",
        "ARXIVID": "2508.00493",
        "COMMENT": "Does not match any specific criteria but involves interactive segmentation in medical imaging, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}