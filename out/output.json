{
    "2508.16292": {
        "authors": [
            "Wen-Han Hsieh",
            "Elvis Hsieh",
            "Dantong Niu",
            "Trevor Darrell",
            "Roei Herzig",
            "David M. Chan"
        ],
        "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
        "abstract": "arXiv:2508.16292v1 Announce Type: new  Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong performance on a range of robotic tasks. These models rely on multimodal inputs, with language instructions playing a crucial role -- not only in predicting actions, but also in robustly interpreting user intent, even when the requests are impossible to fulfill. In this work, we investigate how VLAs can recognize, interpret, and respond to false-premise instructions: natural language commands that reference objects or conditions absent from the environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that (i) detects when an instruction cannot be executed due to a false premise, (ii) engages in language-based clarification or correction, and (iii) grounds plausible alternatives in perception and action. Towards this end, we construct a large-scale instruction tuning setup with structured language prompts and train a VLA model capable of handling both accurate and erroneous requests. Our approach leverages a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction. Our experiments show that IVA improves false premise detection accuracy by 97.56% over baselines, while increasing successful responses in false-premise scenarios by 50.78%.",
        "arxiv_id": "2508.16292",
        "ARXIVID": "2508.16292",
        "COMMENT": "Matches criteria 1 and 3 as it proposes a novel framework for vision-language-action models to handle false-premise instructions, improving spatial reasoning and embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.16157": {
        "authors": [
            "Pi-Wei Chen",
            "Jerry Chun-Wei Lin",
            "Wei-Han Chen",
            "Jia Ji",
            "Zih-Ching Chen",
            "Feng-Hao Yeh",
            "Chao-Chun Chen"
        ],
        "title": "Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection",
        "abstract": "arXiv:2508.16157v1 Announce Type: new  Abstract: Pre-trained Vision-Language Models (VLMs) have recently shown promise in detecting anomalies. However, previous approaches are fundamentally limited by their reliance on human-designed prompts and the lack of accessible anomaly samples, leading to significant gaps in context-specific anomaly understanding. In this paper, we propose \\textbf{A}daptive \\textbf{P}rompt \\textbf{T}uning with semantic alignment for anomaly detection (APT), a groundbreaking prior knowledge-free, few-shot framework and overcomes the limitations of traditional prompt-based approaches. APT uses self-generated anomaly samples with noise perturbations to train learnable prompts that capture context-dependent anomalies in different scenarios. To prevent overfitting to synthetic noise, we propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively aligns the prompts with general anomaly semantics while incorporating diverse synthetic anomaly. Our system not only advances pixel-wise anomaly detection, but also achieves state-of-the-art performance on multiple benchmark datasets without requiring prior knowledge for prompt crafting, establishing a robust and versatile solution for real-world anomaly detection.",
        "arxiv_id": "2508.16157",
        "ARXIVID": "2508.16157",
        "COMMENT": "Matches criterion 2 as it explores pre-trained Vision-Language Models (VLMs) and introduces a novel framework for anomaly detection using adaptive prompt tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16051": {
        "authors": [
            "Yiheng Hu",
            "Xiaoyang Wang",
            "Qing Liu",
            "Xiwei Xu",
            "Qian Fu",
            "Wenjie Zhang",
            "Liming Zhu"
        ],
        "title": "MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs",
        "abstract": "arXiv:2508.16051v1 Announce Type: new  Abstract: Multimodal Multi-hop question answering requires integrating information from diverse sources, such as images and texts, to derive answers. Existing methods typically rely on sequential retrieval and reasoning, where each step builds on the previous output. However, this single-path paradigm makes them vulnerable to errors due to misleading intermediate steps. Moreover, developing multimodal models can be computationally expensive, often requiring extensive training. To address these limitations, we propose a training-free framework guided by an Adaptive Planning Graph, which consists of planning, retrieval and reasoning modules. The planning module analyzes the current state of the Adaptive Planning Graph, determines the next action and where to expand the graph, which enables dynamic and flexible exploration of reasoning paths. To handle retrieval of text to unspecified target modalities, we devise modality-specific strategies that dynamically adapt to distinct data types. Our approach preserves the characteristics of multimodal information without costly task-specific training, enabling seamless integration with up-to-date models. Finally, the experiments on MultimodalQA and WebQA show that our approach matches or outperforms existing models that rely on training.",
        "arxiv_id": "2508.16051",
        "ARXIVID": "2508.16051",
        "COMMENT": "Matches criteria 6 as it proposes a training-free framework for multimodal multi-hop question answering, focusing on reasoning and retrieval.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16172": {
        "authors": [
            "Kai Hu",
            "Parfait Atchade-Adelomou",
            "Carlo Adornetto",
            "Adrian Mora-Carrero",
            "Luis Alonso-Pastor",
            "Ariel Noyman",
            "Yubo Liu",
            "Kent Larson"
        ],
        "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain",
        "abstract": "arXiv:2508.16172v1 Announce Type: new  Abstract: Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.",
        "arxiv_id": "2508.16172",
        "ARXIVID": "2508.16172",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for simulating human behavior in urban environments using Graph RAG and LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16577": {
        "authors": [
            "Yosef Dayani",
            "Omer Benishu",
            "Sagie Benaim"
        ],
        "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
        "abstract": "arXiv:2508.16577v1 Announce Type: new  Abstract: Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.",
        "arxiv_id": "2508.16577",
        "ARXIVID": "2508.16577",
        "COMMENT": "Matches criteria 5 as it combines image understanding tasks with text-to-3D generation using a novel retrieval-augmented multiview diffusion model.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.16211": {
        "authors": [
            "Shikang Zheng",
            "Liang Feng",
            "Xinyu Wang",
            "Qinming Zhou",
            "Peiliang Cai",
            "Chang Zou",
            "Jiacheng Liu",
            "Yuqi Lin",
            "Junjie Chen",
            "Yue Ma",
            "Linfeng Zhang"
        ],
        "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers",
        "abstract": "arXiv:2508.16211v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.",
        "arxiv_id": "2508.16211",
        "ARXIVID": "2508.16211",
        "COMMENT": "Matches criterion 6 as it focuses on video generation and proposes a novel method for efficient diffusion transformers, which is relevant to video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.16225": {
        "authors": [
            "Sandeep Gupta",
            "Roberto Passerone"
        ],
        "title": "An Investigation of Visual Foundation Models Robustness",
        "abstract": "arXiv:2508.16225v1 Announce Type: new  Abstract: Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision, powering systems for diverse tasks such as object detection, image classification, segmentation, pose estimation, and motion tracking. VFMs are capitalizing on seminal innovations in deep learning models, such as LeNet-5, AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver superior performance across a range of critical computer vision applications. These include security-sensitive domains like biometric verification, autonomous vehicle perception, and medical image analysis, where robustness is essential to fostering trust between technology and the end-users. This article investigates network robustness requirements crucial in computer vision systems to adapt effectively to dynamic environments influenced by factors such as lighting, weather conditions, and sensor characteristics. We examine the prevalent empirical defenses and robust training employed to enhance vision network robustness against real-world challenges such as distributional shifts, noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we provide a comprehensive analysis of the challenges associated with these defense mechanisms, including network properties and components to guide ablation studies and benchmarking metrics to evaluate network robustness.",
        "arxiv_id": "2508.16225",
        "ARXIVID": "2508.16225",
        "COMMENT": "Matches criteria 7 as it provides a comprehensive investigation into the robustness of visual foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.16433": {
        "authors": [
            "Sara Rojas",
            "Matthieu Armando",
            "Bernard Ghamen",
            "Philippe Weinzaepfel",
            "Vincent Leroy",
            "Gregory Rogez"
        ],
        "title": "HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction",
        "abstract": "arXiv:2508.16433v1 Announce Type: new  Abstract: Recovering the 3D geometry of a scene from a sparse set of uncalibrated images is a long-standing problem in computer vision. While recent learning-based approaches such as DUSt3R and MASt3R have demonstrated impressive results by directly predicting dense scene geometry, they are primarily trained on outdoor scenes with static environments and struggle to handle human-centric scenarios. In this work, we introduce HAMSt3R, an extension of MASt3R for joint human and scene 3D reconstruction from sparse, uncalibrated multi-view images. First, we exploit DUNE, a strong image encoder obtained by distilling, among others, the encoders from MASt3R and from a state-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a better understanding of scene geometry and human bodies. Our method then incorporates additional network heads to segment people, estimate dense correspondences via DensePose, and predict depth in human-centric environments, enabling a more comprehensive 3D reconstruction. By leveraging the outputs of our different heads, HAMSt3R produces a dense point map enriched with human semantic information in 3D. Unlike existing methods that rely on complex optimization pipelines, our approach is fully feed-forward and efficient, making it suitable for real-world applications. We evaluate our model on EgoHumans and EgoExo4D, two challenging benchmarks con taining diverse human-centric scenarios. Additionally, we validate its generalization to traditional multi-view stereo and multi-view pose regression tasks. Our results demonstrate that our method can reconstruct humans effectively while preserving strong performance in general 3D reconstruction tasks, bridging the gap between human and scene understanding in 3D vision.",
        "arxiv_id": "2508.16433",
        "ARXIVID": "2508.16433",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for human-aware 3D reconstruction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.16085": {
        "authors": [
            "Xiangde Luo",
            "Xiyue Wang",
            "Feyisope Eweje",
            "Xiaoming Zhang",
            "Sen Yang",
            "Ryan Quinton",
            "Jinxi Xiang",
            "Yuchen Li",
            "Yuanfeng Ji",
            "Zhe Li",
            "Yijiang Chen",
            "Colin Bergstrom",
            "Ted Kim",
            "Francesca Maria Olguin",
            "Kelley Yuan",
            "Matthew Abikenari",
            "Andrew Heider",
            "Sierra Willens",
            "Sanjeeth Rajaram",
            "Robert West",
            "Joel Neal",
            "Maximilian Diehn",
            "Ruijiang Li"
        ],
        "title": "Ensemble learning of foundation models for precision oncology",
        "abstract": "arXiv:2508.16085v1 Announce Type: new  Abstract: Histopathology is essential for disease diagnosis and treatment decision-making. Recent advances in artificial intelligence (AI) have enabled the development of pathology foundation models that learn rich visual representations from large-scale whole-slide images (WSIs). However, existing models are often trained on disparate datasets using varying strategies, leading to inconsistent performance and limited generalizability. Here, we introduce ELF (Ensemble Learning of Foundation models), a novel framework that integrates five state-of-the-art pathology foundation models to generate unified slide-level representations. Trained on 53,699 WSIs spanning 20 anatomical sites, ELF leverages ensemble learning to capture complementary information from diverse models while maintaining high data efficiency. Unlike traditional tile-level models, ELF's slide-level architecture is particularly advantageous in clinical contexts where data are limited, such as therapeutic response prediction. We evaluated ELF across a wide range of clinical applications, including disease classification, biomarker detection, and response prediction to major anticancer therapies, cytotoxic chemotherapy, targeted therapy, and immunotherapy, across multiple cancer types. ELF consistently outperformed all constituent foundation models and existing slide-level models, demonstrating superior accuracy and robustness. Our results highlight the power of ensemble learning for pathology foundation models and suggest ELF as a scalable and generalizable solution for advancing AI-assisted precision oncology.",
        "arxiv_id": "2508.16085",
        "ARXIVID": "2508.16085",
        "COMMENT": "Matches criteria 4 as it introduces an ensemble learning framework for pathology foundation models, focusing on clinical applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.15960": {
        "authors": [
            "Zhenhao Guo",
            "Rachit Saluja",
            "Tianyuan Yao",
            "Quan Liu",
            "Yuankai Huo",
            "Benjamin Liechty",
            "David J. Pisapia",
            "Kenji Ikemura",
            "Mert R. Sabuncu",
            "Yihe Yang",
            "Ruining Deng"
        ],
        "title": "Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification",
        "abstract": "arXiv:2508.15960v1 Announce Type: new  Abstract: Vision-language models (VLMs) have shown considerable potential in digital pathology, yet their effectiveness remains limited for fine-grained, disease-specific classification tasks such as distinguishing between glomerular subtypes. The subtle morphological variations among these subtypes, combined with the difficulty of aligning visual patterns with precise clinical terminology, make automated diagnosis in renal pathology particularly challenging. In this work, we explore how large pretrained VLMs can be effectively adapted to perform fine-grained glomerular classification, even in scenarios where only a small number of labeled examples are available. In this work, we introduce Glo-VLMs, a systematic framework designed to explore the adaptation of VLMs to fine-grained glomerular classification in data-constrained settings. Our approach leverages curated pathology images alongside clinical text prompts to facilitate joint image-text representation learning for nuanced renal pathology subtypes. By assessing various VLMs architectures and adaptation strategies under a few-shot learning paradigm, we explore how both the choice of method and the amount of labeled data impact model performance in clinically relevant scenarios. To ensure a fair comparison, we evaluate all models using standardized multi-class metrics, aiming to clarify the practical requirements and potential of large pretrained models for specialized clinical research applications. As a result, fine-tuning the VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with only 8 shots per class, demonstrating that even with highly limited supervision, foundation models can be effectively adapted for fine-grained medical image classification.",
        "arxiv_id": "2508.15960",
        "ARXIVID": "2508.15960",
        "COMMENT": "Matches criteria 4 as it focuses on adapting vision-language models for fine-grained medical image classification.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.16158": {
        "authors": [
            "Haodong He",
            "Yancheng Bai",
            "Rui Lan",
            "Xu Duan",
            "Lei Sun",
            "Xiangxiang Chu",
            "Gui-Song Xia"
        ],
        "title": "RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution",
        "abstract": "arXiv:2508.16158v1 Announce Type: new  Abstract: The rich textual information of large vision-language models (VLMs) combined with the powerful generative prior of pre-trained text-to-image (T2I) diffusion models has achieved impressive performance in single-image super-resolution (SISR). However, existing methods still face significant challenges in generating clear and accurate regional details, particularly in scenarios involving multiple objects. This challenge primarily stems from a lack of fine-grained regional descriptions and the models' insufficient ability to capture complex prompts. To address these limitations, we propose a Regional Attention Guided Super-Resolution (RAGSR) method that explicitly extracts localized fine-grained information and effectively encodes it through a novel regional attention mechanism, enabling both enhanced detail and overall visually coherent SR results. Specifically, RAGSR localizes object regions in an image and assigns fine-grained caption to each region, which are formatted as region-text pairs as textual priors for T2I models. A regional guided attention is then leveraged to ensure that each region-text pair is properly considered in the attention process while preventing unwanted interactions between unrelated region-text pairs. By leveraging this attention mechanism, our approach offers finer control over the integration of text and image information, thereby effectively overcoming limitations faced by traditional SISR techniques. Experimental results on benchmark datasets demonstrate that our approach exhibits superior performance in generating perceptually authentic visual details while maintaining contextual consistency compared to existing approaches.",
        "arxiv_id": "2508.16158",
        "ARXIVID": "2508.16158",
        "COMMENT": "Matches criteria 2 as it explores a novel regional attention mechanism for integrating vision-language models in super-resolution tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.16201": {
        "authors": [
            "Yicheng Ji",
            "Jun Zhang",
            "Heming Xia",
            "Jinpeng Chen",
            "Lidan Shou",
            "Gang Chen",
            "Huan Li"
        ],
        "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
        "abstract": "arXiv:2508.16201v1 Announce Type: new  Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens, enabling efficient speculation without sacrificing accuracy. To achieve this, it performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B.",
        "arxiv_id": "2508.16201",
        "ARXIVID": "2508.16201",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a speculative decoding framework for video LLMs with token pruning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.16129": {
        "authors": [
            "Ruiqi Wu",
            "Yuang Yao",
            "Tengfei Ma",
            "Chenran Zhang",
            "Na Su",
            "Tao Zhou",
            "Geng Chen",
            "Wen Fan",
            "Yi Zhou"
        ],
        "title": "Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning",
        "abstract": "arXiv:2508.16129v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have recently demonstrated remarkable reasoning abilities with reinforcement learning paradigm. Although several multimodal reasoning models have been explored in the medical domain, most of them focus exclusively on basic reasoning, which refers to shallow inference based on visual feature matching. However, real-world clinical diagnosis extends beyond basic reasoning, demanding reasoning processes that integrate heterogeneous clinical information (such as chief complaints and medical history) with multimodal medical imaging data. To bridge this gap, we introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the full spectrum of perception and reasoning. It encompasses both basic reasoning tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental reasoning capabilities and emulate realistic clinical thinking patterns. Building upon MM-Retinal-Reason, we propose OphthaReason, the first ophthalmology-specific multimodal reasoning model with step-by-step reasoning traces. To enable flexible adaptation to both basic and complex reasoning tasks, we specifically design a novel method called Uncertainty-Aware Dynamic Thinking (UADT), which estimates sample-level uncertainty via entropy and dynamically modulates the model's exploration depth using a shaped advantage mechanism. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance on both basic and complex reasoning tasks, outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and ophthalmic MLLMs by at least 24.92\\%, 15.00\\%, 21.20\\%, and 17.66\\%. Project Page: \\href{https://github.com/lxirich/OphthaReason}{link}.",
        "arxiv_id": "2508.16129",
        "ARXIVID": "2508.16129",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a multimodal reasoning model for ophthalmic data and explores integration of visual and textual reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.16512": {
        "authors": [
            "Chun-Peng Chang",
            "Chen-Yu Wang",
            "Julian Schmidt",
            "Holger Caesar",
            "Alain Pagani"
        ],
        "title": "Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation",
        "abstract": "arXiv:2508.16512v1 Announce Type: new  Abstract: Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing for applications such as autonomous driving, particularly in the context of driving simulation and so-called \"world models\". In this work, we investigate the effects of existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives. In datasets with diverse scene structures within temporal space, where objects or perspective shift in varied ways, these objectives tend to highly correlated. However, the very regular and repetitive nature of driving scenes allows visual quality to improve by modeling dominant scene motion patterns, without necessarily preserving fine-grained dynamic behavior. As a result, fine-tuning encourages the model to prioritize surface-level realism over dynamic accuracy. To further examine this phenomenon, we show that simple continual learning strategies, such as replay from diverse domains, can offer a balanced alternative by preserving spatial accuracy while maintaining strong visual quality.",
        "arxiv_id": "2508.16512",
        "ARXIVID": "2508.16512",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it investigates video generation for driving simulation and explores spatial accuracy and visual fidelity in dynamic video modeling.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16463": {
        "authors": [
            "Aniello Panariello",
            "Emanuele Frascaroli",
            "Pietro Buzzega",
            "Lorenzo Bonicelli",
            "Angelo Porrello",
            "Simone Calderara"
        ],
        "title": "Modular Embedding Recomposition for Incremental Learning",
        "abstract": "arXiv:2508.16463v1 Announce Type: new  Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by devising an approach that transforms preservation into enhancement of the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets. The codebase is available at https://github.com/aimagelab/mammoth.",
        "arxiv_id": "2508.16463",
        "ARXIVID": "2508.16463",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it enhances zero-shot capabilities of vision-language models through modular embedding recomposition.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.15904": {
        "authors": [
            "Dexuan He",
            "Xiao Zhou",
            "Wenbin Guan",
            "Liyuan Zhang",
            "Xiaoman Zhang",
            "Sinuo Xu",
            "Ge Wang",
            "Lifeng Wang",
            "Xiaojun Yuan",
            "Xin Sun",
            "Yanfeng Wang",
            "Kun Sun",
            "Ya Zhang",
            "Weidi Xie"
        ],
        "title": "Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping",
        "abstract": "arXiv:2508.15904v1 Announce Type: new  Abstract: Rare cancers comprise 20-25% of all malignancies but face major diagnostic challenges due to limited expert availability-especially in pediatric oncology, where they represent over 70% of cases. While pathology vision-language (VL) foundation models show promising zero-shot capabilities for common cancer subtyping, their clinical performance for rare cancers remains limited. Existing multi-instance learning (MIL) methods rely only on visual features, overlooking cross-modal knowledge and compromising interpretability critical for rare cancer diagnosis. To address this limitation, we propose PathPT, a novel framework that fully exploits the potential of vision-language pathology foundation models through spatially-aware visual aggregation and task-specific prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision into fine-grained tile-level guidance by leveraging the zero-shot capabilities of VL models, thereby preserving localization on cancerous regions and enabling cross-modal reasoning through prompts aligned with histopathological semantics. We benchmark PathPT on eight rare cancer datasets(four adult and four pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer datasets, evaluating four state-of-the-art VL models and four MIL frameworks under three few-shot settings. Results show that PathPT consistently delivers superior performance, achieving substantial gains in subtyping accuracy and cancerous region grounding ability. This work advances AI-assisted diagnosis for rare cancers, offering a scalable solution for improving subtyping accuracy in settings with limited access to specialized expertise.",
        "arxiv_id": "2508.15904",
        "ARXIVID": "2508.15904",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on pathology vision-language foundation models and their application to rare cancer subtyping.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16291": {
        "authors": [
            "Fengshun Wang",
            "Qiurui Wang",
            "Peilin Zhao"
        ],
        "title": "Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment",
        "abstract": "arXiv:2508.16291v1 Announce Type: new  Abstract: Technical Element Score (TES) and Program Component Score (PCS) evaluations in figure skating demand precise assessment of athletic actions and artistic interpretation, respectively. Existing methods face three major challenges. Firstly, video and audio cues are regarded as common features for both TES and PCS predictions in previous works without considering the prior evaluation criterion of figure skating. Secondly, action elements in competitions are separated in time, TES should be derived from each element's score, but existing methods try to give an overall TES prediction without evaluating each action element. Thirdly, lengthy competition videos make it difficult and inefficient to handle long-range contexts. To address these challenges, we propose a two-stream Mamba pyramid network that aligns with actual judging criteria to predict TES and PCS by separating visual-feature based TES evaluation stream from audio-visual-feature based PCS evaluation stream. In the PCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee that video-based features remain unaffected when assessing TES, and enhance PCS estimation by fusing visual and auditory cues across each contextual level of the pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and TES head we proposed effectively address the challenges of localizing and evaluating action elements with various temporal scales and give score predictions. With Mamba's superior ability to capture long-range dependencies and its linear computational complexity, our method is ideal for handling lengthy figure skating videos. Comprehensive experimentation demonstrates that our framework attains state-of-the-art performance on the FineFS benchmark. Our source code is available at https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.",
        "arxiv_id": "2508.16291",
        "ARXIVID": "2508.16291",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a novel method for video-based figure skating assessment, addressing long-range dependencies and multi-modal cues.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16217": {
        "authors": [
            "Hohyun Na",
            "Seunghoo Hong",
            "Simon S. Woo"
        ],
        "title": "PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting",
        "abstract": "arXiv:2508.16217v1 Announce Type: new  Abstract: The success of diffusion models has enabled effortless, high-quality image modifications that precisely align with users' intentions, thereby raising concerns about their potential misuse by malicious actors. Previous studies have attempted to mitigate such misuse through adversarial attacks. However, these approaches heavily rely on image-level inconsistencies, which pose fundamental limitations in addressing the influence of textual prompts. In this paper, we propose PromptFlare, a novel adversarial protection method designed to protect images from malicious modifications facilitated by diffusion-based inpainting models. Our approach leverages the cross-attention mechanism to exploit the intrinsic properties of prompt embeddings. Specifically, we identify and target shared token of prompts that is invariant and semantically uninformative, injecting adversarial noise to suppress the sampling process. The injected noise acts as a cross-attention decoy, diverting the model's focus away from meaningful prompt-image alignments and thereby neutralizing the effect of prompt. Extensive experiments on the EditBench dataset demonstrate that our method achieves state-of-the-art performance across various metrics while significantly reducing computational overhead and GPU memory usage. These findings highlight PromptFlare as a robust and efficient protection against unauthorized image manipulations. The code is available at https://github.com/NAHOHYUN-SKKU/PromptFlare.",
        "arxiv_id": "2508.16217",
        "ARXIVID": "2508.16217",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling and adversarial robustness in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.15988": {
        "authors": [
            "Mohamed Ilyes Lakhal",
            "Richard Bowden"
        ],
        "title": "Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production",
        "abstract": "arXiv:2508.15988v1 Announce Type: new  Abstract: The diversity of sign representation is essential for Sign Language Production (SLP) as it captures variations in appearance, facial expressions, and hand movements. However, existing SLP models are often unable to capture diversity while preserving visual quality and modelling non-manual attributes such as emotions. To address this problem, we propose a novel approach that leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital avatars from a generated reference image. We propose a novel sign feature aggregation module that explicitly models the non-manual features (\\textit{e.g.}, the face) and the manual features (\\textit{e.g.}, the hands). We show that our proposed module ensures the preservation of linguistic content while seamlessly using reference images with different ethnic backgrounds to ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics.",
        "arxiv_id": "2508.15988",
        "ARXIVID": "2508.15988",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multimodal learning and diversity in sign language production.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.16089": {
        "authors": [
            "Sun Weikai",
            "Song Shijie",
            "Chi Wenjie"
        ],
        "title": "Two-flow Feedback Multi-scale Progressive Generative Adversarial Network",
        "abstract": "arXiv:2508.16089v1 Announce Type: new  Abstract: Although diffusion model has made good progress in the field of image generation, GAN\\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\\cite{liu2021comparing}, SSGAN\\cite{guibas2021adaptive} \\cite{zhang2022vsa} \\cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\\%,AWUN The dataset is 78.3\\%,IONJ The dataset is 85.5\\%,POKL The dataset is 88.7\\%,OPIN The dataset is 96.4\\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\\% with INJK With strong cross-task capability.",
        "arxiv_id": "2508.16089",
        "ARXIVID": "2508.16089",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling and GAN advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.16212": {
        "authors": [
            "Huanpeng Chu",
            "Wei Wu",
            "Guanyu Fen",
            "Yutao Zhang"
        ],
        "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models",
        "abstract": "arXiv:2508.16212v1 Announce Type: new  Abstract: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure.In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction.Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.",
        "arxiv_id": "2508.16212",
        "ARXIVID": "2508.16212",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling and computational efficiency in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.16213": {
        "authors": [
            "Kaiyuan Ji",
            "Yijin Guo",
            "Zicheng Zhang",
            "Xiangyang Zhu",
            "Yuan Tian",
            "Ning Liu",
            "Guangtao Zhai"
        ],
        "title": "MedOmni-45{\\deg}: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine",
        "abstract": "arXiv:2508.16213v1 Announce Type: new  Abstract: With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.",
        "arxiv_id": "2508.16213",
        "ARXIVID": "2508.16213",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reasoning in large language models, particularly in the medical domain.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16124": {
        "authors": [
            "Savvas Karatsiolis",
            "Andreas Kamilaris"
        ],
        "title": "Domain Adaptation via Feature Refinement",
        "abstract": "arXiv:2508.16124v1 Announce Type: new  Abstract: We propose Domain Adaptation via Feature Refinement (DAFR2), a simple yet effective framework for unsupervised domain adaptation under distribution shift. The proposed method synergistically combines three key components: adaptation of Batch Normalization statistics using unlabeled target data, feature distillation from a source-trained model and hypothesis transfer. By aligning feature distributions at the statistical and representational levels, DAFR2 produces robust and domain-invariant feature spaces that generalize across similar domains without requiring target labels, complex architectures or sophisticated training objectives. Extensive experiments on benchmark datasets, including CIFAR10-C, CIFAR100-C, MNIST-C and PatchCamelyon-C, demonstrate that the proposed algorithm outperforms prior methods in robustness to corruption. Theoretical and empirical analyses further reveal that our method achieves improved feature alignment, increased mutual information between the domains and reduced sensitivity to input perturbations.",
        "arxiv_id": "2508.16124",
        "ARXIVID": "2508.16124",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning and domain adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16072": {
        "authors": [
            "Zizhen Li",
            "Chuanhao Li",
            "Yibin Wang",
            "Qi Chen",
            "Diping Song",
            "Yukang Feng",
            "Jianwen Sun",
            "Jiaxin Ai",
            "Fanrui Zhang",
            "Mingzhu Sun",
            "Kaipeng Zhang"
        ],
        "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles",
        "abstract": "arXiv:2508.16072v1 Announce Type: new  Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMs' capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction.",
        "arxiv_id": "2508.16072",
        "ARXIVID": "2508.16072",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reasoning and human-centric evaluation of LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16140": {
        "authors": [
            "Jincheng Li",
            "Danyang Dong",
            "Menglin Zheng",
            "Jingbo Zhang",
            "Yueqin Hang",
            "Lichi Zhang",
            "Lili Zhao"
        ],
        "title": "High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection",
        "abstract": "arXiv:2508.16140v1 Announce Type: new  Abstract: Automatic detection of abnormal cervical cells from Thinprep Cytologic Test (TCT) images is a critical component in the development of intelligent computer-aided diagnostic systems. However, existing algorithms typically fail to effectively model the correlations of visual features, while these spatial correlation features actually contain critical diagnostic information. Furthermore, no detection algorithm has the ability to integrate inter-correlation features of cells with intra-discriminative features of cells, lacking a fusion strategy for the end-to-end detection model. In this work, we propose a hypergraph-based cell detection network that effectively fuses different types of features, combining spatial correlation features and deep discriminative features. Specifically, we use a Multi-level Fusion Sub-network (MLF-SNet) to enhance feature extractioncapabilities. Then we introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation module (CLFFS-HC), to integrate mixed features. Finally, we conducted experiments on three publicly available datasets, and the results demonstrate that our method significantly improves the performance of cervical abnormal cell detection.",
        "arxiv_id": "2508.16140",
        "ARXIVID": "2508.16140",
        "COMMENT": "Does not match any specific criterion but is generally relevant to feature fusion and abnormal cell detection in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16383": {
        "authors": [
            "Xinyu Yang",
            "Chenlong Deng",
            "Zhicheng Dou"
        ],
        "title": "GLARE: Agentic Reasoning for Legal Judgment Prediction",
        "abstract": "arXiv:2508.16383v1 Announce Type: new  Abstract: Legal judgment prediction (LJP) has become increasingly important in the legal field. In this paper, we identify that existing large language models (LLMs) have significant problems of insufficient reasoning due to a lack of legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning framework that dynamically acquires key legal knowledge by invoking different modules, thereby improving the breadth and depth of reasoning. Experiments conducted on the real-world dataset verify the effectiveness of our method. Furthermore, the reasoning chain generated during the analysis process can increase interpretability and provide the possibility for practical applications.",
        "arxiv_id": "2508.16383",
        "ARXIVID": "2508.16383",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reasoning and legal judgment prediction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16279": {
        "authors": [
            "Dawei Gao",
            "Zitao Li",
            "Yuexiang Xie",
            "Weirui Kuang",
            "Liuyi Yao",
            "Bingchen Qian",
            "Zhijian Ma",
            "Yue Cui",
            "Haohao Luo",
            "Shen Li",
            "Lu Yi",
            "Yi Yu",
            "Shiqi He",
            "Zhiling Luo",
            "Wenmeng Zhou",
            "Zhicheng Zhang",
            "Xuguang He",
            "Ziqian Chen",
            "Weikai Liao",
            "Farruh Isakulovich Kushnazarov",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications",
        "abstract": "arXiv:2508.16279v1 Announce Type: new  Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications.",
        "arxiv_id": "2508.16279",
        "ARXIVID": "2508.16279",
        "COMMENT": "Does not match any specific criterion but is generally relevant to agentic applications and tool-based interactions.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16054": {
        "authors": [
            "Sonish Sivarajkumar",
            "Hang Zhang",
            "Yuelyu Ji",
            "Maneesh Bilalpur",
            "Xizhi Wu",
            "Chenyu Li",
            "Min Gu Kwak",
            "Shyam Visweswaran",
            "Yanshan Wang"
        ],
        "title": "Generative Foundation Model for Structured and Unstructured Electronic Health Records",
        "abstract": "arXiv:2508.16054v1 Announce Type: new  Abstract: Electronic health records (EHRs) are rich clinical data sources but complex repositories of patient data, spanning structured elements (demographics, vitals, lab results, codes), unstructured clinical notes and other modalities of data. Harnessing this heterogeneity is critical for improving patient outcomes. Recent advances in large language models (LLMs) have enabled foundation models that can learn from multiple data modalities and support clinical tasks. However, most current approaches simply serialize numeric EHR data into text, which risks losing temporal and quantitative detail. We introduce Generative Deep Patient (GDP), a multimodal foundation model that natively encodes structured EHR time-series via a CNN-Transformer encoder and fuses it with unstructured EHRs through cross-modal attention into a LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining, where it learns to produce clinical narratives from raw patient timelines while also performing masked feature prediction (MFP) and next time-step prediction (NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day readmission). In clinical prediction, GDP demonstrated superior performance on MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and 30-day readmission AUROC = 0.627. For narrative generation, GDP achieved ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation, GDP-Instruct scored highest on faithfulness, fluency, and overall clinical utility, suggesting reduced hospital documentation workload without sacrificing accuracy. Our results demonstrate that a single multimodal foundation model can both predict clinically actionable events and generate high-quality clinical narratives. Furthermore, GDP's flexible architecture can be extended to additional modalities.",
        "arxiv_id": "2508.16054",
        "ARXIVID": "2508.16054",
        "COMMENT": "Does not closely match any specific criteria but is generally relevant to multimodal learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16059": {
        "authors": [
            "Zhuomin Chen",
            "Dan Li",
            "Jiahui Zhou",
            "Shunyu Wu",
            "Haozheng Ye",
            "Jian Lou",
            "See-Kiong Ng"
        ],
        "title": "Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting",
        "abstract": "arXiv:2508.16059v1 Announce Type: new  Abstract: Time series (TS) data are ubiquitous across various application areas, rendering time series forecasting (TSF) a fundamental task. With the astounding advances in large language models (LLMs), a variety of methods have been developed to adapt LLMs for time series forecasting. Despite unlocking the potential of LLMs in comprehending TS data, existing methods are inherently constrained by their shallow integration of TS information, wherein LLMs typically access TS representations at shallow layers, primarily at the input layer. This causes the influence of TS representations to progressively fade in deeper layers and eventually leads to ineffective adaptation between textual embeddings and TS representations. In this paper, we propose the Multi-layer Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to directly access time series patterns at all depths, thereby mitigating the progressive loss of TS information in deeper layers. Specifically, MSEF leverages off-the-shelf time series foundation models to extract semantically rich embeddings, which are fused with intermediate text representations across LLM layers via layer-specific steering vectors. These steering vectors are designed to continuously optimize the alignment between time series and textual modalities and facilitate a layer-specific adaptation mechanism that ensures efficient few-shot learning capabilities. Experimental results on seven benchmarks demonstrate significant performance improvements by MSEF compared with baselines, with an average reduction of 31.8% in terms of MSE. The code is available at https://github.com/One1sAll/MSEF.",
        "arxiv_id": "2508.16059",
        "ARXIVID": "2508.16059",
        "COMMENT": "Does not closely match any specific criteria but is generally relevant to multimodal learning and LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16159": {
        "authors": [
            "Jiaqi Ma",
            "Guo-Sen Xie",
            "Fang Zhao",
            "Zechao Li"
        ],
        "title": "Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation",
        "abstract": "arXiv:2508.16159v1 Announce Type: new  Abstract: Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\\% improvement on Pascal-5\\textsuperscript{i} and a 9.7\\% improvement on COCO-20\\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.",
        "arxiv_id": "2508.16159",
        "ARXIVID": "2508.16159",
        "COMMENT": "Does not match any specific criteria. Focuses on weakly-supervised few-shot segmentation, which is tangentially related to computer vision but not directly relevant to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.16317": {
        "authors": [
            "Nedyalko Prisadnikov",
            "Danda Pani Paudel",
            "Yuqian Fu",
            "Luc Van Gool"
        ],
        "title": "Vision encoders should be image size agnostic and task driven",
        "abstract": "arXiv:2508.16317v1 Announce Type: new  Abstract: This position paper argues that the next generation of vision encoders should be image size agnostic and task driven. The source of our inspiration is biological. Not a structural aspect of biological vision, but a behavioral trait -- efficiency. We focus on a couple of ways in which vision in nature is efficient, but modern vision encoders not. We -- humans and animals -- deal with vast quantities of visual data, and need to be smart where we focus our limited energy -- it depends on the task. It is our belief that vision encoders should be dynamic and the computational complexity should depend on the task at hand rather than the size of the image. We, also, provide concrete first steps towards our vision -- a proof-of-concept solution for image classification. Despite classification being not very representative for what we are trying to achieve, it shows that our approach is feasible and promising.",
        "arxiv_id": "2508.16317",
        "ARXIVID": "2508.16317",
        "COMMENT": "Does not match any specific criteria. This is a position paper on vision encoders, which is tangentially related to computer vision but not directly relevant to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}