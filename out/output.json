{
    "2509.09958": {
        "authors": [
            "Jeffrey Liu",
            "Rongbin Hu"
        ],
        "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification",
        "abstract": "arXiv:2509.09958v1 Announce Type: new  Abstract: Referring Expression Comprehension (REC) is usually addressed with task-trained grounding models. We show that a zero-shot workflow, without any REC-specific training, can achieve competitive or superior performance. Our approach reformulates REC as box-wise visual-language verification: given proposals from a COCO-clean generic detector (YOLO-World), a general-purpose VLM independently answers True/False queries for each region. This simple procedure reduces cross-box interference, supports abstention and multiple matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our method not only surpasses a zero-shot GroundingDINO baseline but also exceeds reported results for GroundingDINO trained on REC and GroundingDINO+CRG. Controlled studies with identical proposals confirm that verification significantly outperforms selection-based prompting, and results hold with open VLMs. Overall, we show that workflow design, rather than task-specific pretraining, drives strong zero-shot REC performance.",
        "arxiv_id": "2509.09958",
        "ARXIVID": "2509.09958",
        "COMMENT": "Matches criterion 5 as it explores a zero-shot method for referring expression comprehension using visual-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10026": {
        "authors": [
            "Jing Huang",
            "Zhiya Tan",
            "Shutao Gong",
            "Fanwei Zeng",
            "Jianshu Li"
        ],
        "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA",
        "abstract": "arXiv:2509.10026v1 Announce Type: new  Abstract: As large vision language models (VLMs) advance, their capabilities in multilingual visual question answering (mVQA) have significantly improved. Chain-of-thought (CoT) reasoning has been proven to enhance interpretability and complex reasoning. However, most existing approaches rely primarily on textual CoT and provide limited support for multilingual multimodal reasoning, constraining their deployment in real-world applications. To address this gap, we introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable multi-stage reasoning pipeline consisting of Text Summary with Bounding Box (BBox), Language Identification, Spatial Object-level Captioning, and Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an automated data curation method that generates multilingual CoT annotations through iterative generation, correction, and refinement, enabling scalable and high-quality training data. To improve reasoning and generalization, LaV-CoT adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT) with Language-aware Group Relative Policy Optimization (GRPO), guided by verifiable multi-aspect rewards including language consistency, structural accuracy, and semantic alignment. Extensive evaluations on public datasets including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up to \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar size and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%. Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513 and Gemini-2.5-flash. We further conducted an online A/B test to validate our method on real-world data, highlighting its effectiveness for industrial deployment. Our code is available at this link: \\href{https://github.com/HJNVR/LaV-CoT}",
        "arxiv_id": "2509.10026",
        "ARXIVID": "2509.10026",
        "COMMENT": "Matches criterion 2 as it introduces a novel multilingual visual question answering framework with reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10059": {
        "authors": [
            "Yue Zhou",
            "Litong Feng",
            "Mengcheng Lan",
            "Xue Yang",
            "Qingyun Li",
            "Yiping Ke",
            "Xue Jiang",
            "Wayne Zhang"
        ],
        "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration",
        "abstract": "arXiv:2509.10059v1 Announce Type: new  Abstract: Mathematical reasoning is critical for tasks such as precise distance and area computations, trajectory estimations, and spatial analysis in unmanned aerial vehicle (UAV) based remote sensing, yet current vision-language models (VLMs) have not been adequately tested in this domain. To address this gap, we introduce AVI-Math, the first benchmark to rigorously evaluate multimodal mathematical reasoning in aerial vehicle imagery, moving beyond simple counting tasks to include domain-specific knowledge in areas such as geometry, logic, and algebra. The dataset comprises 3,773 high-quality vehicle-related questions captured from UAV views, covering 6 mathematical subjects and 20 topics. The data, collected at varying altitudes and from multiple UAV angles, reflects real-world UAV scenarios, ensuring the diversity and complexity of the constructed mathematical problems. In this paper, we benchmark 14 prominent VLMs through a comprehensive evaluation and demonstrate that, despite their success on previous multimodal benchmarks, these models struggle with the reasoning tasks in AVI-Math. Our detailed analysis highlights significant limitations in the mathematical reasoning capabilities of current VLMs and suggests avenues for future research. Furthermore, we explore the use of Chain-of-Thought prompting and fine-tuning techniques, which show promise in addressing the reasoning challenges in AVI-Math. Our findings not only expose the limitations of VLMs in mathematical reasoning but also offer valuable insights for advancing UAV-based trustworthy VLMs in real-world applications. The code, and datasets will be released at https://github.com/VisionXLab/avi-math",
        "arxiv_id": "2509.10059",
        "ARXIVID": "2509.10059",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark for multimodal mathematical reasoning in UAV imagery, focusing on video-based tasks and reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10080": {
        "authors": [
            "Minsang Kong",
            "Myeongjun Kim",
            "Sang Gu Kang",
            "Sang Hun Lee"
        ],
        "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals",
        "abstract": "arXiv:2509.10080v1 Announce Type: new  Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe and efficient navigation. To improve prediction accuracy, recent approaches often rely on pre-built high-definition (HD) maps or real-time local map construction modules to incorporate static environmental information. However, pre-built HD maps are limited to specific regions and cannot adapt to transient changes. In addition, local map construction modules, which recognize only predefined elements, may fail to capture critical scene details or introduce errors that degrade prediction performance. To overcome these limitations, we propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory prediction framework that operates directly in the bird's-eye view (BEV) space utilizing real-time sensor data without relying on any pre-built maps. The BEVTraj leverages deformable attention to efficiently extract relevant context from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate Proposal (SGCP) module, which enables full end-to-end prediction without requiring any post-processing steps. Extensive experiments demonstrate that the BEVTraj achieves performance comparable to state-of-the-art HD map-based models while offering greater flexibility by eliminating the dependency on pre-built maps. The source code is available at https://github.com/Kongminsang/bevtraj.",
        "arxiv_id": "2509.10080",
        "ARXIVID": "2509.10080",
        "COMMENT": "Matches criterion 3 as it proposes a novel trajectory prediction framework for autonomous driving, addressing challenges in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09977": {
        "authors": [
            "Siying Liu",
            "Zikai Wang",
            "Hanle Zheng",
            "Yifan Hu",
            "Xilin Wang",
            "Qingkai Yang",
            "Jibin Wu",
            "Hao Guo",
            "Lei Deng"
        ],
        "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking",
        "abstract": "arXiv:2509.09977v1 Announce Type: new  Abstract: RGB-Event tracking has become a promising trend in visual object tracking to leverage the complementary strengths of both RGB images and dynamic spike events for improved performance. However, existing artificial neural networks (ANNs) struggle to fully exploit the sparse and asynchronous nature of event streams. Recent efforts toward hybrid architectures combining ANNs and spiking neural networks (SNNs) have emerged as a promising solution in RGB-Event perception, yet effectively fusing features across heterogeneous paradigms remains a challenge. In this work, we propose ISTASTrack, the first transformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped with \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model employs a vision transformer to extract spatial context from RGB inputs and a spiking transformer to capture spatio-temporal dynamics from event streams. To bridge the modality and paradigm gap between ANN and SNN features, we systematically design a model-based ISTA adapter for bidirectional feature interaction between the two branches, derived from sparse representation theory by unfolding the iterative shrinkage thresholding algorithm. Additionally, we incorporate a temporal downsampling attention module within the adapter to align multi-step SNN features with single-step ANN features in the latent space, improving temporal fusion. Experimental results on RGB-Event tracking benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that ISTASTrack achieves state-of-the-art performance while maintaining high energy efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking. The code is publicly available at https://github.com/lsying009/ISTASTrack.git.",
        "arxiv_id": "2509.09977",
        "ARXIVID": "2509.09977",
        "COMMENT": "Matches criterion 3 as it introduces a novel hybrid ANN-SNN method for RGB-Event tracking, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.09828": {
        "authors": [
            "Tim Broedermannn",
            "Christos Sakaridis",
            "Luigi Piccinelli",
            "Wim Abbeloos",
            "Luc Van Gool"
        ],
        "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception",
        "abstract": "arXiv:2509.09828v1 Announce Type: new  Abstract: Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DELIVER datasets. Code and models will be available at https://github.com/timbroed/DGFusion",
        "arxiv_id": "2509.09828",
        "ARXIVID": "2509.09828",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for sensor fusion in embodied AI, specifically for autonomous vehicles.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.09730": {
        "authors": [
            "Kaikai Zhao",
            "Zhaoxiang Liu",
            "Peng Wang",
            "Xin Wang",
            "Zhicheng Ma",
            "Yajun Xu",
            "Wenjing Zhang",
            "Yibing Nan",
            "Kai Wang",
            "Shiguo Lian"
        ],
        "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance",
        "abstract": "arXiv:2509.09730v1 Announce Type: new  Abstract: General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs, addressing five critical ITS tasks: object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source, providing high-value resources to advance both ITS and LMM research.",
        "arxiv_id": "2509.09730",
        "ARXIVID": "2509.09730",
        "COMMENT": "Matches criterion 3 as it introduces a new multimodal benchmark dataset for intelligent traffic surveillance, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.10408": {
        "authors": [
            "Iacopo Curti",
            "Pierluigi Zama Ramirez",
            "Alioscia Petrelli",
            "Luigi Di Stefano"
        ],
        "title": "Multimodal SAM-adapter for Semantic Segmentation",
        "abstract": "arXiv:2509.10408v1 Announce Type: new  Abstract: Semantic segmentation, a key task in computer vision with broad applications in autonomous driving, medical imaging, and robotics, has advanced substantially with deep learning. Nevertheless, current approaches remain vulnerable to challenging conditions such as poor lighting, occlusions, and adverse weather. To address these limitations, multimodal methods that integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged, providing complementary information that enhances robustness. In this work, we present MM SAM-adapter, a novel framework that extends the capabilities of the Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed method employs an adapter network that injects fused multimodal features into SAM's rich RGB features. This design enables the model to retain the strong generalization ability of RGB features while selectively incorporating auxiliary modalities only when they contribute additional cues. As a result, MM SAM-adapter achieves a balanced and efficient use of multimodal information. We evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES, where MM SAM-adapter delivers state-of-the-art performance. To further analyze modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard subsets. Results consistently demonstrate that our framework outperforms competing methods in both favorable and adverse conditions, highlighting the effectiveness of multimodal adaptation for robust scene understanding. The code is available at the following link: https://github.com/iacopo97/Multimodal-SAM-Adapter.",
        "arxiv_id": "2509.10408",
        "ARXIVID": "2509.10408",
        "COMMENT": "Matches criterion 2 as it extends the Segment Anything Model (SAM) for multimodal semantic segmentation, integrating vision and auxiliary modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.10021": {
        "authors": [
            "Jonas K\\\"uhne",
            "Christian Vogt",
            "Michele Magno",
            "Luca Benini"
        ],
        "title": "Efficient and Accurate Downfacing Visual Inertial Odometry",
        "abstract": "arXiv:2509.10021v1 Announce Type: new  Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that determines an agent's movement through a camera and an IMU sensor. This paper presents an efficient and accurate VIO pipeline optimized for applications on micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and quantized for emerging RISC-V-based ultra-low-power parallel systems on chips (SoCs). Furthermore, by employing a rigid body motion model, the pipeline reduces estimation errors and achieves improved accuracy in planar motion scenarios. The pipeline's suitability for real-time VIO is assessed on an ultra-low-power SoC in terms of compute requirements and tracking accuracy after quantization. The pipeline, including the three feature tracking methods, was implemented on the SoC for real-world validation. This design bridges the gap between high-accuracy VIO pipelines that are traditionally run on computationally powerful systems and lightweight implementations suitable for microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates an average reduction in RMSE of up to a factor of 3.65x over the baseline pipeline when using the ORB feature tracker. The analysis of the computational complexity of the feature trackers further shows that PX4FLOW achieves on-par tracking accuracy with ORB at a lower runtime for movement speeds below 24 pixels/frame.",
        "arxiv_id": "2509.10021",
        "ARXIVID": "2509.10021",
        "COMMENT": "Matches criterion 3 as it introduces a novel VIO pipeline optimized for embodied AI applications like UAVs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.10266": {
        "authors": [
            "Wenfang Wu",
            "Tingting Yuan",
            "Yupeng Li",
            "Daling Wang",
            "Xiaoming Fu"
        ],
        "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion",
        "abstract": "arXiv:2509.10266v1 Announce Type: new  Abstract: Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.",
        "arxiv_id": "2509.10266",
        "ARXIVID": "2509.10266",
        "COMMENT": "Matches criterion 2 as it explores multimodal learning by fusing manual and non-manual cues for sign language translation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.10260": {
        "authors": [
            "Jia Wang",
            "Jie Hu",
            "Xiaoqi Ma",
            "Hanghang Ma",
            "Yanbing Zeng",
            "Xiaoming Wei"
        ],
        "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation",
        "abstract": "arXiv:2509.10260v1 Announce Type: new  Abstract: Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: https://wj-inf.github.io/MagicMirror-page/.",
        "arxiv_id": "2509.10260",
        "ARXIVID": "2509.10260",
        "COMMENT": "Matches criterion 4 as it introduces a new benchmark and dataset for evaluating text-to-image generation models, which are related to vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.10140": {
        "authors": [
            "Yifan Chang",
            "Jie Qin",
            "Limeng Qiao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Lin Ma",
            "Xingang Wang"
        ],
        "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization",
        "abstract": "arXiv:2509.10140v1 Announce Type: new  Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.",
        "arxiv_id": "2509.10140",
        "ARXIVID": "2509.10140",
        "COMMENT": "Matches criterion 5 as it focuses on improving vector-quantized networks for image generation, which involves integration of image understanding and generation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.10105": {
        "authors": [
            "Young-rok Cha",
            "Jeongho Ju",
            "SunYoung Park",
            "Jong-Hyeon Lee",
            "Younghyun Yu",
            "Youngjune Kim"
        ],
        "title": "VARCO-VISION-2.0 Technical Report",
        "abstract": "arXiv:2509.10105v1 Announce Type: new  Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model (VLM) for Korean and English with improved capabilities compared to the previous model VARCO-VISION-14B. The model supports multi-image understanding for complex inputs such as documents, charts, and tables, and delivers layoutaware OCR by predicting both textual content and its spatial location. Trained with a four-stage curriculum with memory-efficient techniques, the model achieves enhanced multimodal alignment, while preserving core language abilities and improving safety via preference optimization. Extensive benchmark evaluations demonstrate strong spatial grounding and competitive results for both languages, with the 14B model achieving 8th place on the OpenCompass VLM leaderboard among models of comparable scale. Alongside the 14B-scale model, we release a 1.7B version optimized for on-device deployment. We believe these models advance the development of bilingual VLMs and their practical applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a full-scale 14B model and a lightweight 1.7B model.",
        "arxiv_id": "2509.10105",
        "ARXIVID": "2509.10105",
        "COMMENT": "Matches criterion 2 as it introduces a bilingual vision-language model with multimodal capabilities.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.09742": {
        "authors": [
            "Md Fazle Rasul",
            "Alanood Alqobaisi",
            "Bruhadeshwar Bezawada",
            "Indrakshi Ray"
        ],
        "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning",
        "abstract": "arXiv:2509.09742v1 Announce Type: new  Abstract: Federated learning (FL) allows multiple entities to train a shared model collaboratively. Its core, privacy-preserving principle is that participants only exchange model updates, such as gradients, and never their raw, sensitive data. This approach is fundamental for applications in domains where privacy and confidentiality are important. However, the security of this very mechanism is threatened by gradient inversion attacks, which can reverse-engineer private training data directly from the shared gradients, defeating the purpose of FL. While the impact of these attacks is known for image, text, and tabular data, their effect on video data remains an unexamined area of research. This paper presents the first analysis of video data leakage in FL using gradient inversion attacks. We evaluate two common video classification approaches: one employing pre-trained feature extractors and another that processes raw video frames with simple transformations. Our initial results indicate that the use of feature extractors offers greater resilience against gradient inversion attacks. We also demonstrate that image super-resolution techniques can enhance the frames extracted through gradient inversion attacks, enabling attackers to reconstruct higher-quality videos. Our experiments validate this across scenarios where the attacker has access to zero, one, or more reference frames from the target environment. We find that although feature extractors make attacks more challenging, leakage is still possible if the classifier lacks sufficient complexity. We, therefore, conclude that video data leakage in FL is a viable threat, and the conditions under which it occurs warrant further investigation.",
        "arxiv_id": "2509.09742",
        "ARXIVID": "2509.09742",
        "COMMENT": "Matches criterion 6 as it explores video data leakage in federated learning, which is a novel perspective on video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.10312": {
        "authors": [
            "Zhixin Zheng",
            "Xinyu Wang",
            "Chang Zou",
            "Shaobo Wang",
            "Linfeng Zhang"
        ],
        "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching",
        "abstract": "arXiv:2509.10312v1 Announce Type: new  Abstract: Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.",
        "arxiv_id": "2509.10312",
        "ARXIVID": "2509.10312",
        "COMMENT": "Matches criterion 5 as it introduces a technique combining image generation tasks with diffusion transformers, which aligns with integrating image understanding and generation with LLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.09919": {
        "authors": [
            "Franklin Yiu",
            "Mohan Lu",
            "Nina Li",
            "Kevin Joseph",
            "Tianxu Zhang",
            "Julian Togelius",
            "Timothy Merino",
            "Sam Earle"
        ],
        "title": "A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments",
        "abstract": "arXiv:2509.09919v1 Announce Type: new  Abstract: Procedural content generation often requires satisfying both designer-specified objectives and adjacency constraints implicitly imposed by the underlying tile set. To address the challenges of jointly optimizing both constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a Markov Decision Process (MDP), enabling external optimization algorithms to focus exclusively on objective maximization while leveraging WFC's propagation mechanism to enforce constraint satisfaction. We empirically compare optimizing this MDP to traditional evolutionary approaches that jointly optimize global metrics and local tile placement. Across multiple domains with various difficulties, we find that joint optimization not only struggles as task complexity increases, but consistently underperforms relative to optimization over the WFC-MDP, underscoring the advantages of decoupling local constraint satisfaction from global objective optimization.",
        "arxiv_id": "2509.09919",
        "ARXIVID": "2509.09919",
        "COMMENT": "Matches criterion 1 as it presents a novel method for spatial reasoning in procedural content generation, which could be relevant to spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.10054": {
        "authors": [
            "Hailong Yang",
            "Mingxian Gu",
            "Jianqi Wang",
            "Guanjin Wang",
            "Zhaohong Deng"
        ],
        "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph",
        "abstract": "arXiv:2509.10054v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans with complex, real-world tasks. However, MAS still face challenges in effective task planning when handling highly complex tasks with uncertainty, often resulting in misleading or incorrect outputs that hinder task execution. To address this, we propose XAgents, a unified multi-agent cooperative framework built on a multipolar task processing graph and IF-THEN rules. XAgents uses the multipolar task processing graph to enable dynamic task planning and handle task uncertainty. During subtask processing, it integrates domain-specific IF-THEN rules to constrain agent behaviors, while global rules enhance inter-agent collaboration. We evaluate the performance of XAgents across three distinct datasets, demonstrating that it consistently surpasses state-of-the-art single-agent and multi-agent approaches in both knowledge-typed and logic-typed question-answering tasks. The codes for XAgents are available at: https://github.com/AGI-FHBC/XAgents.",
        "arxiv_id": "2509.10054",
        "ARXIVID": "2509.10054",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for multi-agent cooperation, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.10278": {
        "authors": [
            "Vidit Vidit",
            "Pavel Korshunov",
            "Amir Mohammadi",
            "Christophe Ecabert",
            "Ketan Kotwal",
            "S\\'ebastien Marcel"
        ],
        "title": "Detecting Text Manipulation in Images using Vision Language Models",
        "abstract": "arXiv:2509.10278v1 Announce Type: new  Abstract: Recent works have shown the effectiveness of Large Vision Language Models (VLMs or LVLMs) in image manipulation detection. However, text manipulation detection is largely missing in these studies. We bridge this knowledge gap by analyzing closed- and open-source VLMs on different text manipulation datasets. Our results suggest that open-source models are getting closer, but still behind closed-source ones like GPT- 4o. Additionally, we benchmark image manipulation detection-specific VLMs for text manipulation detection and show that they suffer from the generalization problem. We benchmark VLMs for manipulations done on in-the-wild scene texts and on fantasy ID cards, where the latter mimic a challenging real-world misuse.",
        "arxiv_id": "2509.10278",
        "ARXIVID": "2509.10278",
        "COMMENT": "Matches criterion 2 as it explores the use of Vision Language Models (VLMs) for text manipulation detection, which is a vision\u2013language integration task.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.09971": {
        "authors": [
            "Aupendu Kar",
            "Vishnu Raj",
            "Guan-Ming Su"
        ],
        "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey",
        "abstract": "arXiv:2509.09971v1 Announce Type: new  Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture per-pixel brightness changes and output a stream of events encoding the polarity, location and time of these changes. These systems are witnessing rapid advancements as an emerging field, driven by their low latency, reduced power consumption, and ultra-high capture rates. This survey explores the evolution of fusing event-stream captured with traditional frame-based capture, highlighting how this synergy significantly benefits various video restoration and 3D reconstruction tasks. The paper systematically reviews major deep learning contributions to image/video enhancement and restoration, focusing on two dimensions: temporal enhancement (such as frame interpolation and motion deblurring) and spatial enhancement (including super-resolution, low-light and HDR enhancement, and artifact reduction). This paper also explores how the 3D reconstruction domain evolves with the advancement of event driven fusion. Diverse topics are covered, with in-depth discussions on recent works for improving visual quality under challenging conditions. Additionally, the survey compiles a comprehensive list of openly available datasets, enabling reproducible research and benchmarking. By consolidating recent progress and insights, this survey aims to inspire further research into leveraging event camera systems, especially in combination with deep learning, for advanced visual media restoration and enhancement.",
        "arxiv_id": "2509.09971",
        "ARXIVID": "2509.09971",
        "COMMENT": "Matches criterion 7 as it is a survey paper on event camera-guided visual media restoration and 3D reconstruction, synthesizing the state of the art in this area.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2509.10441": {
        "authors": [
            "Tao Han",
            "Wanghan Xu",
            "Junchao Gong",
            "Xiaoyu Yue",
            "Song Guo",
            "Luping Zhou",
            "Lei Bai"
        ],
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "abstract": "arXiv:2509.10441v1 Announce Type: new  Abstract: Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \\textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.",
        "arxiv_id": "2509.10441",
        "ARXIVID": "2509.10441",
        "COMMENT": "Does not match any specific criteria but is related to scalable image synthesis and diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.10156": {
        "authors": [
            "Goker Erdogan",
            "Nikhil Parthasarathy",
            "Catalin Ionescu",
            "Drew Hudson",
            "Alexander Lerchner",
            "Andrew Zisserman",
            "Mehdi Sajjadi",
            "Joao Carreira"
        ],
        "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing",
        "abstract": "arXiv:2509.10156v1 Announce Type: new  Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions from pixel to latent prediction through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from \"representation collapse\". We apply our proposed approach, LayerLock, to large models of up to 4B parameters with results surpassing those of non-latent masked prediction on the 4DS perception suite.",
        "arxiv_id": "2509.10156",
        "ARXIVID": "2509.10156",
        "COMMENT": "Does not match any specific criteria but is related to self-supervised learning and video masked autoencoding.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.10222": {
        "authors": [
            "Ma\\\"el Jullien",
            "Lei Xu",
            "Marco Valentino",
            "Andr\\'e Freitas"
        ],
        "title": "Compartmentalised Agentic Reasoning for Clinical NLI",
        "abstract": "arXiv:2509.10222v1 Announce Type: new  Abstract: A common assumption holds that scaling data and parameters yields increasingly structured, generalisable internal representations. We interrogate this assumption in clinical natural language inference (NLI) by adopting a benchmark decomposed into four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction, and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI that separates knowledge access from principled inference. CARENLI routes each premise, statement pair to a family specific solver and enforces auditable procedures via a planner, verifier, and refiner.   Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching 98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag violations with near-ceiling reliability, while refiners correct a substantial share of epistemic errors. Remaining failures cluster in routing, identifying family classification as the main bottleneck. These results show that LLMs often retain relevant facts but default to heuristics when inference is underspecified, a dissociation CARENLI makes explicit while offering a framework for safer, auditable reasoning.",
        "arxiv_id": "2509.10222",
        "ARXIVID": "2509.10222",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of natural language inference and reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.10453": {
        "authors": [
            "Emily Kaczmarek",
            "Justin Szeto",
            "Brennan Nichyporuk",
            "Tal Arbel"
        ],
        "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets",
        "abstract": "arXiv:2509.10453v1 Announce Type: new  Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes memory loss and cognitive decline. While there has been extensive research in applying deep learning models to Alzheimer's prediction tasks, these models remain limited by lack of available labeled data, poor generalization across datasets, and inflexibility to varying numbers of input scans and time intervals between scans. In this study, we adapt three state-of-the-art temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis, and add novel extensions designed to handle variable-length inputs and learn robust spatial features. We aggregate four publicly available datasets comprising 3,161 patients for pre-training, and show the performance of our model across multiple Alzheimer's prediction tasks including diagnosis classification, conversion detection, and future conversion prediction. Importantly, our SSL model implemented with temporal order prediction and contrastive learning outperforms supervised learning on six out of seven downstream tasks. It demonstrates adaptability and generalizability across tasks and number of input images with varying time intervals, highlighting its capacity for robust performance across clinical applications. We release our code and model publicly at https://github.com/emilykaczmarek/SSL-AD.",
        "arxiv_id": "2509.10453",
        "ARXIVID": "2509.10453",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and its applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09869": {
        "authors": [
            "Yihao Liu",
            "Junyu Chen",
            "Lianrui Zuo",
            "Shuwen Wei",
            "Brian D. Boyd",
            "Carmen Andreescu",
            "Olusola Ajilore",
            "Warren D. Taylor",
            "Aaron Carass",
            "Bennett A. Landman"
        ],
        "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration",
        "abstract": "arXiv:2509.09869v1 Announce Type: new  Abstract: Objective: Deep learning-based deformable image registration has achieved strong accuracy, but remains sensitive to variations in input image characteristics such as artifacts, field-of-view mismatch, or modality difference. We aim to develop a general training paradigm that improves the robustness and generalizability of registration networks. Methods: We introduce surrogate supervision, which decouples the input domain from the supervision domain by applying estimated spatial transformations to surrogate images. This allows training on heterogeneous inputs while ensuring supervision is computed in domains where similarity is well defined. We evaluate the framework through three representative applications: artifact-robust brain MR registration, mask-agnostic lung CT registration, and multi-modal MR registration. Results: Across tasks, surrogate supervision demonstrated strong resilience to input variations including inhomogeneity field, inconsistent field-of-view, and modality differences, while maintaining high performance on well-curated data. Conclusions: Surrogate supervision provides a principled framework for training robust and generalizable deep learning-based registration models without increasing complexity. Significance: Surrogate supervision offers a practical pathway to more robust and generalizable medical image registration, enabling broader applicability in diverse biomedical imaging scenarios.",
        "arxiv_id": "2509.09869",
        "ARXIVID": "2509.09869",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.10259": {
        "authors": [
            "Hua Yuan",
            "Jin Yuan",
            "Yicheng Jiang",
            "Yao Zhang",
            "Xin Geng",
            "Yong Rui"
        ],
        "title": "Mask Consistency Regularization in Object Removal",
        "abstract": "arXiv:2509.10259v1 Announce Type: new  Abstract: Object removal, a challenging task within image inpainting, involves seamlessly filling the removed region with content that matches the surrounding context. Despite advancements in diffusion models, current methods still face two critical challenges. The first is mask hallucination, where the model generates irrelevant or spurious content inside the masked region, and the second is mask-shape bias, where the model fills the masked area with an object that mimics the mask's shape rather than surrounding content. To address these issues, we propose Mask Consistency Regularization (MCR), a novel training strategy designed specifically for object removal tasks. During training, our approach introduces two mask perturbations: dilation and reshape, enforcing consistency between the outputs of these perturbed branches and the original mask. The dilated masks help align the model's output with the surrounding content, while reshaped masks encourage the model to break the mask-shape bias. This combination of strategies enables MCR to produce more robust and contextually coherent inpainting results. Our experiments demonstrate that MCR significantly reduces hallucinations and mask-shape bias, leading to improved performance in object removal.",
        "arxiv_id": "2509.10259",
        "ARXIVID": "2509.10259",
        "COMMENT": "Does not match any specific criterion but is related to image inpainting and object removal, which are tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.10297": {
        "authors": [
            "Eoin O'Doherty",
            "Nicole Weinrauch",
            "Andrew Talone",
            "Uri Klempner",
            "Xiaoyuan Yi",
            "Xing Xie",
            "Yi Zeng"
        ],
        "title": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis",
        "abstract": "arXiv:2509.10297v1 Announce Type: new  Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent questions about how to align machine decision-making with human moral values. This working paper investigates how leading AI systems prioritize moral outcomes and what this reveals about the prospects for human-AI symbiosis. We address two central questions: (1) What moral values do state-of-the-art large language models (LLMs) implicitly favour when confronted with dilemmas? (2) How do differences in model architecture, cultural origin, and explainability affect these moral preferences? To explore these questions, we conduct a quantitative experiment with six LLMs, ranking and scoring outcomes across 18 dilemmas representing five moral frameworks. Our findings uncover strikingly consistent value biases. Across all models, Care and Virtue values outcomes were rated most moral, while libertarian choices were consistently penalized. Reasoning-enabled models exhibited greater sensitivity to context and provided richer explanations, whereas non-reasoning models produced more uniform but opaque judgments. This research makes three contributions: (i) Empirically, it delivers a large-scale comparison of moral reasoning across culturally distinct LLMs; (ii) Theoretically, it links probabilistic model behaviour with underlying value encodings; (iii) Practically, it highlights the need for explainability and cultural awareness as critical design principles to guide AI toward a transparent, aligned, and symbiotic future.",
        "arxiv_id": "2509.10297",
        "ARXIVID": "2509.10297",
        "COMMENT": "Does not match any specific criterion but is tangentially related to large language models and moral reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09946": {
        "authors": [
            "Vu-Minh Le",
            "Thao-Anh Tran",
            "Duc Huy Do",
            "Xuan Canh Do",
            "Huong Ninh",
            "Hai Tran"
        ],
        "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation",
        "abstract": "arXiv:2509.09946v1 Announce Type: new  Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision task for automating large-scale surveillance. With camera calibration and depth information, the targets in the scene can be projected into 3D space, offering unparalleled levels of automatic perception of a 3D environment. However, tracking in the 3D space requires replacing all 2D tracking components from the ground up, which may be infeasible for existing MTMC systems. In this paper, we present an approach for extending any online 2D multi-camera tracking system into 3D space by utilizing depth information to reconstruct a target in point-cloud space, and recovering its 3D box through clustering and yaw refinement following tracking. We also introduced an enhanced online data association mechanism that leverages the target's local ID consistency to assign global IDs across frames. The proposed framework is evaluated on the 2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the leaderboard.",
        "arxiv_id": "2509.09946",
        "ARXIVID": "2509.09946",
        "COMMENT": "Does not match any specific criteria but is related to 3D multi-camera tracking and perception.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09785": {
        "authors": [
            "Moslem Yazdanpanah",
            "Ali Bahri",
            "Mehrdad Noori",
            "Sahar Dastani",
            "Gustavo Adolfo Vargas Hakim",
            "David Osowiechi",
            "Ismail Ben Ayed",
            "Christian Desrosiers"
        ],
        "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging",
        "abstract": "arXiv:2509.09785v1 Announce Type: new  Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation caused by distribution shifts in 3D point cloud classification. In this work, we introduce Token Purging (PG), a novel backpropagation-free approach that removes tokens highly affected by domain shifts before they reach attention layers. Unlike existing TTA methods, PG operates at the token level, ensuring robust adaptation without iterative updates. We propose two variants: PG-SP, which leverages source statistics, and PG-SF, a fully source-free version relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of +10.3\\% higher accuracy than state-of-the-art backpropagation-free methods, while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is 12.4 times faster and 5.5 times more memory efficient than our baseline, making it suitable for real-world deployment. Code is available at \\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}",
        "arxiv_id": "2509.09785",
        "ARXIVID": "2509.09785",
        "COMMENT": "Does not match any specific criteria but is related to test-time adaptation and point cloud classification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}