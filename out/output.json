{
    "2506.07966": {
        "authors": [
            "Ziyang Gong",
            "Wenhao Li",
            "Oliver Ma",
            "Songyuan Li",
            "Jiayi Ji",
            "Xue Yang",
            "Gen Luo",
            "Junchi Yan",
            "Rongrong Ji"
        ],
        "title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence",
        "abstract": "arXiv:2506.07966v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in various multimodal tasks. To pursue higher intelligence in space, MLLMs require integrating multiple atomic spatial capabilities to handle complex and dynamic tasks. However, existing benchmarks struggle to comprehensively evaluate the spatial intelligence of common MLLMs from the atomic level to the compositional level. To fill this gap, we present SpaCE-10, a comprehensive benchmark for compositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial capabilities, which are combined to form 8 compositional capabilities. Based on these definitions, we propose a novel hierarchical annotation pipeline to generate high-quality and diverse question-answer (QA) pairs. With over 150+ hours of human expert effort, we obtain over 5k QA pairs for 811 real indoor scenes in SpaCE-10, which covers various evaluation settings like point cloud input and multi-choice QA. We conduct an extensive evaluation of common MLLMs on SpaCE-10 and find that even the most advanced MLLM still lags behind humans by large margins. Through our careful study, we also draw several significant findings that benefit the MLLM community. For example, we reveal that the shortcoming of counting capability greatly limits the compositional spatial capabilities of existing MLLMs. The evaluation code and benchmark datasets are available at https://github.com/Cuzyoung/SpaCE-10.",
        "arxiv_id": "2506.07966",
        "ARXIVID": "2506.07966",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for spatial intelligence in MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.07886": {
        "authors": [
            "Gen Li",
            "Yutong Chen",
            "Yiqian Wu",
            "Kaifeng Zhao",
            "Marc Pollefeys",
            "Siyu Tang"
        ],
        "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining",
        "abstract": "arXiv:2506.07886v1 Announce Type: new  Abstract: Understanding multimodal signals in egocentric vision, such as RGB video, depth, camera poses, and gaze, is essential for applications in augmented reality, robotics, and human-computer interaction. These capabilities enable systems to better interpret the camera wearer's actions, intentions, and surrounding environment. However, building large-scale egocentric multimodal and multitask models presents unique challenges. Egocentric data are inherently heterogeneous, with large variations in modality coverage across devices and settings. Generating pseudo-labels for missing modalities, such as gaze or head-mounted camera trajectories, is often infeasible, making standard supervised learning approaches difficult to scale. Furthermore, dynamic camera motion and the complex temporal and spatial structure of first-person video pose additional challenges for the direct application of existing multimodal foundation models.   To address these challenges, we introduce a set of efficient temporal tokenizers and propose EgoM2P, a masked modeling framework that learns from temporally aware multimodal tokens to train a large, general-purpose model for egocentric 4D understanding. This unified design supports multitasking across diverse egocentric perception and synthesis tasks, including gaze prediction, egocentric camera tracking, and monocular depth estimation from egocentric video. EgoM2P also serves as a generative model for conditional egocentric video synthesis. Across these tasks, EgoM2P matches or outperforms specialist models while being an order of magnitude faster. We will fully open-source EgoM2P to support the community and advance egocentric vision research. Project page: https://egom2p.github.io/",
        "arxiv_id": "2506.07886",
        "ARXIVID": "2506.07886",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a multimodal pretraining framework for egocentric vision tasks, relevant to embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.07227": {
        "authors": [
            "Tianyi Bai",
            "Yuxuan Fan",
            "Jiantao Qiu",
            "Fupeng Sun",
            "Jiayi Song",
            "Junlin Han",
            "Zichen Liu",
            "Conghui He",
            "Wentao Zhang",
            "Binhang Yuan"
        ],
        "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning",
        "abstract": "arXiv:2506.07227v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have achieved strong performance on vision-language tasks but still struggle with fine-grained visual differences, leading to hallucinations or missed semantic shifts. We attribute this to limitations in both training data and learning objectives. To address these issues, we propose a controlled data generation pipeline that produces minimally edited image pairs with semantically aligned captions. Using this pipeline, we construct the Micro Edit Dataset (MED), containing over 50K image-text pairs spanning 11 fine-grained edit categories, including attribute, count, position, and object presence changes. Building on MED, we introduce a supervised fine-tuning (SFT) framework with a feature-level consistency loss that promotes stable visual embeddings under small edits. We evaluate our approach on the Micro Edit Detection benchmark, which includes carefully balanced evaluation pairs designed to test sensitivity to subtle visual variations across the same edit categories. Our method improves difference detection accuracy and reduces hallucinations compared to strong baselines, including GPT-4o. Moreover, it yields consistent gains on standard vision-language tasks such as image captioning and visual question answering. These results demonstrate the effectiveness of combining targeted data and alignment objectives for enhancing fine-grained visual reasoning in MLLMs.",
        "arxiv_id": "2506.07227",
        "ARXIVID": "2506.07227",
        "COMMENT": "Matches criteria 2 as it focuses on improving fine-grained visual reasoning in multimodal large language models (MLLMs) with a novel dataset and training strategy.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.07848": {
        "authors": [
            "Teng Hu",
            "Zhentao Yu",
            "Zhengguang Zhou",
            "Jiangning Zhang",
            "Yuan Zhou",
            "Qinglin Lu",
            "Ran Yi"
        ],
        "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement",
        "abstract": "arXiv:2506.07848v1 Announce Type: new  Abstract: Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines.",
        "arxiv_id": "2506.07848",
        "ARXIVID": "2506.07848",
        "COMMENT": "Matches criteria 5 as it proposes a framework for multi-subject video generation with cross-modal interaction, integrating image and language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.08002": {
        "authors": [
            "Aadarsh Sahoo",
            "Vansh Tibrewal",
            "Georgia Gkioxari"
        ],
        "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
        "abstract": "arXiv:2506.08002v1 Announce Type: new  Abstract: Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/",
        "arxiv_id": "2506.08002",
        "ARXIVID": "2506.08002",
        "COMMENT": "Matches criteria 1 and 5 as it focuses on aligning text, images, and 3D structure for tasks like rendering, recognition, and question-answering, which involves spatial intelligence and integration of image and language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.07603": {
        "authors": [
            "Jianhui Wei",
            "Zikai Xiao",
            "Danyu Sun",
            "Luqi Gong",
            "Zongxin Yang",
            "Zuozhu Liu",
            "Jian Wu"
        ],
        "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis",
        "abstract": "arXiv:2506.07603v1 Announce Type: new  Abstract: Surgical video understanding is pivotal for enabling automated intraoperative decision-making, skill assessment, and postoperative quality improvement. However, progress in developing surgical video foundation models (FMs) remains hindered by the scarcity of large-scale, diverse datasets for pretraining and systematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a unified surgical video benchmarking framework comprising a pretraining dataset, \\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}. SurgBench offers extensive coverage of diverse surgical scenarios, with SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11 specialties, and SurgBench-E providing robust evaluation across six categories (phase classification, camera motion, tool recognition, disease diagnosis, action classification, and organ detection) spanning 72 fine-grained tasks. Extensive experiments reveal that existing video FMs struggle to generalize across varied surgical video analysis tasks, whereas pretraining on SurgBench-P yields substantial performance improvements and superior cross-domain generalization to unseen procedures and modalities. Our dataset and code are available upon request.",
        "arxiv_id": "2506.07603",
        "ARXIVID": "2506.07603",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a large-scale benchmark for surgical video analysis.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.06970": {
        "authors": [
            "Pengfei Zhao",
            "Rongbo Luan",
            "Wei Zhang",
            "Peng Wu",
            "Sifeng He"
        ],
        "title": "Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment",
        "abstract": "arXiv:2506.06970v1 Announce Type: new  Abstract: Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability to retrieve content across modalities, a substantial modality gap persists in its feature space. Intriguingly, we discover that off-the-shelf MLLMs (Multimodal Large Language Models) demonstrate powerful inherent modality alignment properties. While recent MLLM-based retrievers with unified architectures partially mitigate this gap, their reliance on coarse modality alignment mechanisms fundamentally limits their potential. In this work, We introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel framework that leverages the fine grained alignment priors inherent in MLLM to guide cross modal representation learning. MAPLE formulates the learning process as reinforcement learning with two key components: (1) Automatic preference data construction using off-the-shelf MLLM, and (2) a new Relative Preference Alignment (RPA) loss, which adapts Direct Preference Optimization (DPO) to the embedding learning setting. Experimental results show that our preference-guided alignment achieves substantial gains in fine-grained cross-modal retrieval, underscoring its effectiveness in handling nuanced semantic distinctions.",
        "arxiv_id": "2506.06970",
        "ARXIVID": "2506.06970",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) due to its focus on guiding cross-modal representations with MLLM priors.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.07672": {
        "authors": [
            "Yunhe Yan",
            "Shihe Wang",
            "Jiajun Du",
            "Yexuan Yang",
            "Yuxuan Shan",
            "Qichen Qiu",
            "Xianqing Jia",
            "Xinge Wang",
            "Xin Yuan",
            "Xu Han",
            "Mao Qin",
            "Yinxiao Chen",
            "Chen Peng",
            "Shangguang Wang",
            "Mengwei Xu"
        ],
        "title": "MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents",
        "abstract": "arXiv:2506.07672v1 Announce Type: new  Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative technique to automate human-computer interaction. However, existing CUA benchmarks predominantly target GUI agents, whose evaluation methods are susceptible to UI changes and ignore function interactions exposed by application APIs, e.g., Model Context Protocol (MCP). To this end, we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid agents. A key principle of MCPWorld is the use of \"white-box apps\", i.e., those with source code availability and can be revised/re-compiled as needed (e.g., adding MCP support), with two notable advantages:   (1) It greatly broadens the design space of CUA, such as what and how the app features to be exposed/extracted as CUA-callable APIs.   (2) It allows MCPWorld to programmatically verify task completion by directly monitoring application behavior through techniques like dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled from specific agent implementations or UI states.   Currently, MCPWorld includes 201 well curated and annotated user tasks, covering diversified use cases and difficulty levels. MCPWorld is also fully containerized with GPU acceleration support for flexible adoption on different OS/hardware environments. Our preliminary experiments, using a representative LLM-powered CUA framework, achieve 75.12% task completion accuracy, simultaneously providing initial evidence on the practical effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate and standardize the benchmarking of next-generation computer use agents that can leverage rich external tools. Our code and dataset are publicly available at https://github.com/SAAgent/MCPWorld.",
        "arxiv_id": "2506.07672",
        "ARXIVID": "2506.07672",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces MCPWorld, a unified benchmarking testbed for computer use agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.07982": {
        "authors": [
            "Victor Barres",
            "Honghua Dong",
            "Soham Ray",
            "Xujie Si",
            "Karthik Narasimhan"
        ],
        "title": "$\\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment",
        "abstract": "arXiv:2506.07982v1 Announce Type: new  Abstract: Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $\\tau^2$-bench, with four key contributions:   1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,   2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,   3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,   4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.   In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $\\tau^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.",
        "arxiv_id": "2506.07982",
        "ARXIVID": "2506.07982",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel benchmark for conversational agents in dual-control environments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.07785": {
        "authors": [
            "Qi Yang",
            "Chenghao Zhang",
            "Lubin Fan",
            "Kun Ding",
            "Jieping Ye",
            "Shiming Xiang"
        ],
        "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger",
        "abstract": "arXiv:2506.07785v1 Announce Type: new  Abstract: Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.",
        "arxiv_id": "2506.07785",
        "ARXIVID": "2506.07785",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on improving LVLMs for VQA tasks using a novel multimodal RAG framework.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.07643": {
        "authors": [
            "Jae Sung Park",
            "Zixian Ma",
            "Linjie Li",
            "Chenhao Zheng",
            "Cheng-Yu Hsieh",
            "Ximing Lu",
            "Khyathi Chandu",
            "Quan Kong",
            "Norimasa Kobori",
            "Ali Farhadi",
            "Yejin Choi",
            "Ranjay Krishna"
        ],
        "title": "Synthetic Visual Genome",
        "abstract": "arXiv:2506.07643v1 Announce Type: new  Abstract: Reasoning over visual relationships-spatial, functional, interactional, social, etc.-is considered to be a fundamental component of human cognition. Yet, despite the major advances in visual comprehension in multimodal language models (MLMs), precise reasoning over relationships and their generations remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely annotated relationships capable of constructing high-quality dense scene graphs at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by completing the missing relations of selected objects in existing scene graphs using a teacher MLM and a carefully designed filtering process to ensure high-quality. To generate more accurate and rich scene graphs at scale for any image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further refines ROBIN's predicted scene graphs by removing unlikely relations and/or suggesting relevant ones. In total, our dataset contains 146K images and 5.6M relationships for 2.6M objects. Results show that our ROBIN-3B model, despite being trained on less than 3 million instances, outperforms similar-size models trained on over 300 million instances on relationship understanding benchmarks, and even surpasses larger models up to 13B parameters. Notably, it achieves state-of-the-art performance in referring expression comprehension with a score of 88.9, surpassing the previous best of 87.4. Our results suggest that training on the refined scene graph data is crucial to maintaining high performance across diverse visual reasoning task.",
        "arxiv_id": "2506.07643",
        "ARXIVID": "2506.07643",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on multimodal language models and their ability to reason over visual relationships.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.07338": {
        "authors": [
            "Yijie Deng",
            "Shuaihang Yuan",
            "Geeta Chandra Raju Bethala",
            "Anthony Tzes",
            "Yu-Shen Liu",
            "Yi Fang"
        ],
        "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation",
        "abstract": "arXiv:2506.07338v1 Announce Type: new  Abstract: Instance Image-Goal Navigation (IIN) requires autonomous agents to identify and navigate to a target object or location depicted in a reference image captured from any viewpoint. While recent methods leverage powerful novel view synthesis (NVS) techniques, such as three-dimensional Gaussian splatting (3DGS), they typically rely on randomly sampling multiple viewpoints or trajectories to ensure comprehensive coverage of discriminative visual cues. This approach, however, creates significant redundancy through overlapping image samples and lacks principled view selection, substantially increasing both rendering and comparison overhead. In this paper, we introduce a novel IIN framework with a hierarchical scoring paradigm that estimates optimal viewpoints for target matching. Our approach integrates cross-level semantic scoring, utilizing CLIP-derived relevancy fields to identify regions with high semantic similarity to the target object class, with fine-grained local geometric scoring that performs precise pose estimation within promising regions. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on simulated IIN benchmarks and real-world applicability.",
        "arxiv_id": "2506.07338",
        "ARXIVID": "2506.07338",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel method for instance image-goal navigation, a task in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.07915": {
        "authors": [
            "Dimitris Panagopoulos",
            "Adolfo Perrusquia",
            "Weisi Guo"
        ],
        "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement",
        "abstract": "arXiv:2506.07915v1 Announce Type: new  Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success.",
        "arxiv_id": "2506.07915",
        "ARXIVID": "2506.07915",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it proposes a framework integrating LLMs and RL for hierarchical decision-making in dynamic environments, which involves spatial reasoning and embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.07971": {
        "authors": [
            "Jiahao Meng",
            "Shuyang Sun",
            "Yue Tan",
            "Lu Qi",
            "Yunhai Tong",
            "Xiangtai Li",
            "Longyin Wen"
        ],
        "title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding",
        "abstract": "arXiv:2506.07971v1 Announce Type: new  Abstract: Current Multimodal Large Language Models (MLLMs) may struggle with understanding long or complex videos due to computational demands at test time, lack of robustness, and limited accuracy, primarily stemming from their feed-forward processing nature. These limitations could be more severe for models with fewer parameters. To address these limitations, we propose a novel framework inspired by cybernetic principles, redesigning video MLLMs as adaptive systems capable of self-monitoring, self-correction, and dynamic resource allocation during inference. Our approach, CyberV, introduces a cybernetic loop consisting of an MLLM Inference System, a Sensor, and a Controller. Specifically, the sensor monitors forward processes of the MLLM and collects intermediate interpretations, such as attention drift, then the controller determines when and how to trigger self-correction and generate feedback to guide the next round. This test-time adaptive scaling framework enhances frozen MLLMs without requiring retraining or additional components. Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0% improvement, achieving performance even comparable to human experts. Furthermore, our method demonstrates consistent gains on general-purpose benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and generalization capabilities in making MLLMs more robust and accurate for dynamic video understanding. The code is released at https://github.com/marinero4972/CyberV.",
        "arxiv_id": "2506.07971",
        "ARXIVID": "2506.07971",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 6 (Video Understanding) as it proposes a novel framework for improving video MLLMs with cybernetic principles.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.08015": {
        "authors": [
            "Zhen Xu",
            "Zhengqin Li",
            "Zhao Dong",
            "Xiaowei Zhou",
            "Richard Newcombe",
            "Zhaoyang Lv"
        ],
        "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos",
        "abstract": "arXiv:2506.08015v1 Announce Type: new  Abstract: We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene reconstruction, trained entirely on real-world monocular posed videos. Using 4D Gaussian as an inductive bias, 4DGT unifies static and dynamic components, enabling the modeling of complex, time-varying environments with varying object lifespans. We proposed a novel density control strategy in training, which enables our 4DGT to handle longer space-time input and remain efficient rendering at runtime. Our model processes 64 consecutive posed frames in a rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike optimization-based methods, 4DGT performs purely feed-forward inference, reducing reconstruction time from hours to seconds and scaling effectively to long video sequences. Trained only on large-scale monocular posed video datasets, 4DGT can outperform prior Gaussian-based networks significantly in real-world videos and achieve on-par accuracy with optimization-based methods on cross-domain videos. Project page: https://4dgt.github.io",
        "arxiv_id": "2506.08015",
        "ARXIVID": "2506.08015",
        "COMMENT": "Matches criteria 6 as it proposes a novel 4D Gaussian Transformer for dynamic scene reconstruction from monocular videos, pushing the boundaries of video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.07235": {
        "authors": [
            "Tianyi Bai",
            "Zengjie Hu",
            "Fupeng Sun",
            "Jiantao Qiu",
            "Yizhen Jiang",
            "Guangxin He",
            "Bohan Zeng",
            "Conghui He",
            "Binhang Yuan",
            "Wentao Zhang"
        ],
        "title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification",
        "abstract": "arXiv:2506.07235v1 Announce Type: new  Abstract: Multi-modal large language models (MLLMs) have achieved remarkable capabilities by integrating visual perception with language understanding, enabling applications such as image-grounded dialogue, visual question answering, and scientific analysis. However, most MLLMs adopt a static inference paradigm, encoding the entire image into fixed visual tokens upfront, which limits their ability to iteratively refine understanding or adapt to context during inference. This contrasts sharply with human perception, which is dynamic, selective, and feedback-driven. In this work, we introduce a novel framework for inference-time visual token scaling that enables MLLMs to perform iterative, verifier-guided reasoning over visual content. We formulate the problem as a Markov Decision Process, involving a reasoner that proposes visual actions and a verifier, which is trained via multi-step Direct Preference Optimization (DPO), that evaluates these actions and determines when reasoning should terminate. To support this, we present a new dataset, VTS, comprising supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning comparisons (VTS-DPO). Our method significantly outperforms existing approaches across diverse visual reasoning benchmarks, offering not only improved accuracy but also more interpretable and grounded reasoning processes. These results demonstrate the promise of dynamic inference mechanisms for enabling fine-grained, context-aware visual reasoning in next-generation MLLMs.",
        "arxiv_id": "2506.07235",
        "ARXIVID": "2506.07235",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a novel framework for iterative visual reasoning in MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.07013": {
        "authors": [
            "Wentao Zhao",
            "Yihe Niu",
            "Yanbo Wang",
            "Tianchen Deng",
            "Shenghai Yuan",
            "Zhenli Wang",
            "Rui Guo",
            "Jingchuan Wang"
        ],
        "title": "UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment",
        "abstract": "arXiv:2506.07013v1 Announce Type: new  Abstract: This work presents UNO, a unified monocular visual odometry framework that enables robust and adaptable pose estimation across diverse environments, platforms, and motion patterns. Unlike traditional methods that rely on deployment-specific tuning or predefined motion priors, our approach generalizes effectively across a wide range of real-world scenarios, including autonomous vehicles, aerial drones, mobile robots, and handheld devices. To this end, we introduce a Mixture-of-Experts strategy for local state estimation, with several specialized decoders that each handle a distinct class of ego-motion patterns. Moreover, we introduce a fully differentiable Gumbel-Softmax module that constructs a robust inter-frame correlation graph, selects the optimal expert decoder, and prunes erroneous estimates. These cues are then fed into a unified back-end that combines pre-trained, scale-independent depth priors with a lightweight bundling adjustment to enforce geometric consistency. We extensively evaluate our method on three major benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV (indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating state-of-the-art performance.",
        "arxiv_id": "2506.07013",
        "ARXIVID": "2506.07013",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a unified monocular visual odometry framework for diverse platforms.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.06537": {
        "authors": [
            "Seung-jae Lee",
            "Paul Hongsuck Seo"
        ],
        "title": "Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models",
        "abstract": "arXiv:2506.06537v1 Announce Type: new  Abstract: Audiovisual segmentation (AVS) aims to identify visual regions corresponding to sound sources, playing a vital role in video understanding, surveillance, and human-computer interaction. Traditional AVS methods depend on large-scale pixel-level annotations, which are costly and time-consuming to obtain. To address this, we propose a novel zero-shot AVS framework that eliminates task-specific training by leveraging multiple pretrained models. Our approach integrates audio, vision, and text representations to bridge modality gaps, enabling precise sound source segmentation without AVS-specific annotations. We systematically explore different strategies for connecting pretrained models and evaluate their efficacy across multiple datasets. Experimental results demonstrate that our framework achieves state-of-the-art zero-shot AVS performance, highlighting the effectiveness of multimodal model integration for finegrained audiovisual segmentation.",
        "arxiv_id": "2506.06537",
        "ARXIVID": "2506.06537",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a zero-shot audiovisual segmentation framework by connecting pretrained models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.06287": {
        "authors": [
            "FutureSearch",
            ":",
            "Nikos I. Bosse",
            "Jon Evans",
            "Robert G. Gambee",
            "Daniel Hnyk",
            "Peter M\\\"uhlbacher",
            "Lawrence Phillips",
            "Dan Schwarz",
            "Jack Wildman"
        ],
        "title": "Deep Research Bench: Evaluating AI Web Research Agents",
        "abstract": "arXiv:2506.06287v1 Announce Type: new  Abstract: Amongst the most common use cases of modern AI is LLM chat with web search enabled. However, no direct evaluations of the quality of web research agents exist that control for the continually-changing web. We introduce Deep Research Bench, consisting of 89 multi-step web research task instances of varying difficulty across 8 diverse task categories, with the answers carefully worked out by skilled humans. We provide a \"RetroSearch\" environment with a large frozen set of scraped web pages, and demonstrate that offline \"RetroSearch\" agents perform comparably to \"live web\" agents, enabling reliable evaluations of models over time. We provide robust agent tooling and scaffolding to benchmark major LLMs as they are released, including \"thinking\" models like o3 and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent traces to report progress over time in hallucinations, tool use, and forgetting. Finally, we evaluate the major web research products branded as \"Deep Research\", \"Deep Search\", \"Search\", or \"Research.\" Results are available on a public leaderboard at https://drb.futuresearch.ai/.",
        "arxiv_id": "2506.06287",
        "ARXIVID": "2506.06287",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces Deep Research Bench, a benchmark for AI web research agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.07371": {
        "authors": [
            "Ruchit Rawal",
            "Reza Shirkavand",
            "Heng Huang",
            "Gowthami Somepalli",
            "Tom Goldstein"
        ],
        "title": "ARGUS: Hallucination and Omission Evaluation in Video-LLMs",
        "abstract": "arXiv:2506.07371v1 Announce Type: new  Abstract: Video large language models have not yet been widely deployed, largely due to their tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on multiple-choice questions. Unfortunately, VideoLLMs hallucinate far more aggressively on freeform text generation tasks like video captioning than they do on multiple choice verification tasks. To address this weakness, we propose ARGUS, a VideoLLM benchmark that measures freeform video captioning performance. By comparing VideoLLM outputs to human ground truth captions, ARGUS quantifies dual metrics. First, we measure the rate of hallucinations in the form of incorrect statements about video content or temporal relationships. Second, we measure the rate at which the model omits important descriptive details. Together, these dual metrics form a comprehensive view of video captioning performance.",
        "arxiv_id": "2506.07371",
        "ARXIVID": "2506.07371",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for evaluating hallucination and omission in Video-LLMs, focusing on video captioning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.07196": {
        "authors": [
            "Mengya Xu",
            "Zhongzhen Huang",
            "Dillan Imans",
            "Yiru Ye",
            "Xiaofan Zhang",
            "Qi Dou"
        ],
        "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning",
        "abstract": "arXiv:2506.07196v1 Announce Type: new  Abstract: Effective evaluation is critical for driving advancements in MLLM research. The surgical action planning (SAP) task, which aims to generate future action sequences from visual inputs, demands precise and sophisticated analytical capabilities. Unlike mathematical reasoning, surgical decision-making operates in life-critical domains and requires meticulous, verifiable processes to ensure reliability and patient safety. This task demands the ability to distinguish between atomic visual actions and coordinate complex, long-horizon procedures, capabilities that are inadequately evaluated by current benchmarks. To address this gap, we introduce SAP-Bench, a large-scale, high-quality dataset designed to enable multimodal large language models (MLLMs) to perform interpretable surgical action planning. Our SAP-Bench benchmark, derived from the cholecystectomy procedures context with the mean duration of 1137.5s, and introduces temporally-grounded surgical action annotations, comprising the 1,226 clinically validated action clips (mean duration: 68.7s) capturing five fundamental surgical actions across 74 procedures. The dataset provides 1,152 strategically sampled current frames, each paired with the corresponding next action as multimodal analysis anchors. We propose the MLLM-SAP framework that leverages MLLMs to generate next action recommendations from the current surgical scene and natural language instructions, enhanced with injected surgical domain knowledge. To assess our dataset's effectiveness and the broader capabilities of current models, we evaluate seven state-of-the-art MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5, Step-1o, and GLM-4v) and reveal critical gaps in next action prediction performance.",
        "arxiv_id": "2506.07196",
        "ARXIVID": "2506.07196",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for MLLMs in surgical action planning, focusing on multimodal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.07491": {
        "authors": [
            "Yongsen Mao",
            "Junhao Zhong",
            "Chuan Fang",
            "Jia Zheng",
            "Rui Tang",
            "Hao Zhu",
            "Ping Tan",
            "Zihan Zhou"
        ],
        "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
        "abstract": "arXiv:2506.07491v1 Announce Type: new  Abstract: SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.",
        "arxiv_id": "2506.07491",
        "ARXIVID": "2506.07491",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a large language model for structured indoor modeling and spatial understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.07986": {
        "authors": [
            "Zhengyao Lv",
            "Tianlin Pan",
            "Chenyang Si",
            "Zhaoxi Chen",
            "Wangmeng Zuo",
            "Ziwei Liu",
            "Kwan-Yee K. Wong"
        ],
        "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
        "abstract": "arXiv:2506.07986v1 Announce Type: new  Abstract: Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention (TACA)}, a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at \\href{https://github.com/Vchitect/TACA}",
        "arxiv_id": "2506.07986",
        "ARXIVID": "2506.07986",
        "COMMENT": "Matches criteria 5 as it proposes a method to improve cross-modal interaction in multimodal diffusion transformers for text-to-image generation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.06830": {
        "authors": [
            "Guankun Wang",
            "Rui Tang",
            "Mengya Xu",
            "Long Bai",
            "Huxin Gao",
            "Hongliang Ren"
        ],
        "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery",
        "abstract": "arXiv:2506.06830v1 Announce Type: new  Abstract: Endoscopic surgery is the gold standard for robotic-assisted minimally invasive surgery, offering significant advantages in early disease detection and precise interventions. However, the complexity of surgical scenes, characterized by high variability in different surgical activity scenarios and confused image features between targets and the background, presents challenges for surgical environment understanding. Traditional deep learning models often struggle with cross-activity interference, leading to suboptimal performance in each downstream task. To address this limitation, we explore multi-task learning, which utilizes the interrelated features between tasks to enhance overall task performance. In this paper, we propose EndoARSS, a novel multi-task learning framework specifically designed for endoscopy surgery activity recognition and semantic segmentation. Built upon the DINOv2 foundation model, our approach integrates Low-Rank Adaptation to facilitate efficient fine-tuning while incorporating Task Efficient Shared Low-Rank Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we introduce the Spatially-Aware Multi-Scale Attention that enhances feature representation discrimination by enabling cross-spatial learning of global information. In order to evaluate the effectiveness of our framework, we present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored for endoscopic surgery scenarios with detailed annotations for both activity recognition and semantic segmentation tasks. Extensive experiments demonstrate that EndoARSS achieves remarkable performance across multiple benchmarks, significantly improving both accuracy and robustness in comparison to existing models. These results underscore the potential of EndoARSS to advance AI-driven endoscopic surgical systems, offering valuable insights for enhancing surgical safety and efficiency.",
        "arxiv_id": "2506.06830",
        "ARXIVID": "2506.06830",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel spatially-aware multi-scale attention mechanism for embodied agents in endoscopic surgery and provides new benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.06780": {
        "authors": [
            "Lennart Bastian",
            "Mohammad Rashed",
            "Nassir Navab",
            "Tolga Birdal"
        ],
        "title": "Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations",
        "abstract": "arXiv:2506.06780v1 Announce Type: new  Abstract: Tracking and forecasting the rotation of objects is fundamental in computer vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor observations can be noisy and sparse, (2) motion patterns can be governed by complex dynamics, and (3) application settings can demand long-term forecasting. This work proposes modeling continuous-time rotational object dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by Savitzky-Golay paths. Unlike existing methods that rely on simplified motion assumptions, our method learns a general latent dynamical system of the underlying object trajectory while respecting the geometric structure of rotations. Experimental results on real-world data demonstrate compelling forecasting capabilities compared to existing approaches.",
        "arxiv_id": "2506.06780",
        "ARXIVID": "2506.06780",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel method for SO(3) forecasting relevant to robotics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07155": {
        "authors": [
            "Van Nguyen Nguyen",
            "Christian Forster",
            "Sindi Shkodrani",
            "Vincent Lepetit",
            "Bugra Tekin",
            "Cem Keskin",
            "Tomas Hodan"
        ],
        "title": "GoTrack: Generic 6DoF Object Pose Refinement and Tracking",
        "abstract": "arXiv:2506.07155v1 Announce Type: new  Abstract: We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF object pose refinement and tracking, which can handle diverse objects without any object-specific training. Unlike existing tracking methods that rely solely on an analysis-by-synthesis approach for model-to-frame registration, GoTrack additionally integrates frame-to-frame registration, which saves compute and stabilizes tracking. Both types of registration are realized by optical flow estimation. The model-to-frame registration is noticeably simpler than in existing methods, relying only on standard neural network blocks (a transformer is trained on top of DINOv2) and producing reliable pose confidence scores without a scoring network. For the frame-to-frame registration, which is an easier problem as consecutive video frames are typically nearly identical, we employ a light off-the-shelf optical flow model. We demonstrate that GoTrack can be seamlessly combined with existing coarse pose estimation methods to create a minimal pipeline that reaches state-of-the-art RGB-only results on standard benchmarks for 6DoF object pose estimation and tracking. Our source code and trained models are publicly available at https://github.com/facebookresearch/gotrack",
        "arxiv_id": "2506.07155",
        "ARXIVID": "2506.07155",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (GoTrack) for 6DoF object pose refinement and tracking, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07811": {
        "authors": [
            "Tieyuan Chen",
            "Huabin Liu",
            "Yi Wang",
            "Chaofan Gan",
            "Mingxi Lyu",
            "Gui Zou",
            "Weiyao Lin"
        ],
        "title": "Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning",
        "abstract": "arXiv:2506.07811v1 Announce Type: new  Abstract: Video Question Answering (VideoQA) aims to answer natural language questions based on the given video, with prior work primarily focusing on identifying the duration of relevant segments, referred to as explicit visual evidence. However, explicit visual evidence is not always directly available, particularly when questions target symbolic meanings or deeper intentions, leading to significant performance degradation. To fill this gap, we introduce a novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo $\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering questions in scenarios where explicit visual evidence is inaccessible. Given an implicit question and its corresponding video, I-VQA requires answering based on the contextual visual cues present within the video. To tackle I-VQA, we propose a novel reasoning framework, IRM (Implicit Reasoning Model), incorporating dual-stream modeling of contextual actions and intent clues as implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the Visual Enhancement Module (VEM). AIM deduces and preserves question-related dual clues by generating clue candidates and performing relation deduction. VEM enhances contextual visual representation by leveraging key contextual clues. Extensive experiments validate the effectiveness of our IRM in I-VQA tasks, outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$, $1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on similar implicit advertisement understanding and future prediction in traffic-VQA. Datasets and codes are available for double-blind review in anonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.",
        "arxiv_id": "2506.07811",
        "ARXIVID": "2506.07811",
        "COMMENT": "Matches criterion 6 as it introduces a novel task and dataset for implicit video question answering, focusing on video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07460": {
        "authors": [
            "Taeryung Lee",
            "Hyeongjin Nam",
            "Gyeongsik Moon",
            "Kyoung Mu Lee"
        ],
        "title": "GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning",
        "abstract": "arXiv:2506.07460v1 Announce Type: new  Abstract: Sign language generation (SLG), or text-to-sign generation, bridges the gap between signers and non-signers. Despite recent progress in SLG, existing methods still often suffer from incorrect lexical ordering and low semantic accuracy. This is primarily due to sentence-level condition, which encodes the entire sentence of the input text into a single feature vector as a condition for SLG. This approach fails to capture the temporal structure of sign language and lacks the granularity of word-level semantics, often leading to disordered sign sequences and ambiguous motions. To overcome these limitations, we propose GLOS, a sign language generation framework with temporally aligned gloss-level conditioning. First, we employ gloss-level conditions, which we define as sequences of gloss embeddings temporally aligned with the motion sequence. This enables the model to access both the temporal structure of sign language and word-level semantics at each timestep. As a result, this allows for fine-grained control of signs and better preservation of lexical order. Second, we introduce a condition fusion module, temporal alignment conditioning (TAC), to efficiently deliver the word-level semantic and temporal structure provided by the gloss-level condition to the corresponding motion timesteps. Our method, which is composed of gloss-level conditions and TAC, generates signs with correct lexical order and high semantic accuracy, outperforming prior methods on CSL-Daily and Phoenix-2014T.",
        "arxiv_id": "2506.07460",
        "ARXIVID": "2506.07460",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a framework for sign language generation with temporally aligned gloss-level conditioning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07576": {
        "authors": [
            "Boyu Chen",
            "Siran Chen",
            "Kunchang Li",
            "Qinglin Xu",
            "Yu Qiao",
            "Yali Wang"
        ],
        "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding",
        "abstract": "arXiv:2506.07576v1 Announce Type: new  Abstract: Video understanding has been considered as one critical step towards world modeling, which is an important long-term problem in AI research. Recently, multi-modal foundation models have shown such potential via large-scale pretraining. However, these models simply align encoders of different modalities via contrastive learning, while lacking deeper multi-modal interactions, which is critical for understanding complex target movements with diversified video scenes. To fill this gap, we propose a unified Super Encoding Network (SEN) for video understanding, which builds up such distinct interactions through recursive association of multi-modal encoders in the foundation models. Specifically, we creatively treat those well-trained encoders as \"super neurons\" in our SEN. Via designing a Recursive Association (RA) block, we progressively fuse multi-modalities with the input video, based on knowledge integrating, distributing, and prompting of super neurons in a recursive manner. In this way, our SEN can effectively encode deeper multi-modal interactions, for prompting various video understanding tasks in downstream. Extensive experiments show that, our SEN can remarkably boost the four most representative video tasks, including tracking, recognition, chatting, and editing, e.g., for pixel-level tracking, the average jaccard index improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular CaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%, and frame consistency increases 4.1% compared to the popular TuneA-Video approach.",
        "arxiv_id": "2506.07576",
        "ARXIVID": "2506.07576",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a novel recursive association of multi-modal encoders for video understanding tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.06725": {
        "authors": [
            "Guillaume Levy",
            "Cedric Colas",
            "Pierre-Yves Oudeyer",
            "Thomas Carta",
            "Clement Romac"
        ],
        "title": "WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making",
        "abstract": "arXiv:2506.06725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model's predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.",
        "arxiv_id": "2506.06725",
        "ARXIVID": "2506.06725",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it presents a novel framework for improving LLM-based world modeling using curiosity-driven exploration and Bayesian inference.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.06928": {
        "authors": [
            "George Lydakis",
            "Alexander Hermans",
            "Ali Athar",
            "Daan de Geus",
            "Bastian Leibe"
        ],
        "title": "How Important are Videos for Training Video LLMs?",
        "abstract": "arXiv:2506.06928v1 Announce Type: new  Abstract: Research into Video Large Language Models (LLMs) has progressed rapidly, with numerous models and benchmarks emerging in just a few years. Typically, these models are initialized with a pretrained text-only LLM and finetuned on both image- and video-caption datasets. In this paper, we present findings indicating that Video LLMs are more capable of temporal reasoning after image-only training than one would assume, and that improvements from video-specific training are surprisingly small. Specifically, we show that image-trained versions of two LLMs trained with the recent LongVU algorithm perform significantly above chance level on TVBench, a temporal reasoning benchmark. Additionally, we introduce a simple finetuning scheme involving sequences of annotated images and questions targeting temporal capabilities. This baseline results in temporal reasoning performance close to, and occasionally higher than, what is achieved by video-trained LLMs. This suggests suboptimal utilization of rich temporal features found in real video by current models. Our analysis motivates further research into the mechanisms that allow image-trained LLMs to perform temporal reasoning, as well as into the bottlenecks that render current video training schemes inefficient.",
        "arxiv_id": "2506.06928",
        "ARXIVID": "2506.06928",
        "COMMENT": "Matches criteria 2 as it explores Video Large Language Models (Video LLMs) and their training strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.08006": {
        "authors": [
            "Sicheng Mo",
            "Ziyang Leng",
            "Leon Liu",
            "Weizhen Wang",
            "Honglin He",
            "Bolei Zhou"
        ],
        "title": "Dreamland: Controllable World Creation with Simulator and Generative Models",
        "abstract": "arXiv:2506.08006v1 Announce Type: new  Abstract: Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available.",
        "arxiv_id": "2506.08006",
        "ARXIVID": "2506.08006",
        "COMMENT": "Matches criterion 3 as it introduces a hybrid framework for world generation combining simulators and generative models, which can enhance embodied agent training.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2506.06988": {
        "authors": [
            "Binxiao Huang",
            "Zhihao Li",
            "Shiyong Liu",
            "Xiao Tang",
            "Jiajun Tang",
            "Jiaqi Lin",
            "Yuxin Cheng",
            "Zhenyu Chen",
            "Xiaofei Wu",
            "Ngai Wong"
        ],
        "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction",
        "abstract": "arXiv:2506.06988v1 Announce Type: new  Abstract: 3D Gaussian splatting (3DGS) has demonstrated exceptional performance in image-based 3D reconstruction and real-time rendering. However, regions with complex textures require numerous Gaussians to capture significant color variations accurately, leading to inefficiencies in rendering speed. To address this challenge, we introduce a hybrid representation for indoor scenes that combines 3DGS with textured meshes. Our approach uses textured meshes to handle texture-rich flat areas, while retaining Gaussians to model intricate geometries. The proposed method begins by pruning and refining the extracted mesh to eliminate geometrically complex regions. We then employ a joint optimization for 3DGS and mesh, incorporating a warm-up strategy and transmittance-aware supervision to balance their contributions seamlessly.Extensive experiments demonstrate that the hybrid representation maintains comparable rendering quality and achieves superior frames per second FPS with fewer Gaussian primitives.",
        "arxiv_id": "2506.06988",
        "ARXIVID": "2506.06988",
        "COMMENT": "Matches criteria 1 as it focuses on efficient indoor scene reconstruction using hybrid mesh-Gaussian representation, which involves spatial intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07002": {
        "authors": [
            "Yunxiao Shi",
            "Hong Cai",
            "Jisoo Jeong",
            "Yinhao Zhu",
            "Shizhong Han",
            "Amin Ansari",
            "Fatih Porikli"
        ],
        "title": "BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction",
        "abstract": "arXiv:2506.07002v1 Announce Type: new  Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene understanding which is critical for autonomous driving. Most existing methods, however, carry high compute costs, requiring dense 3D feature volume and cross-attention to effectively aggregate information. More recent works have adopted Bird's Eye View (BEV) or sparse points as scene representation with much reduced cost, but still suffer from their respective shortcomings. More concretely, BEV struggles with small objects that often experience significant information loss after being projected to the ground plane. On the other hand, points can flexibly model little objects in 3D, but is inefficient at capturing flat surfaces or large objects. To address these challenges, in this paper, we present a novel 3D occupancy prediction approach, BePo, which combines BEV and sparse points based representations. We propose a dual-branch design: a query-based sparse points branch and a BEV branch. The 3D information learned in the sparse points branch is shared with the BEV stream via cross-attention, which enriches the weakened signals of difficult objects on the BEV plane. The outputs of both branches are finally fused to generate predicted 3D occupancy. We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks that demonstrate the superiority of our proposed BePo. Moreover, BePo also delivers competitive inference speed when compared to the latest efficient approaches.",
        "arxiv_id": "2506.07002",
        "ARXIVID": "2506.07002",
        "COMMENT": "Matches criteria 1 as it focuses on 3D occupancy prediction for autonomous driving, which involves spatial intelligence and reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07375": {
        "authors": [
            "Xunjie He",
            "Christina Dao Wen Lee",
            "Meiling Wang",
            "Chengran Yuan",
            "Zefan Huang",
            "Yufeng Yue",
            "Marcelo H. Ang Jr"
        ],
        "title": "DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models",
        "abstract": "arXiv:2506.07375v1 Announce Type: new  Abstract: Collaborative perception plays a crucial role in enhancing environmental understanding by expanding the perceptual range and improving robustness against sensor failures, which primarily involves collaborative 3D detection and tracking tasks. The former focuses on object recognition in individual frames, while the latter captures continuous instance tracklets over time. However, existing works in both areas predominantly focus on the vehicle superclass, lacking effective solutions for both multi-class collaborative detection and tracking. This limitation hinders their applicability in real-world scenarios, which involve diverse object classes with varying appearances and motion patterns. To overcome these limitations, we propose a multi-class collaborative detection and tracking framework tailored for diverse road users. We first present a detector with a global spatial attention fusion (GSAF) module, enhancing multi-scale feature learning for objects of varying sizes. Next, we introduce a tracklet RE-IDentification (REID) module that leverages visual semantics with a vision foundation model to effectively reduce ID SWitch (IDSW) errors, in cases of erroneous mismatches involving small objects like pedestrians. We further design a velocity-based adaptive tracklet management (VATM) module that adjusts the tracking interval dynamically based on object motion. Extensive experiments on the V2X-Real and OPV2V datasets show that our approach significantly outperforms existing state-of-the-art methods in both detection and tracking accuracy.",
        "arxiv_id": "2506.07375",
        "ARXIVID": "2506.07375",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it leverages vision foundation models for collaborative detection and tracking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07943": {
        "authors": [
            "Yizhen Li",
            "Dell Zhang",
            "Xuelong Li",
            "Yiqing Shen"
        ],
        "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations",
        "abstract": "arXiv:2506.07943v1 Announce Type: new  Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires segmenting objects based on implicit text queries, demanding both precise visual perception and vision-text reasoning capabilities. Current RS approaches rely on fine-tuning vision-language models (VLMs) for both perception and reasoning, but their tokenization of images fundamentally disrupts continuous spatial relationships between objects. We introduce DTwinSeger, a novel RS approach that leverages Digital Twin (DT) representation as an intermediate layer to decouple perception from reasoning. Innovatively, DTwinSeger reformulates RS as a two-stage process, where the first transforms the image into a structured DT representation that preserves spatial relationships and semantic properties and then employs a Large Language Model (LLM) to perform explicit reasoning over this representation to identify target objects. We propose a supervised fine-tuning method specifically for LLM with DT representation, together with a corresponding fine-tuning dataset Seg-DT, to enhance the LLM's reasoning capabilities with DT representations. Experiments show that our method can achieve state-of-the-art performance on two image RS benchmarks and three image referring segmentation benchmarks. It yields that DT representation functions as an effective bridge between vision and text, enabling complex multimodal reasoning tasks to be accomplished solely with an LLM.",
        "arxiv_id": "2506.07943",
        "ARXIVID": "2506.07943",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a novel approach for reasoning segmentation using digital twin representations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.06822": {
        "authors": [
            "Chenlu Zhan",
            "Yufei Zhang",
            "Gaoang Wang",
            "Hongwei Wang"
        ],
        "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting",
        "abstract": "arXiv:2506.06822v1 Announce Type: new  Abstract: Modeling 3D language fields with Gaussian Splatting for open-ended language queries has recently garnered increasing attention. However, recent 3DGS-based models leverage view-dependent 2D foundation models to refine 3D semantics but lack a unified 3D representation, leading to view inconsistencies. Additionally, inherent open-vocabulary challenges cause inconsistencies in object and relational descriptions, impeding hierarchical semantic understanding. In this paper, we propose Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. To achieve view-consistent 3D hierarchical semantics, we first lift 2D features to 3D features by constructing a 3D hierarchical semantic tree with layered instance clustering, which addresses the view inconsistency issue caused by 2D semantic features. Besides, we introduce instance-wise and part-wise contrastive losses to capture all-sided hierarchical semantic representations. Notably, we construct two hierarchical semantic datasets to better assess the model's ability to distinguish different semantic levels. Extensive experiments highlight our method's superiority in 3D open-vocabulary segmentation and localization. Its strong performance on hierarchical semantic datasets underscores its ability to capture complex hierarchical semantics within 3D scenes.",
        "arxiv_id": "2506.06822",
        "ARXIVID": "2506.06822",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a method for hierarchical 3D language querying with a focus on view-consistent 3D semantics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.06944": {
        "authors": [
            "Mellon M. Zhang",
            "Glen Chou",
            "Saibal Mukhopadhyay"
        ],
        "title": "Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences",
        "abstract": "arXiv:2506.06944v1 Announce Type: new  Abstract: Accurate and efficient object detection is essential for autonomous vehicles, where real-time perception requires low latency and high throughput. LiDAR sensors provide robust depth information, but conventional methods process full 360{\\deg} scans in a single pass, introducing significant delay. Streaming approaches address this by sequentially processing partial scans in the native polar coordinate system, yet they rely on translation-invariant convolutions that are misaligned with polar geometry -- resulting in degraded performance or requiring complex distortion mitigation. Recent Mamba-based state space models (SSMs) have shown promise for LiDAR perception, but only in the full-scan setting, relying on geometric serialization and positional embeddings that are memory-intensive and ill-suited to streaming. We propose Polar Hierarchical Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial encoding and a global forward Mamba for inter-sector temporal modeling, replacing convolutions and positional encodings with distortion-aware, dimensionally-decomposed operations. PHiM sets a new state-of-the-art among streaming detectors on the Waymo Open Dataset, outperforming the previous best by 10\\% and matching full-scan baselines at twice the throughput. Code will be available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .",
        "arxiv_id": "2506.06944",
        "ARXIVID": "2506.06944",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel architecture for streaming LiDAR object detection in autonomous vehicles.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07548": {
        "authors": [
            "Weiqiang Jin",
            "Hongyang Du",
            "Guizhong Liu",
            "Dong In Kim"
        ],
        "title": "Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2506.07548v1 Announce Type: new  Abstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at https://github.com/NICE-HKU/CL2MARL-SMAC.",
        "arxiv_id": "2506.07548",
        "ARXIVID": "2506.07548",
        "COMMENT": "Matches criteria 3 as it introduces a novel curriculum learning framework for multi-agent reinforcement learning, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07464": {
        "authors": [
            "Jinyoung Park",
            "Jeehye Na",
            "Jinyoung Kim",
            "Hyunwoo J. Kim"
        ],
        "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO",
        "abstract": "arXiv:2506.07464v1 Announce Type: new  Abstract: Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks.",
        "arxiv_id": "2506.07464",
        "ARXIVID": "2506.07464",
        "COMMENT": "Matches criteria 6 as it focuses on video reasoning tasks and introduces a novel reinforcement learning-based method for video LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.08011": {
        "authors": [
            "Yunfei Xie",
            "Yinsong Ma",
            "Shiyi Lan",
            "Alan Yuille",
            "Junfei Xiao",
            "Chen Wei"
        ],
        "title": "Play to Generalize: Learning to Reason Through Game Play",
        "abstract": "arXiv:2506.08011v1 Announce Type: new  Abstract: Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs.",
        "arxiv_id": "2506.08011",
        "ARXIVID": "2506.08011",
        "COMMENT": "Matches criteria 2 as it explores a novel post-training paradigm for multimodal reasoning in large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.06918": {
        "authors": [
            "Carl Brander",
            "Giovanni Cioffi",
            "Nico Messikommer",
            "Davide Scaramuzza"
        ],
        "title": "Reading in the Dark with Foveated Event Vision",
        "abstract": "arXiv:2506.06918v1 Announce Type: new  Abstract: Current smart glasses equipped with RGB cameras struggle to perceive the environment in low-light and high-speed motion scenarios due to motion blur and the limited dynamic range of frame cameras. Additionally, capturing dense images with a frame camera requires large bandwidth and power consumption, consequently draining the battery faster. These challenges are especially relevant for developing algorithms that can read text from images. In this work, we propose a novel event-based Optical Character Recognition (OCR) approach for smart glasses. By using the eye gaze of the user, we foveate the event stream to significantly reduce bandwidth by around 98% while exploiting the benefits of event cameras in high-dynamic and fast scenes. Our proposed method performs deep binary reconstruction trained on synthetic data and leverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our results demonstrate the ability to read text in low light environments where RGB cameras struggle while using up to 2400 times less bandwidth than a wearable RGB camera.",
        "arxiv_id": "2506.06918",
        "ARXIVID": "2506.06918",
        "COMMENT": "Matches criteria 1 and 2 closely as it involves spatial intelligence for embodied agents and multimodal LLMs for OCR in challenging environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07091": {
        "authors": [
            "Yangkai Lin",
            "Jiabao Lei",
            "Kui Jia"
        ],
        "title": "SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model",
        "abstract": "arXiv:2506.07091v1 Announce Type: new  Abstract: Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation of complex, interactive indoor scenes tailored to user prompt remains a formidable challenge. While existing methods achieve indoor scene synthesis, they struggle with rigid editing constraints, physical incoherence, excessive human effort, single-room limitations, and suboptimal material quality. To address these limitations, we propose SceneLCM, an end-to-end framework that synergizes Large Language Model (LLM) for layout design with Latent Consistency Model(LCM) for scene optimization. Our approach decomposes scene generation into four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D spatial reasoning to convert textual descriptions into parametric blueprints(3D layout). And an iterative programmatic validation mechanism iteratively refines layout parameters through LLM-mediated dialogue loops; (2) Furniture Generation. SceneLCM employs Consistency Trajectory Sampling(CTS), a consistency distillation sampling loss guided by LCM, to form fast, semantically rich, and high-quality representations. We also offer two theoretical justification to demonstrate that our CTS loss is equivalent to consistency loss and its distillation error is bounded by the truncation error of the Euler solver; (3) Environment Optimization. We use a multiresolution texture field to encode the appearance of the scene, and optimize via CTS loss. To maintain cross-geometric texture coherence, we introduce a normal-aware cross-attention decoder to predict RGB by cross-attending to the anchors locations in geometrically heterogeneous instance. (4)Physically Editing. SceneLCM supports physically editing by integrating physical simulation, achieved persistent physical realism. Extensive experiments validate SceneLCM's superiority over state-of-the-art techniques, showing its wide-ranging potential for diverse applications.",
        "arxiv_id": "2506.07091",
        "ARXIVID": "2506.07091",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes a framework combining LLMs and latent consistency models for indoor scene generation, which is a real-world application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07564": {
        "authors": [
            "Peiran Li",
            "Xinkai Zou",
            "Zhuohang Wu",
            "Ruifeng Li",
            "Shuo Xing",
            "Hanwen Zheng",
            "Zhikai Hu",
            "Yuping Wang",
            "Haoxi Li",
            "Qin Yuan",
            "Yingmo Zhang",
            "Zhengzhong Tu"
        ],
        "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems",
        "abstract": "arXiv:2506.07564v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.",
        "arxiv_id": "2506.07564",
        "ARXIVID": "2506.07564",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a protocol for secure and robust multi-modal agent systems using LLMs and VLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07310": {
        "authors": [
            "Adam W. Harley",
            "Yang You",
            "Xinglong Sun",
            "Yang Zheng",
            "Nikhil Raghuraman",
            "Yunqi Gu",
            "Sheldon Liang",
            "Wen-Hsuan Chu",
            "Achal Dave",
            "Pavel Tokmakov",
            "Suya You",
            "Rares Ambrus",
            "Katerina Fragkiadaki",
            "Leonidas J. Guibas"
        ],
        "title": "AllTracker: Efficient Dense Point Tracking at High Resolution",
        "abstract": "arXiv:2506.07310v1 Announce Type: new  Abstract: We introduce AllTracker: a model that estimates long-range point tracks by way of estimating the flow field between a query frame and every other frame of a video. Unlike existing point tracking methods, our approach delivers high-resolution and dense (all-pixel) correspondence fields, which can be visualized as flow maps. Unlike existing optical flow methods, our approach corresponds one frame to hundreds of subsequent frames, rather than just the next frame. We develop a new architecture for this task, blending techniques from existing work in optical flow and point tracking: the model performs iterative inference on low-resolution grids of correspondence estimates, propagating information spatially via 2D convolution layers, and propagating information temporally via pixel-aligned attention layers. The model is fast and parameter-efficient (16 million parameters), and delivers state-of-the-art point tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on a 40G GPU). A benefit of our design is that we can train on a wider set of datasets, and we find that doing so is crucial for top performance. We provide an extensive ablation study on our architecture details and training recipe, making it clear which details matter most. Our code and model weights are available at https://alltracker.github.io .",
        "arxiv_id": "2506.07310",
        "ARXIVID": "2506.07310",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel method for long-range point tracking in videos, which is relevant for video-based tasks like correspondence and flow estimation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07698": {
        "authors": [
            "Yuxiao Yang",
            "Peihao Li",
            "Yuhong Zhang",
            "Junzhe Lu",
            "Xianglong He",
            "Minghan Qin",
            "Weitao Wang",
            "Haoqian Wang"
        ],
        "title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation",
        "abstract": "arXiv:2506.07698v1 Announce Type: new  Abstract: 3D AI-generated content (AIGC) has made it increasingly accessible for anyone to become a 3D content creator. While recent methods leverage Score Distillation Sampling to distill 3D objects from pretrained image diffusion models, they often suffer from inadequate 3D priors, leading to insufficient multi-view consistency. In this work, we introduce NOVA3D, an innovative single-image-to-3D generation framework. Our key insight lies in leveraging strong 3D priors from a pretrained video diffusion model and integrating geometric information during multi-view video fine-tuning. To facilitate information exchange between color and geometric domains, we propose the Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving generalization and multi-view consistency. Moreover, we introduce the de-conflict geometry fusion algorithm, which improves texture fidelity by addressing multi-view inaccuracies and resolving discrepancies in pose alignment. Extensive experiments validate the superiority of NOVA3D over existing baselines.",
        "arxiv_id": "2506.07698",
        "ARXIVID": "2506.07698",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a framework for single-image-to-3D generation leveraging video diffusion models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07570": {
        "authors": [
            "Yixuan Yang",
            "Zhen Luo",
            "Tongsheng Ding",
            "Junru Lu",
            "Mingqi Gao",
            "Jinyu Yang",
            "Victor Sanchez",
            "Feng Zheng"
        ],
        "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization",
        "abstract": "arXiv:2506.07570v1 Announce Type: new  Abstract: Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.",
        "arxiv_id": "2506.07570",
        "ARXIVID": "2506.07570",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on indoor scene layout generation and spatial reasoning for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07177": {
        "authors": [
            "Sangwon Jang",
            "Taekyung Ki",
            "Jaehyeong Jo",
            "Jaehong Yoon",
            "Soo Ye Kim",
            "Zhe Lin",
            "Sung Ju Hwang"
        ],
        "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models",
        "abstract": "arXiv:2506.07177v1 Announce Type: new  Abstract: Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.",
        "arxiv_id": "2506.07177",
        "ARXIVID": "2506.07177",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel training-free guidance for controllable video generation using diffusion models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07489": {
        "authors": [
            "Yahao Shi",
            "Yang Liu",
            "Yanmin Wu",
            "Xing Liu",
            "Chen Zhao",
            "Jie Luo",
            "Bin Zhou"
        ],
        "title": "Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video",
        "abstract": "arXiv:2506.07489v1 Announce Type: new  Abstract: We propose DriveAnyMesh, a method for driving mesh guided by monocular video. Current 4D generation techniques encounter challenges with modern rendering engines. Implicit methods have low rendering efficiency and are unfriendly to rasterization-based engines, while skeletal methods demand significant manual effort and lack cross-category generalization. Animating existing 3D assets, instead of creating 4D assets from scratch, demands a deep understanding of the input's 3D structure. To tackle these challenges, we present a 4D diffusion model that denoises sequences of latent sets, which are then decoded to produce mesh animations from point cloud trajectory sequences. These latent sets leverage a transformer-based variational autoencoder, simultaneously capturing 3D shape and motion information. By employing a spatiotemporal, transformer-based diffusion model, information is exchanged across multiple latent frames, enhancing the efficiency and generalization of the generated results. Our experimental results demonstrate that DriveAnyMesh can rapidly produce high-quality animations for complex motions and is compatible with modern rendering engines. This method holds potential for applications in both the gaming and filming industries.",
        "arxiv_id": "2506.07489",
        "ARXIVID": "2506.07489",
        "COMMENT": "Matches criteria 6 as it introduces a 4D diffusion model for mesh deformation from video, advancing video-based tasks and applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07778": {
        "authors": [
            "Yichang Xu",
            "Gaowen Liu",
            "Ramana Rao Kompella",
            "Sihao Hu",
            "Tiansheng Huang",
            "Fatih Ilhan",
            "Selim Furkan Tekin",
            "Zachary Yahn",
            "Ling Liu"
        ],
        "title": "Language-Vision Planner and Executor for Text-to-Visual Reasoning",
        "abstract": "arXiv:2506.07778v1 Announce Type: new  Abstract: The advancement in large language models (LLMs) and large vision models has fueled the rapid progress in multi-modal visual-text reasoning capabilities. However, existing vision-language models (VLMs) to date suffer from generalization performance. Inspired by recent development in LLMs for visual reasoning, this paper presents VLAgent, an AI system that can create a step-by-step visual reasoning plan with an easy-to-understand script and execute each step of the plan in real time by integrating planning script with execution verifications via an automated process supported by VLAgent. In the task planning phase, VLAgent fine-tunes an LLM through in-context learning to generate a step-by-step planner for each user-submitted text-visual reasoning task. During the plan execution phase, VLAgent progressively refines the composition of neuro-symbolic executable modules to generate high-confidence reasoning results. VLAgent has three unique design characteristics: First, we improve the quality of plan generation through in-context learning, improving logic reasoning by reducing erroneous logic steps, incorrect programs, and LLM hallucinations. Second, we design a syntax-semantics parser to identify and correct additional logic errors of the LLM-generated planning script prior to launching the plan executor. Finally, we employ the ensemble method to improve the generalization performance of our step-executor. Extensive experiments with four visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent achieves significant performance enhancement for multimodal text-visual reasoning applications, compared to the exiting representative VLMs and LLM based visual composition approaches like ViperGPT and VisProg, thanks to the novel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer, Output Verifiers). Code and data will be made available upon paper acceptance.",
        "arxiv_id": "2506.07778",
        "ARXIVID": "2506.07778",
        "COMMENT": "Matches criteria 2 and 5 as it explores a language-vision planner and executor for text-to-visual reasoning, integrating vision and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07280": {
        "authors": [
            "Pablo Acuaviva",
            "Aram Davtyan",
            "Mariam Hassan",
            "Sebastian Stapf",
            "Ahmad Rahimi",
            "Alexandre Alahi",
            "Paolo Favaro"
        ],
        "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models",
        "abstract": "arXiv:2506.07280v1 Announce Type: new  Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools, capable of synthesizing high-quality spatiotemporal content. Yet, their potential goes far beyond mere video generation. We argue that the training dynamics of VDMs, driven by the need to model coherent sequences, naturally pushes them to internalize structured representations and an implicit understanding of the visual world. To probe the extent of this internal knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs for new tasks using only a handful of examples. Our method transforms each task into a visual transition, enabling the training of LoRA weights on short input-output sequences without altering the generative interface of a frozen VDM. Despite minimal supervision, the model exhibits strong generalization across diverse tasks, from low-level vision (for example, segmentation and pose estimation) to high-level reasoning (for example, on ARC-AGI). These results reframe VDMs as more than generative engines. They are adaptable visual learners with the potential to serve as the backbone for future foundation models in vision.",
        "arxiv_id": "2506.07280",
        "ARXIVID": "2506.07280",
        "COMMENT": "Matches criteria 2 and 6 as it explores Video Diffusion Models (VDMs) for video understanding tasks and highlights their potential as adaptable visual learners.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.06698": {
        "authors": [
            "Yitao Liu",
            "Chenglei Si",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "title": "Contextual Experience Replay for Self-Improvement of Language Agents",
        "abstract": "arXiv:2506.06698v1 Announce Type: new  Abstract: Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.",
        "arxiv_id": "2506.06698",
        "ARXIVID": "2506.06698",
        "COMMENT": "Matches criteria 3 as it introduces a training-free framework for self-improvement in language agents, relevant to embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07255": {
        "authors": [
            "Jake Tuero",
            "Michael Buro",
            "Levi H. S. Lelis"
        ],
        "title": "Subgoal-Guided Policy Heuristic Search with Learned Subgoals",
        "abstract": "arXiv:2506.07255v1 Announce Type: new  Abstract: Policy tree search is a family of tree search algorithms that use a policy to guide the search. These algorithms provide guarantees on the number of expansions required to solve a given problem that are based on the quality of the policy. While these algorithms have shown promising results, the process in which they are trained requires complete solution trajectories to train the policy. Search trajectories are obtained during a trial-and-error search process. When the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. As a result, search samples are wasted in failed attempts to solve these hard instances. This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts. We empirically show that our policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in this online setting.",
        "arxiv_id": "2506.07255",
        "ARXIVID": "2506.07255",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for learning subgoal-based policies in embodied AI, improving sample efficiency in policy tree search.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.07223": {
        "authors": [
            "Yangqing Zheng",
            "Shunqi Mao",
            "Dingxin Zhang",
            "Weidong Cai"
        ],
        "title": "LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments",
        "abstract": "arXiv:2506.07223v1 Announce Type: new  Abstract: In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.",
        "arxiv_id": "2506.07223",
        "ARXIVID": "2506.07223",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for embodied agents in latency-sensitive scenarios.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.07436": {
        "authors": [
            "Nishi Chaudhary",
            "S M Jamil Uddin",
            "Sathvik Sharath Chandra",
            "Anto Ovid",
            "Alex Albert"
        ],
        "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
        "abstract": "arXiv:2506.07436v1 Announce Type: new  Abstract: The recent emergence of multimodal large language models (LLMs) has introduced new opportunities for improving visual hazard recognition on construction sites. Unlike traditional computer vision models that rely on domain-specific training and extensive datasets, modern LLMs can interpret and describe complex visual scenes using simple natural language prompts. However, despite growing interest in their applications, there has been limited investigation into how different LLMs perform in safety-critical visual tasks within the construction domain. To address this gap, this study conducts a comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify potential hazards from real-world construction images. Each model was tested under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated basic safety context and a hazard source mnemonic, and CoT provided step-by-step reasoning examples to scaffold model thinking. Quantitative analysis was performed using precision, recall, and F1-score metrics across all conditions. Results reveal that prompting strategy significantly influenced performance, with CoT prompting consistently producing higher accuracy across models. Additionally, LLM performance varied under different conditions, with GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also demonstrate the critical role of prompt design in enhancing the accuracy and consistency of multimodal LLMs for construction safety applications. This study offers actionable insights into the integration of prompt engineering and LLMs for practical hazard recognition, contributing to the development of more reliable AI-assisted safety systems.",
        "arxiv_id": "2506.07436",
        "ARXIVID": "2506.07436",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it evaluates multimodal LLMs for hazard recognition in construction.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.06981": {
        "authors": [
            "Riley Simmons-Edler",
            "Ryan P. Badman",
            "Felix Baastad Berg",
            "Raymond Chua",
            "John J. Vastola",
            "Joshua Lunger",
            "William Qian",
            "Kanaka Rajan"
        ],
        "title": "Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments",
        "abstract": "arXiv:2506.06981v1 Announce Type: new  Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents -- particularly as task and agent sophistication increase -- requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging -- including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics -- without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals -- analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics -- uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential -- not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied.",
        "arxiv_id": "2506.06981",
        "ARXIVID": "2506.06981",
        "COMMENT": "Matches criteria 1 as it explores spatial reasoning and planning-like behavior in reinforcement learning agents.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.07497": {
        "authors": [
            "Xiangyu Guo",
            "Zhanqian Wu",
            "Kaixin Xiong",
            "Ziyang Xu",
            "Lijun Zhou",
            "Gangwei Xu",
            "Shaoqing Xu",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency",
        "abstract": "arXiv:2506.07497v1 Announce Type: new  Abstract: We present Genesis, a unified framework for joint generation of multi-view driving videos and LiDAR sequences with spatio-temporal and cross-modal consistency. Genesis employs a two-stage architecture that integrates a DiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR generator with NeRF-based rendering and adaptive sampling. Both modalities are directly coupled through a shared latent space, enabling coherent evolution across visual and geometric domains. To guide the generation with structured semantics, we introduce DataCrafter, a captioning module built on vision-language models that provides scene-level and instance-level supervision. Extensive experiments on the nuScenes benchmark demonstrate that Genesis achieves state-of-the-art performance across video and LiDAR metrics (FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including segmentation and 3D detection, validating the semantic fidelity and practical utility of the generated data.",
        "arxiv_id": "2506.07497",
        "ARXIVID": "2506.07497",
        "COMMENT": "Matches criteria 2 and 5 as it proposes a unified framework for multimodal driving scene generation with spatio-temporal and cross-modal consistency.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.06898": {
        "authors": [
            "Reese Kneeland",
            "Paul S. Scotti",
            "Ghislain St-Yves",
            "Jesse Breedlove",
            "Kendrick Kay",
            "Thomas Naselaris"
        ],
        "title": "NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery",
        "abstract": "arXiv:2506.06898v1 Announce Type: new  Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired with mental images, to complement the existing Natural Scenes Dataset (NSD), a large-scale dataset of fMRI activity paired with seen images that enabled unprecedented improvements in fMRI-to-image reconstruction efforts. Recent models trained on NSD have been evaluated only on seen image reconstruction. Using NSD-Imagery, it is possible to assess how well these models perform on mental image reconstruction. This is a challenging generalization requirement because mental images are encoded in human brain activity with relatively lower signal-to-noise and spatial resolution; however, generalization from seen to mental imagery is critical for real-world applications in medical domains and brain-computer interfaces, where the desired information is always internally generated. We provide benchmarks for a suite of recent NSD-trained open-source visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.) on NSD-Imagery, and show that the performance of decoding methods on mental images is largely decoupled from performance on vision reconstruction. We further demonstrate that architectural choices significantly impact cross-decoding performance: models employing simple linear decoding architectures and multimodal feature decoding generalize better to mental imagery, while complex architectures tend to overfit visual training data. Our findings indicate that mental imagery datasets are critical for the development of practical applications, and establish NSD-Imagery as a useful resource for better aligning visual decoding methods with this goal.",
        "arxiv_id": "2506.06898",
        "ARXIVID": "2506.06898",
        "COMMENT": "Matches criteria 6 as it introduces a benchmark dataset for mental imagery decoding, which is related to video understanding and vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.07636": {
        "authors": [
            "Haoran Wang",
            "Zhenyu Hou",
            "Yao Wei",
            "Jie Tang",
            "Yuxiao Dong"
        ],
        "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling",
        "abstract": "arXiv:2506.07636v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at https://github.com/THUDM/SWE-Dev.",
        "arxiv_id": "2506.07636",
        "ARXIVID": "2506.07636",
        "COMMENT": "Matches criteria 3 as it introduces SWE-Dev, a new method and benchmark for software engineering agents, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.07116": {
        "authors": [
            "Liyang Chen",
            "Yujun Cai",
            "Jieqiong Dong",
            "Yiwei Wang"
        ],
        "title": "BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite",
        "abstract": "arXiv:2506.07116v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both structurally clean and semantically coherent. BRIGHT is a recent and influential benchmark designed to evaluate complex multi-hop retrieval across diverse, high-reasoning domains. However, its practical effectiveness is limited by common web-crawled artifacts - such as content redundancy and semantic discontinuity - that impair retrieval accuracy and downstream reasoning. Notably, we find that such issues are concentrated in seven StackExchange-derived subdomains, while other domains (e.g., Coding and Theorem-based content) remain relatively clean.   In this study, we present MARCUS, a multi-agent pipeline that leverages large language models (LLMs) to systematically clean and re-chunk BRIGHT into a higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for structural noise removal and semantic segmentation, preserving answer-bearing spans while improving contextual integrity. Experimental evaluations demonstrate that BRIGHT-Plus yields consistent and significant improvements in both retrieval accuracy and multi-hop reasoning across a diverse set of retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to support future research on robust, reasoning-centric retrieval.",
        "arxiv_id": "2506.07116",
        "ARXIVID": "2506.07116",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it upgrades a benchmark for retrieval-augmented generation systems with a focus on reasoning-centric retrieval.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.07045": {
        "authors": [
            "Yikun Ji",
            "Hong Yan",
            "Jun Lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Qi Fan",
            "Liqing Zhang",
            "Jianfu Zhang"
        ],
        "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs",
        "abstract": "arXiv:2506.07045v1 Announce Type: new  Abstract: The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing human-understandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AI-generated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct a dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing a foundation for human-aligned visual-textual grounded reasoning. We then finetune MLLMs through a multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods.",
        "arxiv_id": "2506.07045",
        "ARXIVID": "2506.07045",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on detecting AI-generated images using grounded reasoning in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.06909": {
        "authors": [
            "Vladimir Yugay",
            "Thies Kersten",
            "Luca Carlone",
            "Theo Gevers",
            "Martin R. Oswald",
            "Lukas Schmid"
        ],
        "title": "Gaussian Mapping for Evolving Scenes",
        "abstract": "arXiv:2506.06909v1 Announce Type: new  Abstract: Mapping systems with novel view synthesis (NVS) capabilities are widely used in computer vision, with augmented reality, robotics, and autonomous driving applications. Most notably, 3D Gaussian Splatting-based systems show high NVS performance; however, many current approaches are limited to static scenes. While recent works have started addressing short-term dynamics (motion within the view of the camera), long-term dynamics (the scene evolving through changes out of view) remain less explored. To overcome this limitation, we introduce a dynamic scene adaptation mechanism that continuously updates the 3D representation to reflect the latest changes. In addition, since maintaining geometric and semantic consistency remains challenging due to stale observations disrupting the reconstruction process, we propose a novel keyframe management mechanism that discards outdated observations while preserving as much information as possible. We evaluate Gaussian Mapping for Evolving Scenes (GaME) on both synthetic and real-world datasets and find it to be more accurate than the state of the art.",
        "arxiv_id": "2506.06909",
        "ARXIVID": "2506.06909",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it addresses dynamic scene adaptation and novel view synthesis in evolving scenes.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.07999": {
        "authors": [
            "Junhao Chen",
            "Yulia Tsvetkov",
            "Xiaochuang Han"
        ],
        "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation",
        "abstract": "arXiv:2506.07999v1 Announce Type: new  Abstract: Recent progress in multimodal generation has increasingly combined autoregressive (AR) and diffusion-based approaches, leveraging their complementary strengths: AR models capture long-range dependencies and produce fluent, context-aware outputs, while diffusion models operate in continuous latent spaces to refine high-fidelity visual details. However, existing hybrids often lack systematic guidance on how and why to allocate model capacity between these paradigms. In this work, we introduce MADFormer, a Mixed Autoregressive and Diffusion Transformer that serves as a testbed for analyzing AR-diffusion trade-offs. MADFormer partitions image generation into spatial blocks, using AR layers for one-pass global conditioning across blocks and diffusion layers for iterative local refinement within each block. Through controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights: (1) block-wise partitioning significantly improves performance on high-resolution images, and (2) vertically mixing AR and diffusion layers yields better quality-efficiency balances--improving FID by up to 75% under constrained inference compute. Our findings offer practical design principles for future hybrid generative models.",
        "arxiv_id": "2506.07999",
        "ARXIVID": "2506.07999",
        "COMMENT": "Matches criteria 5 as it introduces MADFormer, a hybrid generative model combining autoregressive and diffusion approaches for image generation, relevant to vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.07905": {
        "authors": [
            "Jie Yang",
            "Feipeng Ma",
            "Zitian Wang",
            "Dacheng Yin",
            "Kang Rong",
            "Fengyun Rao",
            "Ruimao Zhang"
        ],
        "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning",
        "abstract": "arXiv:2506.07905v1 Announce Type: new  Abstract: Building on the success of text-based reasoning models like DeepSeek-R1, extending these capabilities to multimodal reasoning holds great promise. While recent works have attempted to adapt DeepSeek-R1-style reinforcement learning (RL) training paradigms to multimodal large language models (MLLM), focusing on domain-specific tasks like math and visual perception, a critical question remains: How can we achieve the general-purpose visual-language reasoning through RL? To address this challenge, we make three key efforts: (1) A novel Scalable Multimodal QA Synthesis pipeline that autonomously generates context-aware, reasoning-centric question-answer (QA) pairs directly from the given images. (2) The open-source WeThink dataset containing over 120K multimodal QA pairs with annotated reasoning paths, curated from 18 diverse dataset sources and covering various question domains. (3) A comprehensive exploration of RL on our dataset, incorporating a hybrid reward mechanism that combines rule-based verification with model-based assessment to optimize RL training efficiency across various task domains. Across 14 diverse MLLM benchmarks, we demonstrate that our WeThink dataset significantly enhances performance, from mathematical reasoning to diverse general multimodal tasks. Moreover, we show that our automated data pipeline can continuously increase data diversity to further improve model performance.",
        "arxiv_id": "2506.07905",
        "ARXIVID": "2506.07905",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores reinforcement learning for general-purpose vision-language reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.07016": {
        "authors": [
            "Sanjoy Chowdhury",
            "Mohamed Elmoghany",
            "Yohan Abeysinghe",
            "Junjie Fei",
            "Sayan Nag",
            "Salman Khan",
            "Mohamed Elhoseiny",
            "Dinesh Manocha"
        ],
        "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks",
        "abstract": "arXiv:2506.07016v1 Announce Type: new  Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual understanding, yet they struggle with real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video question answering remain limited in scope, typically involving one clip per query, which falls short of representing the challenges of large-scale, audio-visual retrieval and reasoning encountered in practical applications. To bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance. Project: https://schowdhury671.github.io/magnet_project/",
        "arxiv_id": "2506.07016",
        "ARXIVID": "2506.07016",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 6 (Video Understanding) as it introduces a multimodal framework for reasoning over multi-video collections.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.08009": {
        "authors": [
            "Xun Huang",
            "Zhengqi Li",
            "Guande He",
            "Mingyuan Zhou",
            "Eli Shechtman"
        ],
        "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
        "abstract": "arXiv:2506.08009v1 Announce Type: new  Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
        "arxiv_id": "2506.08009",
        "ARXIVID": "2506.08009",
        "COMMENT": "Matches criterion 6 as it introduces a novel training paradigm for autoregressive video diffusion models, advancing video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 8
    },
    "2506.06966": {
        "authors": [
            "Siyuan Jing",
            "Guangxue Wang",
            "Haoyang Zhai",
            "Qin Tao",
            "Jun Yang",
            "Bing Wang",
            "Peng Jin"
        ],
        "title": "Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition",
        "abstract": "arXiv:2506.06966v1 Announce Type: new  Abstract: Due to the emergence of many sign language datasets, isolated sign language recognition (ISLR) has made significant progress in recent years. In addition, the development of various advanced deep neural networks is another reason for this breakthrough. However, challenges remain in applying the technique in the real world. First, existing sign language datasets do not cover the whole sign vocabulary. Second, most of the sign language datasets provide only single view RGB videos, which makes it difficult to handle hand occlusions when performing ISLR. To fill this gap, this paper presents a dual-view sign language dataset for ISLR named NationalCSL-DP, which fully covers the Chinese national sign language vocabulary. The dataset consists of 134140 sign videos recorded by ten signers with respect to two vertical views, namely, the front side and the left side. Furthermore, a CNN transformer network is also proposed as a strong baseline and an extremely simple but effective fusion strategy for prediction. Extensive experiments were conducted to prove the effectiveness of the datasets as well as the baseline. The results show that the proposed fusion strategy can significantly increase the performance of the ISLR, but it is not easy for the sequence-to-sequence model, regardless of whether the early-fusion or late-fusion strategy is applied, to learn the complementary features from the sign videos of two vertical views.",
        "arxiv_id": "2506.06966",
        "ARXIVID": "2506.06966",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a dual-view dataset and a method for sign language recognition, a video-based task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.07936": {
        "authors": [
            "Chengyue Huang",
            "Yuchen Zhu",
            "Sichen Zhu",
            "Jingyun Xiao",
            "Moises Andrade",
            "Shivang Chopra",
            "Zsolt Kira"
        ],
        "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models",
        "abstract": "arXiv:2506.07936v1 Announce Type: new  Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context learning (ICL), a property similar to that of their language-only counterparts. While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies show they often rely on shallow heuristics -- such as copying or majority voting -- rather than true task understanding. We revisit this assumption by evaluating VLMs under distribution shifts, where support examples come from a dataset different from the query. Surprisingly, performance often degrades with more demonstrations, and models tend to copy answers rather than learn from them. To investigate further, we propose a new MM-ICL with Reasoning pipeline that augments each demonstration with a generated rationale alongside the answer. We conduct extensive and comprehensive experiments on both perception- and reasoning-required datasets with open-source VLMs ranging from 3B to 72B and proprietary models such as Gemini 2.0. We conduct controlled studies varying shot count, retrieval method, rationale quality, and distribution. Our results show limited performance sensitivity across these factors, suggesting that current VLMs do not effectively utilize demonstration-level information as intended in MM-ICL.",
        "arxiv_id": "2506.07936",
        "ARXIVID": "2506.07936",
        "COMMENT": "Matches criterion 2 as it evaluates multimodal in-context learning in Vision-Language Models (VLMs), focusing on reasoning and task understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.07202": {
        "authors": [
            "Ming Liu",
            "Wensheng Zhang"
        ],
        "title": "Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation",
        "abstract": "arXiv:2506.07202v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language benchmark performance, yet growing concerns about data contamination (test set exposure during training) risk masking true generalization. This concern extends to reasoning MLLMs, often fine-tuned via reinforcement learning from potentially contaminated base models. We propose a novel dynamic evaluation framework to rigorously assess MLLM generalization, moving beyond static benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the same visual input, models are evaluated across a family of tasks (e.g., QA, captioning, question posing, verification) to probe diverse capabilities. This task perturbation reveals whether model performance is robust or reliant on superficial task-specific cues. Our approach is analogous to loss landscape sharpness: models overfit or contaminated for a single task (sharp minima) falter under task shifts, unlike models with generalizable solutions (flatter minima). We developed an automated pipeline with a calibrated judge scoring open-ended generations (captions, questions) using paraphrase and corruption sampling. Applying this framework to leading image/video MLLMs on benchmarks including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task \"ability vector.\" We demonstrate that fine-tuning on simulated test data (extreme contamination) drastically sharpens task-specific performance but harms overall generalization. Our dynamic task perturbation offers deeper insights into MLLM generalization, distinguishing genuine understanding from spurious leakage or overfitting.",
        "arxiv_id": "2506.07202",
        "ARXIVID": "2506.07202",
        "COMMENT": "Matches criterion 2 as it explores generalization and evaluation strategies for Multimodal Large Language Models (MLLMs).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.07739": {
        "authors": [
            "Jing Zhong",
            "Jun Yin",
            "Peilin Li",
            "Pengyu Zeng",
            "Miao Zhang",
            "Shuai Lu",
            "Ran Luo"
        ],
        "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models",
        "abstract": "arXiv:2506.07739v1 Announce Type: new  Abstract: Architectural cultures across regions are characterized by stylistic diversity, shaped by historical, social, and technological contexts in addition to geograph-ical conditions. Understanding architectural styles requires the ability to describe and analyze the stylistic features of different architects from various regions through visual observations of architectural imagery. However, traditional studies of architectural culture have largely relied on subjective expert interpretations and historical literature reviews, often suffering from regional biases and limited ex-planatory scope. To address these challenges, this study proposes three core contributions: (1) We construct a professional architectural style dataset named ArchDiffBench, which comprises 1,765 high-quality architectural images and their corresponding style annotations, collected from different regions and historical periods. (2) We propose ArchiLense, an analytical framework grounded in Vision-Language Models and constructed using the ArchDiffBench dataset. By integrating ad-vanced computer vision techniques, deep learning, and machine learning algo-rithms, ArchiLense enables automatic recognition, comparison, and precise classi-fication of architectural imagery, producing descriptive language outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show that ArchiLense achieves strong performance in architectural style recognition, with a 92.4% con-sistency rate with expert annotations and 84.5% classification accuracy, effec-tively capturing stylistic distinctions across images. The proposed approach transcends the subjectivity inherent in traditional analyses and offers a more objective and accurate perspective for comparative studies of architectural culture.",
        "arxiv_id": "2506.07739",
        "ARXIVID": "2506.07739",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores a framework based on Vision-Language Models for analyzing architectural styles.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.07996": {
        "authors": [
            "Ming-Feng Li",
            "Xin Yang",
            "Fu-En Wang",
            "Hritam Basak",
            "Yuyin Sun",
            "Shreekant Gayaka",
            "Min Sun",
            "Cheng-Hao Kuo"
        ],
        "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References",
        "abstract": "arXiv:2506.07996v1 Announce Type: new  Abstract: 6D object pose estimation has shown strong generalizability to novel objects. However, existing methods often require either a complete, well-reconstructed 3D model or numerous reference images that fully cover the object. Estimating 6D poses from partial references, which capture only fragments of an object's appearance and geometry, remains challenging. To address this, we propose UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online object completion specifically designed for partial references. We assume access to either (1) a limited set of RGBD images with known poses or (2) a single 2D image. For the first case, we initialize a partial object 3D model based on the provided images and poses, while for the second, we use image-to-3D techniques to generate an initial object 3D model. Our method integrates uncertainty into the incomplete 3D model, distinguishing between seen and unseen regions. This uncertainty enables confidence assessment in pose estimation and guides an uncertainty-aware sampling strategy for online object completion, enhancing robustness in pose estimation accuracy and improving object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and HO3D datasets, including RGBD sequences of YCB objects manipulated by robots and human hands. Experimental results demonstrate significant performance improvements over existing methods, particularly when object observations are incomplete or partially captured. Project page: https://minfenli.github.io/UA-Pose/",
        "arxiv_id": "2506.07996",
        "ARXIVID": "2506.07996",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 6D object pose estimation and online object completion, relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2506.07539": {
        "authors": [
            "Xiaomeng Zhu",
            "Jacob Henningsson",
            "Duruo Li",
            "P\\\"ar M{\\aa}rtensson",
            "Lars Hanson",
            "M{\\aa}rten Bj\\\"orkman",
            "Atsuto Maki"
        ],
        "title": "Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study",
        "abstract": "arXiv:2506.07539v1 Announce Type: new  Abstract: This paper addresses key aspects of domain randomization in generating synthetic data for manufacturing object detection applications. To this end, we present a comprehensive data generation pipeline that reflects different factors: object characteristics, background, illumination, camera settings, and post-processing. We also introduce the Synthetic Industrial Parts Object Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use cases under varying environments as a test bed for the study, while also employing an industrial dataset publicly available for robotic applications. In our experiments, we present more abundant results and insights into the feasibility as well as challenges of sim-to-real object detection. In particular, we identified material properties, rendering methods, post-processing, and distractors as important factors. Our method, leveraging these, achieves top performance on the public dataset with Yolov8 models trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases, respectively. The results showcase the effectiveness of the proposed domain randomization, potentially covering the distribution close to real data for the applications.",
        "arxiv_id": "2506.07539",
        "ARXIVID": "2506.07539",
        "COMMENT": "Matches criteria 3 as it introduces a comprehensive study on domain randomization for object detection in manufacturing, which is relevant to embodied/robotic AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.06631": {
        "authors": [
            "Minghao Zou",
            "Qingtian Zeng",
            "Yongping Miao",
            "Shangkun Liu",
            "Zilong Wang",
            "Hantao Liu",
            "Wei Zhou"
        ],
        "title": "PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments",
        "abstract": "arXiv:2506.06631v1 Announce Type: new  Abstract: Visual parsing of images and videos is critical for a wide range of real-world applications. However, progress in this field is constrained by limitations of existing datasets: (1) insufficient annotation granularity, which impedes fine-grained scene understanding and high-level reasoning; (2) limited coverage of domains, particularly a lack of datasets tailored for educational scenarios; and (3) lack of explicit procedural guidance, with minimal logical rules and insufficient representation of structured task process. To address these gaps, we introduce PhysLab, the first video dataset that captures students conducting complex physics experiments. The dataset includes four representative experiments that feature diverse scientific instruments and rich human-object interaction (HOI) patterns. PhysLab comprises 620 long-form videos and provides multilevel annotations that support a variety of vision tasks, including action recognition, object detection, HOI analysis, etc. We establish strong baselines and perform extensive evaluations to highlight key challenges in the parsing of procedural educational videos. We expect PhysLab to serve as a valuable resource for advancing fine-grained visual parsing, facilitating intelligent classroom systems, and fostering closer integration between computer vision and educational technologies. The dataset and the evaluation toolkit are publicly available at https://github.com/ZMH-SDUST/PhysLab.",
        "arxiv_id": "2506.06631",
        "ARXIVID": "2506.06631",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark dataset for visual parsing in educational physics experiments, which is relevant for embodied AI and fine-grained visual understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.06952": {
        "authors": [
            "Ying Shen",
            "Zhiyang Xu",
            "Jiuhai Chen",
            "Shizhe Diao",
            "Jiaxin Zhang",
            "Yuguang Yao",
            "Joy Rimchala",
            "Ismini Lourentzou",
            "Lifu Huang"
        ],
        "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer",
        "abstract": "arXiv:2506.06952v1 Announce Type: new  Abstract: Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.",
        "arxiv_id": "2506.06952",
        "ARXIVID": "2506.06952",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a unified multimodal model for image understanding and generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.07600": {
        "authors": [
            "Nianbo Zeng",
            "Haowen Hou",
            "Fei Richard Yu",
            "Si Shi",
            "Ying Tiffany He"
        ],
        "title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding",
        "abstract": "arXiv:2506.07600v1 Announce Type: new  Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video understanding, effectively understanding long-form video content remains underexplored due to the vast scale and high complexity of video data. Current RAG approaches typically segment videos into fixed-length chunks, which often disrupts the continuity of contextual information and fails to capture authentic scene boundaries. Inspired by the human ability to naturally organize continuous experiences into coherent scenes, we present SceneRAG, a unified framework that leverages large language models to segment videos into narrative-consistent scenes by processing ASR transcripts alongside temporal metadata. SceneRAG further sharpens these initial boundaries through lightweight heuristics and iterative correction. For each scene, the framework fuses information from both visual and textual modalities to extract entity relations and dynamically builds a knowledge graph, enabling robust multi-hop retrieval and generation that account for long-range dependencies. Experiments on the LongerVideos benchmark, featuring over 134 hours of diverse content, confirm that SceneRAG substantially outperforms prior baselines, achieving a win rate of up to 72.5 percent on generation tasks.",
        "arxiv_id": "2506.07600",
        "ARXIVID": "2506.07600",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel framework for scene-level video understanding using multimodal data.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.08013": {
        "authors": [
            "Anh-Quan Cao",
            "Ivan Lopes",
            "Raoul de Charette"
        ],
        "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets",
        "abstract": "arXiv:2506.08013v1 Announce Type: new  Abstract: Multi-task learning for dense prediction is limited by the need for extensive annotation for every task, though recent works have explored training with partial task labels. Leveraging the generalization power of diffusion models, we extend the partial learning setup to a zero-shot setting, training a multi-task model on multiple synthetic datasets, each labeled for only a subset of tasks. Our method, StableMTL, repurposes image generators for latent regression. Adapting a denoising framework with task encoding, per-task conditioning and a tailored training scheme. Instead of per-task losses requiring careful balancing, a unified latent loss is adopted, enabling seamless scaling to more tasks. To encourage inter-task synergy, we introduce a multi-stream model with a task-attention mechanism that converts N-to-N task interactions into efficient 1-to-N attention, promoting effective cross-task sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.",
        "arxiv_id": "2506.08013",
        "ARXIVID": "2506.08013",
        "COMMENT": "Matches criteria 3 as it introduces a novel multi-task learning framework leveraging diffusion models and addresses challenges in partially annotated datasets.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.08010": {
        "authors": [
            "Nick Jiang",
            "Amil Dravid",
            "Alexei Efros",
            "Yossi Gandelsman"
        ],
        "title": "Vision Transformers Don't Need Trained Registers",
        "abstract": "arXiv:2506.08010v1 Announce Type: new  Abstract: We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.",
        "arxiv_id": "2506.08010",
        "ARXIVID": "2506.08010",
        "COMMENT": "Matches criteria 4 as it explores improvements in Vision Transformers, focusing on mitigating noisy attention maps and enhancing downstream tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.07565": {
        "authors": [
            "Jinlu Zhang",
            "Zixi Kang",
            "Yizhou Wang"
        ],
        "title": "OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data",
        "abstract": "arXiv:2506.07565v1 Announce Type: new  Abstract: Music-driven dance generation offers significant creative potential yet faces considerable challenges. The absence of fine-grained multimodal data and the difficulty of flexible multi-conditional generation limit previous works on generation controllability and diversity in practice. In this paper, we build OpenDance5D, an extensive human dance dataset comprising over 101 hours across 14 distinct genres. Each sample has five modalities to facilitate robust cross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and fine-grained textual descriptions from human arts. Furthermore, we propose OpenDanceNet, a unified masked modeling framework for controllable dance generation conditioned on music and arbitrary combinations of text prompts, keypoints, or character positioning. Comprehensive experiments demonstrate that OpenDanceNet achieves high-fidelity and flexible controllability.",
        "arxiv_id": "2506.07565",
        "ARXIVID": "2506.07565",
        "COMMENT": "Matches criterion 6 as it focuses on multimodal controllable 3D dance generation, which involves video understanding and generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.08003": {
        "authors": [
            "Shuchen Weng",
            "Haojie Zheng",
            "Zheng Chang",
            "Si Li",
            "Boxin Shi",
            "Xinlong Wang"
        ],
        "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control",
        "abstract": "arXiv:2506.08003v1 Announce Type: new  Abstract: Audio is inherently temporal and closely synchronized with the visual world, making it a naturally aligned and expressive control signal for controllable video generation (e.g., movies). Beyond control, directly translating audio into video is essential for understanding and visualizing rich audio narratives (e.g., Podcasts or historical recordings). However, existing approaches fall short in generating high-quality videos with precise audio-visual synchronization, especially across diverse and complex audio types. In this work, we introduce MTV, a versatile framework for audio-sync video generation. MTV explicitly separates audios into speech, effects, and music tracks, enabling disentangled control over lip motion, event timing, and visual mood, respectively -- resulting in fine-grained and semantically aligned video generation. To support the framework, we additionally present DEMIX, a dataset comprising high-quality cinematic videos and demixed audio tracks. DEMIX is structured into five overlapped subsets, enabling scalable multi-stage training for diverse generation scenarios. Extensive experiments demonstrate that MTV achieves state-of-the-art performance across six standard metrics spanning video quality, text-video consistency, and audio-video alignment. Project page: https://hjzheng.net/projects/MTV/.",
        "arxiv_id": "2506.08003",
        "ARXIVID": "2506.08003",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding through audio-sync video generation with a novel framework (MTV).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.07697": {
        "authors": [
            "Jens Piekenbrinck",
            "Christian Schmidt",
            "Alexander Hermans",
            "Narunas Vaskevicius",
            "Timm Linder",
            "Bastian Leibe"
        ],
        "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting",
        "abstract": "arXiv:2506.07697v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for neural scene reconstruction, offering high-quality novel view synthesis while maintaining computational efficiency. In this paper, we extend the capabilities of 3DGS beyond pure scene representation by introducing an approach for open-vocabulary 3D instance segmentation without requiring manual labeling, termed OpenSplat3D. Our method leverages feature-splatting techniques to associate semantic information with individual Gaussians, enabling fine-grained scene understanding. We incorporate Segment Anything Model instance masks with a contrastive loss formulation as guidance for the instance features to achieve accurate instance-level segmentation. Furthermore, we utilize language embeddings of a vision-language model, allowing for flexible, text-driven instance identification. This combination enables our system to identify and segment arbitrary objects in 3D scenes based on natural language descriptions. We show results on LERF-mask and LERF-OVS as well as the full ScanNet++ validation set, demonstrating the effectiveness of our approach.",
        "arxiv_id": "2506.07697",
        "ARXIVID": "2506.07697",
        "COMMENT": "Matches criterion 4 as it extends 3D Gaussian Splatting for open-vocabulary 3D instance segmentation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.07304": {
        "authors": [
            "Kavitha Viswanathan",
            "Vrinda Goel",
            "Shlesh Gholap",
            "Devayan Ghosh",
            "Madhav Gupta",
            "Dhruvi Ganatra",
            "Sanket Potdar",
            "Amit Sethi"
        ],
        "title": "FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos",
        "abstract": "arXiv:2506.07304v1 Announce Type: new  Abstract: Real-world surveillance often renders faces and license plates unrecognizable in individual low-resolution (LR) frames, hindering reliable identification. To advance temporal recognition models, we present FANVID, a novel video-based benchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63 identities and 49 license plates from three English-speaking countries. Each video includes distractor faces and plates, increasing task difficulty and realism. The dataset contains 31,096 manually verified bounding boxes and labels.   FANVID defines two tasks: (1) face matching -- detecting LR faces and matching them to high-resolution mugshots, and (2) license plate recognition -- extracting text from LR plates without a predefined database. Videos are downsampled from high-resolution sources to ensure that faces and text are indecipherable in single frames, requiring models to exploit temporal information. We introduce evaluation metrics adapted from mean Average Precision at IoU > 0.5, prioritizing identity correctness for faces and character-level accuracy for text.   A baseline method with pre-trained video super-resolution, detection, and recognition achieved performance scores of 0.58 (face matching) and 0.42 (plate recognition), highlighting both the feasibility and challenge of the tasks. FANVID's selection of faces and plates balances diversity with recognition challenge. We release the software for data access, evaluation, baseline, and annotation to support reproducibility and extension. FANVID aims to catalyze innovation in temporal modeling for LR recognition, with applications in surveillance, forensics, and autonomous vehicles.",
        "arxiv_id": "2506.07304",
        "ARXIVID": "2506.07304",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (FANVID) for low-resolution video-based face and license plate recognition, relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.07205": {
        "authors": [
            "Min-Jung Kim",
            "Dongjin Kim",
            "Seokju Yun",
            "Jaegul Choo"
        ],
        "title": "TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation",
        "abstract": "arXiv:2506.07205v1 Announce Type: new  Abstract: Video editing has garnered increasing attention alongside the rapid progress of diffusion-based video generation models. As part of these advancements, there is a growing demand for more accessible and controllable forms of video editing, such as prompt-based editing. Previous studies have primarily focused on tasks such as style transfer, background replacement, object substitution, and attribute modification, while maintaining the content structure of the source video. However, more complex tasks, including the addition of novel objects and nonrigid transformations, remain relatively unexplored. In this paper, we present TV-LiVE, a Training-free and text-guided Video editing framework via Layerinformed Vitality Exploitation. We empirically identify vital layers within the video generation model that significantly influence the quality of generated outputs. Notably, these layers are closely associated with Rotary Position Embeddings (RoPE). Based on this observation, our method enables both object addition and non-rigid video editing by selectively injecting key and value features from the source model into the corresponding layers of the target model guided by the layer vitality. For object addition, we further identify prominent layers to extract the mask regions corresponding to the newly added target prompt. We found that the extracted masks from the prominent layers faithfully indicate the region to be edited. Experimental results demonstrate that TV-LiVE outperforms existing approaches for both object addition and non-rigid video editing. Project Page: https://emjay73.github.io/TV_LiVE/",
        "arxiv_id": "2506.07205",
        "ARXIVID": "2506.07205",
        "COMMENT": "Matches criterion 6 as it focuses on video editing tasks and novel methodologies for video understanding.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.06843": {
        "authors": [
            "HaoYang Shang",
            "Xuan Liu",
            "Zi Liang",
            "Jie Zhang",
            "Haibo Hu",
            "Song Guo"
        ],
        "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory",
        "abstract": "arXiv:2506.06843v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM's effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.",
        "arxiv_id": "2506.06843",
        "ARXIVID": "2506.06843",
        "COMMENT": "Matches criterion 1 as it introduces a novel multi-agent framework (CoThinker) for improving reasoning and problem-solving in LLMs, which could be relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.08005": {
        "authors": [
            "Lei Lai",
            "Zekai Yin",
            "Eshed Ohn-Bar"
        ],
        "title": "ZeroVO: Visual Odometry with Minimal Assumptions",
        "abstract": "arXiv:2506.08005v1 Announce Type: new  Abstract: We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves zero-shot generalization across diverse cameras and environments, overcoming limitations in existing methods that depend on predefined or static camera calibration setups. Our approach incorporates three main innovations. First, we design a calibration-free, geometry-aware network structure capable of handling noise in estimated depth and camera parameters. Second, we introduce a language-based prior that infuses semantic information to enhance robust feature extraction and generalization to previously unseen domains. Third, we develop a flexible, semi-supervised training paradigm that iteratively adapts to new scenes using unlabeled data, further boosting the models' ability to generalize across diverse real-world scenarios. We analyze complex autonomous driving contexts, demonstrating over 30% improvement against prior methods on three standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly introduced, high-fidelity synthetic dataset derived from Grand Theft Auto (GTA). By not requiring fine-tuning or camera calibration, our work broadens the applicability of VO, providing a versatile solution for real-world deployment at scale.",
        "arxiv_id": "2506.08005",
        "ARXIVID": "2506.08005",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel visual odometry algorithm with zero-shot generalization.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.07891": {
        "authors": [
            "Simone Facchiano",
            "Stefano Saravalle",
            "Matteo Migliarini",
            "Edoardo De Matteis",
            "Alessio Sampieri",
            "Andrea Pilzer",
            "Emanuele Rodol\\`a",
            "Indro Spinelli",
            "Luca Franco",
            "Fabio Galasso"
        ],
        "title": "Video Unlearning via Low-Rank Refusal Vector",
        "abstract": "arXiv:2506.07891v1 Announce Type: new  Abstract: Video generative models democratize the creation of visual content through intuitive instruction following, but they also inherit the biases and harmful concepts embedded within their web-scale training data. This inheritance creates a significant risk, as users can readily generate undesirable and even illegal content. This work introduces the first unlearning technique tailored explicitly for video diffusion models to address this critical issue. Our method requires 5 multi-modal prompt pairs only. Each pair contains a \"safe\" and an \"unsafe\" example that differ only by the target concept. Averaging their per-layer latent differences produces a \"refusal vector\", which, once subtracted from the model parameters, neutralizes the unsafe concept. We introduce a novel low-rank factorization approach on the covariance difference of embeddings that yields robust refusal vectors. This isolates the target concept while minimizing collateral unlearning of other semantics, thus preserving the visual quality of the generated video. Our method preserves the model's generation quality while operating without retraining or access to the original training data. By embedding the refusal direction directly into the model's weights, the suppression mechanism becomes inherently more robust against adversarial bypass attempts compared to surface-level input-output filters. In a thorough qualitative and quantitative evaluation, we show that we can neutralize a variety of harmful contents, including explicit nudity, graphic violence, copyrights, and trademarks. Project page: https://www.pinlab.org/video-unlearning.",
        "arxiv_id": "2506.07891",
        "ARXIVID": "2506.07891",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel unlearning technique for video diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.07977": {
        "authors": [
            "Jingjing Chang",
            "Yixiao Fang",
            "Peng Xing",
            "Shuhan Wu",
            "Wei Cheng",
            "Rui Wang",
            "Xianfang Zeng",
            "Gang Yu",
            "Hai-Bao Chen"
        ],
        "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
        "abstract": "arXiv:2506.07977v1 Announce Type: new  Abstract: Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community.",
        "arxiv_id": "2506.07977",
        "ARXIVID": "2506.07977",
        "COMMENT": "Matches criteria 4 as it introduces a benchmark for evaluating text-to-image models, focusing on their reasoning and alignment capabilities.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.07878": {
        "authors": [
            "Muhammad Ahmed Humais",
            "Xiaoqian Huang",
            "Hussain Sajwani",
            "Sajid Javed",
            "Yahya Zweiri"
        ],
        "title": "Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow",
        "abstract": "arXiv:2506.07878v1 Announce Type: new  Abstract: Event cameras unlock new frontiers that were previously unthinkable with standard frame-based cameras. One notable example is low-latency motion estimation (optical flow), which is critical for many real-time applications. In such applications, the computational efficiency of algorithms is paramount. Although recent deep learning paradigms such as CNN, RNN, or ViT have shown remarkable performance, they often lack the desired computational efficiency. Conversely, asynchronous event-based methods including SNNs and GNNs are computationally efficient; however, these approaches fail to capture sufficient spatio-temporal information, a powerful feature required to achieve better performance for optical flow estimation. In this work, we introduce Spatio-Temporal State Space Model (STSSM) module along with a novel network architecture to develop an extremely efficient solution with competitive performance. Our STSSM module leverages state-space models to effectively capture spatio-temporal correlations in event data, offering higher performance with lower complexity compared to ViT, CNN-based architectures in similar settings. Our model achieves 4.5x faster inference and 8x lower computations compared to TMA and 2x lower computations compared to EV-FlowNet with competitive performance on the DSEC benchmark. Our code will be available at https://github.com/AhmedHumais/E-STMFlow",
        "arxiv_id": "2506.07878",
        "ARXIVID": "2506.07878",
        "COMMENT": "Matches criteria 3 as it introduces a novel spatio-temporal model for event-based optical flow, relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.07826": {
        "authors": [
            "William Ljungbergh",
            "Bernardo Taveira",
            "Wenzhao Zheng",
            "Adam Tonderski",
            "Chensheng Peng",
            "Fredrik Kahl",
            "Christoffer Petersson",
            "Michael Felsberg",
            "Kurt Keutzer",
            "Masayoshi Tomizuka",
            "Wei Zhan"
        ],
        "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation",
        "abstract": "arXiv:2506.07826v1 Announce Type: new  Abstract: Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene/dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see https://research.zenseact.com/publications/R3D2/.",
        "arxiv_id": "2506.07826",
        "ARXIVID": "2506.07826",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for realistic 3D asset insertion in autonomous driving simulation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.06836": {
        "authors": [
            "Zelin He",
            "Sarah Alnegheimish",
            "Matthew Reimherr"
        ],
        "title": "Harnessing Vision-Language Models for Time Series Anomaly Detection",
        "abstract": "arXiv:2506.06836v1 Announce Type: new  Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of fields, including healthcare, finance, and industrial monitoring. Prior methods, which mainly focus on training domain-specific models on numerical data, lack the visual-temporal reasoning capacity that human experts have to identify contextual anomalies. To fill this gap, we explore a solution based on vision language models (VLMs). Recent studies have shown the ability of VLMs for visual reasoning tasks, yet their direct application to time series has fallen short on both accuracy and efficiency. To harness the power of VLMs for TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening stage built on a relatively lightweight pretrained vision encoder, which leverages 2-D time-series representations to accurately localize candidate anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal context and VLM reasoning capacity to refine the detection upon the candidates provided by ViT4TS. We show that without any time-series training, VLM4TS outperforms time-series pretrained and from-scratch baselines in most cases, yielding a 24.6 percent improvement in F1-max score over the best baseline. Moreover, VLM4TS also consistently outperforms existing language-model-based TSAD methods and is on average 36 times more efficient in token usage.",
        "arxiv_id": "2506.06836",
        "ARXIVID": "2506.06836",
        "COMMENT": "Matches criterion 2 as it explores Vision-Language Models (VLMs) and their application to time-series anomaly detection.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.07865": {
        "authors": [
            "Jinxi Li",
            "Ziyang Song",
            "Siyuan Zhou",
            "Bo Yang"
        ],
        "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity",
        "abstract": "arXiv:2506.07865v1 Announce Type: new  Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the underlying physics purely from multi-view videos. By applying various governing PDEs as PINN losses or incorporating physics simulation into neural networks, existing works often fail to learn complex physical motions at boundaries or require object priors such as masks or types. In this paper, we propose FreeGave to learn the physics of complex dynamic 3D scenes without needing any object priors. The key to our approach is to introduce a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on the inefficient PINN losses. Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of our method for future frame extrapolation and motion segmentation. Most notably, our investigation into the learned physics codes reveals that they truly learn meaningful 3D physical motion patterns in the absence of any human labels in training.",
        "arxiv_id": "2506.07865",
        "ARXIVID": "2506.07865",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding and modeling 3D scene dynamics from videos.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.07136": {
        "authors": [
            "Huaize Liu",
            "Wenzhang Sun",
            "Qiyuan Zhang",
            "Donglin Di",
            "Biao Gong",
            "Hao Li",
            "Chen Wei",
            "Changqing Zou"
        ],
        "title": "Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion",
        "abstract": "arXiv:2506.07136v1 Announce Type: new  Abstract: Recent breakthroughs in video autoencoders (Video AEs) have advanced video generation, but existing methods fail to efficiently model spatio-temporal redundancies in dynamics, resulting in suboptimal compression factors. This shortfall leads to excessive training costs for downstream tasks. To address this, we introduce Hi-VAE, an efficient video autoencoding framework that hierarchically encode coarse-to-fine motion representations of video dynamics and formulate the decoding process as a conditional generation task. Specifically, Hi-VAE decomposes video dynamics into two latent spaces: Global Motion, capturing overarching motion patterns, and Detailed Motion, encoding high-frequency spatial details. Using separate self-supervised motion encoders, we compress video latents into compact motion representations to reduce redundancy significantly. A conditional diffusion decoder then reconstructs videos by combining hierarchical global and detailed motions, enabling high-fidelity video reconstructions. Extensive experiments demonstrate that Hi-VAE achieves a high compression factor of 1428$\\times$, almost 30$\\times$ higher than baseline methods (e.g., Cosmos-VAE at 48$\\times$), validating the efficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction quality at such high compression rates and performs effectively in downstream generative tasks. Moreover, Hi-VAE exhibits interpretability and scalability, providing new perspectives for future exploration in video latent representation and generation.",
        "arxiv_id": "2506.07136",
        "ARXIVID": "2506.07136",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding through efficient video autoencoding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.06905": {
        "authors": [
            "Akash Gupta",
            "Amos Storkey",
            "Mirella Lapata"
        ],
        "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering",
        "abstract": "arXiv:2506.06905v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.",
        "arxiv_id": "2506.06905",
        "ARXIVID": "2506.06905",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on improving few-shot visual question answering using multimodal models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.07164": {
        "authors": [
            "Qiong Chang",
            "Xinyuan Chen",
            "Xiang Li",
            "Weimin Wang",
            "Jun Miyazaki"
        ],
        "title": "Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs",
        "abstract": "arXiv:2506.07164v1 Announce Type: new  Abstract: The visual-based SLAM (Simultaneous Localization and Mapping) is a technology widely used in applications such as robotic navigation and virtual reality, which primarily focuses on detecting feature points from visual images to construct an unknown environmental map and simultaneously determines its own location. It usually imposes stringent requirements on hardware power consumption, processing speed and accuracy. Currently, the ORB (Oriented FAST and Rotated BRIEF)-based SLAM systems have exhibited superior performance in terms of processing speed and robustness. However, they still fall short of meeting the demands for real-time processing on mobile platforms. This limitation is primarily due to the time-consuming Oriented FAST calculations accounting for approximately half of the entire SLAM system. This paper presents two methods to accelerate the Oriented FAST feature detection on low-end embedded GPUs. These methods optimize the most time-consuming steps in Oriented FAST feature detection: FAST feature point detection and Harris corner detection, which is achieved by implementing a binary-level encoding strategy to determine candidate points quickly and a separable Harris detection strategy with efficient low-level GPU hardware-specific instructions. Extensive experiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over 7.3 times compared to widely used OpenCV with GPU support. This significant improvement highlights its effectiveness and potential for real-time applications in mobile and resource-constrained environments.",
        "arxiv_id": "2506.07164",
        "ARXIVID": "2506.07164",
        "COMMENT": "Matches criteria 3 as it proposes methods to accelerate feature detection for SLAM systems, relevant to embodied/robotic AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2506.07803": {
        "authors": [
            "Eduard Allakhverdov",
            "Dmitrii Tarasov",
            "Elizaveta Goncharova",
            "Andrey Kuznetsov"
        ],
        "title": "Image Reconstruction as a Tool for Feature Analysis",
        "abstract": "arXiv:2506.07803v1 Announce Type: new  Abstract: Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub.",
        "arxiv_id": "2506.07803",
        "ARXIVID": "2506.07803",
        "COMMENT": "Matches criteria 7 as it provides insights into vision encoder feature analysis through image reconstruction, contributing to understanding vision models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.06727": {
        "authors": [
            "Can Li",
            "Ting Zhang",
            "Mei Wang",
            "Hua Huang"
        ],
        "title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs",
        "abstract": "arXiv:2506.06727v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving capabilities across various domains. However, their ability to perform mathematical reasoning when answer options are represented as images--an essential aspect of multi-image comprehension--remains underexplored. To bridge this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical reasoning in multimodal contexts involving image-based answer choices. VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where each answer option is an image, presenting unique challenges to existing LMMs. To the best of our knowledge, VisioMath is the first dataset specifically tailored for mathematical reasoning in image-based-option scenarios, where fine-grained distinctions between answer choices are critical for accurate problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath and find that even the most advanced models struggle with this task. Notably, GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current models in reasoning over visually similar answer choices. By addressing a crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed for future research, driving advancements in multimodal reasoning.",
        "arxiv_id": "2506.06727",
        "ARXIVID": "2506.06727",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it benchmarks mathematical reasoning in multimodal contexts.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.07627": {
        "authors": [
            "Haotong Qin",
            "Cheng Hu",
            "Michele Magno"
        ],
        "title": "Event-Priori-Based Vision-Language Model for Efficient Visual Understanding",
        "abstract": "arXiv:2506.07627v1 Announce Type: new  Abstract: Large Language Model (LLM)-based Vision-Language Models (VLMs) have substantially extended the boundaries of visual understanding capabilities. However, their high computational demands hinder deployment on resource-constrained edge devices. A key source of inefficiency stems from the VLM's need to process dense and redundant visual information. Visual inputs contain significant regions irrelevant to text semantics, rendering the associated computations ineffective for inference. This paper introduces a novel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core contribution is a novel mechanism leveraging motion priors derived from dynamic event vision to enhance VLM efficiency. Inspired by human visual cognition, EP-VLM first employs event data to guide the patch-wise sparsification of RGB visual inputs, progressively concentrating VLM computation on salient regions of the visual input. Subsequently, we construct a position-preserving tokenization strategy for the visual encoder within the VLM architecture. This strategy processes the event-guided, unstructured, sparse visual input while accurately preserving positional understanding within the visual input. Experimental results demonstrate that EP-VLM achieves significant efficiency improvements while maintaining nearly lossless accuracy compared to baseline models from the Qwen2-VL series. For instance, against the original Qwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the original accuracy on the RealWorldQA dataset. This work demonstrates the potential of event-based vision priors for improving VLM inference efficiency, paving the way for creating more efficient and deployable VLMs for sustainable visual understanding at the edge.",
        "arxiv_id": "2506.07627",
        "ARXIVID": "2506.07627",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a novel mechanism for improving efficiency in vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.06826": {
        "authors": [
            "Chenfei Yuan",
            "Nanshan Jia",
            "Hangqi Li",
            "Peter W. Glynn",
            "Zeyu Zheng"
        ],
        "title": "Controllable Coupled Image Generation via Diffusion Models",
        "abstract": "arXiv:2506.06826v1 Announce Type: new  Abstract: We provide an attention-level control method for the task of coupled image generation, where \"coupled\" means that multiple simultaneously generated images are expected to have the same or very similar backgrounds. While backgrounds coupled, the centered objects in the generated images are still expected to enjoy the flexibility raised from different text prompts. The proposed method disentangles the background and entity components in the model's cross-attention modules, attached with a sequence of time-varying weight control parameters depending on the time step of sampling. We optimize this sequence of weight control parameters with a combined objective that assesses how coupled the backgrounds are as well as text-to-image alignment and overall visual quality. Empirical results demonstrate that our method outperforms existing approaches across these criteria.",
        "arxiv_id": "2506.06826",
        "ARXIVID": "2506.06826",
        "COMMENT": "Matches criterion 5 as it focuses on techniques combining image generation tasks and text prompts.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.08008": {
        "authors": [
            "Stephanie Fu",
            "Tyler Bonnen",
            "Devin Guillory",
            "Trevor Darrell"
        ],
        "title": "Hidden in plain sight: VLMs overlook their visual representations",
        "abstract": "arXiv:2506.08008v1 Announce Type: new  Abstract: Language provides a natural interface to specify and evaluate performance on visual tasks. To realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information. Our work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the entire model, and they inherit the language priors present in the LLM. Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.",
        "arxiv_id": "2506.08008",
        "ARXIVID": "2506.08008",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it evaluates the integration of visual and linguistic information in VLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.06786": {
        "authors": [
            "Dimitris Panagopoulos",
            "Adolfo Perrusquia",
            "Weisi Guo"
        ],
        "title": "Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain",
        "abstract": "arXiv:2506.06786v1 Announce Type: new  Abstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions must continuously gather mission-critical information while flexibly adapting to shifting operational priorities. We propose CA-MIQ (Context-Aware Max-Information Q-learning), a lightweight dual-critic reinforcement learning (RL) framework that dynamically adjusts its exploration strategy whenever mission priorities change. CA-MIQ pairs a standard extrinsic critic for task reward with an intrinsic critic that fuses state-novelty, information-location awareness, and real-time priority alignment. A built-in shift detector triggers transient exploration boosts and selective critic resets, allowing the agent to re-focus after a priority revision. In a simulated SAR grid-world, where experiments specifically test adaptation to changes in the priority order of information types the agent is expected to focus on, CA-MIQ achieves nearly four times higher mission-success rates than baselines after a single priority shift and more than three times better performance in multiple-shift scenarios, achieving 100% recovery while baseline methods fail to adapt. These results highlight CA-MIQ's effectiveness in any discrete environment with piecewise-stationary information-value distributions.",
        "arxiv_id": "2506.06786",
        "ARXIVID": "2506.06786",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel RL framework for context-aware information gain in SAR missions.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.06596": {
        "authors": [
            "Youssef Farah",
            "Federico Paredes-Vall\\'es",
            "Guido De Croon",
            "Muhammad Ahmed Humais",
            "Hussain Sajwani",
            "Yahya Zweiri"
        ],
        "title": "EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras",
        "abstract": "arXiv:2506.06596v1 Announce Type: new  Abstract: Event cameras are novel bio-inspired sensors that capture motion dynamics with much higher temporal resolution than traditional cameras, since pixels react asynchronously to brightness changes. They are therefore better suited for tasks involving motion such as motion segmentation. However, training event-based networks still represents a difficult challenge, as obtaining ground truth is very expensive, error-prone and limited in frequency. In this article, we introduce EV-LayerSegNet, a self-supervised CNN for event-based motion segmentation. Inspired by a layered representation of the scene dynamics, we show that it is possible to learn affine optical flow and segmentation masks separately, and use them to deblur the input events. The deblurring quality is then measured and used as self-supervised learning loss. We train and test the network on a simulated dataset with only affine motion, achieving IoU and detection rate up to 71% and 87% respectively.",
        "arxiv_id": "2506.06596",
        "ARXIVID": "2506.06596",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a self-supervised motion segmentation method using event cameras.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07837": {
        "authors": [
            "Shijie Wang",
            "Yilun Zhang",
            "Zeyu Lai",
            "Dexing Kong"
        ],
        "title": "HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains",
        "abstract": "arXiv:2506.07837v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD, addressing the data shortage issue in specific domain MLLMs.",
        "arxiv_id": "2506.07837",
        "ARXIVID": "2506.07837",
        "COMMENT": "Matches criteria 2 as it focuses on multimodal large language models in the medical ultrasound domain.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.06733": {
        "authors": [
            "Ruoxuan Zhang",
            "Jidong Gao",
            "Bin Wen",
            "Hongxia Xie",
            "Chenming Zhang",
            "Honghan-shuai",
            "Wen-Huang Cheng"
        ],
        "title": "RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation",
        "abstract": "arXiv:2506.06733v1 Announce Type: new  Abstract: Creating recipe images is a key challenge in food computing, with applications in culinary education and multimodal recipe assistants. However, existing datasets lack fine-grained alignment between recipe goals, step-wise instructions, and visual content. We present RecipeGen, the first large-scale, real-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video (I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes, 196,724 images, and 4,491 videos, covering diverse ingredients, cooking procedures, styles, and dish types. We further propose domain-specific evaluation metrics to assess ingredient fidelity and interaction modeling, benchmark representative T2I, I2V, and T2V models, and provide insights for future recipe generation models. Project page is available now.",
        "arxiv_id": "2506.06733",
        "ARXIVID": "2506.06733",
        "COMMENT": "Matches criteria 5 as it combines image, video, and text generation tasks with multimodal benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07740": {
        "authors": [
            "Yingping Liang",
            "Ying Fu",
            "Yutao Hu",
            "Wenqi Shao",
            "Jiaming Liu",
            "Debing Zhang"
        ],
        "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images",
        "abstract": "arXiv:2506.07740v1 Announce Type: new  Abstract: Optical flow estimation is a crucial subfield of computer vision, serving as a foundation for video tasks. However, the real-world robustness is limited by animated synthetic datasets for training. This introduces domain gaps when applied to real-world applications and limits the benefits of scaling up datasets. To address these challenges, we propose \\textbf{Flow-Anything}, a large-scale data generation framework designed to learn optical flow estimation from any single-view images in the real world. We employ two effective steps to make data scaling-up promising. First, we convert a single-view image into a 3D representation using advanced monocular depth estimation networks. This allows us to render optical flow and novel view images under a virtual camera. Second, we develop an Object-Independent Volume Rendering module and a Depth-Aware Inpainting module to model the dynamic objects in the 3D representation. These two steps allow us to generate realistic datasets for training from large-scale single-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we demonstrate the benefits of generating optical flow training data from large-scale real-world images, outperforming the most advanced unsupervised methods and supervised methods on synthetic datasets. Moreover, our models serve as a foundation model and enhance the performance of various downstream video tasks.",
        "arxiv_id": "2506.07740",
        "ARXIVID": "2506.07740",
        "COMMENT": "Matches criterion 6 as it focuses on optical flow estimation for video understanding and introduces a novel data generation framework.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07575": {
        "authors": [
            "Ruiyang Zhang",
            "Hu Zhang",
            "Hao Fei",
            "Zhedong Zheng"
        ],
        "title": "Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models",
        "abstract": "arXiv:2506.07575v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse modalities, are often considered more robust than pure Language Large Models (LLMs); yet do LMMs know what they do not know? There are three key open questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to quantify uncertainty for downstream tasks. In an attempt to address these challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed to reveal uncertainty in LMMs regardless of their modalities, architectures, or capabilities, (2) an empirical exploration of multimodal prompt perturbations to uncover LMM uncertainty, offering insights and findings, and (3) derive the formulation of multimodal semantic uncertainty, which enables quantifying uncertainty from multimodal responses. Experiments across 18 benchmarks spanning various modalities and 10 LMMs (both open- and closed-source) demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM uncertainty, thereby enhancing downstream tasks such as hallucination detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought reasoning.",
        "arxiv_id": "2506.07575",
        "ARXIVID": "2506.07575",
        "COMMENT": "Matches criterion 2 as it explores uncertainty in Large Multimodal Models (LMMs) and proposes a model-agnostic framework.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07418": {
        "authors": [
            "Arnau Igualde S\\'aez",
            "Lamyae Rhomrasi",
            "Yusef Ahsini",
            "Ricardo Vinuesa",
            "Sergio Hoyas",
            "Jose P. Garc\\'ia Sabater",
            "Marius J. Fullana i Alfonso",
            "J. Alberto Conejero"
        ],
        "title": "Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests",
        "abstract": "arXiv:2506.07418v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language capabilities, yet their effectiveness in visually presented mathematics remains underexplored. This paper analyzes the development and evaluation of MLLMs for mathematical problem solving, focusing on diagrams, multilingual text, and symbolic notation. We then assess several models, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our experiments reveal four key findings. First, overall precision remains moderate across geometry, visual algebra, logic, patterns, and combinatorics: no single model excels in every topic. Second, while most models see improved accuracy with questions that do not have images, the gain is often limited; performance for some remains nearly unchanged without visual input, indicating underutilization of diagrammatic information. Third, substantial variation exists across languages and difficulty levels: models frequently handle easier items but struggle with advanced geometry and combinatorial reasoning. Notably, Gemini 2.0 Flash achieves the highest precision on image based tasks, followed by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance. Fourth, a complementary analysis aimed at distinguishing whether models reason or simply recite reveals that Gemini and GPT 4o stand out for their structured reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less consistent reasoning, often defaulting to heuristics or randomness when unable to align their outputs with the given answer options.",
        "arxiv_id": "2506.07418",
        "ARXIVID": "2506.07418",
        "COMMENT": "Matches criterion 2 as it evaluates multimodal LLMs for visual mathematics, focusing on vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07963": {
        "authors": [
            "Jixiang Hong",
            "Yiran Zhang",
            "Guanzhong Wang",
            "Yi Liu",
            "Ji-Rong Wen",
            "Rui Yan"
        ],
        "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards",
        "abstract": "arXiv:2506.07963v1 Announce Type: new  Abstract: Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate image-text alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are inverse dual tasks, we introduce a self-supervised dual reward mechanism to reinforce the understanding and generation capabilities of LMMs. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood of the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.",
        "arxiv_id": "2506.07963",
        "ARXIVID": "2506.07963",
        "COMMENT": "Matches criterion 2 as it focuses on improving multimodal large language models (LMMs) for both understanding and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07542": {
        "authors": [
            "Bowen Liu",
            "Weiyi Zhang",
            "Peranut Chotcomwongse",
            "Xiaolan Chen",
            "Ruoyu Chen",
            "Pawin Pakaymaskul",
            "Niracha Arjkongharn",
            "Nattaporn Vongsa",
            "Xuelian Cheng",
            "Zongyuan Ge",
            "Kun Huang",
            "Xiaohui Li",
            "Yiru Duan",
            "Zhenbang Wang",
            "BaoYe Xie",
            "Qiang Chen",
            "Huazhu Fu",
            "Michael A. Mahr",
            "Jiaqi Qu",
            "Wangyiyang Chen",
            "Shiye Wang",
            "Yubo Tan",
            "Yongjie Li",
            "Mingguang He",
            "Danli Shi",
            "Paisan Ruamviboonsuk"
        ],
        "title": "APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs",
        "abstract": "arXiv:2506.07542v1 Announce Type: new  Abstract: Optical Coherence Tomography (OCT) provides high-resolution, 3D, and non-invasive visualization of retinal layers in vivo, serving as a critical tool for lesion localization and disease diagnosis. However, its widespread adoption is limited by equipment costs and the need for specialized operators. In comparison, 2D color fundus photography offers faster acquisition and greater accessibility with less dependence on expensive devices. Although generative artificial intelligence has demonstrated promising results in medical image synthesis, translating 2D fundus images into 3D OCT images presents unique challenges due to inherent differences in data dimensionality and biological information between modalities. To advance generative models in the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society (APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT Generation from Fundus Images. This paper details the challenge framework (referred to as APTOS-2024 Challenge), including: the benchmark dataset, evaluation methodology featuring two fidelity metrics-image-based distance (pixel-level OCT B-scan similarity) and video-based distance (semantic-level volumetric consistency), and analysis of top-performing solutions. The challenge attracted 342 participating teams, with 42 preliminary submissions and 9 finalists. Leading methodologies incorporated innovations in hybrid data preprocessing or augmentation (cross-modality collaborative paradigms), pre-training on external ophthalmic imaging datasets, integration of vision foundation models, and model architecture improvement. The APTOS-2024 Challenge is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT synthesis as a potential solution for improving ophthalmic care accessibility in under-resourced healthcare settings, while helping to expedite medical research and clinical applications.",
        "arxiv_id": "2506.07542",
        "ARXIVID": "2506.07542",
        "COMMENT": "Partially matches criterion 4 (Vision Foundation Models and Their Applications) as it discusses the integration of vision foundation models in medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07860": {
        "authors": [
            "Ivan Alberico",
            "Marco Cannici",
            "Giovanni Cioffi",
            "Davide Scaramuzza"
        ],
        "title": "Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction",
        "abstract": "arXiv:2506.07860v1 Announce Type: new  Abstract: In this paper, we present a real-time egocentric trajectory prediction system for table tennis using event cameras. Unlike standard cameras, which suffer from high latency and motion blur at fast ball speeds, event cameras provide higher temporal resolution, allowing more frequent state updates, greater robustness to outliers, and accurate trajectory predictions using just a short time window after the opponent's impact. We collect a dataset of ping-pong game sequences, including 3D ground-truth trajectories of the ball, synchronized with sensor data from the Meta Project Aria glasses and event streams. Our system leverages foveated vision, using eye-gaze data from the glasses to process only events in the viewer's fovea. This biologically inspired approach improves ball detection performance and significantly reduces computational latency, as it efficiently allocates resources to the most perceptually relevant regions, achieving a reduction factor of 10.81 on the collected trajectories. Our detection pipeline has a worst-case total latency of 4.5 ms, including computation and perception - significantly lower than a frame-based 30 FPS system, which, in the worst case, takes 66 ms solely for perception. Finally, we fit a trajectory prediction model to the estimated states of the ball, enabling 3D trajectory forecasting in the future. To the best of our knowledge, this is the first approach to predict table tennis trajectories from an egocentric perspective using event cameras.",
        "arxiv_id": "2506.07860",
        "ARXIVID": "2506.07860",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for trajectory prediction in embodied AI using event-based vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07184": {
        "authors": [
            "Liangliang You",
            "Junchi Yao",
            "Shu Yang",
            "Guimin Hu",
            "Lijie Hu",
            "Di Wang"
        ],
        "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images",
        "abstract": "arXiv:2506.07184v1 Announce Type: new  Abstract: While multimodal large language models excel at various tasks, they still suffer from hallucinations, which limit their reliability and scalability for broader domain applications. To address this issue, recent research mainly focuses on objective hallucination. However, for sequential images, besides objective hallucination, there is also behavioral hallucination, which is less studied. This work aims to fill in the gap. We first reveal that behavioral hallucinations mainly arise from two key factors: prior-driven bias and the snowball effect. Based on these observations, we introduce SHE (Sequence Hallucination Eradication), a lightweight, two-stage framework that (1) detects hallucinations via visual-textual alignment check using our proposed adaptive temporal window and (2) mitigates them via orthogonal projection onto the joint embedding space. We also propose a new metric (BEACH) to quantify behavioral hallucination severity. Empirical results on standard benchmarks demonstrate that SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy.",
        "arxiv_id": "2506.07184",
        "ARXIVID": "2506.07184",
        "COMMENT": "Matches criterion 2 as it addresses hallucination in multimodal large language models for sequential images.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.08012": {
        "authors": [
            "Penghao Wu",
            "Shengnan Ma",
            "Bo Wang",
            "Jiaheng Yu",
            "Lewei Lu",
            "Ziwei Liu"
        ],
        "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior",
        "abstract": "arXiv:2506.08012v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.",
        "arxiv_id": "2506.08012",
        "ARXIVID": "2506.08012",
        "COMMENT": "Matches criterion 2 as it focuses on multimodal large language models (MLLMs) and their applications in GUI automation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.06823": {
        "authors": [
            "Qi Li",
            "Liangzhi Li",
            "Zhouqiang Jiang",
            "Bowen Wang",
            "Keke Tang"
        ],
        "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond",
        "abstract": "arXiv:2506.06823v1 Announce Type: new  Abstract: Visual Prompting (VP), an efficient method for transfer learning, has shown its potential in vision tasks. However, previous works focus exclusively on VP from standard source models, it is still unknown how it performs under the scenario of a robust source model: Can the robustness of the source model be successfully inherited? Does VP also encounter the same trade-off between robustness and generalization ability as the source model during this process? If such a trade-off exists, is there a strategy specifically tailored to VP to mitigate this limitation? In this paper, we thoroughly explore these three questions for the first time and provide affirmative answers to them. To mitigate the trade-off faced by VP, we propose a strategy called Prompt Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally compatible with VP, PBL effectively ensures the successful inheritance of robustness when the source model is a robust model, while significantly enhancing VP's generalization ability across various downstream datasets. Extensive experiments across various datasets show that our findings are universal and demonstrate the significant benefits of the proposed strategy.",
        "arxiv_id": "2506.06823",
        "ARXIVID": "2506.06823",
        "COMMENT": "Matches criterion 4 as it explores visual prompting, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07484": {
        "authors": [
            "Dasol Hong",
            "Wooju Lee",
            "Hyun Myung"
        ],
        "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization",
        "abstract": "arXiv:2506.07484v1 Announce Type: new  Abstract: Prompt tuning, which adapts vision-language models by freezing model parameters and optimizing only the prompt, has proven effective for task-specific adaptations. The core challenge in prompt tuning is improving specialization for a specific task and generalization for unseen domains. However, frozen encoders often produce misaligned features, leading to confusion between classes and limiting specialization. To overcome this issue, we propose a confusion-aware loss (CoA-loss) that improves specialization by refining the decision boundaries between confusing classes. Additionally, we mathematically demonstrate that a mixture model can enhance generalization without compromising specialization. This is achieved using confidence-aware weights (CoA-weights), which adjust the weights of each prediction in the mixture model based on its confidence within the class domains. Extensive experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights, outperforms state-of-the-art methods by enhancing specialization and generalization. Our code is publicly available at https://github.com/url-kaist/CoCoA-Mix.",
        "arxiv_id": "2506.07484",
        "ARXIVID": "2506.07484",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on improving prompt tuning for vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.06667": {
        "authors": [
            "Yu-Hsuan Ho",
            "Ali Mostafavi"
        ],
        "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery",
        "abstract": "arXiv:2506.06667v1 Announce Type: new  Abstract: Most post-disaster damage classifiers succeed only when destructive forces leave clear spectral or structural signatures -- conditions rarely present after inundation. Consequently, existing models perform poorly at identifying flood-related building damages. The model presented in this study, Flood-DamageSense, addresses this gap as the first deep-learning framework purpose-built for building-level flood-damage assessment. The architecture fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical basemaps and an inherent flood-risk layer that encodes long-term exposure probabilities, guiding the network toward plausibly affected structures even when compositional change is minimal. A multimodal Mamba backbone with a semi-Siamese encoder and task-specific decoders jointly predicts (1) graded building-damage states, (2) floodwater extent, and (3) building footprints. Training and evaluation on Hurricane Harvey (2017) imagery from Harris County, Texas -- supported by insurance-derived property-damage extents -- show a mean F1 improvement of up to 19 percentage points over state-of-the-art baselines, with the largest gains in the frequently misclassified \"minor\" and \"moderate\" damage categories. Ablation studies identify the inherent-risk feature as the single most significant contributor to this performance boost. An end-to-end post-processing pipeline converts pixel-level outputs to actionable, building-scale damage maps within minutes of image acquisition. By combining risk-aware modeling with SAR's all-weather capability, Flood-DamageSense delivers faster, finer-grained, and more reliable flood-damage intelligence to support post-disaster decision-making and resource allocation.",
        "arxiv_id": "2506.06667",
        "ARXIVID": "2506.06667",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates SAR imagery and multimodal data for flood damage assessment.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.07456": {
        "authors": [
            "Wei Yao",
            "Yunlian Sun",
            "Chang Liu",
            "Hongwen Zhang",
            "Jinhui Tang"
        ],
        "title": "PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation",
        "abstract": "arXiv:2506.07456v1 Announce Type: new  Abstract: Driven by advancements in motion capture and generative artificial intelligence, leveraging large-scale MoCap datasets to train generative models for synthesizing diverse, realistic human motions has become a promising research direction. However, existing motion-capture techniques and generative models often neglect physical constraints, leading to artifacts such as interpenetration, sliding, and floating. These issues are exacerbated in multi-person motion generation, where complex interactions are involved. To address these limitations, we introduce physical mapping, integrated throughout the human interaction generation pipeline. Specifically, motion imitation within a physics-based simulation environment is used to project target motions into a physically valid space. The resulting motions are adjusted to adhere to real-world physics constraints while retaining their original semantic meaning. This mapping not only improves MoCap data quality but also directly informs post-processing of generated motions. Given the unique interactivity of multi-person scenarios, we propose a tailored motion representation framework. Motion Consistency (MC) and Marker-based Interaction (MI) loss functions are introduced to improve model performance. Experiments show our method achieves impressive results in generated human motion quality, with a 3%-89% improvement in physical fidelity. Project page http://yw0208.github.io/physiinter",
        "arxiv_id": "2506.07456",
        "ARXIVID": "2506.07456",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a physics-based simulation for human interaction generation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.06759": {
        "authors": [
            "Nidheesh Gorthi",
            "Kartik Thakral",
            "Rishabh Ranjan",
            "Richa Singh",
            "Mayank Vatsa"
        ],
        "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security",
        "abstract": "arXiv:2506.06759v1 Announce Type: new  Abstract: Biometric authentication systems are increasingly being deployed in critical applications, but they remain susceptible to spoofing. Since most of the research efforts focus on modality-specific anti-spoofing techniques, building a unified, resource-efficient solution across multiple biometric modalities remains a challenge. To address this, we propose LitMAS, a $\\textbf{Li}$gh$\\textbf{t}$ weight and generalizable $\\textbf{M}$ulti-modal $\\textbf{A}$nti-$\\textbf{S}$poofing framework designed to detect spoofing attacks in speech, face, iris, and fingerprint-based biometric systems. At the core of LitMAS is a Modality-Aligned Concentration Loss, which enhances inter-class separability while preserving cross-modal consistency and enabling robust spoof detection across diverse biometric traits. With just 6M parameters, LitMAS surpasses state-of-the-art methods by $1.36\\%$ in average EER across seven datasets, demonstrating high efficiency, strong generalizability, and suitability for edge deployment. Code and trained models are available at https://github.com/IAB-IITJ/LitMAS.",
        "arxiv_id": "2506.06759",
        "ARXIVID": "2506.06759",
        "COMMENT": "Matches criteria 5 as it introduces a lightweight multi-modal anti-spoofing framework integrating biometric modalities, relevant to vision-language integration.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.06480": {
        "authors": [
            "A. Postlmayr",
            "P. Cosman",
            "S. Dey"
        ],
        "title": "(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training",
        "abstract": "arXiv:2506.06480v1 Announce Type: new  Abstract: We introduce a fitness tracking system that enables remote monitoring for exercises using only a RGB smartphone camera, making fitness tracking more private, scalable, and cost effective. Although prior work explored automated exercise supervision, existing models are either too limited in exercise variety or too complex for real-world deployment. Prior approaches typically focus on a small set of exercises and fail to generalize across diverse movements. In contrast, we develop a robust, multitask motion analysis model capable of performing exercise detection and repetition counting across hundreds of exercises, a scale far beyond previous methods. We overcome previous data limitations by assembling a large-scale fitness dataset, Olympia covering more than 1,900 exercises. To our knowledge, our vision-language model is the first that can perform multiple tasks on skeletal fitness data. On Olympia, our model can detect exercises with 76.5% accuracy and count repetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting a single vision-language transformer model for both exercise identification and rep counting, we take a significant step toward democratizing AI-powered fitness tracking.",
        "arxiv_id": "2506.06480",
        "ARXIVID": "2506.06480",
        "COMMENT": "Matches criteria 5 as it presents a vision-language model for fitness tracking, integrating image understanding and language-based tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.07369": {
        "authors": [
            "Bolin Chen",
            "Shanzhi Yin",
            "Goluck Konuko",
            "Giuseppe Valenzise",
            "Zihan Zhang",
            "Shiqi Wang",
            "Yan Ye"
        ],
        "title": "Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding",
        "abstract": "arXiv:2506.07369v1 Announce Type: new  Abstract: The rise of deep generative models has greatly advanced video compression, reshaping the paradigm of face video coding through their powerful capability for semantic-aware representation and lifelike synthesis. Generative Face Video Coding (GFVC) stands at the forefront of this revolution, which could characterize complex facial dynamics into compact latent codes for bitstream compactness at the encoder side and leverages powerful deep generative models to reconstruct high-fidelity face signal from the compressed latent codes at the decoder side. As such, this well-designed GFVC paradigm could enable high-fidelity face video communication at ultra-low bitrate ranges, far surpassing the capabilities of the latest Versatile Video Coding (VVC) standard. To pioneer foundational research and accelerate the evolution of GFVC, this paper presents the first comprehensive survey of GFVC technologies, systematically bridging critical gaps between theoretical innovation and industrial standardization. In particular, we first review a broad range of existing GFVC methods with different feature representations and optimization strategies, and conduct a thorough benchmarking analysis. In addition, we construct a large-scale GFVC-compressed face video database with subjective Mean Opinion Scores (MOSs) based on human perception, aiming to identify the most appropriate quality metrics tailored to GFVC. Moreover, we summarize the GFVC standardization potentials with a unified high-level syntax and develop a low-complexity GFVC system which are both expected to push forward future practical deployments and applications. Finally, we envision the potential of GFVC in industrial applications and deliberate on the current challenges and future opportunities.",
        "arxiv_id": "2506.07369",
        "ARXIVID": "2506.07369",
        "COMMENT": "Matches criteria 7 as it is a comprehensive survey on generative face video coding, synthesizing state-of-the-art advancements and challenges.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.07713": {
        "authors": [
            "Ge Wang",
            "Songlin Fan",
            "Hangxu Liu",
            "Quanjian Song",
            "Hewei Wang",
            "Jinfeng Xu"
        ],
        "title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation",
        "abstract": "arXiv:2506.07713v1 Announce Type: new  Abstract: With the prosper of video diffusion models, down-stream applications like video editing have been significantly promoted without consuming much computational cost. One particular challenge in this task lies at the motion transfer process from the source video to the edited one, where it requires the consideration of the shape deformation in between, meanwhile maintaining the temporal consistency in the generated video sequence. However, existing methods fail to model complicated motion patterns for video editing, and are fundamentally limited to object replacement, where tasks with non-rigid object motions like multi-object and portrait editing are largely neglected. In this paper, we observe that optical flows offer a promising alternative in complex motion modeling, and present FlowV2V to re-investigate video editing as a task of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V decomposes the entire pipeline into first-frame editing and conditional I2V generation, and simulates pseudo flow sequence that aligns with the deformed shape, thus ensuring the consistency during editing. Experimental results on DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error illustrate the superior temporal consistency and sample quality of FlowV2V compared to existing state-of-the-art ones. Furthermore, we conduct comprehensive ablation studies to analyze the internal functionalities of the first-frame paradigm and flow alignment in the proposed method.",
        "arxiv_id": "2506.07713",
        "ARXIVID": "2506.07713",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video editing with temporal consistency and motion modeling.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.07984": {
        "authors": [
            "Mingquan Lin",
            "Gregory Holste",
            "Song Wang",
            "Yiliang Zhou",
            "Yishu Wei",
            "Imon Banerjee",
            "Pengyi Chen",
            "Tianjie Dai",
            "Yuexi Du",
            "Nicha C. Dvornek",
            "Yuyan Ge",
            "Zuowei Guo",
            "Shouhei Hanaoka",
            "Dongkyun Kim",
            "Pablo Messina",
            "Yang Lu",
            "Denis Parra",
            "Donghyun Son",
            "\\'Alvaro Soto",
            "Aisha Urooj",
            "Ren\\'e Vidal",
            "Yosuke Yamagishi",
            "Zefan Yang",
            "Ruichi Zhang",
            "Yang Zhou",
            "Leo Anthony Celi",
            "Ronald M. Summers",
            "Zhiyong Lu",
            "Hao Chen",
            "Adam Flanders",
            "George Shih",
            "Zhangyang Wang",
            "Yifan Peng"
        ],
        "title": "CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray",
        "abstract": "arXiv:2506.07984v1 Announce Type: new  Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung disease classification using chest X-rays (CXR). It tackles challenges in open long-tailed lung disease classification and enhances the measurability of state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve these goals by providing high-quality benchmark CXR data for model development and conducting comprehensive evaluations to identify ongoing issues impacting lung disease classification performance. Building on the success of CXR-LT 2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45 disease labels, including 19 new rare disease findings. It also introduces a new focus on zero-shot learning to address limitations identified in the previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed classification on a large, noisy test set, (ii) long-tailed classification on a manually annotated \"gold standard\" subset, and (iii) zero-shot generalization to five previously unseen disease findings. This paper provides an overview of CXR-LT 2024, detailing the data curation process and consolidating state-of-the-art solutions, including the use of multimodal models for rare disease detection, advanced generative approaches to handle noisy labels, and zero-shot learning strategies for unseen diseases. Additionally, the expanded dataset enhances disease coverage to better represent real-world clinical settings, offering a valuable resource for future research. By synthesizing the insights and innovations of participating teams, we aim to advance the development of clinically realistic and generalizable diagnostic models for chest radiography.",
        "arxiv_id": "2506.07984",
        "ARXIVID": "2506.07984",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for chest X-ray disease classification.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2506.07738": {
        "authors": [
            "Lanjiong Li",
            "Guanhua Zhao",
            "Lingting Zhu",
            "Zeyu Cai",
            "Lequan Yu",
            "Jian Zhang",
            "Zeyu Wang"
        ],
        "title": "AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization",
        "abstract": "arXiv:2506.07738v1 Announce Type: new  Abstract: Recent research on generative models has primarily focused on creating product-ready visual outputs; however, designers often favor access to standardized asset libraries, a domain that has yet to be significantly enhanced by generative capabilities. Although open-world scenes provide ample raw materials for designers, efficiently extracting high-quality, standardized assets remains a challenge. To address this, we introduce AssetDropper, the first framework designed to extract assets from reference images, providing artists with an open-world asset palette. Our model adeptly extracts a front view of selected subjects from input images, effectively handling complex scenarios such as perspective distortion and subject occlusion. We establish a synthetic dataset of more than 200,000 image-subject pairs and a real-world benchmark with thousands more for evaluation, facilitating the exploration of future research in downstream tasks. Furthermore, to ensure precise asset extraction that aligns well with the image prompts, we employ a pre-trained reward model to fulfill a closed-loop with feedback. We design the reward model to perform an inverse task that pastes the extracted assets back into the reference sources, which assists training with additional consistency and mitigates hallucination. Extensive experiments show that, with the aid of reward-driven optimization, AssetDropper achieves the state-of-the-art results in asset extraction. Project page: AssetDropper.github.io.",
        "arxiv_id": "2506.07738",
        "ARXIVID": "2506.07738",
        "COMMENT": "Does not match any specific criteria. Focuses on asset extraction using diffusion models, which is not directly related to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06832": {
        "authors": [
            "Cl\\'ement Hongler",
            "Andrew Emil"
        ],
        "title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures",
        "abstract": "arXiv:2506.06832v1 Announce Type: new  Abstract: Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.",
        "arxiv_id": "2506.06832",
        "ARXIVID": "2506.06832",
        "COMMENT": "Does not match any specific criteria but discusses novel tasks and benchmarks for LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07047": {
        "authors": [
            "Yu Xuejun",
            "Jianyuan Zhong",
            "Zijin Feng",
            "Pengyi Zhai",
            "Roozbeh Yousefzadeh",
            "Wei Chong Ng",
            "Haoxiong Liu",
            "Ziyi Shou",
            "Jing Xiong",
            "Yudong Zhou",
            "Claudia Beth Ong",
            "Austen Jeremy Sugiarto",
            "Yaoxi Zhang",
            "Wai Ming Tai",
            "Huan Cao",
            "Dongcai Lu",
            "Jiacheng Sun",
            "Qiang Xu",
            "Shen Xin",
            "Zhenguo Li"
        ],
        "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
        "abstract": "arXiv:2506.07047v1 Announce Type: new  Abstract: Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.",
        "arxiv_id": "2506.07047",
        "ARXIVID": "2506.07047",
        "COMMENT": "Does not match any specific criterion but focuses on theorem proving and natural language processing, which are tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07555": {
        "authors": [
            "Haoxiang Wang",
            "Zinan Lin",
            "Da Yu",
            "Huishuai Zhang"
        ],
        "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries",
        "abstract": "arXiv:2506.07555v1 Announce Type: new  Abstract: Generating high fidelity, differentially private (DP) synthetic images offers a promising route to share and analyze sensitive visual data without compromising individual privacy. However, existing DP image synthesis methods struggle to produce high resolution outputs that faithfully capture the structure of the original data. In this paper, we introduce a novel method, referred to as Synthesis via Private Textual Intermediaries (SPTI), that can generate high resolution DP images with easy adoption. The key idea is to shift the challenge of DP image synthesis from the image domain to the text domain by leveraging state of the art DP text generation methods. SPTI first summarizes each private image into a concise textual description using image to text models, then applies a modified Private Evolution algorithm to generate DP text, and finally reconstructs images using text to image models. Notably, SPTI requires no model training, only inference with off the shelf models. Given a private dataset, SPTI produces synthetic images of substantially higher quality than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less than or equal to 26.71 under epsilon equal to 1.0, improving over Private Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine tuning baselines. Overall, our results demonstrate that Synthesis via Private Textual Intermediaries provides a resource efficient and proprietary model compatible framework for generating high resolution DP synthetic images, greatly expanding access to private visual datasets.",
        "arxiv_id": "2506.07555",
        "ARXIVID": "2506.07555",
        "COMMENT": "Does not match any specific criteria. Focuses on privacy-preserving image synthesis, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06710": {
        "authors": [
            "Qianqian Zhao",
            "Chunle Guo",
            "Tianyi Zhang",
            "Junpei Zhang",
            "Peiyang Jia",
            "Tan Su",
            "Wenjie Jiang",
            "Chongyi Li"
        ],
        "title": "A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution",
        "abstract": "arXiv:2506.06710v1 Announce Type: new  Abstract: Omnidirectional image and video super-resolution is a crucial research topic in low-level vision, playing an essential role in virtual reality and augmented reality applications. Its goal is to reconstruct high-resolution images or video frames from low-resolution inputs, thereby enhancing detail preservation and enabling more accurate scene analysis and interpretation. In recent years, numerous innovative and effective approaches have been proposed, predominantly based on deep learning techniques, involving diverse network architectures, loss functions, projection strategies, and training datasets. This paper presents a systematic review of recent progress in omnidirectional image and video super-resolution, focusing on deep learning-based methods. Given that existing datasets predominantly rely on synthetic degradation and fall short in capturing real-world distortions, we introduce a new dataset, 360Insta, that comprises authentically degraded omnidirectional images and videos collected under diverse conditions, including varying lighting, motion, and exposure settings. This dataset addresses a critical gap in current omnidirectional benchmarks and enables more robust evaluation of the generalization capabilities of omnidirectional super-resolution methods. We conduct comprehensive qualitative and quantitative evaluations of existing methods on both public datasets and our proposed dataset. Furthermore, we provide a systematic overview of the current status of research and discuss promising directions for future exploration. All datasets, methods, and evaluation metrics introduced in this work are publicly available and will be regularly updated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.",
        "arxiv_id": "2506.06710",
        "ARXIVID": "2506.06710",
        "COMMENT": "Matches criteria 7 as it is a systematic review of omnidirectional image and video super-resolution methods.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.06992": {
        "authors": [
            "Yanting Gao",
            "Yepeng Liu",
            "Junming Liu",
            "Qi Zhang",
            "Hongyun Zhang",
            "Duoqian Miao",
            "Cairong Zhao"
        ],
        "title": "Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization",
        "abstract": "arXiv:2506.06992v1 Announce Type: new  Abstract: Exploring effective and transferable adversarial examples is vital for understanding the characteristics and mechanisms of Vision Transformers (ViTs). However, adversarial examples generated from surrogate models often exhibit weak transferability in black-box settings due to overfitting. Existing methods improve transferability by diversifying perturbation inputs or applying uniform gradient regularization within surrogate models, yet they have not fully leveraged the shared and unique features of surrogate models trained on the same task, leading to suboptimal transfer performance. Therefore, enhancing perturbations of common information shared by surrogate models and suppressing those tied to individual characteristics offers an effective way to improve transferability. Accordingly, we propose a commonality-oriented gradient optimization strategy (COGO) consisting of two components: Commonality Enhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low frequency regions, leveraging the fact that ViTs trained on the same dataset tend to rely more on mid-to-low frequency information for classification. IS employs adaptive thresholds to evaluate the correlation between backpropagated gradients and model individuality, assigning weights to gradients accordingly. Extensive experiments demonstrate that COGO significantly improves the transfer success rates of adversarial attacks, outperforming current state-of-the-art methods.",
        "arxiv_id": "2506.06992",
        "ARXIVID": "2506.06992",
        "COMMENT": "Does not match any specific criteria but proposes a novel strategy for improving adversarial transferability in vision models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07188": {
        "authors": [
            "Ni Ding",
            "Lei He",
            "Shengbo Eben Li",
            "Keqiang Li"
        ],
        "title": "Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks",
        "abstract": "arXiv:2506.07188v1 Announce Type: new  Abstract: End-to-end autonomous driving has emerged as a dominant paradigm, yet its highly entangled black-box models pose significant challenges in terms of interpretability and safety assurance. To improve model transparency and training flexibility, this paper proposes a hierarchical and decoupled post-training framework tailored for pretrained neural networks. By reconstructing intermediate feature maps from ground-truth labels, surrogate supervisory signals are introduced at transitional layers to enable independent training of specific components, thereby avoiding the complexity and coupling of conventional end-to-end backpropagation and providing interpretable insights into networks' internal mechanisms. To the best of our knowledge, this is the first method to formalize feature-level reverse computation as well-posed optimization problems, which we rigorously reformulate as systems of linear equations or least squares problems. This establishes a novel and efficient training paradigm that extends gradient backpropagation to feature backpropagation. Extensive experiments on multiple standard image classification benchmarks demonstrate that the proposed method achieves superior generalization performance and computational efficiency compared to traditional training approaches, validating its effectiveness and potential.",
        "arxiv_id": "2506.07188",
        "ARXIVID": "2506.07188",
        "COMMENT": "Does not match any specific criteria but discusses a novel training paradigm for neural networks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06580": {
        "authors": [
            "Xiaoran Liu",
            "Istvan David"
        ],
        "title": "AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture",
        "abstract": "arXiv:2506.06580v1 Announce Type: new  Abstract: Insufficient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efficiently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-fidelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our findings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.",
        "arxiv_id": "2506.06580",
        "ARXIVID": "2506.06580",
        "COMMENT": "Matches criterion 7 as it is a survey paper on digital twins and AI simulation, which could be relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.06470": {
        "authors": [
            "Yanwei Ren",
            "Haotian Zhang",
            "Fuxiang Wu",
            "Jiayan Qiu",
            "Jiaxing Huang",
            "Baosheng Yu",
            "Liu Liu"
        ],
        "title": "SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation",
        "abstract": "arXiv:2506.06470v1 Announce Type: new  Abstract: Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.",
        "arxiv_id": "2506.06470",
        "ARXIVID": "2506.06470",
        "COMMENT": "Does not match any specific criteria but focuses on improving reasoning in LLMs, which is tangentially related to spatial intelligence and embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07820": {
        "authors": [
            "Jiaxiang CHen",
            "Zhuo Wang",
            "Mingxi Zou",
            "Qifan Wang",
            "Zenglin Xu"
        ],
        "title": "Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation",
        "abstract": "arXiv:2506.07820v1 Announce Type: new  Abstract: Human reasoning is flexible, adaptive, and grounded in prior experience-qualities that large language models (LLMs) still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner. We propose Guideline Forest, a framework that enhances LLMs reasoning by inducing structured reasoning strategies-called guidelines-from verified examples and executing them via step-wise aggregation. Unlike test-time search or single-path distillation, our method draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants. Much like human reasoning, these variants reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step-enabling the model to adaptively resolve uncertainty and synthesize robust solutions.We evaluate Guideline Forest on four benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and programmatic reasoning. Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation, underscoring the Guideline Forest's adaptability and generalization potential.",
        "arxiv_id": "2506.07820",
        "ARXIVID": "2506.07820",
        "COMMENT": "Does not match any specific criteria but explores reasoning strategies in LLMs, which is tangentially related to your friend's interest in clever statistical tricks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07964": {
        "authors": [
            "Wenxin Tang",
            "Jingyu Xiao",
            "Wenxuan Jiang",
            "Xi Xiao",
            "Yuhang Wang",
            "Xuxin Tang",
            "Qing Li",
            "Yuehe Ma",
            "Junliang Liu",
            "Shisong Tang",
            "Michael R. Lyu"
        ],
        "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design",
        "abstract": "arXiv:2506.07964v1 Announce Type: new  Abstract: Manual slide creation is labor-intensive and requires expert prior knowledge. Existing natural language-based LLM generation methods struggle to capture the visual and structural nuances of slide designs. To address this, we formalize the Reference Image to Slide Generation task and propose Slide2Code, the first benchmark with difficulty-tiered samples based on a novel Slide Complexity Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework for generating editable slides from reference images. SlideCoder integrates a Color Gradient-based Segmentation algorithm and a Hierarchical Retrieval-Augmented Generation method to decompose complex tasks and enhance code generation. We also release SlideMaster, a 7B open-source model fine-tuned with improved reverse-engineered data. Experiments show that SlideCoder outperforms state-of-the-art baselines by up to 40.5 points, demonstrating strong performance across layout fidelity, execution accuracy, and visual consistency. Our code is available at https://github.com/vinsontang1/SlideCoder.",
        "arxiv_id": "2506.07964",
        "ARXIVID": "2506.07964",
        "COMMENT": "Does not match any specific criterion but is relevant to multimodal learning due to its focus on layout-aware slide generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06645": {
        "authors": [
            "Cheng Peng",
            "Jingxiang Sun",
            "Yushuo Chen",
            "Zhaoqi Su",
            "Zhuo Su",
            "Yebin Liu"
        ],
        "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling",
        "abstract": "arXiv:2506.06645v1 Announce Type: new  Abstract: Photorealistic and animatable human avatars are a key enabler for virtual/augmented reality, telepresence, and digital entertainment. While recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering quality and efficiency, existing methods still face fundamental challenges, including time-consuming per-subject optimization and poor generalization under sparse monocular inputs. In this work, we present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. PGHM introduces two core components: (1) a UV-aligned latent identity map that compactly encodes subject-specific geometry and appearance into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that predicts Gaussian attributes by decomposing static, pose-dependent, and view-dependent components via conditioned decoders. This design enables robust rendering quality under challenging poses and viewpoints, while allowing efficient subject adaptation without requiring multi-view capture or long optimization time. Experiments show that PGHM is significantly more efficient than optimization-from-scratch methods, requiring only approximately 20 minutes per subject to produce avatars with comparable visual quality, thereby demonstrating its practical applicability for real-world monocular avatar creation.",
        "arxiv_id": "2506.06645",
        "ARXIVID": "2506.06645",
        "COMMENT": "Does not match any specific criteria but is related to human avatar modeling, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07414": {
        "authors": [
            "Sheng-Kai Huang",
            "Jiun-Feng Chang",
            "Chun-Rong Huang"
        ],
        "title": "DPFormer: Dynamic Prompt Transformer for Continual Learning",
        "abstract": "arXiv:2506.07414v1 Announce Type: new  Abstract: In continual learning, solving the catastrophic forgetting problem may make the models fall into the stability-plasticity dilemma. Moreover, inter-task confusion will also occur due to the lack of knowledge exchanges between different tasks. In order to solve the aforementioned problems, we propose a novel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt schemes help the DPFormer memorize learned knowledge of previous classes and tasks, and keep on learning new knowledge from new classes and tasks under a single network structure with a nearly fixed number of model parameters. Moreover, they also provide discrepant information to represent different tasks to solve the inter-task confusion problem. Based on prompt schemes, a unified classification module with the binary cross entropy loss, the knowledge distillation loss and the auxiliary loss is proposed to train the whole model in an end-to-end trainable manner. Compared with state-of-the-art methods, our method achieves the best performance in the CIFAR-100, ImageNet100 and ImageNet1K datasets under different class-incremental settings in continual learning. The source code will be available at our GitHub after acceptance.",
        "arxiv_id": "2506.07414",
        "ARXIVID": "2506.07414",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning and continual learning, which aligns with your friend's broader interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07527": {
        "authors": [
            "Lu Ma",
            "Hao Liang",
            "Meiyi Qiang",
            "Lexiang Tang",
            "Xiaochen Ma",
            "Zhen Hao Wong",
            "Junbo Niu",
            "Chengyu Shen",
            "Runming He",
            "Bin Cui",
            "Wentao Zhang"
        ],
        "title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions",
        "abstract": "arXiv:2506.07527v1 Announce Type: new  Abstract: Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.",
        "arxiv_id": "2506.07527",
        "ARXIVID": "2506.07527",
        "COMMENT": "Does not match any specific criteria. Focuses on reinforcement learning and fine-tuning for reasoning tasks, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.06748": {
        "authors": [
            "Mingqi Gao",
            "Haoran Duan",
            "Tianlu Zhang",
            "Jungong Han"
        ],
        "title": "THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation",
        "abstract": "arXiv:2506.06748v1 Announce Type: new  Abstract: In this report, we describe our approach to egocentric video object segmentation. Our method combines large-scale visual pretraining from SAM2 with depth-based geometric cues to handle complex scenes and long-term tracking. By integrating these signals in a unified framework, we achieve strong segmentation performance. On the VISOR test set, our method reaches a J&F score of 90.1%.",
        "arxiv_id": "2506.06748",
        "ARXIVID": "2506.06748",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video object segmentation in egocentric videos.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.07611": {
        "authors": [
            "Yuan Zhou",
            "Junbao Zhou",
            "Qingshan Xu",
            "Kesen Zhao",
            "Yuxuan Wang",
            "Hao Fei",
            "Richang Hong",
            "Hanwang Zhang"
        ],
        "title": "DragNeXt: Rethinking Drag-Based Image Editing",
        "abstract": "arXiv:2506.07611v1 Announce Type: new  Abstract: Drag-Based Image Editing (DBIE), which allows users to manipulate images by directly dragging objects within them, has recently attracted much attention from the community. However, it faces two key challenges: (\\emph{\\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and difficult to align with users' intentions; (\\emph{\\textcolor{magenta}{ii}}) current DBIE methods primarily rely on alternating between motion supervision and point tracking, which is not only cumbersome but also fails to produce high-quality results. These limitations motivate us to explore DBIE from a new perspective -- redefining it as deformation, rotation, and translation of user-specified handle regions. Thereby, by requiring users to explicitly specify both drag areas and types, we can effectively address the ambiguity issue. Furthermore, we propose a simple-yet-effective editing framework, dubbed \\textcolor{SkyBlue}{\\textbf{DragNeXt}}. It unifies DBIE as a Latent Region Optimization (LRO) problem and solves it through Progressive Backward Self-Intervention (PBSI), simplifying the overall procedure of DBIE while further enhancing quality by fully leveraging region-level structure information and progressive guidance from intermediate drag states. We validate \\textcolor{SkyBlue}{\\textbf{DragNeXt}} on our NextBench, and extensive experiments demonstrate that our proposed method can significantly outperform existing approaches. Code will be released on github.",
        "arxiv_id": "2506.07611",
        "ARXIVID": "2506.07611",
        "COMMENT": "Does not closely match any specific criteria but is relevant to the general interest area of generative modeling and image editing.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.07056": {
        "authors": [
            "Zhenyu Liu",
            "Huizhi Liang",
            "Rajiv Ranjan",
            "Zhanxing Zhu",
            "Vaclav Snasel",
            "Varun Ojha"
        ],
        "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness",
        "abstract": "arXiv:2506.07056v1 Announce Type: new  Abstract: The robustness of Deep Neural Network models is crucial for defending models against adversarial attacks. Recent defense methods have employed collaborative learning frameworks to enhance model robustness. Two key limitations of existing methods are (i) insufficient guidance of the target model via loss functions and (ii) non-collaborative adversarial generation. We, therefore, propose a dual regularization loss (D2R Loss) method and a collaborative adversarial generation (CAG) strategy for adversarial training. D2R loss includes two optimization steps. The adversarial distribution and clean distribution optimizations enhance the target model's robustness by leveraging the strengths of different loss functions obtained via a suitable function space exploration to focus more precisely on the target model's distribution. CAG generates adversarial samples using a gradient-based collaboration between guidance and target models. We conducted extensive experiments on three benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two popular target models, WideResNet34-10 and PreActResNet18. Our results show that D2R loss with CAG produces highly robust models.",
        "arxiv_id": "2506.07056",
        "ARXIVID": "2506.07056",
        "COMMENT": "Does not match any specific criteria but is relevant to adversarial robustness in deep learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07327": {
        "authors": [
            "Dane Williamson",
            "Yangfeng Ji",
            "Matthew Dwyer"
        ],
        "title": "\"CASE: Contrastive Activation for Saliency Estimation",
        "abstract": "arXiv:2506.07327v1 Announce Type: new  Abstract: Saliency methods are widely used to visualize which input features are deemed relevant to a model's prediction. However, their visual plausibility can obscure critical limitations. In this work, we propose a diagnostic test for class sensitivity: a method's ability to distinguish between competing class labels on the same input. Through extensive experiments, we show that many widely used saliency methods produce nearly identical explanations regardless of the class label, calling into question their reliability. We find that class-insensitive behavior persists across architectures and datasets, suggesting the failure mode is structural rather than model-specific. Motivated by these findings, we introduce CASE, a contrastive explanation method that isolates features uniquely discriminative for the predicted class. We evaluate CASE using the proposed diagnostic and a perturbation-based fidelity test, and show that it produces faithful and more class-specific explanations than existing methods.",
        "arxiv_id": "2506.07327",
        "ARXIVID": "2506.07327",
        "COMMENT": "Does not match any specific criteria but is relevant to saliency estimation and interpretability in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06326": {
        "authors": [
            "Jiazheng Kang",
            "Mingming Ji",
            "Zhe Zhao",
            "Ting Bai"
        ],
        "title": "Memory OS of AI Agent",
        "abstract": "arXiv:2506.06326v1 Announce Type: new  Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at https://github.com/BAI-LAB/MemoryOS.",
        "arxiv_id": "2506.06326",
        "ARXIVID": "2506.06326",
        "COMMENT": "Does not match any specific criteria but discusses memory management for LLMs, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06802": {
        "authors": [
            "Mohammad Ali Rezaei",
            "Helia Hajikazem",
            "Saeed Khanehgir",
            "Mahdi Javanmardi"
        ],
        "title": "Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models",
        "abstract": "arXiv:2506.06802v1 Announce Type: new  Abstract: While diffusion models have demonstrated remarkable generative capabilities, existing style transfer techniques often struggle to maintain identity while achieving high-quality stylization. This limitation is particularly acute for images where faces are small or exhibit significant camera-to-face distances, frequently leading to inadequate identity preservation. To address this, we introduce a novel, training-free framework for identity-preserved stylized image synthesis using diffusion models. Key contributions include: (1) the \"Mosaic Restored Content Image\" technique, significantly enhancing identity retention, especially in complex scenes; and (2) a training-free content consistency loss that enhances the preservation of fine-grained content details by directing more attention to the original image during stylization. Our experiments reveal that the proposed approach substantially surpasses the baseline model in concurrently maintaining high stylistic fidelity and robust identity integrity, particularly under conditions of small facial regions or significant camera-to-face distances, all without necessitating model retraining or fine-tuning.",
        "arxiv_id": "2506.06802",
        "ARXIVID": "2506.06802",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and identity preservation in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06953": {
        "authors": [
            "Maciej Zyrek",
            "Tomasz Tarasiewicz",
            "Jakub Sadel",
            "Aleksandra Krzywon",
            "Michal Kawulok"
        ],
        "title": "Task-driven real-world super-resolution of document scans",
        "abstract": "arXiv:2506.06953v1 Announce Type: new  Abstract: Single-image super-resolution refers to the reconstruction of a high-resolution image from a single low-resolution observation. Although recent deep learning-based methods have demonstrated notable success on simulated datasets -- with low-resolution images obtained by degrading and downsampling high-resolution ones -- they frequently fail to generalize to real-world settings, such as document scans, which are affected by complex degradations and semantic variability. In this study, we introduce a task-driven, multi-task learning framework for training a super-resolution network specifically optimized for optical character recognition tasks. We propose to incorporate auxiliary loss functions derived from high-level vision tasks, including text detection using the connectionist text proposal network, text recognition via a convolutional recurrent neural network, keypoints localization using Key.Net, and hue consistency. To balance these diverse objectives, we employ dynamic weight averaging mechanism, which adaptively adjusts the relative importance of each loss term based on its convergence behavior. We validate our approach upon the SRResNet architecture, which is a well-established technique for single-image super-resolution. Experimental evaluations on both simulated and real-world scanned document datasets demonstrate that the proposed approach improves text detection, measured with intersection over union, while preserving overall image fidelity. These findings underscore the value of multi-objective optimization in super-resolution models for bridging the gap between simulated training regimes and practical deployment in real-world scenarios.",
        "arxiv_id": "2506.06953",
        "ARXIVID": "2506.06953",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and task-driven optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07705": {
        "authors": [
            "Weilei Wen",
            "Chunle Guo",
            "Wenqi Ren",
            "Hongpeng Wang",
            "Xiuli Shao"
        ],
        "title": "Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations",
        "abstract": "arXiv:2506.07705v1 Announce Type: new  Abstract: Prior methodologies have disregarded the diversities among distinct degradation types during image reconstruction, employing a uniform network model to handle multiple deteriorations. Nevertheless, we discover that prevalent degradation modalities, including sampling, blurring, and noise, can be roughly categorized into two classes. We classify the first class as spatial-agnostic dominant degradations, less affected by regional changes in image space, such as downsampling and noise degradation. The second class degradation type is intimately associated with the spatial position of the image, such as blurring, and we identify them as spatial-specific dominant degradations. We introduce a dynamic filter network integrating global and local branches to address these two degradation types. This network can greatly alleviate the practical degradation problem. Specifically, the global dynamic filtering layer can perceive the spatial-agnostic dominant degradation in different images by applying weights generated by the attention mechanism to multiple parallel standard convolution kernels, enhancing the network's representation ability. Meanwhile, the local dynamic filtering layer converts feature maps of the image into a spatially specific dynamic filtering operator, which performs spatially specific convolution operations on the image features to handle spatial-specific dominant degradations. By effectively integrating both global and local dynamic filtering operators, our proposed method outperforms state-of-the-art blind super-resolution algorithms in both synthetic and real image datasets.",
        "arxiv_id": "2506.07705",
        "ARXIVID": "2506.07705",
        "COMMENT": "Does not match any specific criteria but focuses on super-resolution methods, which is tangentially related to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07992": {
        "authors": [
            "Haoguang Lu",
            "Jiacheng Chen",
            "Zhenguo Yang",
            "Aurele Tohokantche Gnanha",
            "Fu Lee Wang",
            "Li Qing",
            "Xudong Mao"
        ],
        "title": "PairEdit: Learning Semantic Variations for Exemplar-based Image Editing",
        "abstract": "arXiv:2506.07992v1 Announce Type: new  Abstract: Recent advancements in text-guided image editing have achieved notable success by leveraging natural language prompts for fine-grained semantic control. However, certain editing semantics are challenging to specify precisely using textual descriptions alone. A practical alternative involves learning editing semantics from paired source-target examples. Existing exemplar-based editing methods still rely on text prompts describing the change within paired examples or learning implicit text-based editing instructions. In this paper, we introduce PairEdit, a novel visual editing method designed to effectively learn complex editing semantics from a limited number of image pairs or even a single image pair, without using any textual guidance. We propose a target noise prediction that explicitly models semantic variations within paired images through a guidance direction term. Moreover, we introduce a content-preserving noise schedule to facilitate more effective semantic learning. We also propose optimizing distinct LoRAs to disentangle the learning of semantic variations from content. Extensive qualitative and quantitative evaluations demonstrate that PairEdit successfully learns intricate semantics while significantly improving content consistency compared to baseline methods. Code will be available at https://github.com/xudonmao/PairEdit.",
        "arxiv_id": "2506.07992",
        "ARXIVID": "2506.07992",
        "COMMENT": "Does not match any specific criteria but focuses on exemplar-based image editing, which is tangentially related to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06923": {
        "authors": [
            "Xutong Zhao",
            "Tengyu Xu",
            "Xuewei Wang",
            "Zhengxing Chen",
            "Di Jin",
            "Liang Tan",
            "Yen-Ting",
            "Zishun Yu",
            "Zhuokai Zhao",
            "Yun He",
            "Sinong Wang",
            "Han Fang",
            "Sarath Chandar",
            "Chen Zhu"
        ],
        "title": "Boosting LLM Reasoning via Spontaneous Self-Correction",
        "abstract": "arXiv:2506.06923v1 Announce Type: new  Abstract: While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles -- solution proposer and verifier -- to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.",
        "arxiv_id": "2506.06923",
        "ARXIVID": "2506.06923",
        "COMMENT": "Does not match any specific criteria but explores reasoning improvements in LLMs, which is tangentially related to general interest in language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07390": {
        "authors": [
            "Xin-Cheng Wen",
            "Yijun Yang",
            "Cuiyun Gao",
            "Yang Xiao",
            "Deheng Ye"
        ],
        "title": "Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data",
        "abstract": "arXiv:2506.07390v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models' ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24\\%-22.77\\% improvement in the accuracy. The source code and data are available at https://github.com/Xin-Cheng-Wen/PO4Vul.",
        "arxiv_id": "2506.07390",
        "ARXIVID": "2506.07390",
        "COMMENT": "Does not match any specific criteria. Focuses on vulnerability detection in LLMs, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07399": {
        "authors": [
            "Peiru Yang",
            "Jinhua Yin",
            "Haoran Zheng",
            "Xueying Bai",
            "Huili Wang",
            "Yufei Sun",
            "Xintian Li",
            "Shangguang Wang",
            "Yongfeng Huang",
            "Tao Qi"
        ],
        "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems",
        "abstract": "arXiv:2506.07399v1 Announce Type: new  Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large vision-language models by integrating cross-modal knowledge, enabling their increasing adoption across real-world multimodal tasks. These knowledge databases may contain sensitive information that requires privacy protection. However, multimodal RAG systems inherently grant external users indirect access to such data, making them potentially vulnerable to privacy attacks, particularly membership inference attacks (MIAs). % Existing MIA methods targeting RAG systems predominantly focus on the textual modality, while the visual modality remains relatively underexplored. To bridge this gap, we propose MrM, the first black-box MIA framework targeted at multimodal RAG systems. It utilizes a multi-object data perturbation framework constrained by counterfactual attacks, which can concurrently induce the RAG systems to retrieve the target data and generate information that leaks the membership information. Our method first employs an object-aware data perturbation method to constrain the perturbation to key semantics and ensure successful retrieval. Building on this, we design a counterfact-informed mask selection strategy to prioritize the most informative masked regions, aiming to eliminate the interference of model self-knowledge and amplify attack efficacy. Finally, we perform statistical membership inference by modeling query trials to extract features that reflect the reconstruction of masked semantics from response patterns. Experiments on two visual datasets and eight mainstream commercial visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves consistently strong performance across both sample-level and set-level evaluations, and remains robust under adaptive defenses.",
        "arxiv_id": "2506.07399",
        "ARXIVID": "2506.07399",
        "COMMENT": "Does not match any specific criteria. Focuses on membership inference attacks in multimodal RAG systems, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07214": {
        "authors": [
            "Zhiyuan Zhong",
            "Zhen Sun",
            "Yepang Liu",
            "Xinlei He",
            "Guanhong Tao"
        ],
        "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation",
        "abstract": "arXiv:2506.07214v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have shown remarkable performance, but are also vulnerable to backdoor attacks whereby the adversary can manipulate the model's outputs through hidden triggers. Prior attacks primarily rely on single-modality triggers, leaving the crucial cross-modal fusion nature of VLMs largely unexplored. Unlike prior work, we identify a novel attack surface that leverages cross-modal semantic mismatches as implicit triggers. Based on this insight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data poisoning attack that injects stealthy backdoors by deliberately misaligning image-text pairs during training. To perform the attack, we construct SIMBad, a dataset tailored for semantic manipulation involving color and object attributes. Extensive experiments across four widely used VLMs show that BadSem achieves over 98% average ASR, generalizes well to out-of-distribution datasets, and can transfer across poisoning modalities. Our detailed analysis using attention visualization shows that backdoored models focus on semantically sensitive regions under mismatched conditions while maintaining normal behavior on clean inputs. To mitigate the attack, we try two defense strategies based on system prompt and supervised fine-tuning but find that both of them fail to mitigate the semantic backdoor. Our findings highlight the urgent need to address semantic vulnerabilities in VLMs for their safer deployment.",
        "arxiv_id": "2506.07214",
        "ARXIVID": "2506.07214",
        "COMMENT": "Does not match any specific criteria. Focuses on backdoor attacks in Vision Language Models, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07750": {
        "authors": [
            "Hyunsoo Kim",
            "Donghyun Kim",
            "Suhyun Kim"
        ],
        "title": "Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation",
        "abstract": "arXiv:2506.07750v1 Announce Type: new  Abstract: How can we generate an image B' that satisfies A:A'::B:B', given the input images A,A' and B? Recent works have tackled this challenge through approaches like visual in-context learning or visual instruction. However, these methods are typically limited to specific models (e.g. InstructPix2Pix. Inpainting models) rather than general diffusion models (e.g. Stable Diffusion, SDXL). This dependency may lead to inherited biases or lower editing capabilities. In this paper, we propose Difference Inversion, a method that isolates only the difference from A and A' and applies it to B to generate a plausible B'. To address model dependency, it is crucial to structure prompts in the form of a \"Full Prompt\" suitable for input to stable diffusion models, rather than using an \"Instruction Prompt\". To this end, we accurately extract the Difference between A and A' and combine it with the prompt of B, enabling a plug-and-play application of the difference. To extract a precise difference, we first identify it through 1) Delta Interpolation. Additionally, to ensure accurate training, we propose the 2) Token Consistency Loss and 3) Zero Initialization of Token Embeddings. Our extensive experiments demonstrate that Difference Inversion outperforms existing baselines both quantitatively and qualitatively, indicating its ability to generate more feasible B' in a model-agnostic manner.",
        "arxiv_id": "2506.07750",
        "ARXIVID": "2506.07750",
        "COMMENT": "Does not match any specific criteria. Focuses on image analogy generation, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06524": {
        "authors": [
            "Sam Earle",
            "Ahmed Khalifa",
            "Muhammad Umair Nasir",
            "Zehua Jiang",
            "Graham Todd",
            "Andrzej Banburski-Fahey",
            "Julian Togelius"
        ],
        "title": "ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search",
        "abstract": "arXiv:2506.06524v1 Announce Type: new  Abstract: There is much interest in using large pre-trained models in Automatic Game Design (AGD), whether via the generation of code, assets, or more abstract conceptualization of design ideas. But so far this interest largely stems from the ad hoc use of such generative models under persistent human supervision. Much work remains to show how these tools can be integrated into longer-time-horizon AGD pipelines, in which systems interface with game engines to test generated content autonomously. To this end, we introduce ScriptDoctor, a Large Language Model (LLM)-driven system for automatically generating and testing games in PuzzleScript, an expressive but highly constrained description language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates and tests game design ideas in an iterative loop, where human-authored examples are used to ground the system's output, compilation errors from the PuzzleScript engine are used to elicit functional code, and search-based agents play-test generated games. ScriptDoctor serves as a concrete example of the potential of automated, open-ended LLM-based workflows in generating novel game content.",
        "arxiv_id": "2506.06524",
        "ARXIVID": "2506.06524",
        "COMMENT": "Does not match any specific criterion but discusses game design using LLMs, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06910": {
        "authors": [
            "Mahnaz Koupaee",
            "Xueying Bai",
            "Mudan Chen",
            "Greg Durrett",
            "Nathanael Chambers",
            "Niranjan Balasubramanian"
        ],
        "title": "Causal Graph based Event Reasoning using Semantic Relation Experts",
        "abstract": "arXiv:2506.06910v1 Announce Type: new  Abstract: Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.",
        "arxiv_id": "2506.06910",
        "ARXIVID": "2506.06910",
        "COMMENT": "Does not match any specific criteria. Focuses on causal graph generation and event reasoning, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07814": {
        "authors": [
            "Yongzhen Wang",
            "Yongjun Li",
            "Zhuoran Zheng",
            "Xiao-Ping Zhang",
            "Mingqiang Wei"
        ],
        "title": "M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration",
        "abstract": "arXiv:2506.07814v1 Announce Type: new  Abstract: Natural images are often degraded by complex, composite degradations such as rain, snow, and haze, which adversely impact downstream vision applications. While existing image restoration efforts have achieved notable success, they are still hindered by two critical challenges: limited generalization across dynamically varying degradation scenarios and a suboptimal balance between preserving local details and modeling global dependencies. To overcome these challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based Mamba-CNN fusion framework for efficient and robust all-in-one image restoration. M2Restore introduces three key contributions: First, to boost the model's generalization across diverse degradation conditions, we exploit a CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with CLIP-derived semantic priors. This mechanism is further refined via cross-modal feature calibration, which enables precise expert selection for various degradation types. Second, to jointly capture global contextual dependencies and fine-grained local details, we design a dual-stream architecture that integrates the localized representational strength of CNNs with the long-range modeling efficiency of Mamba. This integration enables collaborative optimization of global semantic relationships and local structural fidelity, preserving global coherence while enhancing detail restoration. Third, we introduce an edge-aware dynamic gating mechanism that adaptively balances global modeling and local enhancement by reallocating computational attention to degradation-sensitive regions. This targeted focus leads to more efficient and precise restoration. Extensive experiments across multiple image restoration benchmarks validate the superiority of M2Restore in both visual quality and quantitative performance.",
        "arxiv_id": "2506.07814",
        "ARXIVID": "2506.07814",
        "COMMENT": "Does not match any specific criteria. Focuses on image restoration, not spatial intelligence, multimodal models, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07720": {
        "authors": [
            "Yufei Guo",
            "Yuhan Zhang",
            "Zhou Jie",
            "Xiaode Liu",
            "Xin Tong",
            "Yuanpei Chen",
            "Weihang Peng",
            "Zhe Ma"
        ],
        "title": "ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks",
        "abstract": "arXiv:2506.07720v1 Announce Type: new  Abstract: The Spiking Neural Network (SNN), a biologically inspired neural network infrastructure, has garnered significant attention recently. SNNs utilize binary spike activations for efficient information transmission, replacing multiplications with additions, thereby enhancing energy efficiency. However, binary spike activation maps often fail to capture sufficient data information, resulting in reduced accuracy. To address this challenge, we advocate reversing the bit of the weight and activation for SNNs, called \\textbf{ReverB-SNN}, inspired by recent findings that highlight greater accuracy degradation from quantizing activations compared to weights. Specifically, our method employs real-valued spike activations alongside binary weights in SNNs. This preserves the event-driven and multiplication-free advantages of standard SNNs while enhancing the information capacity of activations. Additionally, we introduce a trainable factor within binary weights to adaptively learn suitable weight amplitudes during training, thereby increasing network capacity. To maintain efficiency akin to vanilla \\textbf{ReverB-SNN}, our trainable binary weight SNNs are converted back to standard form using a re-parameterization technique during inference. Extensive experiments across various network architectures and datasets, both static and dynamic, demonstrate that our approach consistently outperforms state-of-the-art methods.",
        "arxiv_id": "2506.07720",
        "ARXIVID": "2506.07720",
        "COMMENT": "Does not closely match any specific criteria but is related to efficient neural network methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06757": {
        "authors": [
            "Ziyu Yue",
            "Ruixi You",
            "Feng Xu"
        ],
        "title": "SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image",
        "abstract": "arXiv:2506.06757v1 Announce Type: new  Abstract: To translate synthetic aperture radar (SAR) image into interpretable forms for human understanding is the ultimate goal of SAR advanced information retrieval. Existing methods mainly focus on 3D surface reconstruction or local geometric feature extraction of targets, neglecting the role of structural modeling in capturing semantic information. This paper proposes a novel task: SAR target structure recovery, which aims to infer the components of a target and the structural relationships between its components, specifically symmetry and adjacency, from a single-view SAR image. Through learning the structural consistency and geometric diversity across the same type of targets as observed in different SAR images, it aims to derive the semantic representation of target directly from its 2D SAR image. To solve this challenging task, a two-step algorithmic framework based on structural descriptors is developed. Specifically, in the training phase, it first detects 2D keypoints from real SAR images, and then learns the mapping from these keypoints to 3D hierarchical structures using simulated data. During the testing phase, these two steps are integrated to infer the 3D structure from real SAR images. Experimental results validated the effectiveness of each step and demonstrated, for the first time, that 3D semantic structural representation of aircraft targets can be directly derived from a single-view SAR image.",
        "arxiv_id": "2506.06757",
        "ARXIVID": "2506.06757",
        "COMMENT": "Does not match any specific criteria but focuses on extracting 3D semantic structural representation from SAR images.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06739": {
        "authors": [
            "Andrew Cropper",
            "Filipe Gouveia",
            "David M. Cerna"
        ],
        "title": "Honey, I shrunk the hypothesis space (through logical preprocessing)",
        "abstract": "arXiv:2506.06739v1 Announce Type: new  Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The goal is to search a hypothesis space for a hypothesis that generalises training examples and background knowledge. We introduce an approach that 'shrinks' the hypothesis space before an ILP system searches it. Our approach uses background knowledge to find rules that cannot be in an optimal hypothesis regardless of the training examples. For instance, our approach discovers relationships such as \"even numbers cannot be odd\" and \"prime numbers greater than 2 are odd\". It then removes violating rules from the hypothesis space. We implement our approach using answer set programming and use it to shrink the hypothesis space of a constraint-based ILP system. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can substantially reduce learning times whilst maintaining predictive accuracies. For instance, given just 10 seconds of preprocessing time, our approach can reduce learning times from over 10 hours to only 2 seconds.",
        "arxiv_id": "2506.06739",
        "ARXIVID": "2506.06739",
        "COMMENT": "Does not match any specific criteria but introduces a preprocessing method to reduce hypothesis space in inductive logic programming.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06864": {
        "authors": [
            "Junyu Liu",
            "Jianfeng Ren",
            "Sunhong Liang",
            "Xudong Jiang"
        ],
        "title": "Face recognition on point cloud with cgan-top for denoising",
        "abstract": "arXiv:2506.06864v1 Announce Type: new  Abstract: Face recognition using 3D point clouds is gaining growing interest, while raw point clouds often contain a significant amount of noise due to imperfect sensors. In this paper, an end-to-end 3D face recognition on a noisy point cloud is proposed, which synergistically integrates the denoising and recognition modules. Specifically, a Conditional Generative Adversarial Network on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the noise in the point cloud, and recover the underlying features for subsequent recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is then adapted to recognize faces from the processed point cloud, which hierarchically links both the local point features and neighboring features of multiple scales. The proposed method is validated on the Bosphorus dataset. It significantly improves the recognition accuracy under all noise settings, with a maximum gain of 14.81%.",
        "arxiv_id": "2506.06864",
        "ARXIVID": "2506.06864",
        "COMMENT": "Does not match any specific criteria but focuses on 3D face recognition using point clouds.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07412": {
        "authors": [
            "Changsheng Gao",
            "Wei Zhou",
            "Guosheng Lin",
            "Weisi Lin"
        ],
        "title": "Compressed Feature Quality Assessment: Dataset and Baselines",
        "abstract": "arXiv:2506.07412v1 Announce Type: new  Abstract: The widespread deployment of large models in resource-constrained environments has underscored the need for efficient transmission of intermediate feature representations. In this context, feature coding, which compresses features into compact bitstreams, becomes a critical component for scenarios involving feature transmission, storage, and reuse. However, this compression process introduces inherent semantic degradation that is notoriously difficult to quantify with traditional metrics. To address this, this paper introduces the research problem of Compressed Feature Quality Assessment (CFQA), which seeks to evaluate the semantic fidelity of compressed features. To advance CFQA research, we propose the first benchmark dataset, comprising 300 original features and 12000 compressed features derived from three vision tasks and four feature codecs. Task-specific performance drops are provided as true semantic distortion for the evaluation of CFQA metrics. We assess the performance of three widely used metrics (MSE, cosine similarity, and Centered Kernel Alignment) in capturing semantic degradation. The results underscore the representativeness of the dataset and highlight the need for more refined metrics capable of addressing the nuances of semantic distortion in compressed features. To facilitate the ongoing development of CFQA research, we release the dataset and all accompanying source code at \\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}. This contribution aims to advance the field and provide a foundational resource for the community to explore CFQA.",
        "arxiv_id": "2506.07412",
        "ARXIVID": "2506.07412",
        "COMMENT": "Does not match any specific criteria but introduces a new benchmark dataset for compressed feature quality assessment.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07364": {
        "authors": [
            "Chengchao Shen",
            "Dawei Liu",
            "Jianxin Wang"
        ],
        "title": "Multiple Object Stitching for Unsupervised Representation Learning",
        "abstract": "arXiv:2506.07364v1 Announce Type: new  Abstract: Contrastive learning for single object centric images has achieved remarkable progress on unsupervised representation, but suffering inferior performance on the widespread images with multiple objects. In this paper, we propose a simple but effective method, Multiple Object Stitching (MOS), to refine the unsupervised representation for multi-object images. Specifically, we construct the multi-object images by stitching the single object centric ones, where the objects in the synthesized multi-object images are predetermined. Hence, compared to the existing contrastive methods, our method provides additional object correspondences between multi-object images without human annotations. In this manner, our method pays more attention to the representations of each object in multi-object image, thus providing more detailed representations for complicated downstream tasks, such as object detection and semantic segmentation. Experimental results on ImageNet, CIFAR and COCO datasets demonstrate that our proposed method achieves the leading unsupervised representation performance on both single object centric images and multi-object ones. The source code is available at https://github.com/visresearch/MultipleObjectStitching.",
        "arxiv_id": "2506.07364",
        "ARXIVID": "2506.07364",
        "COMMENT": "Does not match any specific criteria but focuses on unsupervised representation learning for multi-object images, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07075": {
        "authors": [
            "Liwen Zheng",
            "Chaozhuo Li",
            "Haoran Jia",
            "Xi Zhang"
        ],
        "title": "Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression",
        "abstract": "arXiv:2506.07075v1 Announce Type: new  Abstract: The growing complexity of factual claims in real-world scenarios presents significant challenges for automated fact verification systems, particularly in accurately aggregating and reasoning over multi-hop evidence. Existing approaches often rely on static or shallow models that fail to capture the evolving structure of reasoning paths, leading to fragmented retrieval and limited interpretability. To address these issues, we propose a Structural Reasoning framework for Multi-hop Fact Verification that explicitly models reasoning paths as structured graphs throughout both evidence retrieval and claim verification stages. Our method comprises two key modules: a structure-enhanced retrieval mechanism that constructs reasoning graphs to guide evidence collection, and a reasoning-path-guided verification module that incrementally builds subgraphs to represent evolving inference trajectories. We further incorporate a structure-aware reasoning mechanism that captures long-range dependencies across multi-hop evidence chains, enabling more precise verification. Extensive experiments on the FEVER and HoVer datasets demonstrate that our approach consistently outperforms strong baselines, highlighting the effectiveness of reasoning-path modeling in enhancing retrieval precision and verification accuracy.",
        "arxiv_id": "2506.07075",
        "ARXIVID": "2506.07075",
        "COMMENT": "Does not match any specific criteria but focuses on reasoning paths for fact verification, which is tangentially related to reasoning in embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07216": {
        "authors": [
            "Nada Aboudeshish",
            "Dmitry Ignatov",
            "Radu Timofte"
        ],
        "title": "AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?",
        "abstract": "arXiv:2506.07216v1 Announce Type: new  Abstract: Data augmentation is a crucial technique in deep learning, particularly for tasks with limited dataset diversity, such as skeleton-based datasets. This paper proposes a comprehensive data augmentation framework that integrates geometric transformations, random cropping, rotation, zooming and intensity-based transformations, brightness and contrast adjustments to simulate real-world variations. Random cropping ensures the preservation of spatio-temporal integrity while addressing challenges such as viewpoint bias and occlusions. The augmentation pipeline generates three augmented versions for each sample in addition to the data set sample, thus quadrupling the data set size and enriching the diversity of gesture representations. The proposed augmentation strategy is evaluated on three models: multi-stream e2eET, FPPR point cloud-based hand gesture recognition (HGR), and DD-Network. Experiments are conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB. The e2eET model, recognized as the state-of-the-art for hand gesture recognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best performing model on SHREC'17, excels in point cloud-based gesture recognition. DD-Net, a lightweight and efficient architecture for skeleton-based action recognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB). The results underline the effectiveness and versatility of the proposed augmentation strategy, significantly improving model generalization and robustness across diverse datasets and architectures. This framework not only establishes state-of-the-art results on all three evaluated models but also offers a scalable solution to advance HGR and action recognition applications in real-world scenarios. The framework is available at https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR",
        "arxiv_id": "2506.07216",
        "ARXIVID": "2506.07216",
        "COMMENT": "Does not match any specific criteria but focuses on data augmentation for gesture recognition, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07112": {
        "authors": [
            "Changhong Fu",
            "Hua Lin",
            "Haobo Zuo",
            "Liangliang Yao",
            "Liguo Zhang"
        ],
        "title": "EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring",
        "abstract": "arXiv:2506.07112v1 Announce Type: new  Abstract: Text spotting for industrial panels is a key task for intelligent monitoring. However, achieving efficient and accurate text spotting for complex industrial panels remains challenging due to issues such as cross-scale localization and ambiguous boundaries in dense text regions. Moreover, most existing methods primarily focus on representing a single text shape, neglecting a comprehensive exploration of multi-scale feature information across different texts. To address these issues, this work proposes a novel multi-scale dense text spotter for edge AI-based vision system (EdgeSpotter) to achieve accurate and robust industrial panel monitoring. Specifically, a novel Transformer with efficient mixer is developed to learn the interdependencies among multi-level features, integrating multi-layer spatial and semantic cues. In addition, a new feature sampling with catmull-rom splines is designed, which explicitly encodes the shape, position, and semantic information of text, thereby alleviating missed detections and reducing recognition errors caused by multi-scale or dense text regions. Furthermore, a new benchmark dataset for industrial panel monitoring (IPM) is constructed. Extensive qualitative and quantitative evaluations on this challenging benchmark dataset validate the superior performance of the proposed method in different challenging panel monitoring tasks. Finally, practical tests based on the self-designed edge AI-based vision system demonstrate the practicality of the method. The code and demo will be available at https://github.com/vision4robotics/EdgeSpotter.",
        "arxiv_id": "2506.07112",
        "ARXIVID": "2506.07112",
        "COMMENT": "Does not match any specific criteria but focuses on text spotting for industrial panels, which is tangentially related to computer vision but not directly relevant to the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07731": {
        "authors": [
            "Mouadh Yagoubi",
            "Yasser Dahou",
            "Billel Mokeddem",
            "Younes Belkada",
            "Phuc H. Le-Khac",
            "Basma El Amel Boussaha",
            "Reda Alami",
            "Jingwei Zuo",
            "Damiano Marsili",
            "Mugariya Farooq",
            "Mounia Lalmas",
            "Georgia Gkioxari",
            "Patrick Gallinari",
            "Philip Torr",
            "Hakim Hacid"
        ],
        "title": "NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models",
        "abstract": "arXiv:2506.07731v1 Announce Type: new  Abstract: Existing benchmarks have proven effective for assessing the performance of fully trained large language models. However, we find striking differences in the early training stages of small models, where benchmarks often fail to provide meaningful or discriminative signals. To explore how these differences arise, this competition tackles the challenge of designing scientific knowledge evaluation tasks specifically tailored for measuring early training progress of language models. Participants are invited to develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models. To support this effort, we provide three pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate checkpoints sampled during training up to 200B tokens. All experiments and development work can be run on widely available free cloud-based GPU platforms, making participation accessible to researchers with limited computational resources. Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain. By promoting the design of tailored evaluation strategies for early training, this competition aims to attract a broad range of participants from various disciplines, including those who may not be machine learning experts or have access to dedicated GPU resources. Ultimately, this initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development.",
        "arxiv_id": "2506.07731",
        "ARXIVID": "2506.07731",
        "COMMENT": "Does not match any specific criteria but introduces a competition for early training evaluation of LLMs, which is tangentially related to benchmarks but not specific to embodied or robotic AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06993": {
        "authors": [
            "Cong Guan",
            "Jiacheng Ying",
            "Yuya Ieiri",
            "Osamu Yoshie"
        ],
        "title": "DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching",
        "abstract": "arXiv:2506.06993v1 Announce Type: new  Abstract: Dual-camera super-resolution is highly practical for smartphone photography that primarily super-resolve the wide-angle images using the telephoto image as a reference. In this paper, we propose DM$^3$Net, a novel dual-camera super-resolution network based on Domain Modulation and Multi-scale Matching. To bridge the domain gap between the high-resolution domain and the degraded domain, we learn two compressed global representations from image pairs corresponding to the two domains. To enable reliable transfer of high-frequency structural details from the reference image, we design a multi-scale matching module that conducts patch-level feature matching and retrieval across multiple receptive fields to improve matching accuracy and robustness. Moreover, we also introduce Key Pruning to achieve a significant reduction in memory usage and inference time with little model performance sacrificed. Experimental results on three real-world datasets demonstrate that our DM$^3$Net outperforms the state-of-the-art approaches.",
        "arxiv_id": "2506.06993",
        "ARXIVID": "2506.06993",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning due to its focus on dual-camera super-resolution.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06850": {
        "authors": [
            "Sara M. Cerqueira",
            "Manuel Palermo",
            "Cristina P. Santos"
        ],
        "title": "Deep Inertial Pose: A deep learning approach for human pose estimation",
        "abstract": "arXiv:2506.06850v1 Announce Type: new  Abstract: Inertial-based Motion capture system has been attracting growing attention due to its wearability and unsconstrained use. However, accurate human joint estimation demands several complex and expertise demanding steps, which leads to expensive software such as the state-of-the-art MVN Awinda from Xsens Technologies. This work aims to study the use of Neural Networks to abstract the complex biomechanical models and analytical mathematics required for pose estimation. Thus, it presents a comparison of different Neural Network architectures and methodologies to understand how accurately these methods can estimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda) Magnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method was the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle distance error of 7.96, using Mtw Awinda data. Also, an ablation study was conducted to study the impact of data augmentation, output representation, window size, loss function and magnetometer data on the pose estimation error. This work indicates that Neural Networks can be trained to estimate human pose, with results comparable to the state-of-the-art fusion filters.",
        "arxiv_id": "2506.06850",
        "ARXIVID": "2506.06850",
        "COMMENT": "Does not match any specific criteria but is related to human pose estimation, which is tangentially relevant to computer vision and embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07572": {
        "authors": [
            "Yu Li",
            "Feng Xue",
            "Shujie Li",
            "Jinrui Zhang",
            "Shuang Yang",
            "Dan Guo",
            "Richang Hong"
        ],
        "title": "Learning Speaker-Invariant Visual Features for Lipreading",
        "abstract": "arXiv:2506.07572v1 Announce Type: new  Abstract: Lipreading is a challenging cross-modal task that aims to convert visual lip movements into spoken text. Existing lipreading methods often extract visual features that include speaker-specific lip attributes (e.g., shape, color, texture), which introduce spurious correlations between vision and text. These correlations lead to suboptimal lipreading accuracy and restrict model generalization. To address this challenge, we introduce SIFLip, a speaker-invariant visual feature learning framework that disentangles speaker-specific attributes using two complementary disentanglement modules (Implicit Disentanglement and Explicit Disentanglement) to improve generalization. Specifically, since different speakers exhibit semantic consistency between lip movements and phonetic text when pronouncing the same words, our implicit disentanglement module leverages stable text embeddings as supervisory signals to learn common visual representations across speakers, implicitly decoupling speaker-specific features. Additionally, we design a speaker recognition sub-task within the main lipreading pipeline to filter speaker-specific features, then further explicitly disentangle these personalized visual features from the backbone network via gradient reversal. Experimental results demonstrate that SIFLip significantly enhances generalization performance across multiple public datasets. Experimental results demonstrate that SIFLip significantly improves generalization performance across multiple public datasets, outperforming state-of-the-art methods.",
        "arxiv_id": "2506.07572",
        "ARXIVID": "2506.07572",
        "COMMENT": "Does not match any specific criteria but is related to cross-modal tasks and visual feature learning, which is tangentially relevant to your friend's interest in vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07446": {
        "authors": [
            "Liwen Zheng",
            "Chaozhuo Li",
            "Zheng Liu",
            "Feiran Huang",
            "Haoran Jia",
            "Zaisheng Ye",
            "Xi Zhang"
        ],
        "title": "Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification",
        "abstract": "arXiv:2506.07446v1 Announce Type: new  Abstract: Fact verification plays a vital role in combating misinformation by assessing the veracity of claims through evidence retrieval and reasoning. However, traditional methods struggle with complex claims requiring multi-hop reasoning over fragmented evidence, as they often rely on static decomposition strategies and surface-level semantic retrieval, which fail to capture the nuanced structure and intent of the claim. This results in accumulated reasoning errors, noisy evidence contamination, and limited adaptability to diverse claims, ultimately undermining verification accuracy in complex scenarios. To address this, we propose Atomic Fact Extraction and Verification (AFEV), a novel framework that iteratively decomposes complex claims into atomic facts, enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically refines claim understanding and reduces error propagation through iterative fact extraction, reranks evidence to filter noise, and leverages context-specific demonstrations to guide the reasoning process. Extensive experiments on five benchmark datasets demonstrate that AFEV achieves state-of-the-art performance in both accuracy and interpretability.",
        "arxiv_id": "2506.07446",
        "ARXIVID": "2506.07446",
        "COMMENT": "Does not match any specific criteria but is related to fact verification and reasoning, which is tangentially relevant to your friend's interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07809": {
        "authors": [
            "Weilei Wen",
            "Tianyi Zhang",
            "Qianqian Zhao",
            "Zhaohui Zheng",
            "Chunle Guo",
            "Xiuli Shao",
            "Chongyi Li"
        ],
        "title": "Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution",
        "abstract": "arXiv:2506.07809v1 Announce Type: new  Abstract: Recent advancements in codebook-based real image super-resolution (SR) have shown promising results in real-world applications. The core idea involves matching high-quality image features from a codebook based on low-resolution (LR) image features. However, existing methods face two major challenges: inaccurate feature matching with the codebook and poor texture detail reconstruction. To address these issues, we propose a novel Uncertainty-Guided and Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key components: (1) an uncertainty learning mechanism that guides the model to focus on texture-rich regions, (2) a Top-k feature matching strategy that enhances feature matching accuracy by fusing multiple candidate features, and (3) an Align-Attention module that enhances the alignment of information between LR and HR features. Experimental results demonstrate significant improvements in texture realism and reconstruction fidelity compared to existing methods. We will release the code upon formal publication.",
        "arxiv_id": "2506.07809",
        "ARXIVID": "2506.07809",
        "COMMENT": "Does not match any specific criteria but is related to image super-resolution, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06712": {
        "authors": [
            "Saiyu Hu",
            "Chunlei He",
            "Jianfeng Zhang",
            "Dexing Kong",
            "Shoujun Huang"
        ],
        "title": "Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation",
        "abstract": "arXiv:2506.06712v1 Announce Type: new  Abstract: Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are widely used in image segmentation, which however depend heavily on the selection of initial curve configurations. In this paper, we firstly propose several hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce tunable initial velocity fields, enabling adaptive optimization for diverse segmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows and establish the numerical equivalence between dissipative HMCF formulations and certain wave equations using the level set method with signed distance function. Building on this framework, we furthermore develop hyperbolic dual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth Heaviside functions for edge-aware force modulation to suppress over-diffusion near weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta algorithm with nine-point stencil spatial discretization when solving the above-mentioned wave equations. Experiments show that both HMCF-ACMs and HDRF-ACMs could achieve more precise segmentations with superior noise resistance and numerical stability due to task-adaptive configurations of initial velocities and initial contours.",
        "arxiv_id": "2506.06712",
        "ARXIVID": "2506.06712",
        "COMMENT": "Does not match any specific criteria but is related to image segmentation, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07087": {
        "authors": [
            "Weiqi Yan",
            "Lvhai Chen",
            "Huaijia Kou",
            "Shengchuan Zhang",
            "Yan Zhang",
            "Liujuan Cao"
        ],
        "title": "UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning",
        "abstract": "arXiv:2506.07087v1 Announce Type: new  Abstract: Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it doesn't need to rely on extensive pixel-level labels. Existing UCOD methods typically generate pseudo-labels using fixed strategies and train 1 x1 convolutional layers as a simple decoder, leading to low performance compared to fully-supervised methods. We emphasize two drawbacks in these approaches: 1). The model is prone to fitting incorrect knowledge due to the pseudo-label containing substantial noise. 2). The simple decoder fails to capture and learn the semantic features of camouflaged objects, especially for small-sized objects, due to the low-resolution pseudo-labels and severe confusion between foreground and background pixels. To this end, we propose a UCOD method with a teacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL, which contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial (DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines pseudo-labels generated by fixed strategies and the teacher model to prevent the model from overfitting incorrect knowledge while preserving the ability for self-correction; the DBA decoder takes adversarial learning of different segmentation objectives, guides the model to overcome the foreground-background confusion of camouflaged objects, and the Look-Twice mechanism mimics the human tendency to zoom in on camouflaged objects and performs secondary refinement on small-sized objects. Extensive experiments show that our method demonstrates outstanding performance, even surpassing some existing fully supervised methods. The code is available now.",
        "arxiv_id": "2506.07087",
        "ARXIVID": "2506.07087",
        "COMMENT": "Does not match any specific criterion but is related to unsupervised object detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07376": {
        "authors": [
            "Jintao Tong",
            "Ran Ma",
            "Yixiong Zou",
            "Guangyao Chen",
            "Yuhua Li",
            "Ruixuan Li"
        ],
        "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation",
        "abstract": "arXiv:2506.07376v1 Announce Type: new  Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few samples are available for efficient fine-tuning. There are majorly two challenges in this task: (1) the domain gap and (2) fine-tuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and find the model's inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the model's attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn target-specific knowledge specific. Extensive experiments demonstrate that our method surpasses the state-of-the-art method in CD-FSS significantly by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.",
        "arxiv_id": "2506.07376",
        "ARXIVID": "2506.07376",
        "COMMENT": "Does not match any specific criterion but is related to cross-domain few-shot learning, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06846": {
        "authors": [
            "Yangkai Lin",
            "Jiabao Lei",
            "Kui jia"
        ],
        "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles",
        "abstract": "arXiv:2506.06846v1 Announce Type: new  Abstract: In recent years, there has been a growing demand to stylize a given 3D scene to align with the artistic style of reference images for creative purposes. While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method for realistic 3D scene modeling, there remains a challenge in adapting it to stylize 3D GS to match with multiple styles through automatic local style transfer or manual designation, while maintaining memory efficiency for stylization training. In this paper, we introduce a novel 3D GS stylization solution termed Multi-StyleGS to tackle these challenges. In particular, we employ a bipartite matching mechanism to au tomatically identify correspondences between the style images and the local regions of the rendered images. To facilitate local style transfer, we introduce a novel semantic style loss function that employs a segmentation network to apply distinct styles to various objects of the scene and propose a local-global feature matching to enhance the multi-view consistency. Furthermore, this technique can achieve memory efficient training, more texture details and better color match. To better assign a robust semantic label to each Gaussian, we propose several techniques to regularize the segmentation network. As demonstrated by our comprehensive experiments, our approach outperforms existing ones in producing plausible stylization results and offering flexible editing.",
        "arxiv_id": "2506.06846",
        "ARXIVID": "2506.06846",
        "COMMENT": "Does not match any specific criterion but is related to 3D scene stylization, which is tangentially relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07759": {
        "authors": [
            "Diego Forni\\'es-Tabuenca",
            "Alejandro Uribe",
            "Urtzi Otamendi",
            "Arkaitz Artetxe",
            "Juan Carlos Rivera",
            "Oier Lopez de Lacalle"
        ],
        "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models",
        "abstract": "arXiv:2506.07759v1 Announce Type: new  Abstract: Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.",
        "arxiv_id": "2506.07759",
        "ARXIVID": "2506.07759",
        "COMMENT": "Does not match any specific criterion but explores the integration of LLMs with optimization, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07670": {
        "authors": [
            "Xiaohan Lu",
            "Jiaye Fu",
            "Jiaqi Zhang",
            "Zetian Song",
            "Chuanmin Jia",
            "Siwei Ma"
        ],
        "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views",
        "abstract": "arXiv:2506.07670v1 Announce Type: new  Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising results for novel view synthesis (NVS) from sparse input views, particularly under narrow-baseline conditions. However, its performance significantly degrades in wide-baseline scenarios due to limited texture details and geometric inconsistencies across views. To address these challenges, in this paper, we propose ProSplat, a two-stage feed-forward framework designed for high-fidelity rendering under wide-baseline conditions. The first stage involves generating 3D Gaussian primitives via a 3DGS generator. In the second stage, rendered views from these primitives are enhanced through an improvement model. Specifically, this improvement model is based on a one-step diffusion model, further optimized by our proposed Maximum Overlap Reference view Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI supplements missing texture and color by strategically selecting a reference view with maximum viewpoint overlap, while DWEA enforces geometric consistency using epipolar constraints. Additionally, we introduce a divide-and-conquer training strategy that aligns data distributions between the two stages through joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K datasets under wide-baseline settings. Experimental results demonstrate that ProSplat achieves an average improvement of 1 dB in PSNR compared to recent SOTA methods.",
        "arxiv_id": "2506.07670",
        "ARXIVID": "2506.07670",
        "COMMENT": "Does not closely match any specific criterion but is relevant to novel view synthesis and 3D rendering.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07533": {
        "authors": [
            "Wei Tao",
            "Haocheng Lu",
            "Xiaoyang Qu",
            "Bin Zhang",
            "Kai Lu",
            "Jiguang Wan",
            "Jianzong Wang"
        ],
        "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts",
        "abstract": "arXiv:2506.07533v1 Announce Type: new  Abstract: One of the primary challenges in optimizing large language models (LLMs) for long-context inference lies in the high memory consumption of the Key-Value (KV) cache. Existing approaches, such as quantization, have demonstrated promising results in reducing memory usage. However, current quantization methods cannot take both effectiveness and efficiency into account. In this paper, we propose MoQAE, a novel mixed-precision quantization method via mixture of quantization-aware experts. First, we view different quantization bit-width configurations as experts and use the traditional mixture of experts (MoE) method to select the optimal configuration. To avoid the inefficiency caused by inputting tokens one by one into the router in the traditional MoE method, we input the tokens into the router chunk by chunk. Second, we design a lightweight router-only fine-tuning process to train MoQAE with a comprehensive loss to learn the trade-off between model accuracy and memory usage. Finally, we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to further reduce the inference overhead. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art KV cache quantization approaches in both efficiency and effectiveness.",
        "arxiv_id": "2506.07533",
        "ARXIVID": "2506.07533",
        "COMMENT": "Does not closely match any specific criteria but is relevant to the general interest area of large language models and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06912": {
        "authors": [
            "Olivier Papillon",
            "Rafik Goubran",
            "James Green",
            "Julien Larivi\\`ere-Chartier",
            "Caitlin Higginson",
            "Frank Knoefel",
            "R\\'ebecca Robillard"
        ],
        "title": "Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM",
        "abstract": "arXiv:2506.06912v1 Announce Type: new  Abstract: Accurate sleep stage classification is essential for diagnosing sleep disorders, particularly in aging populations. While traditional polysomnography (PSG) relies on electroencephalography (EEG) as the gold standard, its complexity and need for specialized equipment make home-based sleep monitoring challenging. To address this limitation, we investigate the use of electrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive alternatives for five-stage sleep-wake classification. This study introduces a novel approach that leverages ImageBind, a multimodal embedding deep learning model, to integrate PSM data with dual-channel EOG signals for sleep stage classification. Our method is the first reported approach that fuses PSM and EOG data for sleep stage classification with ImageBind. Our results demonstrate that fine-tuning ImageBind significantly improves classification accuracy, outperforming existing models based on single-channel EOG (DeepSleepNet), exclusively PSM data (ViViT), and other multimodal deep learning approaches (MBT). Notably, the model also achieved strong performance without fine-tuning, highlighting its adaptability to specific tasks with limited labeled data, making it particularly advantageous for medical applications. We evaluated our method using 85 nights of patient recordings from a sleep clinic. Our findings suggest that pre-trained multimodal embedding models, even those originally developed for non-medical domains, can be effectively adapted for sleep staging, with accuracies approaching systems that require complex EEG data.",
        "arxiv_id": "2506.06912",
        "ARXIVID": "2506.06912",
        "COMMENT": "Does not closely match any specific criteria but is relevant to the general interest area of multimodal learning and medical applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06959": {
        "authors": [
            "Alena Makarova",
            "Houssam Abbas"
        ],
        "title": "Deontically Constrained Policy Improvement in Reinforcement Learning Agents",
        "abstract": "arXiv:2506.06959v1 Announce Type: new  Abstract: Markov Decision Processes (MDPs) are the most common model for decision making under uncertainty in the Machine Learning community. An MDP captures non-determinism, probabilistic uncertainty, and an explicit model of action. A Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a utility function. This paper considers the problem of learning a decision policy that maximizes utility subject to satisfying a constraint expressed in deontic logic. In this setup, the utility captures the agent's mission - such as going quickly from A to B. The deontic formula represents (ethical, social, situational) constraints on how the agent might achieve its mission by prohibiting classes of behaviors. We use the logic of Expected Act Utilitarianism, a probabilistic stit logic that can be interpreted over controlled MDPs. We develop a variation on policy improvement, and show that it reaches a constrained local maximum of the mission utility. Given that in stit logic, an agent's duty is derived from value maximization, this can be seen as a way of acting to simultaneously maximize two value functions, one of which is implicit, in a bi-level structure. We illustrate these results with experiments on sample MDPs.",
        "arxiv_id": "2506.06959",
        "ARXIVID": "2506.06959",
        "COMMENT": "Does not match any specific criteria but involves reinforcement learning with deontic logic constraints.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06367": {
        "authors": [
            "Jiaxin Pan",
            "Mojtaba Nayyeri",
            "Osama Mohammed",
            "Daniel Hernandez",
            "Rongchuan Zhang",
            "Cheng Cheng",
            "Steffen Staab"
        ],
        "title": "Towards Foundation Model on Temporal Knowledge Graph Reasoning",
        "abstract": "arXiv:2506.06367v1 Announce Type: new  Abstract: Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats (s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform link prediction tasks in transductive or semi-inductive settings, which means the entities, relations, and temporal information in the test graph are fully or partially observed during training. Such reliance on seen elements during inference limits the models' ability to transfer to new domains and generalize to real-world scenarios. A central limitation is the difficulty in learning representations for entities, relations, and timestamps that are transferable and not tied to dataset-specific vocabularies. To overcome these limitations, we introduce the first fully-inductive approach to temporal knowledge graph link prediction. Our model employs sinusoidal positional encodings to capture fine-grained temporal patterns and generates adaptive entity and relation representations using message passing conditioned on both local and global temporal contexts. Our model design is agnostic to temporal granularity and time span, effectively addressing temporal discrepancies across TKGs and facilitating time-aware structural information transfer. As a pretrained, scalable, and transferable model, POSTRA demonstrates strong zero-shot performance on unseen temporal knowledge graphs, effectively generalizing to novel entities, relations, and timestamps. Extensive theoretical analysis and empirical results show that a single pretrained model can improve zero-shot performance on various inductive temporal reasoning scenarios, marking a significant step toward a foundation model for temporal KGs.",
        "arxiv_id": "2506.06367",
        "ARXIVID": "2506.06367",
        "COMMENT": "Does not match any specific criteria but involves temporal knowledge graph reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.06906": {
        "authors": [
            "Nima Jamali",
            "Matina Mahdizadeh Sani",
            "Hanieh Naderi",
            "Shohreh Kasaei"
        ],
        "title": "KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search",
        "abstract": "arXiv:2506.06906v1 Announce Type: new  Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in analyzing 3D point cloud data. However, their vulnerability to adversarial attacks-such as point dropping, shifting, and adding-poses a critical challenge to the reliability of 3D vision systems. These attacks can compromise the semantic and structural integrity of point clouds, rendering many existing defense mechanisms ineffective. To address this issue, a defense strategy named KNN-Defense is proposed, grounded in the manifold assumption and nearest-neighbor search in feature space. Instead of reconstructing surface geometry or enforcing uniform point distributions, the method restores perturbed inputs by leveraging the semantic similarity of neighboring samples from the training set. KNN-Defense is lightweight and computationally efficient, enabling fast inference and making it suitable for real-time and practical applications. Empirical results on the ModelNet40 dataset demonstrated that KNN-Defense significantly improves robustness across various attack types. In particular, under point-dropping attacks-where many existing methods underperform due to the targeted removal of critical points-the proposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on PointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that KNN-Defense offers a scalable and effective solution for enhancing the adversarial resilience of 3D point cloud classifiers. (An open-source implementation of the method, including code and data, is available at https://github.com/nimajam41/3d-knn-defense).",
        "arxiv_id": "2506.06906",
        "ARXIVID": "2506.06906",
        "COMMENT": "Does not match any specific criteria but involves adversarial robustness in 3D vision systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.07885": {
        "authors": [
            "Zubin Bhuyan",
            "Yuanchang Xie",
            "AngkeaReach Rith",
            "Xintong Yan",
            "Nasko Apostolov",
            "Jimi Oke",
            "Chengbo Ai"
        ],
        "title": "CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing",
        "abstract": "arXiv:2506.07885v1 Announce Type: new  Abstract: With the increasing availability of aerial and satellite imagery, deep learning presents significant potential for transportation asset management, safety analysis, and urban planning. This study introduces CrosswalkNet, a robust and efficient deep learning framework designed to detect various types of pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet incorporates a novel detection approach that improves upon traditional object detection strategies by utilizing oriented bounding boxes (OBB), enhancing detection precision by accurately capturing crosswalks regardless of their orientation. Several optimization techniques, including Convolutional Block Attention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine annealing, are implemented to maximize performance and efficiency. A comprehensive dataset comprising over 23,000 annotated crosswalk instances is utilized to train and validate the proposed framework. The best-performing model achieves an impressive precision of 96.5% and a recall of 93.3% on aerial imagery from Massachusetts, demonstrating its accuracy and effectiveness. CrosswalkNet has also been successfully applied to datasets from New Hampshire, Virginia, and Maine without transfer learning or fine-tuning, showcasing its robustness and strong generalization capability. Additionally, the crosswalk detection results, processed using High-Performance Computing (HPC) platforms and provided in polygon shapefile format, have been shown to accelerate data processing and detection, supporting real-time analysis for safety and mobility applications. This integration offers policymakers, transportation engineers, and urban planners an effective instrument to enhance pedestrian safety and improve urban mobility.",
        "arxiv_id": "2506.07885",
        "ARXIVID": "2506.07885",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision applications in urban planning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.06389": {
        "authors": [
            "Rifat Sadik",
            "Tanvir Rahman",
            "Arpan Bhattacharjee",
            "Bikash Chandra Halder",
            "Ismail Hossain"
        ],
        "title": "Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images",
        "abstract": "arXiv:2506.06389v1 Announce Type: new  Abstract: Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism -- adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%.",
        "arxiv_id": "2506.06389",
        "ARXIVID": "2506.06389",
        "COMMENT": "Does not match any specific criteria but discusses adversarial robustness in vision transformers, which is tangentially related to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.06301": {
        "authors": [
            "Muhammad Monjurul Karim",
            "Yan Shi",
            "Shucheng Zhang",
            "Bingzhang Wang",
            "Mehrdad Nasri",
            "Yinhai Wang"
        ],
        "title": "Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review",
        "abstract": "arXiv:2506.06301v1 Announce Type: new  Abstract: Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted -- via architectural, training, prompting, and multimodal strategies -- to bridge the \"modality gap\" with transportation's unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control) and safety (e.g., crash analysis, driver behavior assessment,). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs' transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.",
        "arxiv_id": "2506.06301",
        "ARXIVID": "2506.06301",
        "COMMENT": "Does not match any specific criteria but discusses LLMs in a domain-specific application (roadway safety and mobility).",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.06578": {
        "authors": [
            "Anees Nashath Shaik",
            "Barbara Villarini",
            "Vasileios Argyriou"
        ],
        "title": "A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance",
        "abstract": "arXiv:2506.06578v1 Announce Type: new  Abstract: Surveillance systems play a critical role in security and reconnaissance, but their performance is often compromised by low-quality images and videos, leading to reduced accuracy in face recognition. Additionally, existing AI-based facial analysis models suffer from biases related to skin tone variations and partially occluded faces, further limiting their effectiveness in diverse real-world scenarios. These challenges are the results of data limitations and imbalances, where available training datasets lack sufficient diversity, resulting in unfair and unreliable facial recognition performance. To address these issues, we propose a data-driven platform that enhances surveillance capabilities by generating synthetic training data tailored to compensate for dataset biases. Our approach leverages deep learning-based facial attribute manipulation and reconstruction using autoencoders and Generative Adversarial Networks (GANs) to create diverse and high-quality facial datasets. Additionally, our system integrates an image enhancement module, improving the clarity of low-resolution or occluded faces in surveillance footage. We evaluate our approach using the CelebA dataset, demonstrating that the proposed platform enhances both training data diversity and model fairness. This work contributes to reducing bias in AI-based facial analysis and improving surveillance accuracy in challenging environments, leading to fairer and more reliable security applications.",
        "arxiv_id": "2506.06578",
        "ARXIVID": "2506.06578",
        "COMMENT": "Does not closely match any specific criteria but is related to generative modeling for surveillance applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.07055": {
        "authors": [
            "Tarique Dahri",
            "Zulfiqar Ali Memon",
            "Zhenyu Yu",
            "Mohd. Yamani Idna Idris",
            "Sheheryar Khan",
            "Sadiq Ahmad",
            "Maged Shoman",
            "Saddam Aziz",
            "Rizwan Qureshi"
        ],
        "title": "A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge",
        "abstract": "arXiv:2506.07055v1 Announce Type: new  Abstract: We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework for training compact deep learning models. Unlike traditional methods that rely on pre-trained teacher networks, our approach appends auxiliary classifiers to intermediate feature maps, generating diverse self-supervised knowledge and enabling one-to-one transfer across different network stages. Our method achieves an average improvement of 4.54\\% over the state-of-the-art PS-KD method and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on ImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under few-shot learning scenarios also achieve state-of-the-art results. These findings demonstrate the effectiveness of our approach in enhancing model generalization and performance without the need for large over-parameterized teacher networks. Importantly, at the inference stage, all auxiliary classifiers can be removed, yielding no extra computational cost. This makes our model suitable for deploying small language models on affordable low-computing devices. Owing to its lightweight design and adaptability, our framework is particularly suitable for multimodal sensing and cyber-physical environments that require efficient and responsive inference. LSSKD facilitates the development of intelligent agents capable of learning from limited sensory data under weak supervision.",
        "arxiv_id": "2506.07055",
        "ARXIVID": "2506.07055",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.07368": {
        "authors": [
            "Jiaying He",
            "Yitong Lin",
            "Jiahe Chen",
            "Honghui Xu",
            "Jianwei Zheng"
        ],
        "title": "C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation",
        "abstract": "arXiv:2506.07368v1 Announce Type: new  Abstract: For the immanent challenge of insufficiently annotated samples in the medical field, semi-supervised medical image segmentation (SSMIS) offers a promising solution. Despite achieving impressive results in delineating primary target areas, most current methodologies struggle to precisely capture the subtle details of boundaries. This deficiency often leads to significant diagnostic inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised segmentation model that synergistically integrates complementary competition and contrastive selection. This design significantly sharpens boundary delineation and enhances overall precision. Specifically, we develop an $\\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining boundary localization. Additionally, we incorporate a $\\textit{Dynamic Complementary Competition}$ module that leverages two high-performing sub-networks to generate pseudo-labels, thereby further improving segmentation quality. The proposed C3S3 undergoes rigorous validation on two publicly accessible datasets, encompassing the practices of both MRI and CT scans. The results demonstrate that our method achieves superior performance compared to previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our approach achieves a notable improvement of at least $6\\%$, highlighting the significant advancements. The code is available at https://github.com/Y-TARL/C3S3.",
        "arxiv_id": "2506.07368",
        "ARXIVID": "2506.07368",
        "COMMENT": "Does not match any specific criteria but involves semi-supervised medical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.06886": {
        "authors": [
            "Wafaa Kasri",
            "Yassine Himeur",
            "Abigail Copiaco",
            "Wathiq Mansoor",
            "Ammar Albanna",
            "Valsamma Eapen"
        ],
        "title": "Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis",
        "abstract": "arXiv:2506.06886v1 Announce Type: new  Abstract: Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early intervention. This study presents a hybrid deep learning framework combining Vision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking data. The model uses attention-based fusion to integrate visual, speech, and facial cues, capturing both spatial and temporal dynamics. Unlike traditional handcrafted methods, it applies state-of-the-art deep learning and explainable AI techniques to enhance diagnostic accuracy and transparency. Tested on the Saliency4ASD dataset, the proposed ViT-Mamba model outperformed existing methods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94 specificity. These findings show the model's promise for scalable, interpretable ASD screening, especially in resource-constrained or remote clinical settings where access to expert diagnosis is limited.",
        "arxiv_id": "2506.06886",
        "ARXIVID": "2506.06886",
        "COMMENT": "Does not match any specific criteria but involves vision transformers and multimodal analysis for autism diagnosis.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}