{
    "2509.24200": {
        "authors": [
            "Jiabin Luo",
            "Junhui Lin",
            "Zeyu Zhang",
            "Biao Wu",
            "Meng Fang",
            "Ling Chen",
            "Hao Tang"
        ],
        "title": "UniVid: The Open-Source Unified Video Model",
        "abstract": "arXiv:2509.24200v1 Announce Type: new  Abstract: Unified video modeling that combines generation and understanding capabilities is increasingly important but faces two key challenges: maintaining semantic faithfulness during flow-based generation due to text-visual token imbalance and the limitations of uniform cross-modal attention across the flow trajectory, and efficiently extending image-centric MLLMs to video without costly retraining. We present UniVid, a unified architecture that couples an MLLM with a diffusion decoder through a lightweight adapter, enabling both video understanding and generation. We introduce Temperature Modality Alignment to improve prompt adherence and Pyramid Reflection for efficient temporal reasoning via dynamic keyframe selection. Extensive experiments on standard benchmarks demonstrate state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total score compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA and ActivityNet-QA, respectively, compared with the best prior 7B baselines.",
        "arxiv_id": "2509.24200",
        "ARXIVID": "2509.24200",
        "COMMENT": "Matches criteria 2 and 5 as it explores a unified architecture for video understanding and generation, integrating multimodal large language models with a diffusion decoder.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24473": {
        "authors": [
            "Shijie Lian",
            "Changti Wu",
            "Laurence Tianruo Yang",
            "Hang Yuan",
            "Bin Yu",
            "Lei Zhang",
            "Kai Chen"
        ],
        "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks",
        "abstract": "arXiv:2509.24473v1 Announce Type: new  Abstract: Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
        "arxiv_id": "2509.24473",
        "ARXIVID": "2509.24473",
        "COMMENT": "Matches criteria 1 and 2 as it focuses on spatial reasoning improvements in vision-language models and introduces a novel dataset and training strategy for spatial intelligence.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.24910": {
        "authors": [
            "Songze Li",
            "Zun Wang",
            "Gengze Zhou",
            "Jialu Li",
            "Xiangyu Zeng",
            "Limin Wang",
            "Yu Qiao",
            "Qi Wu",
            "Mohit Bansal",
            "Yi Wang"
        ],
        "title": "Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale",
        "abstract": "arXiv:2509.24910v1 Announce Type: new  Abstract: Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.",
        "arxiv_id": "2509.24910",
        "ARXIVID": "2509.24910",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on navigation and exploration in embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.24304": {
        "authors": [
            "Zefeng He",
            "Xiaoye Qu",
            "Yafu Li",
            "Siyuan Huang",
            "Daizong Liu",
            "Yu Cheng"
        ],
        "title": "FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting",
        "abstract": "arXiv:2509.24304v1 Announce Type: new  Abstract: While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. To overcome these challenges, in this paper, we introduce the concept of thinking with long videos and propose a novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. Developing such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. To solve these challenges, we propose a two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize a strategic decision-making policy.Notably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. Extensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker achieves a significant average improvement of +10.4% over baselines while drastically reducing the number of processed frames. Most notably, our 7B model, FrameThinker establishes a new state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness.",
        "arxiv_id": "2509.24304",
        "ARXIVID": "2509.24304",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on reasoning with long videos using LVLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24695": {
        "authors": [
            "Junsong Chen",
            "Yuyang Zhao",
            "Jincheng Yu",
            "Ruihang Chu",
            "Junyu Chen",
            "Shuai Yang",
            "Xianbang Wang",
            "Yicheng Pan",
            "Daquan Zhou",
            "Huan Ling",
            "Haozhe Liu",
            "Hongwei Yi",
            "Hao Zhang",
            "Muyang Li",
            "Yukang Chen",
            "Han Cai",
            "Sanja Fidler",
            "Ping Luo",
            "Song Han",
            "Enze Xie"
        ],
        "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer",
        "abstract": "arXiv:2509.24695v1 Announce Type: new  Abstract: We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.",
        "arxiv_id": "2509.24695",
        "ARXIVID": "2509.24695",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) and criterion 6 (Video Understanding) due to its focus on efficient video generation with text-video alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.23927": {
        "authors": [
            "Yi Yang",
            "Xiaokun Zhang",
            "Qingchen Fang",
            "Ziqi Ye",
            "Rui Li",
            "Li Liu",
            "Haipeng Wang"
        ],
        "title": "SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing",
        "abstract": "arXiv:2509.23927v1 Announce Type: new  Abstract: Cross-modal artificial intelligence has garnered widespread attention in recent years, achieving significant progress in the study of natural images. However, existing methods are mostly designed for RGB imagery, leaving a significant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with its all-day, all-weather imaging capabilities, plays an irreplaceable role in remote sensing scene understanding. To address this gap, this paper proposes SAR-KnowLIP, the first universal SAR multimodal foundational model, along with reusable data and evaluation baselines. Specifically: (1) This work introduces the critical yet long-overlooked attribute of geographic information into remote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection properties), covering multiple satellite platforms, 120,000 images, and 135 cities. (2) Aligned structured text is generated through a hierarchical cognitive chain-of-thought (HCoT), providing more than one million multi-dimensional semantic annotations of landforms, regional functions, target attributes, and spatial relationships. (3) We design a Self-Consistent Iterative Optimization mechanism that continuously enhances cross-modal alignment through a self-supervised closed loop of contrastive, matching, and reconstruction learning on a transferable multimodal encoder. (4) A unified evaluation benchmark is established across 11 representative downstream vision and vision-language tasks, with comparisons against 14 leading foundation models, where SAR-KnowLIP demonstrates leading performance, particularly in object counting and land-cover classification. We expect that SAR-KnowLIP's large-scale multimodal data, transferable model architecture, and comprehensive experimental benchmark will significantly advance the development of SAR multimodal baseline models.",
        "arxiv_id": "2509.23927",
        "ARXIVID": "2509.23927",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal foundation model for SAR imagery, integrating vision and language.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.25177": {
        "authors": [
            "Bingkui Tong",
            "Jiaer Xia",
            "Kaiyang Zhou"
        ],
        "title": "Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding",
        "abstract": "arXiv:2509.25177v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have shown impressive perception and reasoning capabilities, yet they often suffer from hallucinations -- generating outputs that are linguistically coherent but inconsistent with the context of the input image, including inaccuracies in objects, attributes, and relations. To address this challenge, we propose a simple approach called Layer Contrastive Decoding (LayerCD). Our design is motivated by the observation that shallow visual features are much more likely than deep visual features to cause an MLLM to hallucinate as they only capture biased, low-level information that is insufficient for high-level reasoning. Therefore, LayerCD aims to filter out hallucinations by contrasting the output distributions generated from visual features of different levels, specifically those from the shallow and deep layers of the vision encoder, respectively. We conduct extensive experiments on two hallucination benchmarks and show that LayerCD significantly outperforms current state-of-the-art. The code for LayerCD is available at https://github.com/maifoundations/LayerCD .",
        "arxiv_id": "2509.25177",
        "ARXIVID": "2509.25177",
        "COMMENT": "Matches criteria 2 as it proposes a method to mitigate hallucination in multimodal large language models, focusing on vision-language integration.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.23541": {
        "authors": [
            "Hongyang Li",
            "Jinyuan Qu",
            "Lei Zhang"
        ],
        "title": "OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction",
        "abstract": "arXiv:2509.23541v1 Announce Type: new  Abstract: In this paper, we propose a training scheme called OVSeg3R to learn open-vocabulary 3D instance segmentation from well-studied 2D perception models with the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes from 2D videos as input, avoiding costly manual adjustment while aligning input with real-world applications. By exploiting the 2D to 3D correspondences provided by 3D reconstruction models, OVSeg3R projects each view's 2D instance mask predictions, obtained from an open-vocabulary 2D model, onto 3D to generate annotations for the view's corresponding sub-scene. To avoid incorrectly introduced false positives as supervision due to partial annotations from 2D to 3D, we propose a View-wise Instance Partition algorithm, which partitions predictions to their respective views for supervision, stabilizing the training process. Furthermore, since 3D reconstruction models tend to over-smooth geometric details, clustering reconstructed points into representative super-points based solely on geometry, as commonly done in mainstream 3D segmentation methods, may overlook geometrically non-salient objects. We therefore introduce 2D Instance Boundary-aware Superpoint, which leverages 2D masks to constrain the superpoint clustering, preventing superpoints from violating instance boundaries. With these designs, OVSeg3R not only extends a state-of-the-art closed-vocabulary 3D instance segmentation model to open-vocabulary, but also substantially narrows the performance gap between tail and head classes, ultimately leading to an overall improvement of +2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard open-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP on the novel classes, further validating its effectiveness.",
        "arxiv_id": "2509.23541",
        "ARXIVID": "2509.23541",
        "COMMENT": "Matches criteria 3 as it proposes a novel training scheme for open-vocabulary 3D instance segmentation, leveraging 2D to 3D correspondences.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.22761": {
        "authors": [
            "Yapeng Mi",
            "Hengli Li",
            "Yanpeng Zhao",
            "Chenxi Li",
            "Huimin Wu",
            "Xiaojian Ma",
            "Song-Chun Zhu",
            "Ying Nian Wu",
            "Qing Li"
        ],
        "title": "MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning",
        "abstract": "arXiv:2509.22761v1 Announce Type: new  Abstract: Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic. We instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method.",
        "arxiv_id": "2509.22761",
        "ARXIVID": "2509.22761",
        "COMMENT": "Matches criteria 5 as it explores techniques combining image and text reasoning in a unified latent space for multimodal image generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24251": {
        "authors": [
            "Bangzheng Li",
            "Ximeng Sun",
            "Jiang Liu",
            "Ze Wang",
            "Jialian Wu",
            "Xiaodong Yu",
            "Hao Chen",
            "Emad Barsoum",
            "Muhao Chen",
            "Zicheng Liu"
        ],
        "title": "Latent Visual Reasoning",
        "abstract": "arXiv:2509.24251v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved notable gains in various tasks by incorporating Chain-of-Thought (CoT) reasoning in language spaces. Recent work extends this direction by leveraging external tools for visual editing, thereby enhancing the visual signal along the reasoning trajectories. Nevertheless, these approaches remain fundamentally constrained: reasoning is still confined to the language space, with visual information treated as static preconditions. We introduce Latent Visual Reasoning (LVR), a new paradigm that enables autoregressive reasoning directly in the visual embedding space. A visual encoder first projects images into visual tokens within a joint semantic space shared with the language model. The language model is then trained to generate latent states that reconstruct key visual tokens critical for answering the query, constituting the process of latent visual reasoning. By interleaving LVR with standard text generation, our model achieves substantial gains on perception-intensive visual question answering tasks. In addition, we adapt the GRPO algorithm to conduct reinforcement learning on latent reasoning, further balancing LVR and textual generation. We show that LVR substantially improves fine-grained visual understanding and perception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code base and model weights will be released later.",
        "arxiv_id": "2509.24251",
        "ARXIVID": "2509.24251",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces latent visual reasoning for multimodal large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.24709": {
        "authors": [
            "Yang Chen",
            "Minghao Liu",
            "Yufan Shen",
            "Yunwen Li",
            "Tianyuan Huang",
            "Xinyu Fang",
            "Tianyu Zheng",
            "Wenxuan Huang",
            "Cheng Yang",
            "Daocheng Fu",
            "Jianbiao Mei",
            "Rong Wu",
            "Licheng Wen",
            "Xuemeng Yang",
            "Song Mao",
            "Qunshu Lin",
            "Zhi Yu",
            "Yongliang Shen",
            "Yu Qiao",
            "Botian Shi"
        ],
        "title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?",
        "abstract": "arXiv:2509.24709v1 Announce Type: new  Abstract: The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at https://github.com/L-O-I/IWR-Bench.",
        "arxiv_id": "2509.24709",
        "ARXIVID": "2509.24709",
        "COMMENT": "Matches criteria 5 as it explores integration of video understanding and large language models for interactive webpage reconstruction.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24441": {
        "authors": [
            "Yanpeng Zhao",
            "Shanyan Guan",
            "Yunbo Wang",
            "Yanhao Ge",
            "Wei Li",
            "Xiaokang Yang"
        ],
        "title": "NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding",
        "abstract": "arXiv:2509.24441v1 Announce Type: new  Abstract: We introduce NeoWorld, a deep learning framework for generating interactive 3D virtual worlds from a single input image. Inspired by the on-demand worldbuilding concept in the science fiction novel Simulacron-3 (1964), our system constructs expansive environments where only the regions actively explored by the user are rendered with high visual realism through object-centric 3D representations. Unlike previous approaches that rely on global world generation or 2D hallucination, NeoWorld models key foreground objects in full 3D, while synthesizing backgrounds and non-interacted regions in 2D to ensure efficiency. This hybrid scene structure, implemented with cutting-edge representation learning and object-to-3D techniques, enables flexible viewpoint manipulation and physically plausible scene animation, allowing users to control object appearance and dynamics using natural language commands. As users interact with the environment, the virtual world progressively unfolds with increasing 3D detail, delivering a dynamic, immersive, and visually coherent exploration experience. NeoWorld significantly outperforms existing 2D and depth-layered 2.5D methods on the WorldScore benchmark.",
        "arxiv_id": "2509.24441",
        "ARXIVID": "2509.24441",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel method for spatial reasoning and embodied agents through 3D virtual world simulation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.24897": {
        "authors": [
            "Yang Shi",
            "Yuhao Dong",
            "Yue Ding",
            "Yuran Wang",
            "Xuanyu Zhu",
            "Sheng Zhou",
            "Wenting Liu",
            "Haochen Tian",
            "Rundong Wang",
            "Huanqian Wang",
            "Zuyan Liu",
            "Bohan Zeng",
            "Ruizhe Chen",
            "Qixun Wang",
            "Zhuoran Zhang",
            "Xinlong Chen",
            "Chengzhuo Tong",
            "Bozhou Li",
            "Chaoyou Fu",
            "Qiang Liu",
            "Haotian Wang",
            "Wenjing Yang",
            "Yuanxing Zhang",
            "Pengfei Wan",
            "Yi-Fan Zhang",
            "Ziwei Liu"
        ],
        "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark",
        "abstract": "arXiv:2509.24897v1 Announce Type: new  Abstract: The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling.",
        "arxiv_id": "2509.24897",
        "ARXIVID": "2509.24897",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it evaluates unified multimodal models for understanding and generation tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.23098": {
        "authors": [
            "Na Min An",
            "Inha Kang",
            "Minhyun Lee",
            "Hyunjung Shim"
        ],
        "title": "CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP",
        "abstract": "arXiv:2509.23098v1 Announce Type: new  Abstract: Spatial grounding is crucial for referring image segmentation (RIS), where the goal of the task is to localize an object described by language. Current foundational vision-language models (VLMs), such as CLIP, excel at aligning images and text but struggle with understanding spatial relationships. Within the language stream, most existing methods often focus on the primary noun phrase when extracting local text features, undermining contextual tokens. Within the vision stream, CLIP generates similar features for images with different spatial layouts, resulting in limited sensitivity to spatial structure. To address these limitations, we propose \\textsc{CoPatch}, a zero-shot RIS framework that leverages internal model components to enhance spatial representations in both text and image modalities. For language, \\textsc{CoPatch} constructs hybrid text features by incorporating context tokens carrying spatial cues. For vision, it extracts patch-level image features using our novel path discovered from intermediate layers, where spatial structure is better preserved. These enhanced features are fused into a clustered image-text similarity map, \\texttt{CoMap}, enabling precise mask selection. As a result, \\textsc{CoPatch} significantly improves spatial grounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+ 2--7 mIoU) without requiring any additional training. Our findings underscore the importance of recovering and leveraging the untapped spatial knowledge inherently embedded in VLMs, thereby paving the way for opportunities in zero-shot RIS.",
        "arxiv_id": "2509.23098",
        "ARXIVID": "2509.23098",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it leverages spatial knowledge in CLIP for image segmentation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.23589": {
        "authors": [
            "Shu Liu",
            "Wenlin Chen",
            "Weihao Li",
            "Zheng Wang",
            "Lijin Yang",
            "Jianing Huang",
            "Yipin Zhang",
            "Zhongzhan Huang",
            "Ze Cheng",
            "Hao Yang"
        ],
        "title": "BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving",
        "abstract": "arXiv:2509.23589v1 Announce Type: new  Abstract: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 5% over prior arts.",
        "arxiv_id": "2509.23589",
        "ARXIVID": "2509.23589",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel diffusion-based planner for autonomous driving.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.24008": {
        "authors": [
            "Haonan Ge",
            "Yiwei Wang",
            "Kai-Wei Chang",
            "Hang Wu",
            "Yujun Cai"
        ],
        "title": "FrameMind: Frame-Interleaved Chain-of-Thought for Video Reasoning via Reinforcement Learning",
        "abstract": "arXiv:2509.24008v1 Announce Type: new  Abstract: Current video understanding models rely on fixed frame sampling strategies, processing predetermined visual inputs regardless of the specific reasoning requirements of each question. This static approach limits their ability to adaptively gather visual evidence, leading to suboptimal performance on tasks that require either broad temporal coverage or fine-grained spatial detail. In this paper, we introduce FrameMind, an end-to-end framework trained with reinforcement learning that enables models to dynamically request visual information during reasoning through Frame-Interleaved Chain-of-Thought (FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns where the model alternates between textual reasoning and active visual perception, using tools to extract targeted frames or video clips based on identified knowledge gaps. To train effective dynamic sampling policies, we propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to diverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a group-relative policy optimization algorithm that learns from outcome-based rewards without requiring frame-level annotations. Extensive experiments on challenging benchmarks like MLVU and VideoMME demonstrate that our method significantly outperforms existing models, advancing the state of the art in flexible and efficient video understanding.",
        "arxiv_id": "2509.24008",
        "ARXIVID": "2509.24008",
        "COMMENT": "Matches criteria 6 as it focuses on video reasoning and understanding with novel reinforcement learning methods.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.25139": {
        "authors": [
            "Yue Zhang",
            "Tianyi Ma",
            "Zun Wang",
            "Yanyuan Qiao",
            "Parisa Kordjamshidi"
        ],
        "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
        "abstract": "arXiv:2509.25139v1 Announce Type: new  Abstract: Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.",
        "arxiv_id": "2509.25139",
        "ARXIVID": "2509.25139",
        "COMMENT": "Matches criteria 1 and 3 as it focuses on spatial reasoning and embodied agents in Vision-and-Language Navigation (VLN).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.23737": {
        "authors": [
            "Guole Shen",
            "Tianchen Deng",
            "Yanbo Wang",
            "Yongtao Chen",
            "Yilin Shen",
            "Jiuming Liu",
            "Jingchuan Wang"
        ],
        "title": "GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State",
        "abstract": "arXiv:2509.23737v1 Announce Type: new  Abstract: DUSt3R-based end-to-end scene reconstruction has recently shown promising results in dense visual SLAM. However, most existing methods only use image pairs to estimate pointmaps, overlooking spatial memory and global consistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework for dense scene reconstruction and pose estimation from RGB images without any prior knowledge of the scene or camera parameters. Unlike existing DUSt3R-based frameworks, which operate on all image pairs and predict per-pair point maps in local coordinate frames, our method supports sequentialized input and incrementally estimates metric-scale point clouds in the global coordinate. In order to improve consistent spatial correlation, we use a latent state for spatial memory and design a transformer-based gated update module to reset and update the spatial memory that continuously aggregates and tracks relevant 3D information across frames. Furthermore, we partition the scene into submaps, apply local alignment within each submap, and register all submaps into a common world frame using relative constraints, producing a globally consistent map. Experiments on various datasets show that our framework achieves superior reconstruction accuracy while maintaining real-time performance.",
        "arxiv_id": "2509.23737",
        "ARXIVID": "2509.23737",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a novel SLAM framework for spatial reasoning and embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.24081": {
        "authors": [
            "Sucheng Ren",
            "Chen Chen",
            "Zhenbang Wang",
            "Liangchen Song",
            "Xiangxin Zhu",
            "Alan Yuille",
            "Yinfei Yang",
            "Jiasen Lu"
        ],
        "title": "Autoregressive Video Generation beyond Next Frames Prediction",
        "abstract": "arXiv:2509.24081v1 Announce Type: new  Abstract: Autoregressive models for video generation typically operate frame-by-frame, extending next-token prediction from language to video's temporal dimension. We question that unlike word as token is universally agreed in language if frame is a appropriate prediction unit? To address this, we present VideoAR, a unified framework that supports a spectrum of prediction units including full frames, key-detail frames, multiscale refinements, and spatiotemporal cubes. Among these designs, we find model video generation using \\textit{spatiotemporal} cubes as prediction units, which allows autoregressive models to operate across both spatial and temporal dimensions simultaneously. This approach eliminates the assumption that frames are the natural atomic units for video autoregression. We evaluate VideoAR across diverse prediction strategies, finding that cube-based prediction consistently delivers superior quality, speed, and temporal coherence. By removing the frame-by-frame constraint, our video generator surpasses state-of-the-art baselines on VBench while achieving faster inference and enabling seamless scaling to minute-long sequences. We hope this work will motivate rethinking sequence decomposition in video and other spatiotemporal domains.",
        "arxiv_id": "2509.24081",
        "ARXIVID": "2509.24081",
        "COMMENT": "Matches criterion 6 as it introduces a novel methodology for video understanding by rethinking sequence decomposition in video generation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.23038": {
        "authors": [
            "Jingxing Li",
            "Yongjae Lee",
            "Deliang Fan"
        ],
        "title": "GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization",
        "abstract": "arXiv:2509.23038v1 Announce Type: new  Abstract: Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and state-of-the-art regression accuracy, yet our analysis reveals subtle geometric inconsistencies in its internal representations that prevent reaching the precision ceiling of correspondence-based methods like MASt3R (which require 300ms per pair). In this work, we present GeLoc3r, a novel approach to relative camera pose estimation that enhances pose regression methods through Geometric Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma by training regression networks to produce geometrically consistent poses without inference-time geometric computation. During training, GeLoc3r leverages ground-truth depth to generate dense 3D-2D correspondences, weights them using a FusionTransformer that learns correspondence importance, and computes geometrically-consistent poses via weighted RANSAC. This creates a consistency loss that transfers geometric knowledge into the regression network. Unlike FAR method which requires both regression and geometric solving at inference, GeLoc3r only uses the enhanced regression head at test time, maintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving significant improvements including 40.45% vs. 34.85% AUC@5{\\deg} on the CO3Dv2 dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5{\\deg} on RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric consistency during training rather than enforcing it at inference, GeLoc3r represents a paradigm shift in how neural networks learn camera geometry, achieving both the speed of regression and the geometric understanding of correspondence methods.",
        "arxiv_id": "2509.23038",
        "ARXIVID": "2509.23038",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel method for relative camera pose estimation with geometric consistency, which is relevant to spatial reasoning and embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.23661": {
        "authors": [
            "Xiang An",
            "Yin Xie",
            "Kaicheng Yang",
            "Wenkang Zhang",
            "Xiuwei Zhao",
            "Zheng Cheng",
            "Yirui Wang",
            "Songcen Xu",
            "Changrui Chen",
            "Chunsheng Wu",
            "Huajie Tan",
            "Chunyuan Li",
            "Jing Yang",
            "Jie Yu",
            "Xiyao Wang",
            "Bin Qin",
            "Yumeng Wang",
            "Zizhen Yan",
            "Ziyong Feng",
            "Ziwei Liu",
            "Bo Li",
            "Jiankang Deng"
        ],
        "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "abstract": "arXiv:2509.23661v1 Announce Type: new  Abstract: We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale Curated Datasets: We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction dataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed multimodal tokens. (2) Efficient Training Framework: We develop a complete end-to-end efficient training framework leveraging an offline parallel data packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a $16,000 budget. (3) State-of-the-art Performance: Experimental results demonstrate that LLaVA-OneVision1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We anticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community to await further updates.",
        "arxiv_id": "2509.23661",
        "ARXIVID": "2509.23661",
        "COMMENT": "Matches criteria 2 as it presents a novel framework for training multimodal large language models with state-of-the-art performance.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.25172": {
        "authors": [
            "Yuxin Jiang",
            "Yuchao Gu",
            "Yiren Song",
            "Ivor Tsang",
            "Mike Zheng Shou"
        ],
        "title": "Personalized Vision via Visual In-Context Learning",
        "abstract": "arXiv:2509.25172v1 Announce Type: new  Abstract: Modern vision models, trained on large-scale annotated datasets, excel at predefined tasks but struggle with personalized vision -- tasks defined at test time by users with customized objects or novel objectives. Existing personalization approaches rely on costly fine-tuning or synthetic data pipelines, which are inflexible and restricted to fixed task formats. Visual in-context learning (ICL) offers a promising alternative, yet prior methods confine to narrow, in-domain tasks and fail to generalize to open-ended personalization. We introduce Personalized In-Context Operator (PICO), a simple four-panel framework that repurposes diffusion transformers as visual in-context learners. Given a single annotated exemplar, PICO infers the underlying transformation and applies it to new inputs without retraining. To enable this, we construct VisRel, a compact yet diverse tuning dataset, showing that task diversity, rather than scale, drives robust generalization. We further propose an attention-guided seed scorer that improves reliability via efficient inference scaling. Extensive experiments demonstrate that PICO (i) surpasses fine-tuning and synthetic-data baselines, (ii) flexibly adapts to novel user-defined tasks, and (iii) generalizes across both recognition and generation.",
        "arxiv_id": "2509.25172",
        "ARXIVID": "2509.25172",
        "COMMENT": "Matches criteria 2 as it explores visual in-context learning, a novel approach for vision and multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.25140": {
        "authors": [
            "Siru Ouyang",
            "Jun Yan",
            "I-Hung Hsu",
            "Yanfei Chen",
            "Ke Jiang",
            "Zifeng Wang",
            "Rujun Han",
            "Long T. Le",
            "Samira Daruki",
            "Xiangru Tang",
            "Vishy Tirumalashetty",
            "George Lee",
            "Mahsan Rofouei",
            "Hangfei Lin",
            "Jiawei Han",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
        "abstract": "arXiv:2509.25140v1 Announce Type: new  Abstract: With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.",
        "arxiv_id": "2509.25140",
        "ARXIVID": "2509.25140",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a memory framework for agents to self-evolve, which could be relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2509.24361": {
        "authors": [
            "Hao Yang",
            "Weijie Qiu",
            "Ru Zhang",
            "Zhou Fang",
            "Ruichao Mao",
            "Xiaoyu Lin",
            "Maji Huang",
            "Zhaosong Huang",
            "Teng Guo",
            "Shuoyang Liu",
            "Hai Rao"
        ],
        "title": "UI-UG: A Unified MLLM for UI Understanding and Generation",
        "abstract": "arXiv:2509.24361v1 Announce Type: new  Abstract: Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still facing challenges in domain-specific tasks, such as User Interface (UI) understanding accuracy and UI generation quality. In this paper, we introduce UI-UG (a unified MLLM for UI Understanding and Generation), integrating both capabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we further use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we propose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), training strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) performance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks.",
        "arxiv_id": "2509.24361",
        "ARXIVID": "2509.24361",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a unified MLLM for UI understanding and generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22799": {
        "authors": [
            "Xuan He",
            "Dongfu Jiang",
            "Ping Nie",
            "Minghao Liu",
            "Zhengxuan Jiang",
            "Mingyi Su",
            "Wentao Ma",
            "Junru Lin",
            "Chun Ye",
            "Yi Lu",
            "Keming Wu",
            "Benjamin Schneider",
            "Quy Duc Do",
            "Zhuofeng Li",
            "Yiming Jia",
            "Yuxuan Zhang",
            "Guo Cheng",
            "Haozhe Wang",
            "Wangchunshu Zhou",
            "Qunshu Lin",
            "Yuanxing Zhang",
            "Ge Zhang",
            "Wenhao Huang",
            "Wenhu Chen"
        ],
        "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
        "abstract": "arXiv:2509.22799v1 Announce Type: new  Abstract: Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
        "arxiv_id": "2509.22799",
        "ARXIVID": "2509.22799",
        "COMMENT": "Matches criteria 6 as it focuses on video understanding and evaluation with novel methodologies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23879": {
        "authors": [
            "Hitesh Laxmichand Patel",
            "Amit Agarwal",
            "Srikant Panda",
            "Hansa Meghwani",
            "Karan Dua",
            "Paul Li",
            "Tao Sheng",
            "Sujith Ravi",
            "Dan Roth"
        ],
        "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications",
        "abstract": "arXiv:2509.23879v1 Announce Type: new  Abstract: The reliability of Multimodal Large Language Models (MLLMs) in real-world settings is often undermined by sensitivity to irrelevant or distracting visual context, an aspect not captured by existing evaluation metrics. We introduce the \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and interpretable score for quantifying MLLM robustness to variations in visual context granularity, measuring performance changes between localized image patches and full-image input.   Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language benchmarks, we find that most leading models remain brittle to background noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating consistent robustness across tasks. PCRI analysis also highlights how different model architectures handle and integrate visual context, offering actionable diagnostic insight for both researchers and practitioners.   PCRI enables rigorous comparison of context robustness, supporting principled model selection and guiding the development of future architectures and training strategies for robust, real-world deployment.",
        "arxiv_id": "2509.23879",
        "ARXIVID": "2509.23879",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it evaluates MLLMs' robustness to visual context.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25162": {
        "authors": [
            "Bowei Chen",
            "Sai Bi",
            "Hao Tan",
            "He Zhang",
            "Tianyuan Zhang",
            "Zhengqi Li",
            "Yuanjun Xiong",
            "Jianming Zhang",
            "Kai Zhang"
        ],
        "title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models",
        "abstract": "arXiv:2509.25162v1 Announce Type: new  Abstract: In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256$\\times$256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.",
        "arxiv_id": "2509.25162",
        "ARXIVID": "2509.25162",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on aligning visual foundation encoders for diffusion models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23566": {
        "authors": [
            "Pinyuan Feng",
            "Hossein Adeli",
            "Wenxuan Guo",
            "Fan Cheng",
            "Ethan Hwang",
            "Nikolaus Kriegeskorte"
        ],
        "title": "Towards Interpretable Visual Decoding with Attention to Brain Representations",
        "abstract": "arXiv:2509.23566v1 Announce Type: new  Abstract: Recent work has demonstrated that complex visual stimuli can be decoded from human brain activity using deep generative models, helping brain science researchers interpret how the brain represents real-world scenes. However, most current approaches leverage mapping brain signals into intermediate image or text feature spaces before guiding the generative process, masking the effect of contributions from different brain areas on the final reconstruction output. In this work, we propose NeuroAdapter, a visual decoding framework that directly conditions a latent diffusion model on brain representations, bypassing the need for intermediate feature spaces. Our method demonstrates competitive visual reconstruction quality on public fMRI datasets compared to prior work, while providing greater transparency into how brain signals shape the generation process. To this end, we contribute an Image-Brain BI-directional interpretability framework (IBBI) which investigates cross-attention mechanisms across diffusion denoising steps to reveal how different cortical areas influence the unfolding generative trajectory. Our results highlight the potential of end-to-end brain-to-image decoding and establish a path toward interpreting diffusion models through the lens of visual neuroscience.",
        "arxiv_id": "2509.23566",
        "ARXIVID": "2509.23566",
        "COMMENT": "Matches criterion 5 as it integrates brain representations with latent diffusion models for visual decoding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25047": {
        "authors": [
            "Ram Ramrakhya",
            "Andrew Szot",
            "Omar Attia",
            "Yuhao Yang",
            "Anh Nguyen",
            "Bogdan Mazoure",
            "Zhe Gan",
            "Harsh Agrawal",
            "Alexander Toshev"
        ],
        "title": "Scaling Synthetic Task Generation for Agents via Exploration",
        "abstract": "arXiv:2509.25047v1 Announce Type: new  Abstract: Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable. Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited downstream environment information, which is either costly or poorly scalable as it yield tasks with limited coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly explores interactive environments to discover possible interactions and current state information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks across 13 applications Ubuntu applications to train mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis without human annotation by employing an MLLM task executor and verifier. This data enables training MLLM-based UI agents that improve success rates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling reinforcement learning training of UI agents, leading to an additional $5.7\\%$ gain. coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents reducing reliance on human annotation.",
        "arxiv_id": "2509.25047",
        "ARXIVID": "2509.25047",
        "COMMENT": "Matches criterion 3 as it introduces a scalable pipeline for task generation in embodied agents, addressing challenges in interactive environments.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24896": {
        "authors": [
            "Xi Chen",
            "Hongxun Yao",
            "Zhaopan Xu",
            "Kui Jiang"
        ],
        "title": "DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation",
        "abstract": "arXiv:2509.24896v1 Announce Type: new  Abstract: Source-free active domain adaptation (SFADA) enhances knowledge transfer from a source model to an unlabeled target domain using limited manual labels selected via active learning. While recent domain adaptation studies have introduced Vision-and-Language (ViL) models to improve pseudo-label quality or feature alignment, they often treat ViL-based and data supervision as separate sources, lacking effective fusion. To overcome this limitation, we propose Dual Active learning with Multimodal (DAM) foundation model, a novel framework that integrates multimodal supervision from a ViL model to complement sparse human annotations, thereby forming a dual supervisory signal. DAM initializes stable ViL-guided targets and employs a bidirectional distillation mechanism to foster mutual knowledge exchange between the target model and the dual supervisions during iterative adaptation. Extensive experiments demonstrate that DAM consistently outperforms existing methods and sets a new state-of-the-art across multiple SFADA benchmarks and active learning strategies.",
        "arxiv_id": "2509.24896",
        "ARXIVID": "2509.24896",
        "COMMENT": "Matches criterion 2 and 5 as it explores a multimodal foundation model integrating vision and language for domain adaptation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23492": {
        "authors": [
            "Junyi Wu",
            "Jiachen Tao",
            "Haoxuan Wang",
            "Gaowen Liu",
            "Ramana Rao Kompella",
            "Yan Yan"
        ],
        "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos",
        "abstract": "arXiv:2509.23492v1 Announce Type: new  Abstract: We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework for high-quality 4D reconstruction from casually captured monocular videos. While recent advances extend 3D Gaussian Splatting to dynamic scenes via various motion anchors, such as graph nodes or spline control points, they often rely on low-rank assumptions and fall short in modeling complex, region-specific deformations inherent to unconstrained dynamics. OriGS addresses this by introducing a hyperdimensional representation grounded in scene orientation. We first estimate a Global Orientation Field that propagates principal forward directions across space and time, serving as stable structural guidance for dynamic modeling. Built upon this, we propose Orientation-aware Hyper-Gaussian, a unified formulation that embeds time, space, geometry, and orientation into a coherent probabilistic state. This enables inferring region-specific deformation through principled conditioned slicing, adaptively capturing diverse local dynamics in alignment with global motion intent. Experiments demonstrate the superior reconstruction fidelity of OriGS over mainstream methods in challenging real-world dynamic scenes.",
        "arxiv_id": "2509.23492",
        "ARXIVID": "2509.23492",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks with a novel methodology for 4D reconstruction from videos.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23700": {
        "authors": [
            "Yunjiang Xu",
            "Lingzhi Li",
            "Jin Wang",
            "Yupeng Ouyang",
            "Benyuan Yang"
        ],
        "title": "INSTINCT: Instance-Level Interaction Architecture for Query-Based Collaborative Perception",
        "abstract": "arXiv:2509.23700v1 Announce Type: new  Abstract: Collaborative perception systems overcome single-vehicle limitations in long-range detection and occlusion scenarios by integrating multi-agent sensory data, improving accuracy and safety. However, frequent cooperative interactions and real-time requirements impose stringent bandwidth constraints. Previous works proves that query-based instance-level interaction reduces bandwidth demands and manual priors, however, LiDAR-focused implementations in collaborative perception remain underdeveloped, with performance still trailing state-of-the-art approaches. To bridge this gap, we propose INSTINCT (INSTance-level INteraCtion ArchiTecture), a novel collaborative perception framework featuring three core components: 1) a quality-aware filtering mechanism for high-quality instance feature selection; 2) a dual-branch detection routing scheme to decouple collaboration-irrelevant and collaboration-relevant instances; and 3) a Cross Agent Local Instance Fusion module to aggregate local hybrid instance features. Additionally, we enhance the ground truth (GT) sampling technique to facilitate training with diverse hybrid instance features. Extensive experiments across multiple datasets demonstrate that INSTINCT achieves superior performance. Specifically, our method achieves an improvement in accuracy 13.23%/33.08% in DAIR-V2X and V2V4Real while reducing the communication bandwidth to 1/281 and 1/264 compared to state-of-the-art methods. The code is available at https://github.com/CrazyShout/INSTINCT.",
        "arxiv_id": "2509.23700",
        "ARXIVID": "2509.23700",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for collaborative perception in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25180": {
        "authors": [
            "Wenkun He",
            "Yuchao Gu",
            "Junyu Chen",
            "Dongyun Zou",
            "Yujun Lin",
            "Zhekai Zhang",
            "Haocheng Xi",
            "Muyang Li",
            "Ligeng Zhu",
            "Jincheng Yu",
            "Junsong Chen",
            "Enze Xie",
            "Song Han",
            "Han Cai"
        ],
        "title": "DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space",
        "abstract": "arXiv:2509.25180v1 Announce Type: new  Abstract: Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: https://github.com/dc-ai-projects/DC-Gen.",
        "arxiv_id": "2509.25180",
        "ARXIVID": "2509.25180",
        "COMMENT": "Matches criterion 4 as it focuses on accelerating text-to-image diffusion models, which are foundational in vision applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23911": {
        "authors": [
            "Xiyan Xu",
            "Sirui Xu",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "title": "MoReact: Generating Reactive Motion from Textual Descriptions",
        "abstract": "arXiv:2509.23911v1 Announce Type: new  Abstract: Modeling and generating human reactions poses a significant challenge with broad applications for computer vision and human-computer interaction. Existing methods either treat multiple individuals as a single entity, directly generating interactions, or rely solely on one person's motion to generate the other's reaction, failing to integrate the rich semantic information that underpins human interactions. Yet, these methods often fall short in adaptive responsiveness, i.e., the ability to accurately respond to diverse and dynamic interaction scenarios. Recognizing this gap, our work introduces an approach tailored to address the limitations of existing models by focusing on text-driven human reaction generation. Our model specifically generates realistic motion sequences for individuals that responding to the other's actions based on a descriptive text of the interaction scenario. The goal is to produce motion sequences that not only complement the opponent's movements but also semantically fit the described interactions. To achieve this, we present MoReact, a diffusion-based method designed to disentangle the generation of global trajectories and local motions sequentially. This approach stems from the observation that generating global trajectories first is crucial for guiding local motion, ensuring better alignment with given action and text. Furthermore, we introduce a novel interaction loss to enhance the realism of generated close interactions. Our experiments, utilizing data adapted from a two-person motion dataset, demonstrate the efficacy of our approach for this novel task, which is capable of producing realistic, diverse, and controllable reactions that not only closely match the movements of the counterpart but also adhere to the textual guidance. Please find our webpage at https://xiyan-xu.github.io/MoReactWebPage.",
        "arxiv_id": "2509.23911",
        "ARXIVID": "2509.23911",
        "COMMENT": "Matches criterion 6 as it focuses on generating reactive motion from textual descriptions, which is a novel video-based task.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23919": {
        "authors": [
            "Longtao Jiang",
            "Mingfei Han",
            "Lei Chen",
            "Yongqiang Yu",
            "Feng Zhao",
            "Xiaojun Chang",
            "Zhihui Li"
        ],
        "title": "Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models",
        "abstract": "arXiv:2509.23919v1 Announce Type: new  Abstract: Text-guided image inpainting aims to inpaint masked image regions based on a textual prompt while preserving the background. Although diffusion-based methods have become dominant, their property of modeling the entire image in latent space makes it challenging for the results to align well with prompt details and maintain a consistent background. To address these issues, we explore Mask AutoRegressive (MAR) models for this task. MAR naturally supports image inpainting by generating latent tokens corresponding to mask regions, enabling better local controllability without altering the background. However, directly applying MAR to this task makes the inpainting content either ignore the prompts or be disharmonious with the background context. Through analysis of the attention maps from the inpainting images, we identify the impact of background tokens on text tokens during the MAR generation, and leverage this to design \\textbf{Token Painter}, a training-free text-guided image inpainting method based on MAR. Our approach introduces two key components: (1) Dual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and context information from text and background in frequency domain to produce novel guidance tokens, allowing MAR to generate text-faithful inpainting content while keeping harmonious with background context. (2) Adaptive Decoder Attention Score Enhancing (ADAE), which adaptively enhances attention scores on guidance tokens and inpainting tokens to further enhance the alignment of prompt details and the content visual quality. Extensive experiments demonstrate that our training-free method outperforms prior state-of-the-art methods across almost all metrics and delivers superior visual results. Codes will be released.",
        "arxiv_id": "2509.23919",
        "ARXIVID": "2509.23919",
        "COMMENT": "Matches criterion 5 as it explores text-guided image inpainting using mask autoregressive models, combining image understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23646": {
        "authors": [
            "Lu Xiao",
            "Jiale Zhang",
            "Yang Liu",
            "Taicheng Huang",
            "Xin Tian"
        ],
        "title": "Sparse-Up: Learnable Sparse Upsampling for 3D Generation with High-Fidelity Textures",
        "abstract": "arXiv:2509.23646v1 Announce Type: new  Abstract: The creation of high-fidelity 3D assets is often hindered by a 'pixel-level pain point': the loss of high-frequency details. Existing methods often trade off one aspect for another: either sacrificing cross-view consistency, resulting in torn or drifting textures, or remaining trapped by the resolution ceiling of explicit voxels, forfeiting fine texture detail. In this work, we propose Sparse-Up, a memory-efficient, high-fidelity texture modeling framework that effectively preserves high-frequency details. We use sparse voxels to guide texture reconstruction and ensure multi-view consistency, while leveraging surface anchoring and view-domain partitioning to break through resolution constraints. Surface anchoring employs a learnable upsampling strategy to constrain voxels to the mesh surface, eliminating over 70% of redundant voxels present in traditional voxel upsampling. View-domain partitioning introduces an image patch-guided voxel partitioning scheme, supervising and back-propagating gradients only on visible local patches. Through these two strategies, we can significantly reduce memory consumption during high-resolution voxel training without sacrificing geometric consistency, while preserving high-frequency details in textures.",
        "arxiv_id": "2509.23646",
        "ARXIVID": "2509.23646",
        "COMMENT": "Matches criteria 4 as it introduces a novel framework for high-fidelity 3D generation with sparse upsampling, focusing on texture modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23738": {
        "authors": [
            "Cong Chen",
            "Kaixiang Ji",
            "Hao Zhong",
            "Muzhi Zhu",
            "Anzhou Li",
            "Guo Gan",
            "Ziyuan Huang",
            "Cheng Zou",
            "Jiajia Liu",
            "Jingdong Chen",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks",
        "abstract": "arXiv:2509.23738v1 Announce Type: new  Abstract: Autonomous agents for long-sequence Graphical User Interface tasks are hindered by sparse rewards and the intractable credit assignment problem. To address these challenges, we introduce GUI-Shepherd, a Process Reward Model that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is trained on a diverse large-scale data set of $52$k interactions that features human-annotated scores and GPT-4o generated rationales, enabling it to serve both as a reward provider for RL training and as a verifier for inference. As far as we know, we are the first to conduct a systematic study of process supervision in GUI agents, across diverse settings from online long-horizon tasks to offline single-step prediction. On the online AndroidWorld benchmark, GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO, significantly outperforming Outcome Reward Model based competitors. When used as an inference verifier, it brings $5.1$ points improvements. The benefits generalize to the offline AndroidControl benchmark, with gains of $2.2$ points as a reward provider and $4.3$ points as a verifier. Collectively, our results establish that high-fidelity process supervision is critical for building more capable GUI agents and present a generalizable solution.",
        "arxiv_id": "2509.23738",
        "ARXIVID": "2509.23738",
        "COMMENT": "Matches criteria 3 as it introduces a new reward model and verification method for GUI agents, addressing challenges in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.24943": {
        "authors": [
            "Jiahua Li",
            "Kun Wei",
            "Zhe Xu",
            "Zibo Su",
            "Xu Yang",
            "Cheng Deng"
        ],
        "title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents",
        "abstract": "arXiv:2509.24943v1 Announce Type: new  Abstract: Long videos, characterized by temporal complexity and sparse task-relevant information, pose significant reasoning challenges for AI systems. Although various Large Language Model (LLM)-based approaches have advanced long video understanding, they still struggle to achieve both completeness and efficiency in capturing task-critical information. Inspired by human progressive visual cognition, we propose CogniGPT, a framework that leverages an interactive loop between Multi-Granular Perception Agent (MGPA) and Verification-Enhanced Reflection Agent (VERA) for efficient and reliable long video understanding. Specifically, MGPA mimics human visual divergent and focused attention to capture task-related information, while VERA verifies perceived key clues to mitigate hallucination and optimize subsequent perception strategies. Through this interactive process, CogniGPT explores a minimal set of informative and reliable task-related clues. Extensive experiments on EgoSchema, Video-MME, NExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both accuracy and efficiency. Notably, on EgoSchema, it surpasses existing training-free methods using only 11.2 frames and achieves performance comparable to Gemini 1.5-Pro.",
        "arxiv_id": "2509.24943",
        "ARXIVID": "2509.24943",
        "COMMENT": "Matches criteria 6 as it focuses on long video understanding with novel methodologies for efficient and reliable reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23690": {
        "authors": [
            "Siyuan Gao",
            "Jiashu Yao",
            "Haoyu Wen",
            "Yuhang Guo",
            "Zeming Liu",
            "Heyan Huang"
        ],
        "title": "HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection",
        "abstract": "arXiv:2509.23690v1 Announce Type: new  Abstract: Embodied agents can identify and report safety hazards in the home environments. Accurately evaluating their capabilities in home safety inspection tasks is curcial, but existing benchmarks suffer from two key limitations. First, they oversimplify safety inspection tasks by using textual descriptions of the environment instead of direct visual information, which hinders the accurate evaluation of embodied agents based on Vision-Language Models (VLMs). Second, they use a single, static viewpoint for environmental observation, which restricts the agents' free exploration and cause the omission of certain safety hazards, especially those that are occluded from a fixed viewpoint. To alleviate these issues, we propose HomeSafeBench, a benchmark with 12,900 data points covering five common home safety hazards: fire, electric shock, falling object, trips, and child safety. HomeSafeBench provides dynamic first-person perspective images from simulated home environments, enabling the evaluation of VLM capabilities for home safety inspection. By allowing the embodied agents to freely explore the room, HomeSafeBench provides multiple dynamic perspectives in complex environments for a more thorough inspection. Our comprehensive evaluation of mainstream VLMs on HomeSafeBench reveals that even the best-performing model achieves an F1-score of only 10.23%, demonstrating significant limitations in current VLMs. The models particularly struggle with identifying safety hazards and selecting effective exploration strategies. We hope HomeSafeBench will provide valuable reference and support for future research related to home security inspections. Our dataset and code will be publicly available soon.",
        "arxiv_id": "2509.23690",
        "ARXIVID": "2509.23690",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark for embodied vision-language models in home safety inspection tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22692": {
        "authors": [
            "Le Zhang",
            "Ao Li",
            "Qibin Hou",
            "Ce Zhu",
            "Yonina C. Eldar"
        ],
        "title": "Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects",
        "abstract": "arXiv:2509.22692v1 Announce Type: new  Abstract: Super-resolution (SR) has garnered significant attention within the computer vision community, driven by advances in deep learning (DL) techniques and the growing demand for high-quality visual applications. With the expansion of this field, numerous surveys have emerged. Most existing surveys focus on specific domains, lacking a comprehensive overview of this field. Here, we present an in-depth review of diverse SR methods, encompassing single image super-resolution (SISR), video super-resolution (VSR), stereo super-resolution (SSR), and light field super-resolution (LFSR). We extensively cover over 150 SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical results, and complexity. In addition, we conducted a taxonomy based on each backbone structure according to the diverse purposes. We also explore valuable yet under-studied open issues in the field. We believe that this work will serve as a valuable resource and offer guidance to researchers in this domain. To facilitate access to related work, we created a dedicated repository available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.",
        "arxiv_id": "2509.22692",
        "ARXIVID": "2509.22692",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive survey on super-resolution techniques.",
        "RELEVANCE": 10,
        "NOVELTY": 5
    },
    "2509.25160": {
        "authors": [
            "Fan Yuan",
            "Yuchen Yan",
            "Yifan Jiang",
            "Haoran Zhao",
            "Tao Feng",
            "Jinyan Chen",
            "Yanwei Lou",
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts",
        "abstract": "arXiv:2509.25160v1 Announce Type: new  Abstract: Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.",
        "arxiv_id": "2509.25160",
        "ARXIVID": "2509.25160",
        "COMMENT": "Matches criteria 6 as it introduces a benchmark for visual mathematical reasoning in multi-image contexts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23728": {
        "authors": [
            "Yiheng Zhang",
            "Zhuojiang Cai",
            "Mingdao Wang",
            "Meitong Guo",
            "Tianxiao Li",
            "Li Lin",
            "Yuwang Wang"
        ],
        "title": "M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation",
        "abstract": "arXiv:2509.23728v1 Announce Type: new  Abstract: In text-driven 3D scene generation, object layout serves as a crucial intermediate representation that bridges high-level language instructions with detailed geometric output. It not only provides a structural blueprint for ensuring physical plausibility but also supports semantic controllability and interactive editing. However, the learning capabilities of current 3D indoor layout generation models are constrained by the limited scale, diversity, and annotation quality of existing datasets. To address this, we introduce M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation. M3DLayout comprises 15,080 layouts and over 258k object instances, integrating three distinct sources: real-world scans, professional CAD designs, and procedurally generated scenes. Each layout is paired with detailed structured text describing global scene summaries, relational placements of large furniture, and fine-grained arrangements of smaller items. This diverse and richly annotated resource enables models to learn complex spatial and semantic patterns across a wide variety of indoor environments. To assess the potential of M3DLayout, we establish a benchmark using a text-conditioned diffusion model. Experimental results demonstrate that our dataset provides a solid foundation for training layout generation models. Its multi-source composition enhances diversity, notably through the Inf3DLayout subset which provides rich small-object information, enabling the generation of more complex and detailed scenes. We hope that M3DLayout can serve as a valuable resource for advancing research in text-driven 3D scene synthesis.",
        "arxiv_id": "2509.23728",
        "ARXIVID": "2509.23728",
        "COMMENT": "Matches criteria 3 as it introduces a new dataset for 3D indoor layout generation, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23951": {
        "authors": [
            "Siyu Cao",
            "Hangting Chen",
            "Peng Chen",
            "Yiji Cheng",
            "Yutao Cui",
            "Xinchi Deng",
            "Ying Dong",
            "Kipper Gong",
            "Tianpeng Gu",
            "Xiusen Gu",
            "Tiankai Hang",
            "Duojun Huang",
            "Jie Jiang",
            "Zhengkai Jiang",
            "Weijie Kong",
            "Changlin Li",
            "Donghao Li",
            "Junzhe Li",
            "Xin Li",
            "Yang Li",
            "Zhenxi Li",
            "Zhimin Li",
            "Jiaxin Lin",
            "Linus",
            "Lucaz Liu",
            "Shu Liu",
            "Songtao Liu",
            "Yu Liu",
            "Yuhong Liu",
            "Yanxin Long",
            "Fanbin Lu",
            "Qinglin Lu",
            "Yuyang Peng",
            "Yuanbo Peng",
            "Xiangwei Shen",
            "Yixuan Shi",
            "Jiale Tao",
            "Yangyu Tao",
            "Qi Tian",
            "Pengfei Wan",
            "Chunyu Wang",
            "Kai Wang",
            "Lei Wang",
            "Linqing Wang",
            "Lucas Wang",
            "Qixun Wang",
            "Weiyan Wang",
            "Hao Wen",
            "Bing Wu",
            "Jianbing Wu",
            "Yue Wu",
            "Senhao Xie",
            "Fang Yang",
            "Miles Yang",
            "Xiaofeng Yang",
            "Xuan Yang",
            "Zhantao Yang",
            "Jingmiao Yu",
            "Zheng Yuan",
            "Chao Zhang",
            "Jian-Wei Zhang",
            "Peizhen Zhang",
            "Shi-Xue Zhang",
            "Tao Zhang",
            "Weigang Zhang",
            "Yepeng Zhang",
            "Yingfang Zhang",
            "Zihao Zhang",
            "Zijian Zhang",
            "Penghao Zhao",
            "Zhiyuan Zhao",
            "Xuefei Zhe",
            "Jianchen Zhu",
            "Zhao Zhong"
        ],
        "title": "HunyuanImage 3.0 Technical Report",
        "abstract": "arXiv:2509.23951v1 Announce Type: new  Abstract: We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
        "arxiv_id": "2509.23951",
        "ARXIVID": "2509.23951",
        "COMMENT": "Matches criteria 4 as it focuses on a foundation model for computer vision and its applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.23194": {
        "authors": [
            "Yifan Zhang",
            "Wei Zhang",
            "Chuangxin He",
            "Zhonghua Miao",
            "Junhui Hou"
        ],
        "title": "Unsupervised Online 3D Instance Segmentation with Synthetic Sequences and Dynamic Loss",
        "abstract": "arXiv:2509.23194v1 Announce Type: new  Abstract: Unsupervised online 3D instance segmentation is a fundamental yet challenging task, as it requires maintaining consistent object identities across LiDAR scans without relying on annotated training data. Existing methods, such as UNIT, have made progress in this direction but remain constrained by limited training diversity, rigid temporal sampling, and heavy dependence on noisy pseudo-labels. We propose a new framework that enriches the training distribution through synthetic point cloud sequence generation, enabling greater diversity without relying on manual labels or simulation engines. To better capture temporal dynamics, our method incorporates a flexible sampling strategy that leverages both adjacent and non-adjacent frames, allowing the model to learn from long-range dependencies as well as short-term variations. In addition, a dynamic-weighting loss emphasizes confident and informative samples, guiding the network toward more robust representations. Through extensive experiments on SemanticKITTI, nuScenes, and PandaSet, our method consistently outperforms UNIT and other unsupervised baselines, achieving higher segmentation accuracy and more robust temporal associations. The code will be publicly available at github.com/Eaphan/SFT3D.",
        "arxiv_id": "2509.23194",
        "ARXIVID": "2509.23194",
        "COMMENT": "Matches criteria 3 as it introduces a new framework for unsupervised 3D instance segmentation, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.25185": {
        "authors": [
            "Shuoshuo Zhang",
            "Zijian Li",
            "Yizhen Zhang",
            "Jingjing Fu",
            "Lei Song",
            "Jiang Bian",
            "Jun Zhang",
            "Yujiu Yang",
            "Rui Wang"
        ],
        "title": "PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images",
        "abstract": "arXiv:2509.25185v1 Announce Type: new  Abstract: Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structured-image tasks. In this paper, we propose PixelCraft, a novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises a dispatcher, a planner, a reasoner, critics, and a set of visual tool agents. To achieve high-fidelity processing, we construct a high-quality corpus and fine-tune an MLLM into a grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. Building on this foundation, PixelCraft facilitates flexible visual reasoning through a dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. Moreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting a new standard for structured image reasoning. Our code will be available at https://github.com/microsoft/PixelCraft.",
        "arxiv_id": "2509.25185",
        "ARXIVID": "2509.25185",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) with a focus on structured image reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.24314": {
        "authors": [
            "Hongjun Liu",
            "Yinghao Zhu",
            "Yuhui Wang",
            "Yitao Long",
            "Zeyu Lai",
            "Lequan Yu",
            "Chen Zhao"
        ],
        "title": "MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning",
        "abstract": "arXiv:2509.24314v1 Announce Type: new  Abstract: Recent progress in multimodal large language models (MLLMs) has demonstrated promising performance on medical benchmarks and in preliminary trials as clinical assistants. Yet, our pilot audit of diagnostic cases uncovers a critical failure mode: instability in early evidence interpretation precedes hallucination, creating branching reasoning trajectories that cascade into globally inconsistent conclusions. This highlights the need for clinical reasoning agents that constrain stochasticity and hallucination while producing auditable decision flows. We introduce MedMMV, a controllable multimodal multi-agent framework for reliable and verifiable clinical reasoning. MedMMV stabilizes reasoning through diversified short rollouts, grounds intermediate steps in a structured evidence graph under the supervision of a Hallucination Detector, and aggregates candidate paths with a Combined Uncertainty scorer. On six medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more critically, demonstrates superior reliability. Blind physician evaluations confirm that MedMMV substantially increases reasoning truthfulness without sacrificing informational content. By controlling instability through a verifiable, multi-agent process, our framework provides a robust path toward deploying trustworthy AI systems in high-stakes domains like clinical decision support.",
        "arxiv_id": "2509.24314",
        "ARXIVID": "2509.24314",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on multimodal reasoning in clinical applications.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.23673": {
        "authors": [
            "Amit Agarwal",
            "Hitesh Laxmichand Patel",
            "Srikant Panda",
            "Hansa Meghwani",
            "Jyotika Singh",
            "Karan Dua",
            "Paul Li",
            "Tao Sheng",
            "Sujith Ravi",
            "Dan Roth"
        ],
        "title": "RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks",
        "abstract": "arXiv:2509.23673v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive results on vision-language benchmarks, yet it remains unclear whether these benchmarks assess genuine global reasoning or allow success via localized visual cues. Existing evaluation methods do not explicitly measure this distinction, hindering effective dataset curation and real-world focused model development.   We introduce Region Comprehension Index (RCI), the first model-based score to directly quantify a dataset's reliance on global versus local visual information. RCI systematically compares reference-model performance on image patches versus full images, revealing if tasks require holistic image understanding or can be solved with partial or localized visual cues.   When applying RCI to 13 widely used multimodal benchmarks, we observed that most of them favor localized reasoning and exhibit significant spatial biases, indicating potential risks in real-world applications. RCI equips researchers & practitioners with an actionable tool for diagnosing & mitigating these biases, enabling the construction of datasets and benchmarks to foster the development of robust, enterprise-ready multimodal systems.",
        "arxiv_id": "2509.23673",
        "ARXIVID": "2509.23673",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a new evaluation metric for multimodal benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.25142": {
        "authors": [
            "Nicholas Budny",
            "Kia Ghods",
            "Declan Campbell",
            "Raja Marjieh",
            "Amogh Joshi",
            "Sreejan Kumar",
            "Jonathan D. Cohen",
            "Taylor W. Webb",
            "Thomas L. Griffiths"
        ],
        "title": "Visual serial processing deficits explain divergences in human and VLM reasoning",
        "abstract": "arXiv:2509.25142v1 Announce Type: new  Abstract: Why do Vision Language Models (VLMs), despite success on standard benchmarks, often fail to match human performance on surprisingly simple visual reasoning tasks? While the underlying computational principles are still debated, we hypothesize that a crucial factor is a deficit in visually-grounded serial processing. To test this hypothesis, we compared human and VLM performance across tasks designed to vary serial processing demands in three distinct domains: geometric reasoning, perceptual enumeration, and mental rotation. Tasks within each domain varied serial processing load by manipulating factors such as geometric concept complexity, perceptual individuation load, and transformation difficulty. Across all domains, our results revealed a consistent pattern: decreased VLM accuracy was strongly correlated with increased human reaction time (used as a proxy for serial processing load). As tasks require more demanding serial processing -- whether composing concepts, enumerating items, or performing mental transformations -- the VLM-human performance gap widens reliably. These findings support our hypothesis, indicating that limitations in serial, visually grounded reasoning represent a fundamental bottleneck that distinguishes current VLMs from humans.",
        "arxiv_id": "2509.25142",
        "ARXIVID": "2509.25142",
        "COMMENT": "Matches criteria 2 as it explores reasoning limitations in Vision Language Models (VLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.22720": {
        "authors": [
            "Zezhong Fan",
            "Xiaohan Li",
            "Luyi Ma",
            "Kai Zhao",
            "Liang Peng",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kaushiki Nag",
            "Kannan Achan"
        ],
        "title": "LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning",
        "abstract": "arXiv:2509.22720v1 Announce Type: new  Abstract: Designing realistic multi-object scenes requires not only generating images, but also planning spatial layouts that respect semantic relations and physical plausibility. On one hand, while recent advances in diffusion models have enabled high-quality image generation, they lack explicit spatial reasoning, leading to unrealistic object layouts. On the other hand, traditional spatial planning methods in robotics emphasize geometric and relational consistency, but they struggle to capture semantic richness in visual scenes. To bridge this gap, in this paper, we propose LayoutAgent, an agentic framework that unifies vision-language reasoning with compositional diffusion for layout generation. Given multiple input images with target objects in them, our method first employs visual-language model to preprocess the inputs through segmentation, object size estimation, scene graph construction, and prompt rewriting. Then we leverage compositional diffusion-a method traditionally used in robotics-to synthesize bounding boxes that respect object relations encoded in the scene graph for spatial layouts. In the end, a foreground-conditioned image generator composes the complete scene by rendering the objects into the planned layout guided by designed prompts. Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment.",
        "arxiv_id": "2509.22720",
        "ARXIVID": "2509.22720",
        "COMMENT": "Matches criteria 1 (Spatial Intelligence and Embodied Agents) as it focuses on spatial layout planning using vision-language reasoning and compositional diffusion.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.22793": {
        "authors": [
            "Komal Kumar",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Salman Khan",
            "Ivan Laptev",
            "Hisham Cholakkal"
        ],
        "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models",
        "abstract": "arXiv:2509.22793v1 Announce Type: new  Abstract: Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves adjusting the model to suit a particular task or dataset while minimizing computational resources and limiting the number of trainable parameters. However, it often faces challenges in striking a trade-off between aligning with the target distribution: learning a novel concept from a limited image for personalization and retaining the instruction ability needed for unifying multiple tasks, all while maintaining editability (aligning with a variety of prompts or in-context generation). In this work, we introduce DEFT, Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that adapts a pre-trained weight matrix by decomposing its update into two components with two trainable matrices: (1) a projection onto the complement of a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update. The single trainable low-rank matrix defines the subspace, while the other trainable low-rank matrix enables flexible parameter adaptation within that subspace. We conducted extensive experiments on the Dreambooth and Dreambench Plus datasets for personalization, the InsDet dataset for object and scene adaptation, and the VisualCloze dataset for a universal image generation framework through visual in-context learning with both Stable Diffusion and a unified model. Our results demonstrated state-of-the-art performance, highlighting the emergent properties of efficient fine-tuning. Our code is available on \\href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.",
        "arxiv_id": "2509.22793",
        "ARXIVID": "2509.22793",
        "COMMENT": "Matches criterion 5 as it focuses on techniques combining image generation tasks with large language models, specifically in text-to-image fine-tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.23186": {
        "authors": [
            "Qimin Zhong",
            "Hao Liao",
            "Siwei Wang",
            "Mingyang Zhou",
            "Xiaoqun Wu",
            "Rui Mao",
            "Wei Chen"
        ],
        "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction",
        "abstract": "arXiv:2509.23186v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.",
        "arxiv_id": "2509.23186",
        "ARXIVID": "2509.23186",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and planning capabilities in language models, particularly in the Blocksworld planning benchmark.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.23870": {
        "authors": [
            "Jingyu Liu",
            "Xiaopeng Wu",
            "Jingquan Peng",
            "Kehan Chen",
            "Chuan Yu",
            "Lizhong Ding",
            "Yong Liu"
        ],
        "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL",
        "abstract": "arXiv:2509.23870v1 Announce Type: new  Abstract: Building autonomous agents capable of solving long-horizon, real-world tasks has garnered significant research interest. But outcome based rewards may cause reward miscalibration which means it might mistakenly allocate positive reward to flawed middle steps which is regarded as the key reason making the bad actions being reinforced during training. However we reveal that outcome based reward ensures expected negative advantage for those flawed middle steps, which means the flawed actions should be punished during training. Even accounting for the ``squeezing effect\", the probability mass of good actions should increase and the actor should gradually get rid of harmful actions. This shows that flawed actions should be punished during training. We further identify gradient coupling between similar samples as a key issue in agentic RL, the input prompt is extremely similar and the output action space is limited, therefore during training, gradients from well-performing samples can inadvertently strengthen suboptimal or incorrect actions due to similar input observation and output actions. We show that with gradient coupling, some flawed actions might be enhanced. To address this, we propose training the actor to classify good or bad actions to separate the embedding of good/bad actions and alleviate the gradient interference, extensive experiments shows its effectiveness.",
        "arxiv_id": "2509.23870",
        "ARXIVID": "2509.23870",
        "COMMENT": "Matches criteria 3 as it addresses reward miscalibration in reinforcement learning for autonomous agents, which is relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.24258": {
        "authors": [
            "Jinming Liu",
            "Zhaoyang Jia",
            "Jiahao Li",
            "Bin Li",
            "Xin Jin",
            "Wenjun Zeng",
            "Yan Lu"
        ],
        "title": "When MLLMs Meet Compression Distortion: A Coding Paradigm Tailored to MLLMs",
        "abstract": "arXiv:2509.24258v1 Announce Type: new  Abstract: The increasing deployment of powerful Multimodal Large Language Models (MLLMs), typically hosted on cloud platforms, urgently requires effective compression techniques to efficiently transmit signal inputs (e.g., images, videos) from edge devices with minimal bandwidth usage. However, conventional image codecs are optimized for fidelity to serve the Human Visual System (HVS) and ill-suited for MLLMs, in which diverse downstream tasks are jointly considered. In this paper, we first systematically analyze the impact of compression artifacts on several mainstream MLLMs. We find that: Compression distortion unevenly impacts different-level image features, leading to varying effects on MLLMs' downstream tasks depending on their feature-level reliance. Motivated by this discovery, we propose an image Codec TAilored to MLLMs (CoTAM) designed to adaptively protect multi-level features and suit different demands of downstream tasks. The encoder leverages CLIP's shallow-layer attention to generate an importance map for bit allocation, preserving critical semantic regions. Concurrently, the decoder integrates a lightweight adapter with a multi-level loss function to ensure the faithful reconstruction both of low-level details and high-level semantic context for robust synthesis of cross-level features. Extensive experiments validate that our method achieves up to 35.99\\% bitrate saving while maintaining the same performance on the MLLM tasks, outperforming previous SOTA neural codecs.",
        "arxiv_id": "2509.24258",
        "ARXIVID": "2509.24258",
        "COMMENT": "Matches criterion 2 as it explores a coding paradigm tailored to multimodal large language models (MLLMs), focusing on efficient compression for vision-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.24491": {
        "authors": [
            "Yuanshuai Li",
            "Yuping Yan",
            "Junfeng Tang",
            "Yunxuan Li",
            "Zeqi Zheng",
            "Yaochu Jin"
        ],
        "title": "Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs",
        "abstract": "arXiv:2509.24491v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have significantly improved the performance of various tasks, but continue to suffer from visual hallucinations, a critical issue where generated responses contradict visual evidence. While Direct Preference Optimization(DPO) is widely used for alignment, its application to MLLMs often fails to capture fine-grained semantic differences and encourages shortcut learning. To address these challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard curriculum built upon our Semantic Curriculum Preference Pairs dataset, which provides fine-grained semantic contrasts sorted by difficulty. This curriculum is trained with a dynamic reference model and a novel symmetric, bidirectional objective to facilitate simultaneous learning from both textual and visual preferences. To our knowledge, SCPO is the first framework to unify semantics, symmetry, and curriculum for MLLMs alignment, effectively mitigating visual hallucinations. Extensive experiments on LLaVA models across various scales and versions validate that SCPO demonstrates superior performance compared to baseline models on multiple hallucination benchmarks, reducing the hallucination rate by up to 62.9%. Moreover, evaluations on generalized benchmarks show that SCPO improves factuality while preserving general capabilities, with its performance remaining stable across general vision-language benchmarks.",
        "arxiv_id": "2509.24491",
        "ARXIVID": "2509.24491",
        "COMMENT": "Matches criterion 2 as it addresses visual hallucinations in multimodal large language models (MLLMs) and proposes a novel alignment framework.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.25143": {
        "authors": [
            "Junyi Zhang",
            "Jia-Chen Gu",
            "Wenbo Hu",
            "Yu Zhou",
            "Robinson Piramuthu",
            "Nanyun Peng"
        ],
        "title": "TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models",
        "abstract": "arXiv:2509.25143v1 Announce Type: new  Abstract: Existing medical reasoning benchmarks for vision-language models primarily focus on analyzing a patient's condition based on an image from a single visit. However, this setting deviates significantly from real-world clinical practice, where doctors typically refer to a patient's historical conditions to provide a comprehensive assessment by tracking their changes over time. In this paper, we introduce TemMed-Bench, the first benchmark designed for analyzing changes in patients' conditions between different clinical visits, which challenges large vision-language models (LVLMs) to reason over temporal medical images. TemMed-Bench consists of a test set comprising three tasks - visual question-answering (VQA), report generation, and image-pair selection - and a supplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we conduct an evaluation of six proprietary and six open-source LVLMs. Our results show that most LVLMs lack the ability to analyze patients' condition changes over temporal medical images, and a large proportion perform only at a random-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini and Claude 3.5 Sonnet demonstrate comparatively decent performance, though they have yet to reach the desired level. Furthermore, we explore augmenting the input with both retrieved visual and textual modalities in the medical domain. We also show that multi-modal retrieval augmentation yields notably higher performance gains than no retrieval and textual retrieval alone across most models on our benchmark, with the VQA task showing an average improvement of 2.59%. Overall, we compose a benchmark grounded on real-world clinical practice, and it reveals LVLMs' limitations in temporal medical image reasoning, as well as highlighting the use of multi-modal retrieval augmentation as a potentially promising direction worth exploring to address this challenge.",
        "arxiv_id": "2509.25143",
        "ARXIVID": "2509.25143",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (TemMed-Bench) for temporal medical image reasoning in vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.22737": {
        "authors": [
            "Jie Cai",
            "Kangning Yang",
            "Lan Fu",
            "Jiaming Ding",
            "Jinlong Li",
            "Huiming Sun",
            "Daitao Xing",
            "Jinglin Shen",
            "Zibo Meng"
        ],
        "title": "CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models",
        "abstract": "arXiv:2509.22737v1 Announce Type: new  Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.",
        "arxiv_id": "2509.22737",
        "ARXIVID": "2509.22737",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (CompareBench) for visual comparison reasoning in vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23344": {
        "authors": [
            "Zijie Meng",
            "Jin Hao",
            "Xiwei Dai",
            "Yang Feng",
            "Jiaxiang Liu",
            "Bin Feng",
            "Huikai Wu",
            "Xiaotang Gai",
            "Hengchuan Zhu",
            "Tianxiang Hu",
            "Yangyang Wu",
            "Hongxia Xu",
            "Jin Li",
            "Jun Xiao",
            "Xiaoqiang Liu",
            "Joey Tianyi Zhou",
            "Fudong Zhu",
            "Zhihe Zhao",
            "Lunguo Xia",
            "Bing Fang",
            "Jimeng Sun",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "title": "DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice",
        "abstract": "arXiv:2509.23344v1 Announce Type: new  Abstract: Diagnosing and managing oral diseases necessitate advanced visual interpretation across diverse imaging modalities and integrated information synthesis. While current AI models excel at isolated tasks, they often fall short in addressing the complex, multimodal requirements of comprehensive clinical dental practice. Here we introduce DentVLM, a multimodal vision-language model engineered for expert-level oral disease diagnosis. DentVLM was developed using a comprehensive, large-scale, bilingual dataset of 110,447 images and 2.46 million visual question-answering (VQA) pairs. The model is capable of interpreting seven 2D oral imaging modalities across 36 diagnostic tasks, significantly outperforming leading proprietary and open-source models by 19.6% higher accuracy for oral diseases and 27.9% for malocclusions. In a clinical study involving 25 dentists, evaluating 1,946 patients and encompassing 3,105 QA pairs, DentVLM surpassed the diagnostic performance of 13 junior dentists on 21 of 36 tasks and exceeded that of 12 senior dentists on 12 of 36 tasks. When integrated into a collaborative workflow, DentVLM elevated junior dentists' performance to senior levels and reduced diagnostic time for all practitioners by 15-22%. Furthermore, DentVLM exhibited promising performance across three practical utility scenarios, including home-based dental health management, hospital-based intelligent diagnosis and multi-agent collaborative interaction. These findings establish DentVLM as a robust clinical decision support tool, poised to enhance primary dental care, mitigate provider-patient imbalances, and democratize access to specialized medical expertise within the field of dentistry.",
        "arxiv_id": "2509.23344",
        "ARXIVID": "2509.23344",
        "COMMENT": "Matches criterion 2 as it explores a multimodal vision-language model (DentVLM) for dental diagnosis, focusing on vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.23584": {
        "authors": [
            "Shulian Zhang",
            "Yong Guo",
            "Long Peng",
            "Ziyang Wang",
            "Ye Chen",
            "Wenbo Li",
            "Xiao Zhang",
            "Yulun Zhang",
            "Jian Chen"
        ],
        "title": "VividFace: High-Quality and Efficient One-Step Diffusion For Video Face Enhancement",
        "abstract": "arXiv:2509.23584v1 Announce Type: new  Abstract: Video Face Enhancement (VFE) seeks to reconstruct high-quality facial regions from degraded video sequences, a capability that underpins numerous applications including video conferencing, film restoration, and surveillance. Despite substantial progress in the field, current methods that primarily rely on video super-resolution and generative frameworks continue to face three fundamental challenges: (1) faithfully modeling intricate facial textures while preserving temporal consistency; (2) restricted model generalization due to the lack of high-quality face video training data; and (3) low efficiency caused by repeated denoising steps during inference. To address these challenges, we propose VividFace, a novel and efficient one-step diffusion framework for video face enhancement. Built upon the pretrained WANX video generation model, our method leverages powerful spatiotemporal priors through a single-step flow matching paradigm, enabling direct mapping from degraded inputs to high-quality outputs with significantly reduced inference time. To further boost efficiency, we propose a Joint Latent-Pixel Face-Focused Training strategy that employs stochastic switching between facial region optimization and global reconstruction, providing explicit supervision in both latent and pixel spaces through a progressive two-stage training process. Additionally, we introduce an MLLM-driven data curation pipeline for automated selection of high-quality video face datasets, enhancing model generalization. Extensive experiments demonstrate that VividFace achieves state-of-the-art results in perceptual quality, identity preservation, and temporal stability, while offering practical resources for the research community.",
        "arxiv_id": "2509.23584",
        "ARXIVID": "2509.23584",
        "COMMENT": "Matches criteria 6 as it focuses on video-based tasks, specifically video face enhancement with novel methodologies.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.24527": {
        "authors": [
            "Danijar Hafner",
            "Wilson Yan",
            "Timothy Lillicrap"
        ],
        "title": "Training Agents Inside of Scalable World Models",
        "abstract": "arXiv:2509.24527v1 Announce Type: new  Abstract: World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. We introduce Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. We propose the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. Our work provides a scalable recipe for imagination training, marking a step towards intelligent agents.",
        "arxiv_id": "2509.24527",
        "ARXIVID": "2509.24527",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and method for training agents in scalable world models, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.23736": {
        "authors": [
            "Cong Chen",
            "Ziyuan Huang",
            "Cheng Zou",
            "Muzhi Zhu",
            "Kaixiang Ji",
            "Jiajia Liu",
            "Jingdong Chen",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation",
        "abstract": "arXiv:2509.23736v1 Announce Type: new  Abstract: In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\\times$ faster convergence rate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizer's training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks.",
        "arxiv_id": "2509.23736",
        "ARXIVID": "2509.23736",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their applications in image reconstruction and generation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24209": {
        "authors": [
            "Yingdong Hu",
            "Yisheng He",
            "Jinnan Chen",
            "Weihao Yuan",
            "Kejie Qiu",
            "Zehong Lin",
            "Siyu Zhu",
            "Zilong Dong",
            "Jun Zhang"
        ],
        "title": "Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos",
        "abstract": "arXiv:2509.24209v1 Announce Type: new  Abstract: Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view videos is critical for numerous downstream applications. Existing methods, however, are either limited by the slow reconstruction speeds or incapable of generating novel-time representations. To address these challenges, we propose Forge4D, a feed-forward 4D human reconstruction and interpolation model that efficiently reconstructs temporally aligned representations from uncalibrated sparse-view videos, enabling both novel view and novel time synthesis. Our model simplifies the 4D reconstruction and interpolation problem as a joint task of streaming 3D Gaussian reconstruction and dense motion prediction. For the task of streaming 3D Gaussian reconstruction, we first reconstruct static 3D Gaussians from uncalibrated sparse-view images and then introduce learnable state tokens to enforce temporal consistency in a memory-friendly manner by interactively updating shared information across different timestamps. For novel time synthesis, we design a novel motion prediction module to predict dense motions for each 3D Gaussian between two adjacent frames, coupled with an occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at arbitrary timestamps. To overcome the lack of the ground truth for dense motion supervision, we formulate dense motion prediction as a dense point matching task and introduce a self-supervised retargeting loss to optimize this module. An additional occlusion-aware optical flow loss is introduced to ensure motion consistency with plausible human movement, providing stronger regularization. Extensive experiments demonstrate the effectiveness of our model on both in-domain and out-of-domain datasets. Project page and code at: https://zhenliuzju.github.io/huyingdong/Forge4D.",
        "arxiv_id": "2509.24209",
        "ARXIVID": "2509.24209",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 4D human reconstruction and interpolation, relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24142": {
        "authors": [
            "Jianze Li",
            "Yong Guo",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "Asymmetric VAE for One-Step Video Super-Resolution Acceleration",
        "abstract": "arXiv:2509.24142v1 Announce Type: new  Abstract: Diffusion models have significant advantages in the field of real-world video super-resolution and have demonstrated strong performance in past research. In recent diffusion-based video super-resolution (VSR) models, the number of sampling steps has been reduced to just one, yet there remains significant room for further optimization in inference efficiency. In this paper, we propose FastVSR, which achieves substantial reductions in computational cost by implementing a high compression VAE (spatial compression ratio of 16, denoted as f16). We design the structure of the f16 VAE and introduce a stable training framework. We employ pixel shuffle and channel replication to achieve additional upsampling. Furthermore, we propose a lower-bound-guided training strategy, which introduces a simpler training objective as a lower bound for the VAE's performance. It makes the training process more stable and easier to converge. Experimental results show that FastVSR achieves speedups of 111.9 times compared to multi-step models and 3.92 times compared to existing one-step models. We will release code and models at https://github.com/JianzeLi-114/FastVSR.",
        "arxiv_id": "2509.24142",
        "ARXIVID": "2509.24142",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video super-resolution with novel methodologies.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.23770": {
        "authors": [
            "Xiaojie Li",
            "Bei Wang",
            "Jianlong Wu",
            "Yue Yu",
            "Liqiang Nie",
            "Min Zhang"
        ],
        "title": "GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning",
        "abstract": "arXiv:2509.23770v1 Announce Type: new  Abstract: The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair's semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%. The code is available at https://github.com/xiaojieli0903/GenViewPlusPlus.",
        "arxiv_id": "2509.23770",
        "ARXIVID": "2509.23770",
        "COMMENT": "Matches criteria 4 as it focuses on vision foundation models and their applications in contrastive learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.24350": {
        "authors": [
            "Yan Ke",
            "Xin Yu",
            "Heming Du",
            "Scott Chapman",
            "Helen Huang"
        ],
        "title": "Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA",
        "abstract": "arXiv:2509.24350v1 Announce Type: new  Abstract: Agricultural visual question answering is essential for providing farmers and researchers with accurate and timely knowledge. However, many existing approaches are predominantly developed for evidence-constrained settings such as text-only queries or single-image cases. This design prevents them from coping with real-world agricultural scenarios that often require multi-image inputs with complementary views across spatial scales, and growth stages. Moreover, limited access to up-to-date external agricultural context makes these systems struggle to adapt when evidence is incomplete. In addition, rigid pipelines often lack systematic quality control. To address this gap, we propose a self-reflective and self-improving multi-agent framework that integrates four roles, the Retriever, the Reflector, the Answerer, and the Improver. They collaborate to enable context enrichment, reflective reasoning, answer drafting, and iterative improvement.   A Retriever formulates queries and gathers external information, while a Reflector assesses adequacy and triggers sequential reformulation and renewed retrieval. Two Answerers draft candidate responses in parallel to reduce bias. The Improver refines them through iterative checks while ensuring that information from multiple images is effectively aligned and utilized. Experiments on the AgMMU benchmark show that our framework achieves competitive performance on multi-image agricultural QA.",
        "arxiv_id": "2509.24350",
        "ARXIVID": "2509.24350",
        "COMMENT": "Matches criteria 5 as it integrates multi-image understanding with reasoning tasks in a multi-modal framework.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.23014": {
        "authors": [
            "Yihao Sun",
            "Zhilong Zhang",
            "Yang Yu",
            "Pierre-Luc Bacon"
        ],
        "title": "Planning with Unified Multimodal Models",
        "abstract": "arXiv:2509.23014v1 Announce Type: new  Abstract: With the powerful reasoning capabilities of large language models (LLMs) and vision-language models (VLMs), many recent works have explored using them for decision-making. However, most of these approaches rely solely on language-based reasoning, which limits their ability to reason and make informed decisions. Recently, a promising new direction has emerged with unified multimodal models (UMMs), which support both multimodal inputs and outputs. We believe such models have greater potential for decision-making by enabling reasoning through generated visual content. To this end, we propose Uni-Plan, a planning framework built on UMMs. Within this framework, a single model simultaneously serves as the policy, dynamics model, and value function. In addition, to avoid hallucinations in dynamics predictions, we present a novel approach self-discriminated filtering, where the generative model serves as a self-discriminator to filter out invalid dynamics predictions. Experiments on long-horizon planning tasks show that Uni-Plan substantially improves success rates compared to VLM-based methods, while also showing strong data scalability, requiring no expert demonstrations and achieving better performance under the same training-data size. This work lays a foundation for future research in reasoning and decision-making with UMMs.",
        "arxiv_id": "2509.23014",
        "ARXIVID": "2509.23014",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it explores unified multimodal models for decision-making, integrating vision and language reasoning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24702": {
        "authors": [
            "Yutong Hao",
            "Chen Chen",
            "Ajmal Saeed Mian",
            "Chang Xu",
            "Daochang Liu"
        ],
        "title": "Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility",
        "abstract": "arXiv:2509.24702v1 Announce Type: new  Abstract: Diffusion models can generate realistic videos, but existing methods rely on implicitly learning physical reasoning from large-scale text-video datasets, which is costly, difficult to scale, and still prone to producing implausible motions that violate fundamental physical laws. We introduce a training-free framework that improves physical plausibility at inference time by explicitly reasoning about implausibility and guiding the generation away from it. Specifically, we employ a lightweight physics-aware reasoning pipeline to construct counterfactual prompts that deliberately encode physics-violating behaviors. Then, we propose a novel Synchronized Decoupled Guidance (SDG) strategy, which leverages these prompts through synchronized directional normalization to counteract lagged suppression and trajectory-decoupled denoising to mitigate cumulative trajectory bias, ensuring that implausible content is suppressed immediately and consistently throughout denoising. Experiments across different physical domains show that our approach substantially enhances physical fidelity while maintaining photorealism, despite requiring no additional training. Ablation studies confirm the complementary effectiveness of both the physics-aware reasoning component and SDG. In particular, the aforementioned two designs of SDG are also individually validated to contribute critically to the suppression of implausible content and the overall gains in physical plausibility. This establishes a new and plug-and-play physics-aware paradigm for video generation.",
        "arxiv_id": "2509.24702",
        "ARXIVID": "2509.24702",
        "COMMENT": "Matches criteria 6 (Video Understanding) as it focuses on improving physical plausibility in video generation, which is a novel methodology for video-based tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.24927": {
        "authors": [
            "An Guo",
            "Shuoxiao Zhang",
            "Enyi Tang",
            "Xinyu Gao",
            "Haomin Pang",
            "Haoxiang Tian",
            "Yanzhou Mu",
            "Wu Wen",
            "Chunrong Fang",
            "Zhenyu Chen"
        ],
        "title": "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?",
        "abstract": "arXiv:2509.24927v1 Announce Type: new  Abstract: With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) cooperative perception has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. V2X cooperative perception systems are software systems characterized by diverse sensor types and cooperative agents, varying fusion schemes, and operation under different communication conditions. Therefore, their complex composition gives rise to numerous operational challenges. Furthermore, when cooperative perception systems produce erroneous predictions, the types of errors and their underlying causes remain insufficiently explored. To bridge this gap, we take an initial step by conducting an empirical study of V2X cooperative perception. To systematically evaluate the impact of cooperative perception on the ego vehicle's perception performance, we identify and analyze six prevalent error patterns in cooperative perception systems. We further conduct a systematic evaluation of the critical components of these systems through our large-scale study and identify the following key findings: (1) The LiDAR-based cooperation configuration exhibits the highest perception performance; (2) Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication exhibit distinct cooperative perception performance under different fusion schemes; (3) Increased cooperative perception errors may result in a higher frequency of driving violations; (4) Cooperative perception systems are not robust against communication interference when running online. Our results reveal potential risks and vulnerabilities in critical components of cooperative perception systems. We hope that our findings can better promote the design and repair of cooperative perception systems.",
        "arxiv_id": "2509.24927",
        "ARXIVID": "2509.24927",
        "COMMENT": "Matches criterion 3 as it evaluates cooperative perception systems for autonomous vehicles, addressing challenges in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.25026": {
        "authors": [
            "Mustansar Fiaz",
            "Hiyam Debary",
            "Paolo Fraccaro",
            "Danda Paudel",
            "Luc Van Gool",
            "Fahad Khan",
            "Salman Khan"
        ],
        "title": "GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning",
        "abstract": "arXiv:2509.25026v1 Announce Type: new  Abstract: Recent advances in reinforcement learning (RL) have delivered strong reasoning capabilities in natural image domains, yet their potential for Earth Observation (EO) remains largely unexplored. EO tasks introduce unique challenges, spanning referred object detection, image or region captioning, change detection, grounding, and temporal analysis, that demand task aware reasoning. We propose a novel post training framework that incorporates task aware rewards to enable effective adaptation of reasoning based RL models to diverse EO tasks. This training strategy enhances reasoning capabilities for remote sensing images, stabilizes optimization, and improves robustness. Extensive experiments across multiple EO benchmarks show consistent performance gains over state of the art generic and specialized vision language models. Code and models will be released publicly at https://mustansarfiaz.github.io/GeoVLM-R1/ .",
        "arxiv_id": "2509.25026",
        "ARXIVID": "2509.25026",
        "COMMENT": "Matches criterion 6 as it focuses on reasoning in remote sensing tasks, including temporal analysis and grounding, which are video-based tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.24020": {
        "authors": [
            "Jian Chen",
            "Zhuoran Zheng",
            "Han Hu",
            "Guijuan Zhang",
            "Dianjie Lu",
            "Liang Li",
            "Chen Lyu"
        ],
        "title": "Hazy Pedestrian Trajectory Prediction via Physical Priors and Graph-Mamba",
        "abstract": "arXiv:2509.24020v1 Announce Type: new  Abstract: To address the issues of physical information degradation and ineffective pedestrian interaction modeling in pedestrian trajectory prediction under hazy weather conditions, we propose a deep learning model that combines physical priors of atmospheric scattering with topological modeling of pedestrian relationships. Specifically, we first construct a differentiable atmospheric scattering model that decouples haze concentration from light degradation through a network with physical parameter estimation, enabling the learning of haze-mitigated feature representations. Second, we design an adaptive scanning state space model for feature extraction. Our adaptive Mamba variant achieves a 78% inference speed increase over native Mamba while preserving long-range dependency modeling.   Finally, to efficiently model pedestrian relationships, we develop a heterogeneous graph attention network, using graph matrices to model multi-granularity interactions between pedestrians and groups, combined with a spatio-temporal fusion module to capture the collaborative evolution patterns of pedestrian movements. Furthermore, we constructed a new pedestrian trajectory prediction dataset based on ETH/UCY to evaluate the effectiveness of the proposed method. Experiments show that our method reduces the minADE / minFDE metrics by 37.2% and 41.5%, respectively, compared to the SOTA models in dense haze scenarios (visibility < 30m), providing a new modeling paradigm for reliable perception in intelligent transportation systems in adverse environments.",
        "arxiv_id": "2509.24020",
        "ARXIVID": "2509.24020",
        "COMMENT": "Matches criterion 3 as it introduces a new method for pedestrian trajectory prediction under adverse conditions, relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.24250": {
        "authors": [
            "Edward Kim",
            "Daniel He",
            "Jorge Chao",
            "Wiktor Rajca",
            "Mohammed Amin",
            "Nishant Malpani",
            "Ruta Desai",
            "Antti Oulasvirta",
            "Bjoern Hartmann",
            "Sanjit Seshia"
        ],
        "title": "Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations",
        "abstract": "arXiv:2509.24250v1 Announce Type: new  Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.",
        "arxiv_id": "2509.24250",
        "ARXIVID": "2509.24250",
        "COMMENT": "Matches criterion 3 as it addresses collaborative task learning in embodied AI with a novel program synthesis approach.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.23242": {
        "authors": [
            "Yuntian Wu",
            "Xiaonan Hu",
            "Ziqi Zhou",
            "Hao Lu"
        ],
        "title": "TATTOO: Training-free AesTheTic-aware Outfit recOmmendation",
        "abstract": "arXiv:2509.23242v1 Announce Type: new  Abstract: The global fashion e-commerce market relies significantly on intelligent and aesthetic-aware outfit-completion tools to promote sales. While previous studies have approached the problem of fashion outfit-completion and compatible-item retrieval, most of them require expensive, task-specific training on large-scale labeled data, and no effort is made to guide outfit recommendation with explicit human aesthetics. In the era of Multimodal Large Language Models (MLLMs), we show that the conventional training-based pipeline could be streamlined to a training-free paradigm, with better recommendation scores and enhanced aesthetic awareness. We achieve this with TATTOO, a Training-free AesTheTic-aware Outfit recommendation approach. It first generates a target-item description using MLLMs, followed by an aesthetic chain-of-thought used to distill the images into a structured aesthetic profile including color, style, occasion, season, material, and balance. By fusing the visual summary of the outfit with the textual description and aesthetics vectors using a dynamic entropy-gated mechanism, candidate items can be represented in a shared embedding space and be ranked accordingly. Experiments on a real-world evaluation set Aesthetic-100 show that TATTOO achieves state-of-the-art performance compared with existing training-based methods. Another standard Polyvore dataset is also used to measure the advanced zero-shot retrieval capability of our training-free method.",
        "arxiv_id": "2509.23242",
        "ARXIVID": "2509.23242",
        "COMMENT": "Matches criterion 2 as it leverages multimodal large language models for aesthetic-aware outfit recommendation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.23316": {
        "authors": [
            "Siheng Wang",
            "Zhengdao Li",
            "Yanshu Li",
            "Canran Xiao",
            "Haibo Zhan",
            "Zhengtao Yao",
            "Xuzhi Zhang",
            "Jiale Kang",
            "Linshan Li",
            "Weiming Liu",
            "Zhikang Dong",
            "Jifeng Shen",
            "Junhao Dong",
            "Qiang Sun",
            "Piotr Koniusz"
        ],
        "title": "C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection",
        "abstract": "arXiv:2509.23316v1 Announce Type: new  Abstract: Object detection has advanced significantly in the closed-set setting, but real-world deployment remains limited by two challenges: poor generalization to unseen categories and insufficient robustness under adverse conditions. Prior research has explored these issues separately: visible-infrared detection improves robustness but lacks generalization, while open-world detection leverages vision-language alignment strategy for category diversity but struggles under extreme environments. This trade-off leaves robustness and diversity difficult to achieve simultaneously. To mitigate these issues, we propose \\textbf{C3-OWD}, a curriculum cross-modal contrastive learning framework that unifies both strengths. Stage~1 enhances robustness by pretraining with RGBT data, while Stage~2 improves generalization via vision-language alignment. To prevent catastrophic forgetting between two stages, we introduce an Exponential Moving Average (EMA) mechanism that theoretically guarantees preservation of pre-stage performance with bounded parameter lag and function consistency. Experiments on FLIR, OV-COCO, and OV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$ AP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\\text{Novel}}$ on OV-COCO, and $35.7$ mAP$_r$ on OV-LVIS, establishing competitive performance across both robustness and diversity evaluations. Code available at: https://github.com/justin-herry/C3-OWD.git.",
        "arxiv_id": "2509.23316",
        "ARXIVID": "2509.23316",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for open-world detection with a focus on robustness and generalization, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.23922": {
        "authors": [
            "Haibao Yu",
            "Wenxian Yang",
            "Ruiyang Hao",
            "Chuanye Wang",
            "Jiaru Zhong",
            "Ping Luo",
            "Zaiqing Nie"
        ],
        "title": "DriveE2E: Closed-Loop Benchmark for End-to-End Autonomous Driving through Real-to-Simulation",
        "abstract": "arXiv:2509.23922v1 Announce Type: new  Abstract: Closed-loop evaluation is increasingly critical for end-to-end autonomous driving. Current closed-loop benchmarks using the CARLA simulator rely on manually configured traffic scenarios, which can diverge from real-world conditions, limiting their ability to reflect actual driving performance. To address these limitations, we introduce a simple yet challenging closed-loop evaluation framework that closely integrates real-world driving scenarios into the CARLA simulator with infrastructure cooperation. Our approach involves extracting 800 dynamic traffic scenarios selected from a comprehensive 100-hour video dataset captured by high-mounted infrastructure sensors, and creating static digital twin assets for 15 real-world intersections with consistent visual appearance. These digital twins accurately replicate the traffic and environmental characteristics of their real-world counterparts, enabling more realistic simulations in CARLA. This evaluation is challenging due to the diversity of driving behaviors, locations, weather conditions, and times of day at complex urban intersections. In addition, we provide a comprehensive closed-loop benchmark for evaluating end-to-end autonomous driving models. Project URL: \\href{https://github.com/AIR-THU/DriveE2E}{https://github.com/AIR-THU/DriveE2E}.",
        "arxiv_id": "2509.23922",
        "ARXIVID": "2509.23922",
        "COMMENT": "Matches criteria 3 as it introduces a new closed-loop benchmark for autonomous driving, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.23980": {
        "authors": [
            "Jinpei Guo",
            "Yifei Ji",
            "Zheng Chen",
            "Yufei Wang",
            "Sizhuo Ma",
            "Yong Guo",
            "Yulun Zhang",
            "Jian Wang"
        ],
        "title": "Towards Redundancy Reduction in Diffusion Models for Efficient Video Super-Resolution",
        "abstract": "arXiv:2509.23980v1 Announce Type: new  Abstract: Diffusion models have recently shown promising results for video super-resolution (VSR). However, directly adapting generative diffusion models to VSR can result in redundancy, since low-quality videos already preserve substantial content information. Such redundancy leads to increased computational overhead and learning burden, as the model performs superfluous operations and must learn to filter out irrelevant information. To address this problem, we propose OASIS, an efficient $\\textbf{o}$ne-step diffusion model with $\\textbf{a}$ttention $\\textbf{s}$pecialization for real-world v$\\textbf{i}$deo $\\textbf{s}$uper-resolution. OASIS incorporates an attention specialization routing that assigns attention heads to different patterns according to their intrinsic behaviors. This routing mitigates redundancy while effectively preserving pretrained knowledge, allowing diffusion models to better adapt to VSR and achieve stronger performance. Moreover, we propose a simple yet effective progressive training strategy, which starts with temporally consistent degradations and then shifts to inconsistent settings. This strategy facilitates learning under complex degradations. Extensive experiments demonstrate that OASIS achieves state-of-the-art performance on both synthetic and real-world datasets. OASIS also provides superior inference speed, offering a $\\textbf{6.2$\\times$}$ speedup over one-step diffusion baselines such as SeedVR2. The code will be available at \\href{https://github.com/jp-guo/OASIS}{https://github.com/jp-guo/OASIS}.",
        "arxiv_id": "2509.23980",
        "ARXIVID": "2509.23980",
        "COMMENT": "Matches criterion 6 as it focuses on video super-resolution, a video understanding task.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.23517": {
        "authors": [
            "Akila Kadambi",
            "Marco Iacoboni",
            "Lisa Aziz-Zadeh",
            "Srini Narayanan"
        ],
        "title": "Evaluating point-light biological motion in multimodal large language models",
        "abstract": "arXiv:2509.23517v1 Announce Type: new  Abstract: Humans can extract rich semantic information from minimal visual cues, as demonstrated by point-light displays (PLDs), which consist of sparse sets of dots localized to key joints of the human body. This ability emerges early in development and is largely attributed to human embodied experience. Since PLDs isolate body motion as the sole source of meaning, they represent key stimuli for testing the constraints of action understanding in these systems. Here we introduce ActPLD, the first benchmark to evaluate action processing in MLLMs from human PLDs. Tested models include state-of-the-art proprietary and open-source systems on single-actor and socially interacting PLDs. Our results reveal consistently low performance across models, introducing fundamental gaps in action and spatiotemporal understanding.",
        "arxiv_id": "2509.23517",
        "ARXIVID": "2509.23517",
        "COMMENT": "Matches criterion 2 as it evaluates multimodal large language models (MLLMs) on action understanding tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.23958": {
        "authors": [
            "Yang Ye",
            "Tianyu He",
            "Shuo Yang",
            "Jiang Bian"
        ],
        "title": "Reinforcement Learning with Inverse Rewards for World Model Post-training",
        "abstract": "arXiv:2509.23958v1 Announce Type: new  Abstract: World models simulate dynamic environments, enabling agents to interact with diverse input modalities. Although recent advances have improved the visual quality and temporal consistency of video world models, their ability of accurately modeling human-specified actions remains under-explored. Reinforcement learning presents a promising approach for directly improving the suboptimal action-following capability of pre-trained models, assuming that an appropriate reward function can be defined. However, transferring reinforcement learning post-training methods to world model is impractical due to the prohibitive cost of large-scale preference annotations and the infeasibility of constructing rule-based video verifiers. To address this gap, we propose Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework that derives verifiable reward signals by recovering input actions from generated videos using an Inverse Dynamics Model. By mapping high-dimensional video modality to a low-dimensional action space, RLIR provides an objective and verifiable reward for optimization via Group Relative Policy Optimization. Experiments across autoregressive and diffusion paradigms demonstrate 5-10% gains in action-following, up to 10% improvements in visual quality, and higher human preference scores, establishing RLIR as the first post-training method specifically designed to enhance action-following in video world models.",
        "arxiv_id": "2509.23958",
        "ARXIVID": "2509.23958",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding and improving action-following in video world models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.25190": {
        "authors": [
            "Penghao Wu",
            "Yushan Zhang",
            "Haiwen Diao",
            "Bo Li",
            "Lewei Lu",
            "Ziwei Liu"
        ],
        "title": "Visual Jigsaw Post-Training Improves MLLMs",
        "abstract": "arXiv:2509.25190v1 Announce Type: new  Abstract: Reinforcement learning based post-training has recently emerged as a powerful paradigm for enhancing the alignment and reasoning capabilities of multimodal large language models (MLLMs). While vision-centric post-training is crucial for enhancing MLLMs' intrinsic understanding of visual signals, current post-training paradigms are predominantly text-centric, where dense visual inputs are only leveraged to extract sparse cues for text-based reasoning. There exist a few approaches in this direction, however, they often still rely on text as an intermediate mediator or introduce additional visual generative designs. In this work, we introduce Visual Jigsaw, a generic self-supervised post-training framework designed to strengthen visual understanding in MLLMs. Visual Jigsaw is formulated as a general ordering task: visual inputs are partitioned, shuffled, and the model must reconstruct the visual information by producing the correct permutation in natural language. This naturally aligns with reinforcement learning from verifiable rewards (RLVR), requires no additional visual generative components, and derives its supervisory signal automatically without any annotations. We instantiate Visual Jigsaw across three visual modalities, including images, videos, and 3D data. Extensive experiments demonstrate substantial improvements in fine-grained perception, temporal reasoning, and 3D spatial understanding. Our findings highlight the potential of self-supervised vision-centric tasks in post-training MLLMs and aim to inspire further research on vision-centric pretext designs. Project Page: https://penghao-wu.github.io/visual_jigsaw/",
        "arxiv_id": "2509.25190",
        "ARXIVID": "2509.25190",
        "COMMENT": "Matches criterion 2 as it introduces a novel self-supervised post-training framework for multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.24231": {
        "authors": [
            "Yang Bai",
            "Haoran Cheng",
            "Yang Zhou",
            "Jun Zhou",
            "Arun Thirunavukarasu",
            "Yuhe Ke",
            "Jie Yao",
            "Kanae Fukutsu",
            "Chrystie Wan Ning Quek",
            "Ashley Hong",
            "Laura Gutierrez",
            "Zhen Ling Teo",
            "Darren Shu Jeng Ting",
            "Brian T. Soetikno",
            "Christopher S. Nielsen",
            "Tobias Elze",
            "Zengxiang Li",
            "Linh Le Dinh",
            "Hiok Hong Chan",
            "Victor Koh",
            "Marcus Tan",
            "Kelvin Z. Li",
            "Leonard Yip",
            "Ching Yu Cheng",
            "Yih Chung Tham",
            "Gavin Siew Wei Tan",
            "Leopold Schmetterer",
            "Marcus Ang",
            "Rahat Hussain",
            "Jod Mehta",
            "Tin Aung",
            "Lionel Tim-Ee Cheng",
            "Tran Nguyen Tuan Anh",
            "Chee Leong Cheng",
            "Tien Yin Wong",
            "Nan Liu",
            "Iain Beehuat Tan",
            "Soon Thye Lim",
            "Eyal Klang",
            "Tony Kiat Hon Lim",
            "Rick Siow Mong Goh",
            "Yong Liu",
            "Daniel Shu Wei Ting"
        ],
        "title": "EVLF-FM: Explainable Vision Language Foundation Model for Medicine",
        "abstract": "arXiv:2509.24231v1 Announce Type: new  Abstract: Despite the promise of foundation models in medical AI, current systems remain limited - they are modality-specific and lack transparent reasoning processes, hindering clinical adoption. To address this gap, we present EVLF-FM, a multimodal vision-language foundation model (VLM) designed to unify broad diagnostic capability with fine-grain explainability. The development and testing of EVLF-FM encompassed over 1.3 million total samples from 23 global datasets across eleven imaging modalities related to six clinical specialties: dermatology, hepatology, ophthalmology, pathology, pulmonology, and radiology. External validation employed 8,884 independent test samples from 10 additional datasets across five imaging modalities. Technically, EVLF-FM is developed to assist with multiple disease diagnosis and visual question answering with pixel-level visual grounding and reasoning capabilities. In internal validation for disease diagnostics, EVLF-FM achieved the highest average accuracy (0.858) and F1-score (0.797), outperforming leading generalist and specialist models. In medical visual grounding, EVLF-FM also achieved stellar performance across nine modalities with average mIOU of 0.743 and Acc@0.5 of 0.837. External validations further confirmed strong zero-shot and few-shot performance, with competitive F1-scores despite a smaller model size. Through a hybrid training strategy combining supervised and visual reinforcement fine-tuning, EVLF-FM not only achieves state-of-the-art accuracy but also exhibits step-by-step reasoning, aligning outputs with visual evidence. EVLF-FM is an early multi-disease VLM model with explainability and reasoning capabilities that could advance adoption of and trust in foundation models for real-world clinical deployment.",
        "arxiv_id": "2509.24231",
        "ARXIVID": "2509.24231",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language foundation model with multi-modal capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.24423": {
        "authors": [
            "Runmin Zhang",
            "Jialiang Wang",
            "Si-Yuan Cao",
            "Zhu Yu",
            "Junchen Yu",
            "Guangyi Zhang",
            "Hui-Liang Shen"
        ],
        "title": "Rethinking Unsupervised Cross-modal Flow Estimation: Learning from Decoupled Optimization and Consistency Constraint",
        "abstract": "arXiv:2509.24423v1 Announce Type: new  Abstract: This work presents DCFlow, a novel unsupervised cross-modal flow estimation framework that integrates a decoupled optimization strategy and a cross-modal consistency constraint. Unlike previous approaches that implicitly learn flow estimation solely from appearance similarity, we introduce a decoupled optimization strategy with task-specific supervision to address modality discrepancy and geometric misalignment distinctly. This is achieved by collaboratively training a modality transfer network and a flow estimation network. To enable reliable motion supervision without ground-truth flow, we propose a geometry-aware data synthesis pipeline combined with an outlier-robust loss. Additionally, we introduce a cross-modal consistency constraint to jointly optimize both networks, significantly improving flow prediction accuracy. For evaluation, we construct a comprehensive cross-modal flow benchmark by repurposing public datasets. Experimental results demonstrate that DCFlow can be integrated with various flow estimation networks and achieves state-of-the-art performance among unsupervised approaches.",
        "arxiv_id": "2509.24423",
        "ARXIVID": "2509.24423",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for cross-modal flow estimation, which involves spatial reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.24681": {
        "authors": [
            "Hanyu Zhang",
            "Yiming Zhou",
            "Jinxia Zhang"
        ],
        "title": "Classifier-Centric Adaptive Framework for Open-Vocabulary Camouflaged Object Segmentation",
        "abstract": "arXiv:2509.24681v1 Announce Type: new  Abstract: Open-vocabulary camouflaged object segmentation requires models to segment camouflaged objects of arbitrary categories unseen during training, placing extremely high demands on generalization capabilities. Through analysis of existing methods, it is observed that the classification component significantly affects overall segmentation performance. Accordingly, a classifier-centric adaptive framework is proposed to enhance segmentation performance by improving the classification component via a lightweight text adapter with a novel layered asymmetric initialization. Through the classification enhancement, the proposed method achieves substantial improvements in segmentation metrics compared to the OVCoser baseline on the OVCamo benchmark: cIoU increases from 0.443 to 0.493, cSm from 0.579 to 0.658, and cMAE reduces from 0.336 to 0.239. These results demonstrate that targeted classification enhancement provides an effective approach for advancing camouflaged object segmentation performance.",
        "arxiv_id": "2509.24681",
        "ARXIVID": "2509.24681",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning improvements for segmentation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24261": {
        "authors": [
            "Yuhua Jiang",
            "Jiawei Huang",
            "Yufeng Yuan",
            "Xin Mao",
            "Yu Yue",
            "Qianchuan Zhao",
            "Lin Yan"
        ],
        "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models",
        "abstract": "arXiv:2509.24261v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.",
        "arxiv_id": "2509.24261",
        "ARXIVID": "2509.24261",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it discusses reinforcement learning improvements for reasoning tasks, which could be relevant to embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.22917": {
        "authors": [
            "Yuelin Xin",
            "Yuheng Liu",
            "Xiaohui Xie",
            "Xinke Li"
        ],
        "title": "Learning Unified Representation of 3D Gaussian Splatting",
        "abstract": "arXiv:2509.22917v1 Announce Type: new  Abstract: A well-designed vectorized representation is crucial for the learning systems natively based on 3D Gaussian Splatting. While 3DGS enables efficient and explicit 3D reconstruction, its parameter-based representation remains hard to learn as features, especially for neural-network-based models. Directly feeding raw Gaussian parameters into learning frameworks fails to address the non-unique and heterogeneous nature of the Gaussian parameterization, yielding highly data-dependent models. This challenge motivates us to explore a more principled approach to represent 3D Gaussian Splatting in neural networks that preserves the underlying color and geometric structure while enforcing unique mapping and channel homogeneity. In this paper, we propose an embedding representation of 3DGS based on continuous submanifold fields that encapsulate the intrinsic information of Gaussian primitives, thereby benefiting the learning of 3DGS.",
        "arxiv_id": "2509.22917",
        "ARXIVID": "2509.22917",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it explores a new representation for 3D Gaussian Splatting, which is relevant to spatial reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24445": {
        "authors": [
            "Jianxin Liang",
            "Tan Yue",
            "Yuxuan Wang",
            "Yueqian Wang",
            "Zhihan Yin",
            "Huishuai Zhang",
            "Dongyan Zhao"
        ],
        "title": "Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA",
        "abstract": "arXiv:2509.24445v1 Announce Type: new  Abstract: The performance of Video Question Answering (VideoQA) models is fundamentally constrained by the nature of their supervision, which typically consists of isolated, factual question-answer pairs. This \"bag-of-facts\" approach fails to capture the underlying narrative and causal structure of events, limiting models to a shallow understanding of video content. To move beyond this paradigm, we introduce a framework to synthesize richer supervisory signals. We propose two complementary strategies: Question-Based Paraphrasing (QBP), which synthesizes the diverse inquiries (what, how, why) from a video's existing set of question-answer pairs into a holistic narrative paragraph that reconstructs the video's event structure; and Question-Based Captioning (QBC), which generates fine-grained visual rationales, grounding the answer to each question in specific, relevant evidence. Leveraging powerful generative models, we use this synthetic data to train VideoQA models under a unified next-token prediction objective. Extensive experiments on STAR and NExT-QA validate our approach, demonstrating significant accuracy gains and establishing new state-of-the-art results, such as improving a 3B model to 72.5\\% on STAR (+4.9\\%) and a 7B model to 80.8\\% on NExT-QA. Beyond accuracy, our analysis reveals that both QBP and QBC substantially enhance cross-dataset generalization, with QBP additionally accelerating model convergence by over 2.5x. These results demonstrate that shifting data synthesis from isolated facts to narrative coherence and grounded rationales yields a more accurate, efficient, and generalizable training paradigm.",
        "arxiv_id": "2509.24445",
        "ARXIVID": "2509.24445",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a new framework for VideoQA with narrative and grounded supervision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23760": {
        "authors": [
            "Xinyang Song",
            "Libin Wang",
            "Weining Wang",
            "Shaozhen Liu",
            "Dandan Zheng",
            "Jingdong Chen",
            "Qi Li",
            "Zhenan Sun"
        ],
        "title": "UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception",
        "abstract": "arXiv:2509.23760v1 Announce Type: new  Abstract: The remarkable success of diffusion models in text-to-image generation has sparked growing interest in expanding their capabilities to a variety of multi-modal tasks, including image understanding, manipulation, and perception. These tasks require advanced semantic comprehension across both visual and textual modalities, especially in scenarios involving complex semantic instructions. However, existing approaches often rely heavily on vision-language models (VLMs) or modular designs for semantic guidance, leading to fragmented architectures and computational inefficiency. To address these challenges, we propose UniAlignment, a unified multimodal generation framework within a single diffusion transformer. UniAlignment introduces a dual-stream diffusion training strategy that incorporates both intrinsic-modal semantic alignment and cross-modal semantic alignment, thereby enhancing the model's cross-modal consistency and instruction-following robustness. Additionally, we present SemGen-Bench, a new benchmark specifically designed to evaluate multimodal semantic consistency under complex textual instructions. Extensive experiments across multiple tasks and benchmarks demonstrate that UniAlignment outperforms existing baselines, underscoring the significant potential of diffusion models in unified multimodal generation.",
        "arxiv_id": "2509.23760",
        "ARXIVID": "2509.23760",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a unified multimodal generation framework for image and text tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23681": {
        "authors": [
            "Weilun Feng",
            "Chuanguang Yang",
            "Haotong Qin",
            "Mingqiang Wu",
            "Yuqi Li",
            "Xiangqi Li",
            "Zhulin An",
            "Libo Huang",
            "Yulun Zhang",
            "Michele Magno",
            "Yongjun Xu"
        ],
        "title": "QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification",
        "abstract": "arXiv:2509.23681v1 Announce Type: new  Abstract: Diffusion transformers exhibit remarkable video generation capability, yet their prohibitive computational and memory costs hinder practical deployment. Model quantization and attention sparsification are two promising directions for compression, but each alone suffers severe performance degradation under aggressive compression. Combining them promises compounded efficiency gains, but naive integration is ineffective. The sparsity-induced information loss exacerbates quantization noise, leading to amplified attention shifts. To address this, we propose \\textbf{QuantSparse}, a unified framework that integrates model quantization with attention sparsification. Specifically, we introduce \\textit{Multi-Scale Salient Attention Distillation}, which leverages both global structural guidance and local salient supervision to mitigate quantization-induced bias. In addition, we develop \\textit{Second-Order Sparse Attention Reparameterization}, which exploits the temporal stability of second-order residuals to efficiently recover information lost under sparsity. Experiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88 PSNR, substantially outperforming the state-of-the-art quantization baseline Q-VDiT (16.85 PSNR), while simultaneously delivering a \\textbf{3.68$\\times$} reduction in storage and \\textbf{1.88$\\times$} acceleration in end-to-end inference. Our code will be released in https://github.com/wlfeng0509/QuantSparse.",
        "arxiv_id": "2509.23681",
        "ARXIVID": "2509.23681",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video diffusion transformers and their compression for video generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.25001": {
        "authors": [
            "Tooba Imtiaz",
            "Lucy Chai",
            "Kathryn Heal",
            "Xuan Luo",
            "Jungyeon Park",
            "Jennifer Dy",
            "John Flynn"
        ],
        "title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers",
        "abstract": "arXiv:2509.25001v1 Announce Type: new  Abstract: Large transformer models are proving to be a powerful tool for 3D vision and novel view synthesis. However, the standard Transformer's well-known quadratic complexity makes it difficult to scale these methods to large scenes. To address this challenge, we propose the Local View Transformer (LVT), a large-scale scene reconstruction and novel view synthesis architecture that circumvents the need for the quadratic attention operation. Motivated by the insight that spatially nearby views provide more useful signal about the local scene composition than distant views, our model processes all information in a local neighborhood around each view. To attend to tokens in nearby views, we leverage a novel positional encoding that conditions on the relative geometric transformation between the query and nearby views. We decode the output of our model into a 3D Gaussian Splat scene representation that includes both color and opacity view-dependence. Taken together, the Local View Transformer enables reconstruction of arbitrarily large, high-resolution scenes in a single forward pass. See our project page for results and interactive demos https://toobaimt.github.io/lvt/.",
        "arxiv_id": "2509.25001",
        "ARXIVID": "2509.25001",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a novel transformer architecture for large-scale 3D scene reconstruction.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24900": {
        "authors": [
            "Zhihong Chen",
            "Xuehai Bai",
            "Yang Shi",
            "Chaoyou Fu",
            "Huanyu Zhang",
            "Haotian Wang",
            "Xiaoyan Sun",
            "Zhang Zhang",
            "Liang Wang",
            "Yuanxing Zhang",
            "Pengfei Wan",
            "Yi-Fan Zhang"
        ],
        "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing",
        "abstract": "arXiv:2509.24900v1 Announce Type: new  Abstract: The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.",
        "arxiv_id": "2509.24900",
        "ARXIVID": "2509.24900",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it introduces a dataset for multimodal models focusing on image generation and editing, which is relevant to vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24427": {
        "authors": [
            "Ailing Zhang",
            "Lina Lei",
            "Dehong Kong",
            "Zhixin Wang",
            "Jiaqi Xu",
            "Fenglong Song",
            "Chun-Le Guo",
            "Chang Liu",
            "Fan Li",
            "Jie Chen"
        ],
        "title": "UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark",
        "abstract": "arXiv:2509.24427v1 Announce Type: new  Abstract: Generative diffusion models are developing rapidly and attracting increasing attention due to their wide range of applications. Image-to-Video (I2V) generation has become a major focus in the field of video synthesis. However, existing evaluation benchmarks primarily focus on aspects such as video quality and temporal consistency, while largely overlooking the model's ability to understand the semantics of specific subjects in the input image or to ensure that the generated video aligns with physical laws and human commonsense. To address this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V models with a focus on semantic understanding and reasoning. It introduces four primary evaluation dimensions: spatial understanding, attribute binding, category understanding, and reasoning. To assess these dimensions, we design two evaluation methods based on Multimodal Large Language Models (MLLMs): an instance-level pipeline for fine-grained semantic understanding, and a feedback-based reasoning pipeline that enables step-by-step causal assessment for more accurate evaluation. UI2V-Bench includes approximately 500 carefully constructed text-image pairs and evaluates a range of both open source and closed-source I2V models across all defined dimensions. We further incorporate human evaluations, which show strong alignment with the proposed MLLM-based metrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by emphasizing semantic comprehension and reasoning ability, offering a robust framework and dataset to support future research and model development in the field.",
        "arxiv_id": "2509.24427",
        "ARXIVID": "2509.24427",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for Image-to-Video generation with a focus on semantic understanding and reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24230": {
        "authors": [
            "Shaobin Ling",
            "Yun Wang",
            "Chenyou Fan",
            "Tin Lun Lam",
            "Junjie Hu"
        ],
        "title": "ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration",
        "abstract": "arXiv:2509.24230v1 Announce Type: new  Abstract: Large Language Models (LLMs) enable intelligent multi-robot collaboration but face fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods incur prohibitive computational costs that scale poorly with team size and task complexity. In this paper, we propose ELHPlan, a novel framework that introduces Action Chains--sequences of actions explicitly bound to sub-goal intentions--as the fundamental planning primitive. ELHPlan operates via a cyclical process: 1) constructing intention-bound action sequences, 2) proactively validating for conflicts and feasibility, 3) refining issues through targeted mechanisms, and 4) executing validated actions. This design balances adaptability and efficiency by providing sufficient planning horizons while avoiding expensive full re-planning. We further propose comprehensive efficiency metrics, including token consumption and planning time, to more holistically evaluate multi-agent collaboration. Our experiments on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. Our research establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems.",
        "arxiv_id": "2509.24230",
        "ARXIVID": "2509.24230",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for multi-agent task planning, relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23993": {
        "authors": [
            "Muleilan Pei",
            "Shaoshuai Shi",
            "Shaojie Shen"
        ],
        "title": "Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning",
        "abstract": "arXiv:2509.23993v1 Announce Type: new  Abstract: Scalable and realistic simulation of multi-agent traffic behavior is critical for advancing autonomous driving technologies. Although existing data-driven simulators have made significant strides in this domain, they predominantly rely on supervised learning to align simulated distributions with real-world driving scenarios. A persistent challenge, however, lies in the distributional shift that arises between training and testing, which often undermines model generalization in unseen environments. To address this limitation, we propose SMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for next-token prediction models to better align agent behavior with human preferences and evaluation metrics. Our approach introduces a metric-oriented policy optimization algorithm to improve distribution alignment and an iterative \"SFT-RFT-SFT\" training strategy that alternates between Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance gains. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) validate the effectiveness of this simple yet powerful R1-style training framework in enhancing foundation models. The results on the Waymo Open Sim Agents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art performance with an overall realism meta score of 0.7858, ranking first on the leaderboard at the time of submission.",
        "arxiv_id": "2509.23993",
        "ARXIVID": "2509.23993",
        "COMMENT": "Matches criterion 3 as it introduces a novel reinforcement fine-tuning method for multi-agent traffic simulation, relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.25182": {
        "authors": [
            "Junyu Chen",
            "Wenkun He",
            "Yuchao Gu",
            "Yuyang Zhao",
            "Jincheng Yu",
            "Junsong Chen",
            "Dongyun Zou",
            "Yujun Lin",
            "Zhekai Zhang",
            "Muyang Li",
            "Haocheng Xi",
            "Ligeng Zhu",
            "Enze Xie",
            "Song Han",
            "Han Cai"
        ],
        "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder",
        "abstract": "arXiv:2509.25182v1 Announce Type: new  Abstract: We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.",
        "arxiv_id": "2509.25182",
        "ARXIVID": "2509.25182",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a framework for efficient video generation with compression techniques.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.24899": {
        "authors": [
            "Mohsen Ghafoorian",
            "Denis Korzhenkov",
            "Amirhossein Habibian"
        ],
        "title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer",
        "abstract": "arXiv:2509.24899v1 Announce Type: new  Abstract: Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive. While linear attention offers sub-quadratic complexity, prior attempts fail to match the expressiveness of softmax attention without costly retraining. We introduce \\textit{Attention Surgery}, an efficient framework for \\textit{linearizing} or \\textit{hybridizing} attention in pretrained VDMs without training from scratch. Inspired by recent advances in language models, our method combines a novel hybrid attention mechanism-mixing softmax and linear tokens-with a lightweight distillation and fine-tuning pipeline requiring only a few GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery achieves the first competitive sub-quadratic attention video diffusion models, reducing attention cost by up to 40\\% in terms of FLOPs, while maintaining generation quality as measured on the standard VBench and VBench-2.0 benchmarks.",
        "arxiv_id": "2509.24899",
        "ARXIVID": "2509.24899",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video diffusion models and their efficiency improvements.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23663": {
        "authors": [
            "Jingqi Xu",
            "Jingxi Lu",
            "Chenghao Li",
            "Sreetama Sarkar",
            "Peter A. Beerel"
        ],
        "title": "HIVTP: A Training-Free Method to Improve VLMs Efficiency via Hierarchical Visual Token Pruning Using Middle-Layer-Based Importance Score",
        "abstract": "arXiv:2509.23663v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have shown strong capabilities on diverse multimodal tasks. However, the large number of visual tokens output by the vision encoder severely hinders inference efficiency, and prior studies have shown that many of these tokens are not important and can therefore be safely pruned. In this work, we propose HIVTP, a training-free method to improve VLMs efficiency via hierarchical visual token pruning using a novel middle-layer-based importance score. Specifically, we utilize attention maps extracted from the middle layers of the vision encoder, which better reflect fine-grained and object-level attention, to estimate visual token importance. Based on this, we propose a hierarchical visual token pruning method to retain both globally and locally important visual tokens. Specifically, we reshape the 1-D visual token sequence output by the vision encoder into a 2-D spatial layout. In the global retaining stage, we divide the image into regions and retain tokens with higher importance scores in each region; in the local retaining stage, we then divide the image into small windows and retain the most important token in each local window. Experimental results show that our proposed method, HIVTP, can reduce the time-to-first-token (TTFT) of LLaVA-v1.5-7B and LLaVA-Next-7B by up to 50.0% and 55.1%, respectively, and improve the token generation throughput by up to 60.9% and 47.3%, without sacrificing accuracy, and even achieving improvements on certain benchmarks. Compared with prior works, HIVTP achieves better accuracy while offering higher inference efficiency.",
        "arxiv_id": "2509.23663",
        "ARXIVID": "2509.23663",
        "COMMENT": "Matches criterion 2 as it focuses on improving Vision-Language Models (VLMs) efficiency, which is directly related to multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23263": {
        "authors": [
            "Tao Xiong",
            "Xavier Hu",
            "Yurun Chen",
            "Yuhang Liu",
            "Changqiao Wu",
            "Pengzhi Gao",
            "Wei Liu",
            "Jian Luan",
            "Shengyu Zhang"
        ],
        "title": "GUI-PRA: Process Reward Agent for GUI Tasks",
        "abstract": "arXiv:2509.23263v1 Announce Type: new  Abstract: Graphical User Interface (GUI) Agents powered by Multimodal Large Language Models (MLLMs) show significant potential for automating tasks. However, they often struggle with long-horizon tasks, leading to frequent failures. Process Reward Models (PRMs) are a promising solution, as they can guide these agents with crucial process signals during inference. Nevertheless, their application to the GUI domain presents unique challenges. When processing dense artificial inputs with long history data, PRMs suffer from a \"lost in the middle\" phenomenon, where the overwhelming historical context compromises the evaluation of the current step. Furthermore, standard PRMs lacks GUI changing awareness, providing static evaluations that are disconnected from the dynamic consequences of actions, a critical mismatch with the inherently dynamic nature of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process Reward Agent for GUI Tasks), a judge agent designed to better provide process reward than standard PRM by intelligently processing historical context and actively perceiving UI state changes. Specifically, to directly combat the ``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism consisting of two core components: a Relevance-based Retrieval Module to actively fetch pertinent information from long histories and a Progressive Summarization Module to dynamically condense growing interaction data, ensuring the model focuses on relevant context. Moreover, to address the lack of UI changing awareness, we introduce an Aadaptive UI Perception mechanism. This mechanism enables the agent to reason about UI state changes and dynamically select the most appropriate tool to gather grounded visual evidence, ensuring its evaluation is always informed by the current UI context.",
        "arxiv_id": "2509.23263",
        "ARXIVID": "2509.23263",
        "COMMENT": "Matches criterion 2 as it discusses multimodal large language models (MLLMs) for GUI tasks, focusing on process reward models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23909": {
        "authors": [
            "Xin Luo",
            "Jiahao Wang",
            "Chenyuan Wu",
            "Shitao Xiao",
            "Xiyan Jiang",
            "Defu Lian",
            "Jiajun Zhang",
            "Dong Liu",
            "Zheng liu"
        ],
        "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling",
        "abstract": "arXiv:2509.23909v1 Announce Type: new  Abstract: Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges with complex instructions and often require multiple samples to produce a desired result. Reinforcement Learning (RL) offers a promising solution, but its adoption in image editing has been severely hindered by the lack of a high-fidelity, efficient reward signal. In this work, we present a comprehensive methodology to overcome this barrier, centered on the development of a state-of-the-art, specialized reward model. We first introduce EditReward-Bench, a comprehensive benchmark to systematically evaluate reward models on editing quality. Building on this benchmark, we develop EditScore, a series of reward models (7B-72B) for evaluating the quality of instruction-guided image editing. Through meticulous data curation and filtering, EditScore effectively matches the performance of learning proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy tailored for the generative nature of EditScore, our largest variant even surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity reward model is the key to unlocking online RL for image editing. Our experiments show that, while even the largest open-source VLMs fail to provide an effective learning signal, EditScore enables efficient and robust policy optimization. Applying our framework to a strong base model, OmniGen2, results in a final model that shows a substantial and consistent performance uplift. Overall, this work provides the first systematic path from benchmarking to reward modeling to RL training in image editing, showing that a high-fidelity, domain-specialized reward model is the key to unlocking the full potential of RL in this domain.",
        "arxiv_id": "2509.23909",
        "ARXIVID": "2509.23909",
        "COMMENT": "Matches criterion 5 as it focuses on integrating image editing tasks with reinforcement learning and reward modeling, which involves vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.23841": {
        "authors": [
            "Bingyang Cui",
            "Yujie Zhang",
            "Qi Yang",
            "Zhu Li",
            "Yiling Xu"
        ],
        "title": "Towards Fine-Grained Text-to-3D Quality Assessment: A Benchmark and A Two-Stage Rank-Learning Metric",
        "abstract": "arXiv:2509.23841v1 Announce Type: new  Abstract: Recent advances in Text-to-3D (T23D) generative models have enabled the synthesis of diverse, high-fidelity 3D assets from textual prompts. However, existing challenges restrict the development of reliable T23D quality assessment (T23DQA). First, existing benchmarks are outdated, fragmented, and coarse-grained, making fine-grained metric training infeasible. Moreover, current objective metrics exhibit inherent design limitations, resulting in non-representative feature extraction and diminished metric robustness. To address these limitations, we introduce T23D-CompBench, a comprehensive benchmark for compositional T23D generation. We define five components with twelve sub-components for compositional prompts, which are used to generate 3,600 textured meshes from ten state-of-the-art generative models. A large-scale subjective experiment is conducted to collect 129,600 reliable human ratings across different perspectives. Based on T23D-CompBench, we further propose Rank2Score, an effective evaluator with two-stage training for T23DQA. Rank2Score enhances pairwise training via supervised contrastive regression and curriculum learning in the first stage, and subsequently refines predictions using mean opinion scores to achieve closer alignment with human judgments in the second stage. Extensive experiments and downstream applications demonstrate that Rank2Score consistently outperforms existing metrics across multiple dimensions and can additionally serve as a reward function to optimize generative models. The project is available at https://cbysjtu.github.io/Rank2Score/.",
        "arxiv_id": "2509.23841",
        "ARXIVID": "2509.23841",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it introduces a benchmark and metric for fine-grained Text-to-3D quality assessment.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.23867": {
        "authors": [
            "Jiajin Tang",
            "Zhengxuan Wei",
            "Yuchen Zhu",
            "Cheng Shi",
            "Guanbin Li",
            "Liang Lin",
            "Sibei Yang"
        ],
        "title": "Sim-DETR: Unlock DETR for Temporal Sentence Grounding",
        "abstract": "arXiv:2509.23867v1 Announce Type: new  Abstract: Temporal sentence grounding aims to identify exact moments in a video that correspond to a given textual query, typically addressed with detection transformer (DETR) solutions. However, we find that typical strategies designed to enhance DETR do not improve, and may even degrade, its performance in this task. We systematically analyze and identify the root causes of this abnormal behavior: (1) conflicts between queries from similar target moments and (2) internal query conflicts due to the tension between global semantics and local localization. Building on these insights, we propose a simple yet powerful baseline, Sim-DETR, which extends the standard DETR with two minor modifications in the decoder layers: (1) constraining self-attention between queries based on their semantic and positional overlap and (2) adding query-to-frame alignment to bridge the global and local contexts. Experiments demonstrate that Sim-DETR unlocks the full potential of DETR for temporal sentence grounding, offering a strong baseline for future research.",
        "arxiv_id": "2509.23867",
        "ARXIVID": "2509.23867",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on temporal sentence grounding in videos using a novel DETR-based approach.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.23988": {
        "authors": [
            "Zirui Tang",
            "Weizheng Wang",
            "Zihang Zhou",
            "Yang Jiao",
            "Bangrui Xu",
            "Boyu Niu",
            "Xuanhe Zhou",
            "Guoliang Li",
            "Yeye He",
            "Wei Zhou",
            "Yitong Song",
            "Cheng Tan",
            "Bin Wang",
            "Conghui He",
            "Xiaoyang Wang",
            "Fan Wu"
        ],
        "title": "LLM/Agent-as-Data-Analyst: A Survey",
        "abstract": "arXiv:2509.23988v1 Announce Type: new  Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both academica and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. The technical evolution further distills five key design goals for intelligent data analysis agents, namely semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., table question answering for relational data and NL2GQL for graph data), (ii) semi-structured data (e.g., markup languages understanding and semi-structured table modeling), (iii) unstructured data (e.g., chart understanding, document understanding, programming languages vulnerable detection), and (iv) heterogeneous data (e.g., data retrieval and modality alignment for data lakes). Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.",
        "arxiv_id": "2509.23988",
        "ARXIVID": "2509.23988",
        "COMMENT": "Matches criterion 7 as it is a survey paper synthesizing the state of the art in LLMs for data analysis.",
        "RELEVANCE": 6,
        "NOVELTY": 4
    },
    "2509.25079": {
        "authors": [
            "Guanjun Wu",
            "Jiemin Fang",
            "Chen Yang",
            "Sikuang Li",
            "Taoran Yi",
            "Jia Lu",
            "Zanwei Zhou",
            "Jiazhong Cen",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Wei",
            "Wenyu Liu",
            "Xinggang Wang",
            "Qi Tian"
        ],
        "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation",
        "abstract": "arXiv:2509.25079v1 Announce Type: new  Abstract: High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry-texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry-appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation -- UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality. More demos \\& code are available at https://unilat3d.github.io/",
        "arxiv_id": "2509.25079",
        "ARXIVID": "2509.25079",
        "COMMENT": "Does not match any specific criteria but focuses on single-stage 3D generation, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.23258": {
        "authors": [
            "Atakan Topaloglu",
            "Kunyi Li",
            "Michael Niemeyer",
            "Nassir Navab",
            "A. Murat Tekalp",
            "Federico Tombari"
        ],
        "title": "OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting",
        "abstract": "arXiv:2509.23258v1 Announce Type: new  Abstract: Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity. Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies. We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting. Instead of using generative models to patch incomplete reconstructions, our \"propose-and-validate\" framework first leverages a pre-trained 3D-aware diffusion model to synthesize novel views to propose a complete scene. We then repurpose a multi-view stereo (MVS) model as a 3D-aware oracle to validate the 3D uncertainties of generated views, using its attention maps to reveal regions where the generated views are well-supported by multi-view evidence versus where they fall into regions of high uncertainty due to occlusion, lack of texture, or direct inconsistency. This uncertainty signal directly guides the optimization of a 3D Gaussian Splatting model via an uncertainty-weighted loss. Our approach conditions the powerful generative prior on multi-view geometric evidence, filtering hallucinatory artifacts while preserving plausible completions in under-constrained regions, outperforming state-of-the-art methods on datasets including Mip-NeRF 360 and NeRF Synthetic.",
        "arxiv_id": "2509.23258",
        "ARXIVID": "2509.23258",
        "COMMENT": "Does not match any specific criteria but focuses on novel view synthesis using generative priors, which is tangentially related to vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.22888": {
        "authors": [
            "Louie Hong Yao",
            "Nicholas Jarvis",
            "Tiffany Zhan",
            "Saptarshi Ghosh",
            "Linfeng Liu",
            "Tianyu Jiang"
        ],
        "title": "JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory",
        "abstract": "arXiv:2509.22888v1 Announce Type: new  Abstract: Standard LLM evaluation practices compress diverse abilities into single scores, obscuring their inherently multidimensional nature. We present JE-IRT, a geometric item-response framework that embeds both LLMs and questions in a shared space. For question embeddings, the direction encodes semantics and the norm encodes difficulty, while correctness on each question is determined by the geometric interaction between the model and question embeddings. This geometry replaces a global ranking of LLMs with topical specialization and enables smooth variation across related questions. Building on this framework, our experimental results reveal that out-of-distribution behavior can be explained through directional alignment, and that larger norms consistently indicate harder questions. Moreover, JE-IRT naturally supports generalization: once the space is learned, new LLMs are added by fitting a single embedding. The learned space further reveals an LLM-internal taxonomy that only partially aligns with human-defined subject categories. JE-IRT thus establishes a unified and interpretable geometric lens that connects LLM abilities with the structure of questions, offering a distinctive perspective on model evaluation and generalization.",
        "arxiv_id": "2509.22888",
        "ARXIVID": "2509.22888",
        "COMMENT": "Does not match any specific criterion but introduces a geometric framework for LLM evaluation, which is tangentially related to LLM behavior.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25123": {
        "authors": [
            "Lifan Yuan",
            "Weize Chen",
            "Yuchen Zhang",
            "Ganqu Cui",
            "Hanbin Wang",
            "Ziming You",
            "Ning Ding",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Hao Peng"
        ],
        "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones",
        "abstract": "arXiv:2509.25123v1 Announce Type: new  Abstract: Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",
        "arxiv_id": "2509.25123",
        "ARXIVID": "2509.25123",
        "COMMENT": "Does not match any specific criteria but explores reinforcement learning and compositional reasoning in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23304": {
        "authors": [
            "Liwen Hu",
            "Yang Li",
            "Mianzhi Liu",
            "Yijia Guo",
            "Shenghao Xie",
            "Ziluo Ding",
            "Tiejun Huang",
            "Lei Ma"
        ],
        "title": "Seeing the Unseen in Low-light Spike Streams",
        "abstract": "arXiv:2509.23304v1 Announce Type: new  Abstract: Spike camera, a type of neuromorphic sensor with high-temporal resolution, shows great promise for high-speed visual tasks. Unlike traditional cameras, spike camera continuously accumulates photons and fires asynchronous spike streams. Due to unique data modality, spike streams require reconstruction methods to become perceptible to the human eye.   However, lots of methods struggle to handle spike streams in low-light high-speed scenarios due to severe noise and sparse information. In this work, we propose Diff-SPK, the first diffusion-based reconstruction method for spike camera. Diff-SPK effectively leverages generative priors to supplement texture information in low-light conditions. Specifically, it first employs an \\textbf{E}nhanced \\textbf{T}exture \\textbf{f}rom Inter-spike \\textbf{I}nterval (ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI serves as a conditioning input for ControlNet to generate the high-speed scenes. To improve the quality of results, we introduce an ETFI-based feature fusion module during the generation process.   Moreover, we establish the first bona fide benchmark for the low-light spike stream reconstruction task. It significantly surpasses existing reconstruction datasets in scale and provides quantitative illumination information. The performance on real low-light spike streams demonstrates the superiority of Diff-SPK.",
        "arxiv_id": "2509.23304",
        "ARXIVID": "2509.23304",
        "COMMENT": "Does not match any specific criteria but involves generative modeling and novel sensor data.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25122": {
        "authors": [
            "Jan Held",
            "Renaud Vandeghen",
            "Sanghyun Son",
            "Daniel Rebain",
            "Matheus Gadelha",
            "Yi Zhou",
            "Ming C. Lin",
            "Marc Van Droogenbroeck",
            "Andrea Tagliasacchi"
        ],
        "title": "Triangle Splatting+: Differentiable Rendering with Opaque Triangles",
        "abstract": "arXiv:2509.25122v1 Announce Type: new  Abstract: Reconstructing 3D scenes and synthesizing novel views has seen rapid progress in recent years. Neural Radiance Fields demonstrated that continuous volumetric radiance fields can achieve high-quality image synthesis, but their long training and rendering times limit practicality. 3D Gaussian Splatting (3DGS) addressed these issues by representing scenes with millions of Gaussians, enabling real-time rendering and fast optimization. However, Gaussian primitives are not natively compatible with the mesh-based pipelines used in VR headsets, and real-time graphics applications. Existing solutions attempt to convert Gaussians into meshes through post-processing or two-stage pipelines, which increases complexity and degrades visual quality. In this work, we introduce Triangle Splatting+, which directly optimizes triangles, the fundamental primitive of computer graphics, within a differentiable splatting framework. We formulate triangle parametrization to enable connectivity through shared vertices, and we design a training strategy that enforces opaque triangles. The final output is immediately usable in standard graphics engines without post-processing. Experiments on the Mip-NeRF360 and Tanks & Temples datasets show that Triangle Splatting+achieves state-of-the-art performance in mesh-based novel view synthesis. Our method surpasses prior splatting approaches in visual fidelity while remaining efficient and fast to training. Moreover, the resulting semi-connected meshes support downstream applications such as physics-based simulation or interactive walkthroughs. The project page is https://trianglesplatting2.github.io/trianglesplatting2/.",
        "arxiv_id": "2509.25122",
        "ARXIVID": "2509.25122",
        "COMMENT": "Does not match any specific criteria but is related to 3D scene reconstruction and novel view synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.24968": {
        "authors": [
            "Donghwa Kang",
            "Junho Kim",
            "Dongwoo Kang"
        ],
        "title": "Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning",
        "abstract": "arXiv:2509.24968v1 Announce Type: new  Abstract: Event cameras offer unique advantages for facial keypoint alignment under challenging conditions, such as low light and rapid motion, due to their high temporal resolution and robustness to varying illumination. However, existing RGB facial keypoint alignment methods do not perform well on event data, and training solely on event data often leads to suboptimal performance because of its limited spatial information. Moreover, the lack of comprehensive labeled event datasets further hinders progress in this area. To address these issues, we propose a novel framework based on cross-modal fusion attention (CMFA) and self-supervised multi-event representation learning (SSMER) for event-based facial keypoint alignment. Our framework employs CMFA to integrate corresponding RGB data, guiding the model to extract robust facial features from event input images. In parallel, SSMER enables effective feature learning from unlabeled event data, overcoming spatial limitations. Extensive experiments on our real-event E-SIE dataset and a synthetic-event version of the public WFLW-V benchmark show that our approach consistently surpasses state-of-the-art methods across multiple evaluation metrics.",
        "arxiv_id": "2509.24968",
        "ARXIVID": "2509.24968",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23838": {
        "authors": [
            "Zhixiong Zhang",
            "Shuangrui Ding",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Jiaqi Wang"
        ],
        "title": "2nd Place Report of MOSEv2 Challenge 2025: Concept Guided Video Object Segmentation via SeC",
        "abstract": "arXiv:2509.23838v1 Announce Type: new  Abstract: Semi-supervised Video Object Segmentation aims to segment a specified target throughout a video sequence, initialized by a first-frame mask. Previous methods rely heavily on appearance-based pattern matching and thus exhibit limited robustness against challenges such as drastic visual changes, occlusions, and scene shifts. This failure is often attributed to a lack of high-level conceptual understanding of the target. The recently proposed Segment Concept (SeC) framework mitigated this limitation by using a Large Vision-Language Model (LVLM) to establish a deep semantic understanding of the object for more persistent segmentation. In this work, we evaluate its zero-shot performance on the challenging coMplex video Object SEgmentation v2 (MOSEv2) dataset. Without any fine-tuning on the training set, SeC achieved 39.7 \\JFn on the test set and ranked 2nd place in the Complex VOS track of the 7th Large-scale Video Object Segmentation Challenge.",
        "arxiv_id": "2509.23838",
        "ARXIVID": "2509.23838",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video object segmentation using a novel conceptual understanding approach.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2509.23189": {
        "authors": [
            "Zhenxing Xu",
            "Yizhe Zhang",
            "Weidong Bao",
            "Hao Wang",
            "Ming Chen",
            "Haoran Ye",
            "Wenzheng Jiang",
            "Hui Yan",
            "Ji Wang"
        ],
        "title": "AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms",
        "abstract": "arXiv:2509.23189v1 Announce Type: new  Abstract: Dynamically configuring algorithm hyperparameters is a fundamental challenge in computational intelligence. While learning-based methods offer automation, they suffer from prohibitive sample complexity and poor generalization. We introduce AutoEP, a novel framework that bypasses training entirely by leveraging Large Language Models (LLMs) as zero-shot reasoning engines for algorithm control. AutoEP's core innovation lies in a tight synergy between two components: (1) an online Exploratory Landscape Analysis (ELA) module that provides real-time, quantitative feedback on the search dynamics, and (2) a multi-LLM reasoning chain that interprets this feedback to generate adaptive hyperparameter strategies. This approach grounds high-level reasoning in empirical data, mitigating hallucination. Evaluated on three distinct metaheuristics across diverse combinatorial optimization benchmarks, AutoEP consistently outperforms state-of-the-art tuners, including neural evolution and other LLM-based methods. Notably, our framework enables open-source models like Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and accessible new paradigm for automated hyperparameter design. Our code is available at https://anonymous.4open.science/r/AutoEP-3E11",
        "arxiv_id": "2509.23189",
        "ARXIVID": "2509.23189",
        "COMMENT": "Does not match any specific criteria but explores LLMs for hyperparameter evolution, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23962": {
        "authors": [
            "Guanxu Chen",
            "Yafu Li",
            "Yuxian Jiang",
            "Chen Qian",
            "Qihan Ren",
            "Jingyi Yang",
            "Yu Cheng",
            "Dongrui Liu",
            "Jing Shao"
        ],
        "title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models",
        "abstract": "arXiv:2509.23962v1 Announce Type: new  Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyperparameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce Conditional advANtage estimatiON (CANON), amplifying the impact of the target metric without presuming its direction. Specifically, CANON regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, CANON based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, CANON further improves token efficiency, yielding a more favorable Pareto frontier in the performance-cost trade-off.",
        "arxiv_id": "2509.23962",
        "ARXIVID": "2509.23962",
        "COMMENT": "Does not match any specific criteria but discusses reinforcement learning for reasoning in LLMs, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.24509": {
        "authors": [
            "Yihong Liu",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Hongyu Lu",
            "Ji-Rong Wen"
        ],
        "title": "Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design",
        "abstract": "arXiv:2509.24509v1 Announce Type: new  Abstract: Combinatorial optimization problems are traditionally tackled with handcrafted heuristic algorithms, which demand extensive domain expertise and significant implementation effort. Recent progress has highlighted the potential of automatic heuristics design powered by large language models (LLMs), enabling the automatic generation and refinement of heuristics. These approaches typically maintain a population of heuristics and employ LLMs as mutation operators to evolve them across generations. While effective, such methods often risk stagnating in local optima. To address this issue, we propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics (EvoPH) for automatic algorithm design, a novel framework that integrates the island migration model with the elites selection algorithm to simulate diverse heuristics populations. In EvoPH, prompts are co-evolved with heuristic algorithms, guided by performance feedback. We evaluate our framework on two problems, i.e., Traveling Salesman Problem and Bin Packing Problem. Experimental results demonstrate that EvoPH achieves the lowest relative error against optimal solutions across both datasets, advancing the field of automatic algorithm design with LLMs.",
        "arxiv_id": "2509.24509",
        "ARXIVID": "2509.24509",
        "COMMENT": "Does not match any specific criteria but explores LLMs for automatic algorithm design, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25137": {
        "authors": [
            "Chuanyang Jin",
            "Jing Xu",
            "Bo Liu",
            "Leitian Tao",
            "Olga Golovneva",
            "Tianmin Shu",
            "Wenting Zhao",
            "Xian Li",
            "Jason Weston"
        ],
        "title": "The Era of Real-World Human Interaction: RL from User Conversations",
        "abstract": "arXiv:2509.25137v1 Announce Type: new  Abstract: We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.",
        "arxiv_id": "2509.25137",
        "ARXIVID": "2509.25137",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of reinforcement learning and human interaction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22819": {
        "authors": [
            "Sumanth Varambally",
            "Thomas Voice",
            "Yanchao Sun",
            "Zhifeng Chen",
            "Rose Yu",
            "Ke Ye"
        ],
        "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning",
        "abstract": "arXiv:2509.22819v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate impressive mathematical reasoning abilities, but their solutions frequently contain errors that cannot be automatically verified. Formal theorem proving systems such as Lean 4 offer automated verification with complete accuracy, motivating recent efforts to build specialized prover LLMs that generate verifiable proofs in formal languages. However, a significant gap remains: current prover LLMs solve substantially fewer problems than general-purpose LLMs operating in natural language. We introduce Hilbert, an agentic framework that bridges this gap by combining the complementary strengths of informal reasoning and formal verification. Our system orchestrates four components: an informal LLM that excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4 tactics, a formal verifier, and a semantic theorem retriever. Given a problem that the prover is unable to solve, Hilbert employs recursive decomposition to split the problem into subgoals that it solves with the prover or reasoner LLM. It leverages verifier feedback to refine incorrect proofs as necessary. Experimental results demonstrate that Hilbert substantially outperforms existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points above the best publicly available method. Hilbert achieves the best known result on PutnamBench. It solves 462/660 problems (70.0%), outperforming proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement over the best publicly available baseline. Thus, Hilbert effectively narrows the gap between informal reasoning and formal proof generation.",
        "arxiv_id": "2509.22819",
        "ARXIVID": "2509.22819",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of mathematical reasoning and formal proof generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23912": {
        "authors": [
            "Ouns El Harzli",
            "Bernardo Cuenca Grau",
            "Artur d'Avila Garcez",
            "Ian Horrocks",
            "Tarek R. Besold"
        ],
        "title": "From Neural Networks to Logical Theories: The Correspondence between Fibring Modal Logics and Fibring Neural Networks",
        "abstract": "arXiv:2509.23912v1 Announce Type: new  Abstract: Fibring of modal logics is a well-established formalism for combining countable families of modal logics into a single fibred language with common semantics, characterized by fibred models. Inspired by this formalism, fibring of neural networks was introduced as a neurosymbolic framework for combining learning and reasoning in neural networks. Fibring of neural networks uses the (pre-)activations of a trained network to evaluate a fibring function computing the weights of another network whose outputs are injected back into the original network. However, the exact correspondence between fibring of neural networks and fibring of modal logics was never formally established. In this paper, we close this gap by formalizing the idea of fibred models \\emph{compatible} with fibred neural networks. Using this correspondence, we then derive non-uniform logical expressiveness results for Graph Neural Networks (GNNs), Graph Attention Networks (GATs) and Transformer encoders. Longer-term, the goal of this paper is to open the way for the use of fibring as a formalism for interpreting the logical theories learnt by neural networks with the tools of computational logic.",
        "arxiv_id": "2509.23912",
        "ARXIVID": "2509.23912",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to neurosymbolic AI and logical reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.24711": {
        "authors": [
            "Qingjie Zhang",
            "Yujia Fu",
            "Yang Wang",
            "Liu Yan",
            "Tao Wei",
            "Ke Xu",
            "Minlie Huang",
            "Han Qiu"
        ],
        "title": "On the Self-awareness of Large Reasoning Models' Capability Boundaries",
        "abstract": "arXiv:2509.24711v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.",
        "arxiv_id": "2509.24711",
        "ARXIVID": "2509.24711",
        "COMMENT": "Does not directly match any specific criteria but is relevant to reasoning and self-awareness in large reasoning models, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23619": {
        "authors": [
            "Xiangyu Wen",
            "Junhua Huang",
            "Zeju Li",
            "Min Li",
            "Jianyuan Zhong",
            "Zhijian Xu",
            "Mingxuan Yuan",
            "Yongxiang Huang",
            "Qiang Xu"
        ],
        "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
        "abstract": "arXiv:2509.23619v1 Announce Type: new  Abstract: The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.",
        "arxiv_id": "2509.23619",
        "ARXIVID": "2509.23619",
        "COMMENT": "Does not directly match any specific criteria but is relevant to reasoning and distillation in language models, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22691": {
        "authors": [
            "Yan Wen",
            "Peng Ye",
            "Lin Zhang",
            "Baopu Li",
            "Jiakang Yuan",
            "Yaoxin Yang",
            "Tao Chen"
        ],
        "title": "Sequential Token Merging: Revisiting Hidden States",
        "abstract": "arXiv:2509.22691v1 Announce Type: new  Abstract: Vision Mambas (ViMs) achieve remarkable success with sub-quadratic complexity, but their efficiency remains constrained by quadratic token scaling with image resolution. While existing methods address token redundancy, they overlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a critical information flow mechanism revealed in our analysis. We further identify Mamba's selective scan enables gradual information aggregation in hidden states. Based on these insights, we propose Sequential Token Merging (STM), featuring: 1) Bidirectional nearest neighbor merging to preserve sequential dependencies through symmetric spatial aggregation, and 2) Hidden states protection to stabilize the hidden states around the class token. STM strategically leverages Mamba's layer-wise loss convergence to convert temporal forgetfulness into stability. Experiments demonstrate STM's superiority: 1.0% accuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for ViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with minimal complexity, while providing new insights into state-space model dynamics. Codes will be released soon.",
        "arxiv_id": "2509.22691",
        "ARXIVID": "2509.22691",
        "COMMENT": "Does not match any specific criteria but discusses token merging in vision models, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.24850": {
        "authors": [
            "Bo Zhao",
            "Dan Guo",
            "Junzhe Cao",
            "Yong Xu",
            "Tao Tan",
            "Yue Sun",
            "Bochao Zou",
            "Jie Zhang",
            "Zitong Yu"
        ],
        "title": "PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient Remote Photoplethysmography Measurement",
        "abstract": "arXiv:2509.24850v1 Announce Type: new  Abstract: Remote photoplethysmography (rPPG) measurement enables non-contact physiological monitoring but suffers from accuracy degradation under head motion and illumination changes. Existing deep learning methods are mostly heuristic and lack theoretical grounding, which limits robustness and interpretability. In this work, we propose a physics-informed rPPG paradigm derived from the Navier-Stokes equations of hemodynamics, showing that the pulse signal follows a second-order dynamical system whose discrete solution naturally leads to a causal convolution. This provides a theoretical justification for using a Temporal Convolutional Network (TCN). Based on this principle, we design PHASE-Net, a lightweight model with three key components: (1) Zero-FLOPs Axial Swapper module, which swaps or transposes a few spatial channels to mix distant facial regions and enhance cross-region feature interaction without breaking temporal order; (2) Adaptive Spatial Filter, which learns a soft spatial mask per frame to highlight signal-rich areas and suppress noise; and (3) Gated TCN, a causal dilated TCN with gating that models long-range temporal dynamics for accurate pulse recovery. Extensive experiments demonstrate that PHASE-Net achieves state-of-the-art performance with strong efficiency, offering a theoretically grounded and deployment-ready rPPG solution.",
        "arxiv_id": "2509.24850",
        "ARXIVID": "2509.24850",
        "COMMENT": "Does not match any specific criteria but introduces a physics-grounded model for physiological monitoring, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23537": {
        "authors": [
            "Aaron Xuxiang Tian",
            "Ruofan Zhang",
            "Jiayao Tang",
            "Young Min Cho",
            "Xueqian Li",
            "Qiang Yi",
            "Ji Wang",
            "Zhunping Zhang",
            "Danrui Qi",
            "Sharath Chandra Guntuku",
            "Lyle Ungar",
            "Tianyu Shi",
            "Chi Wang"
        ],
        "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks",
        "abstract": "arXiv:2509.23537v1 Announce Type: new  Abstract: We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.",
        "arxiv_id": "2509.23537",
        "ARXIVID": "2509.23537",
        "COMMENT": "Does not match any specific criteria but explores multi-agent orchestration with LLMs, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23122": {
        "authors": [
            "Chenrui Ma",
            "Xi Xiao",
            "Tianyang Wang",
            "Xiao Wang",
            "Yanning Shen"
        ],
        "title": "Stochastic Interpolants via Conditional Dependent Coupling",
        "abstract": "arXiv:2509.23122v1 Announce Type: new  Abstract: Existing image generation models face critical challenges regarding the trade-off between computation and fidelity. Specifically, models relying on a pretrained Variational Autoencoder (VAE) suffer from information loss, limited detail, and the inability to support end-to-end training. In contrast, models operating directly in the pixel space incur prohibitive computational cost. Although cascade models can mitigate computational cost, stage-wise separation prevents effective end-to-end optimization, hampers knowledge sharing, and often results in inaccurate distribution learning within each stage. To address these challenges, we introduce a unified multistage generative framework based on our proposed Conditional Dependent Coupling strategy. It decomposes the generative process into interpolant trajectories at multiple stages, ensuring accurate distribution learning while enabling end-to-end optimization. Importantly, the entire process is modeled as a single unified Diffusion Transformer, eliminating the need for disjoint modules and also enabling knowledge sharing. Extensive experiments demonstrate that our method achieves both high fidelity and efficiency across multiple resolutions.",
        "arxiv_id": "2509.23122",
        "ARXIVID": "2509.23122",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and efficient image generation frameworks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.24526": {
        "authors": [
            "Zheyuan Hu",
            "Chieh-Hsin Lai",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "title": "CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models",
        "abstract": "arXiv:2509.24526v1 Announce Type: new  Abstract: Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable few-step generation by learning the long jump of the ODE solution of diffusion models, yet training remains unstable, sensitive to hyperparameters, and costly. Initializing from a pre-trained diffusion model helps, but still requires converting infinitesimal steps into a long-jump map, leaving instability unresolved. We introduce mid-training, the first concept and practical method that inserts a lightweight intermediate stage between the (diffusion) pre-training and the final flow map training (i.e., post-training) for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact and principled stage that trains a model to map points along a solver trajectory from a pre-trained model, starting from a prior sample, directly to the solver-generated clean sample. It yields a trajectory-consistent and stable initialization. This initializer outperforms random and diffusion-based baselines and enables fast, robust convergence without heuristics. Initializing post-training with CMT weights further simplifies flow map learning. Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10, 1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98% less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT reaches 1-step FID 3.34 while cutting total training time by about 50% compared to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient, and general framework for training flow map models.",
        "arxiv_id": "2509.24526",
        "ARXIVID": "2509.24526",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and efficient training strategies.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.23352": {
        "authors": [
            "Xiaolong Fu",
            "Lichen Ma",
            "Zipeng Guo",
            "Gaojing Zhou",
            "Chongxiao Wang",
            "ShiPing Dong",
            "Shizhe Zhou",
            "Shizhe Zhou",
            "Ximan Liu",
            "Jingling Fu",
            "Tan Lit Sin",
            "Yu Shi",
            "Zhen Chen",
            "Junshi Huang",
            "Jason Li"
        ],
        "title": "Dynamic-TreeRPO: Breaking the Independent Trajectory Bottleneck with Structured Sampling",
        "abstract": "arXiv:2509.23352v1 Announce Type: new  Abstract: The integration of Reinforcement Learning (RL) into flow matching models for text-to-image (T2I) generation has driven substantial advances in generation quality. However, these gains often come at the cost of exhaustive exploration and inefficient sampling strategies due to slight variation in the sampling group. Building on this insight, we propose Dynamic-TreeRPO, which implements the sliding-window sampling strategy as a tree-structured search with dynamic noise intensities along depth. We perform GRPO-guided optimization and constrained Stochastic Differential Equation (SDE) sampling within this tree structure. By sharing prefix paths of the tree, our design effectively amortizes the computational overhead of trajectory search. With well-designed noise intensities for each tree layer, Dynamic-TreeRPO can enhance the variation of exploration without any extra computational cost. Furthermore, we seamlessly integrate Supervised Fine-Tuning (SFT) and RL paradigm within Dynamic-TreeRPO to construct our proposed LayerTuning-RL, reformulating the loss function of SFT as a dynamically weighted Progress Reward Model (PRM) rather than a separate pretraining method. By associating this weighted PRM with dynamic-adaptive clipping bounds, the disruption of exploration process in Dynamic-TreeRPO is avoided. Benefiting from the tree-structured sampling and the LayerTuning-RL paradigm, our model dynamically explores a diverse search space along effective directions. Compared to existing baselines, our approach demonstrates significant superiority in terms of semantic consistency, visual fidelity, and human preference alignment on established benchmarks, including HPS-v2.1, PickScore, and ImageReward. In particular, our model outperforms SoTA by $4.9\\%$, $5.91\\%$, and $8.66\\%$ on those benchmarks, respectively, while improving the training efficiency by nearly $50\\%$.",
        "arxiv_id": "2509.23352",
        "ARXIVID": "2509.23352",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in generative modeling and reinforcement learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23273": {
        "authors": [
            "Yihao Ding",
            "Soyeon Caren Han",
            "Yanbei Jiang",
            "Yan Li",
            "Zechuan Li",
            "Yifan Peng"
        ],
        "title": "SynDoc: A Hybrid Discriminative-Generative Framework for Enhancing Synthetic Domain-Adaptive Document Key Information Extraction",
        "abstract": "arXiv:2509.23273v1 Announce Type: new  Abstract: Domain-specific Visually Rich Document Understanding (VRDU) presents significant challenges due to the complexity and sensitivity of documents in fields such as medicine, finance, and material science. Existing Large (Multimodal) Language Models (LLMs/MLLMs) achieve promising results but face limitations such as hallucinations, inadequate domain adaptation, and reliance on extensive fine-tuning datasets. This paper introduces SynDoc, a novel framework that combines discriminative and generative models to address these challenges. SynDoc employs a robust synthetic data generation workflow, using structural information extraction and domain-specific query generation to produce high-quality annotations. Through adaptive instruction tuning, SynDoc improves the discriminative model's ability to extract domain-specific knowledge. At the same time, a recursive inferencing mechanism iteratively refines the output of both models for stable and accurate predictions. This framework demonstrates scalable, efficient, and precise document understanding and bridges the gap between domain-specific adaptation and general world knowledge for document key information extraction tasks.",
        "arxiv_id": "2509.23273",
        "ARXIVID": "2509.23273",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in multimodal learning and document understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23608": {
        "authors": [
            "Liubing Hu",
            "Chen Wu",
            "Anrui Wang",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Zhuoran Zheng"
        ],
        "title": "FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching",
        "abstract": "arXiv:2509.23608v1 Announce Type: new  Abstract: Deep learning-based image enhancement methods face a fundamental trade-off between computational efficiency and representational capacity. For example, although a conventional three-dimensional Look-Up Table (3D LUT) can process a degraded image in real time, it lacks representational flexibility and depends solely on a fixed prior. To address this problem, we introduce FlowLUT, a novel end-to-end model that integrates the efficiency of LUTs, multiple priors, and the parameter-independent characteristic of flow-matched reconstructed images. Specifically, firstly, the input image is transformed in color space by a collection of differentiable 3D LUTs (containing a large number of 3D LUTs with different priors). Subsequently, a lightweight content-aware dynamically predicts fusion weights, enabling scene-adaptive color correction with $\\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs on multiple 3D LUTs, with $\\mathcal{O}(1)$ complexity for scene-adaptive color correction.Furthermore, to address the inherent representation limitations of LUTs, we design an innovative iterative flow matching method to restore local structural details and eliminate artifacts. Finally, the entire model is jointly optimized under a composite loss function enforcing perceptual and structural fidelity. Extensive experimental results demonstrate the effectiveness of our method on three benchmarks.",
        "arxiv_id": "2509.23608",
        "ARXIVID": "2509.23608",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23058": {
        "authors": [
            "Yikai Wang (Department of Statistics and Operations Research",
            "University of North Carolina)",
            "Xiaocheng Li (Imperial College Business School",
            "Imperial College London)",
            "Guanting Chen (Department of Statistics and Operations Research",
            "University of North Carolina)"
        ],
        "title": "Risk Profiling and Modulation for LLMs",
        "abstract": "arXiv:2509.23058v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used for decision-making tasks under uncertainty; however, their risk profiles and how they are influenced by prompting and alignment methods remain underexplored. Existing studies have primarily examined personality prompting or multi-agent interactions, leaving open the question of how post-training influences the risk behavior of LLMs. In this work, we propose a new pipeline for eliciting, steering, and modulating LLMs' risk profiles, drawing on tools from behavioral economics and finance. Using utility-theoretic models, we compare pre-trained, instruction-tuned, and RLHF-aligned LLMs, and find that while instruction-tuned models exhibit behaviors consistent with some standard utility formulations, pre-trained and RLHF-aligned models deviate more from any utility models fitted. We further evaluate modulation strategies, including prompt engineering, in-context learning, and post-training, and show that post-training provides the most stable and effective modulation of risk preference. Our findings provide insights into the risk profiles of different classes and stages of LLMs and demonstrate how post-training modulates these profiles, laying the groundwork for future research on behavioral alignment and risk-aware LLM design.",
        "arxiv_id": "2509.23058",
        "ARXIVID": "2509.23058",
        "COMMENT": "Does not match any specific criterion but explores risk profiling in LLMs, which is tangentially related to your friend's interest in LLM behavior.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23605": {
        "authors": [
            "Zeren Xiong",
            "Yue Yu",
            "Zedong Zhang",
            "Shuo Chen",
            "Jian Yang",
            "Jun Li"
        ],
        "title": "VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis",
        "abstract": "arXiv:2509.23605v1 Announce Type: new  Abstract: Creating novel images by fusing visual cues from multiple sources is a fundamental yet underexplored problem in image-to-image generation, with broad applications in artistic creation, virtual reality and visual media. Existing methods often face two key challenges: coexistent generation, where multiple objects are simply juxtaposed without true integration, and bias generation, where one object dominates the output due to semantic imbalance. To address these issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet effective diffusion-based framework that synthesizes a single, coherent object by integrating two input images at both noise and latent levels. Our approach comprises: (1) a hybrid sampling process that combines guided denoising, inversion, and spherical interpolation with adjustable parameters to achieve structure-aware fusion, mitigating coexistent generation; and (2) an efficient adaptive adjustment module, which introduces a novel similarity-based score to automatically and adaptively search for optimal parameters, countering semantic bias. Experiments on a curated benchmark of 780 concept pairs demonstrate that our method outperforms strong baselines in visual quality, semantic consistency, and human-rated creativity.",
        "arxiv_id": "2509.23605",
        "ARXIVID": "2509.23605",
        "COMMENT": "Does not match any specific criteria but involves generative modeling and image synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23310": {
        "authors": [
            "Hao Liu",
            "Yongjie Zheng",
            "Yuhan Kang",
            "Mingyang Zhang",
            "Maoguo Gong",
            "Lorenzo Bruzzone"
        ],
        "title": "Balanced Diffusion-Guided Fusion for Multimodal Remote Sensing Classification",
        "abstract": "arXiv:2509.23310v1 Announce Type: new  Abstract: Deep learning-based techniques for the analysis of multimodal remote sensing data have become popular due to their ability to effectively integrate complementary spatial, spectral, and structural information from different sensors. Recently, denoising diffusion probabilistic models (DDPMs) have attracted attention in the remote sensing community due to their powerful ability to capture robust and complex spatial-spectral distributions. However, pre-training multimodal DDPMs may result in modality imbalance, and effectively leveraging diffusion features to guide complementary diversity feature extraction remains an open question. To address these issues, this paper proposes a balanced diffusion-guided fusion (BDGF) framework that leverages multimodal diffusion features to guide a multi-branch network for land-cover classification. Specifically, we propose an adaptive modality masking strategy to encourage the DDPMs to obtain a modality-balanced rather than spectral image-dominated data distribution. Subsequently, these diffusion features hierarchically guide feature extraction among CNN, Mamba, and transformer networks by integrating feature fusion, group channel attention, and cross-attention mechanisms. Finally, a mutual learning strategy is developed to enhance inter-branch collaboration by aligning the probability entropy and feature similarity of individual subnetworks. Extensive experiments on four multimodal remote sensing datasets demonstrate that the proposed method achieves superior classification performance. The code is available at https://github.com/HaoLiu-XDU/BDGF.",
        "arxiv_id": "2509.23310",
        "ARXIVID": "2509.23310",
        "COMMENT": "Does not match any specific criteria but involves multimodal data and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25027": {
        "authors": [
            "Xiaoxiao Ma",
            "Haibo Qiu",
            "Guohui Zhang",
            "Zhixiong Zeng",
            "Siqi Yang",
            "Lin Ma",
            "Feng Zhao"
        ],
        "title": "STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation",
        "abstract": "arXiv:2509.25027v1 Announce Type: new  Abstract: Reinforcement learning has recently been explored to improve text-to-image generation, yet applying existing GRPO algorithms to autoregressive (AR) image models remains challenging. The instability of the training process easily disrupts the pretrained model capability during long runs, resulting in marginal gains, degraded image quality, and poor generalization. In this work, we revisit GRPO for AR image generation and identify two key issues: contradictory gradients from unnecessary tokens and unstable policy entropy dynamics. To address these, we introduce STAGE, a stable and generalizable framework that leverages two targeted solutions: 1) Advantage/KL reweighting. Similarity-aware reweighting to alleviate conflicting updates; and 2) Entropy reward. An entropy-based reward corresponding to reference model to stabilize learning. With the help of alleviating conflicts between tokens and an entropy reward for stabilizing training, we reduce disruption of the pretrained distribution and mitigate reward hacking, which in turn improves generalization and transfer better to other benchmarks. Experiments across multiple benchmarks show that STAGE consistently improves visual quality, stability, and cross-task generalization compared to baseline GRPO.",
        "arxiv_id": "2509.25027",
        "ARXIVID": "2509.25027",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and reinforcement learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24836": {
        "authors": [
            "Zhen Bi",
            "Zhenlin Hu",
            "Jinnan Yang",
            "Mingyang Chen",
            "Cheng Deng",
            "Yida Xue",
            "Zeyu Yang",
            "Qing Shen",
            "Zhenfang Liu",
            "Kang Zhao",
            "Ningyu Zhang",
            "Jungang Lou"
        ],
        "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity",
        "abstract": "arXiv:2509.24836v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training data.Rather than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.",
        "arxiv_id": "2509.24836",
        "ARXIVID": "2509.24836",
        "COMMENT": "Does not match any specific criteria but is related to reasoning in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24248": {
        "authors": [
            "Rubing Yang",
            "Huajun Bai",
            "Song Liu",
            "Guanghua Yu",
            "Runzhi Fan",
            "Yanbin Dang",
            "Jiejing Zhang",
            "Kai Liu",
            "Jianchen Zhu",
            "Peng Chen"
        ],
        "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit",
        "abstract": "arXiv:2509.24248v1 Announce Type: new  Abstract: Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.",
        "arxiv_id": "2509.24248",
        "ARXIVID": "2509.24248",
        "COMMENT": "Does not match any specific criterion but is related to efficient reasoning in large models, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23733": {
        "authors": [
            "Hangtian Zhao",
            "Xiang Chen",
            "Yizhe Li",
            "Qianhao Wang",
            "Haibo Lu",
            "Fei Gao"
        ],
        "title": "FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention",
        "abstract": "arXiv:2509.23733v1 Announce Type: new  Abstract: In this paper we propose FastViDAR, a novel framework that takes four fisheye camera inputs and produces a full $360^\\circ$ depth map along with per-camera depth, fusion depth, and confidence estimates. Our main contributions are: (1) We introduce Alternative Hierarchical Attention (AHA) mechanism that efficiently fuses features across views through separate intra-frame and inter-frame windowed self-attention, achieving cross-view feature mixing with reduced overhead. (2) We propose a novel ERP fusion approach that projects multi-view depth estimates to a shared equirectangular coordinate system to obtain the final fusion depth. (3) We generate ERP image-depth pairs using HM3D and 2D3D-S datasets for comprehensive evaluation, demonstrating competitive zero-shot performance on real datasets while achieving up to 20 FPS on NVIDIA Orin NX embedded hardware. Project page: \\href{https://3f7dfc.github.io/FastVidar/}{https://3f7dfc.github.io/FastVidar/}",
        "arxiv_id": "2509.23733",
        "ARXIVID": "2509.23733",
        "COMMENT": "Does not match any specific criteria but focuses on omnidirectional depth estimation, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24194": {
        "authors": [
            "Zach Eidex",
            "Mojtaba Safari",
            "Jie Ding",
            "Richard Qiu",
            "Justin Roper",
            "David Yu",
            "Hui-Kuo Shu",
            "Zhen Tian",
            "Hui Mao",
            "Xiaofeng Yang"
        ],
        "title": "An Efficient 3D Latent Diffusion Model for T1-contrast Enhanced MRI Generation",
        "abstract": "arXiv:2509.24194v1 Announce Type: new  Abstract: Objective: Gadolinium-based contrast agents (GBCAs) are commonly employed with T1w MRI to enhance lesion visualization but are restricted in patients at risk of nephrogenic systemic fibrosis and variations in GBCA administration can introduce imaging inconsistencies. This study develops an efficient 3D deep-learning framework to generate T1-contrast enhanced images (T1C) from pre-contrast multiparametric MRI. Approach: We propose the 3D latent rectified flow (T1C-RFlow) model for generating high-quality T1C images. First, T1w and T2-FLAIR images are input into a pretrained autoencoder to acquire an efficient latent space representation. A rectified flow diffusion model is then trained in this latent space representation. The T1C-RFlow model was trained on a curated dataset comprised of the BraTS 2024 glioma (GLI; 1480 patients), meningioma (MEN; 1141 patients), and metastases (MET; 1475 patients) datasets. Selected patients were split into train (N=2860), validation (N=612), and test (N=614) sets. Results: Both qualitative and quantitative results demonstrate that the T1C-RFlow model outperforms benchmark 3D models (pix2pix, DDPM, Diffusion Transformers (DiT-3D)) trained in the same latent space. T1C-RFlow achieved the following metrics - GLI: NMSE 0.044 +/- 0.047, SSIM 0.935 +/- 0.025; MEN: NMSE 0.046 +/- 0.029, SSIM 0.937 +/- 0.021; MET: NMSE 0.098 +/- 0.088, SSIM 0.905 +/- 0.082. T1C-RFlow had the best tumor reconstruction performance and significantly faster denoising times (6.9 s/volume, 200 steps) than conventional DDPM models in both latent space (37.7s, 1000 steps) and patch-based in image space (4.3 hr/volume). Significance: Our proposed method generates synthetic T1C images that closely resemble ground truth T1C in much less time than previous diffusion models. Further development may permit a practical method for contrast-agent-free MRI for brain tumors.",
        "arxiv_id": "2509.24194",
        "ARXIVID": "2509.24194",
        "COMMENT": "Does not match any specific criteria but focuses on a specific application of diffusion models for MRI generation, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23488": {
        "authors": [
            "Siyang Wu",
            "Honglin Bao",
            "Sida Li",
            "Ari Holtzman",
            "James A. Evans"
        ],
        "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild",
        "abstract": "arXiv:2509.23488v1 Announce Type: new  Abstract: We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functional overlaps across logic, math, language, instruction following, and world modeling, with coding emerging as the least overlapping domain. Together, these findings provide mechanistic insights into benchmark validity and LLM sensitivities, and sketch the underlying landscape of interconnected LLM capabilities.",
        "arxiv_id": "2509.23488",
        "ARXIVID": "2509.23488",
        "COMMENT": "Does not match any specific criteria but discusses benchmark overlaps and LLM sensitivities, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22925": {
        "authors": [
            "Yuanzhi Zhu",
            "Xi Wang",
            "St\\'ephane Lathuili\\`ere",
            "Vicky Kalogeiton"
        ],
        "title": "Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings",
        "abstract": "arXiv:2509.22925v1 Announce Type: new  Abstract: One-step generators distilled from Masked Diffusion Models (MDMs) compress multiple sampling steps into a single forward pass, enabling efficient text and image synthesis. However, they suffer two key limitations: they inherit modeling bias from the teacher, and their discrete token outputs block gradient flow, preventing post-distillation refinements such as adversarial training, reward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this work, we introduce soft embeddings, a simple relaxation that replaces discrete tokens with the expected embeddings under the generator's output distribution. Soft embeddings preserve representation fidelity for one-step discrete generator while providing a fully differentiable continuous surrogate that is compatible with teacher backbones and tokenizer decoders. Integrating soft embeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes one-step generators end-to-end trainable and enables straightforward application of GAN-based refinement, differentiable reward fine-tuning, and TTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen), Soft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image performance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement, along with higher GenEval and HPS scores on text-to-image with reward fine-tuning, and further gains from TTEO.",
        "arxiv_id": "2509.22925",
        "ARXIVID": "2509.22925",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of generative modeling and efficient synthesis methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25082": {
        "authors": [
            "Xiaoyi Huang",
            "Junwei Wu",
            "Kejia Zhang",
            "Carl Yang",
            "Zhiming Luo"
        ],
        "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification",
        "abstract": "arXiv:2509.25082v1 Announce Type: new  Abstract: Adversarial purification with diffusion models has emerged as a promising defense strategy, but existing methods typically rely on uniform noise injection, which indiscriminately perturbs all frequencies, corrupting semantic structures and undermining robustness. Our empirical study reveals that adversarial perturbations are not uniformly distributed: they are predominantly concentrated in high-frequency regions, with heterogeneous magnitude intensity patterns that vary across frequencies and attack types. Motivated by this observation, we introduce MANI-Pure, a magnitude-adaptive purification framework that leverages the magnitude spectrum of inputs to guide the purification process. Instead of injecting homogeneous noise, MANI-Pure adaptively applies heterogeneous, frequency-targeted noise, effectively suppressing adversarial perturbations in fragile high-frequency, low-magnitude bands while preserving semantically critical low-frequency content. Extensive experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of MANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original classifier, while boosting robust accuracy by 2.15, and achieves the top-1 robust accuracy on the RobustBench leaderboard, surpassing the previous state-of-the-art method.",
        "arxiv_id": "2509.25082",
        "ARXIVID": "2509.25082",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of machine learning and adversarial robustness.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22689": {
        "authors": [
            "Ha-Hieu Pham",
            "Minh Le",
            "Han Huynh",
            "Nguyen Quoc Khanh Le",
            "Huy-Hieu Pham"
        ],
        "title": "Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation",
        "abstract": "arXiv:2509.22689v1 Announce Type: new  Abstract: Semi-supervised semantic segmentation (SSSS) is vital in computational pathology, where dense annotations are costly and limited. Existing methods often rely on pixel-level consistency, which propagates noisy pseudo-labels and produces fragmented or topologically invalid masks. We propose Topology Graph Consistency (TGC), a framework that integrates graph-theoretic constraints by aligning Laplacian spectra, component counts, and adjacency statistics between prediction graphs and references. This enforces global topology and improves segmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC achieves state-of-the-art performance under 5-10% supervision and significantly narrows the gap to full supervision. Code is available at https://github.com/hieuphamha19/TGC.",
        "arxiv_id": "2509.22689",
        "ARXIVID": "2509.22689",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of segmentation and graph-theoretic methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23719": {
        "authors": [
            "Shuai Shao",
            "Shu Jiang",
            "Shiyuan Zhao",
            "Di Yang",
            "Yan Wang",
            "Yutong Bai",
            "Jianguo Zhang",
            "Jiangtao Wang"
        ],
        "title": "PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary Diagnosis of Parkinson's Disease",
        "abstract": "arXiv:2509.23719v1 Announce Type: new  Abstract: Parkinson's disease (PD) is a common neurodegenerative disorder that severely diminishes patients' quality of life. Its global prevalence has increased markedly in recent decades. Current diagnostic workflows are complex and heavily reliant on neurologists' expertise, often resulting in delays in early detection and missed opportunities for timely intervention. To address these issues, we propose an end-to-end automated diagnostic method for PD, termed PD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly from raw MRI scans. This framework first introduces an MRI Pre-processing Module (MRI-Processor) to mitigate inter-subject and inter-scanner variability by flexibly integrating established medical imaging preprocessing tools. It then incorporates two forms of clinical prior knowledge: (1) Brain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions strongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior), which reflects the accelerated aging typically observed in PD-associated regions. Building on these priors, we design two dedicated modules: the Relevance-Prior Guided Feature Aggregation Module (Aggregator), which guides the model to focus on PD-associated regions at the inter-subject level, and the Age-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps as auxiliary constraints at the intra-subject level to enhance diagnostic accuracy and clinical interpretability. Furthermore, we collected external test data from our collaborating hospital. Experimental results show that PD-Diag-Net achieves 86\\% accuracy on external tests and over 96% accuracy in early-stage diagnosis, outperforming existing advanced methods by more than 20%.",
        "arxiv_id": "2509.23719",
        "ARXIVID": "2509.23719",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of medical imaging and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24266": {
        "authors": [
            "Wenjie Wei",
            "Malu Zhang",
            "Jieyuan Zhang",
            "Ammar Belatreche",
            "Shuai Wang",
            "Yimeng Shan",
            "Hanwen Liu",
            "Honglin Cao",
            "Guoqing Wang",
            "Yang Yang",
            "Haizhou Li"
        ],
        "title": "S$^2$NN: Sub-bit Spiking Neural Networks",
        "abstract": "arXiv:2509.24266v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) offer an energy-efficient paradigm for machine intelligence, but their continued scaling poses challenges for resource-limited deployment. Despite recent advances in binary SNNs, the storage and computational demands remain substantial for large-scale networks. To further explore the compression and acceleration potential of SNNs, we propose Sub-bit Spiking Neural Networks (S$^2$NNs) that represent weights with less than one bit. Specifically, we first establish an S$^2$NN baseline by leveraging the clustering patterns of kernels in well-trained binary SNNs. This baseline is highly efficient but suffers from \\textit{outlier-induced codeword selection bias} during training. To mitigate this issue, we propose an \\textit{outlier-aware sub-bit weight quantization} (OS-Quant) method, which optimizes codeword selection by identifying and adaptively scaling outliers. Furthermore, we propose a \\textit{membrane potential-based feature distillation} (MPFD) method, improving the performance of highly compressed S$^2$NN via more precise guidance from a teacher model. Extensive results on vision and non-vision tasks reveal that S$^2$NN outperforms existing quantized SNNs in both performance and efficiency, making it promising for edge computing applications.",
        "arxiv_id": "2509.24266",
        "ARXIVID": "2509.24266",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of efficient neural networks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23601": {
        "authors": [
            "Han Hu",
            "Zhuoran Zheng",
            "Liang Li",
            "Chen Lyu"
        ],
        "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
        "abstract": "arXiv:2509.23601v1 Announce Type: new  Abstract: Recent Mamba-based image restoration methods have achieved promising results but remain   limited by fixed scanning patterns and inefficient feature utilization. Conventional Mamba   architectures rely on predetermined paths that cannot adapt to diverse degradations, constraining   both restoration performance and computational efficiency. To overcome these limitations, we   propose VAMamba, a Visual Adaptive Mamba framework with two key innovations. First,   QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha   FIFO cache that stores historical representations. Similarity between current LoRA-adapted and   cached features guides intelligent fusion, enabling dynamic reuse while effectively controlling   memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning. A   Vision Transformer generates score maps to estimate pixel importance, and a greedy strategy de termines optimal forward and backward scanning paths. These learned trajectories replace rigid   patterns, enabling SS2D to perform targeted feature extraction. The integration of QCLAM and   GPS-SS2D allows VAMamba to adaptively focus on degraded regions while maintaining high   computational efficiency. Extensive experiments across diverse restoration tasks demonstrate   that VAMamba consistently outperforms existing approaches in both restoration quality and   efficiency, establishing new benchmarks for adaptive image restoration. Our code is available   at https://github.com/WaterHQH/VAMamba.",
        "arxiv_id": "2509.23601",
        "ARXIVID": "2509.23601",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to image restoration and adaptive methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23082": {
        "authors": [
            "Yutao Shen",
            "Junkun Yuan",
            "Toru Aonishi",
            "Hideki Nakayama",
            "Yue Ma"
        ],
        "title": "Follow-Your-Preference: Towards Preference-Aligned Image Inpainting",
        "abstract": "arXiv:2509.23082v1 Announce Type: new  Abstract: This paper investigates image inpainting with preference alignment. Instead of introducing a novel method, we go back to basics and revisit fundamental problems in achieving such alignment. We leverage the prominent direct preference optimization approach for alignment training and employ public reward models to construct preference training datasets. Experiments are conducted across nine reward models, two benchmarks, and two baseline models with varying structures and generative algorithms. Our key findings are as follows: (1) Most reward models deliver valid reward scores for constructing preference data, even if some of them are not reliable evaluators. (2) Preference data demonstrates robust trends in both candidate scaling and sample scaling across models and benchmarks. (3) Observable biases in reward models, particularly in brightness, composition, and color scheme, render them susceptible to cause reward hacking. (4) A simple ensemble of these models yields robust and generalizable results by mitigating such biases. Built upon these observations, our alignment models significantly outperform prior models across standard metrics, GPT-4 assessments, and human evaluations, without any changes to model structures or the use of new datasets. We hope our work can set a simple yet solid baseline, pushing this promising frontier. Our code is open-sourced at: https://github.com/shenytzzz/Follow-Your-Preference.",
        "arxiv_id": "2509.23082",
        "ARXIVID": "2509.23082",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and image inpainting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22700": {
        "authors": [
            "Zhuang Qi",
            "Pan Yu",
            "Lei Meng",
            "Sijin Zhou",
            "Han Yu",
            "Xiaoxiao Li",
            "Xiangxu Meng"
        ],
        "title": "Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning",
        "abstract": "arXiv:2509.22700v1 Announce Type: new  Abstract: Federated Prompt Learning (FPL) enables communication-efficient adaptation by tuning lightweight prompts on top of frozen pre-trained models. Existing FPL methods typically rely on global information, which is only available after the second training round, to facilitate collaboration among client models. Therefore, they are inherently dependent on multi-round communication to fully exhibit their strengths. Moreover, existing one-shot federated learning methods typically focus on fitting seen tasks, but lack cross-task generalization. To bridge this gap, we propose the Global Prompt Refinement with Non-Interfering Attention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to design a masking mechanism that restricts excessive interaction between the original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves this through the collaboration of two key modules. Firstly, the attention isolation module suppresses attention from the learnable prompt tokens to the original text tokens, and reweights the reverse attention which preserves generalization across tasks. Secondly, the cross-silo collaborative refinement module integrates decentralized visual knowledge into a unified base and calibrates the global prompt through multi-source cross-modal knowledge alignment, further mitigating the inconsistency caused by data heterogeneity. Extensive experiments conducted on ten benchmark datasets under two tasks show that GPR-NIAM outperforms eight state-of-the-art methods in both class-level and domain-level generalization.",
        "arxiv_id": "2509.22700",
        "ARXIVID": "2509.22700",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to machine learning and federated learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23907": {
        "authors": [
            "You Zhou",
            "Lijiang Chen",
            "Shuchang Lyu",
            "Guangxia Cui",
            "Wenpei Bai",
            "Zheng Zhou",
            "Meng Li",
            "Guangliang Cheng",
            "Huiyu Zhou",
            "Qi Zhao"
        ],
        "title": "Adversarial Versus Federated: An Adversarial Learning based Multi-Modality Cross-Domain Federated Medical Segmentation",
        "abstract": "arXiv:2509.23907v1 Announce Type: new  Abstract: Federated learning enables collaborative training of machine learning models among different clients while ensuring data privacy, emerging as the mainstream for breaking data silos in the healthcare domain. However, the imbalance of medical resources, data corruption or improper data preservation may lead to a situation where different clients possess medical images of different modality. This heterogeneity poses a significant challenge for cross-domain medical image segmentation within the federated learning framework. To address this challenge, we propose a new Federated Domain Adaptation (FedDA) segmentation training framework. Specifically, we propose a feature-level adversarial learning among clients by aligning feature maps across clients through embedding an adversarial training mechanism. This design can enhance the model's generalization on multiple domains and alleviate the negative impact from domain-shift. Comprehensive experiments on three medical image datasets demonstrate that our proposed FedDA substantially achieves cross-domain federated aggregation, endowing single modality client with cross-modality processing capabilities, and consistently delivers robust performance compared to state-of-the-art federated aggregation algorithms in objective and subjective assessment. Our code are available at https://github.com/GGbond-study/FedDA.",
        "arxiv_id": "2509.23907",
        "ARXIVID": "2509.23907",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multimodal learning and federated medical segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24393": {
        "authors": [
            "Yichi Zhang",
            "Yue Ding",
            "Jingwen Yang",
            "Tianwei Luo",
            "Dongbai Li",
            "Ranjie Duan",
            "Qiang Liu",
            "Hang Su",
            "Yinpeng Dong",
            "Jun Zhu"
        ],
        "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention",
        "abstract": "arXiv:2509.24393v1 Announce Type: new  Abstract: Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.",
        "arxiv_id": "2509.24393",
        "ARXIVID": "2509.24393",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to reasoning and safety in large models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22930": {
        "authors": [
            "Chenghan Yang",
            "Peng Zhou",
            "Dong-Sheng Zhang",
            "Yueyun Wang",
            "Hong-Bin Shen",
            "Xiaoyong Pan"
        ],
        "title": "FishAI 2.0: Marine Fish Image Classification with Multi-modal Few-shot Learning",
        "abstract": "arXiv:2509.22930v1 Announce Type: new  Abstract: Traditional marine biological image recognition faces challenges of incomplete datasets and unsatisfactory model accuracy, particularly for few-shot conditions of rare species where data scarcity significantly hampers the performance. To address these issues, this study proposes an intelligent marine fish recognition framework, FishAI 2.0, integrating multimodal few-shot deep learning techniques with image generation for data augmentation. First, a hierarchical marine fish benchmark dataset, which provides a comprehensive data foundation for subsequent model training, is utilized to train the FishAI 2.0 model. To address the data scarcity of rare classes, the large language model DeepSeek was employed to generate high-quality textual descriptions, which are input into Stable Diffusion 2 for image augmentation through a hierarchical diffusion strategy that extracts latent encoding to construct a multimodal feature space. The enhanced visual-textual datasets were then fed into a Contrastive Language-Image Pre-Training (CLIP) based model, enabling robust few-shot image recognition. Experimental results demonstrate that FishAI 2.0 achieves a Top-1 accuracy of 91.67 percent and Top-5 accuracy of 97.97 percent at the family level, outperforming baseline CLIP and ViT models with a substantial margin for the minority classes with fewer than 10 training samples. To better apply FishAI 2.0 to real-world scenarios, at the genus and species level, FishAI 2.0 respectively achieves a Top-1 accuracy of 87.58 percent and 85.42 percent, demonstrating practical utility. In summary, FishAI 2.0 improves the efficiency and accuracy of marine fish identification and provides a scalable technical solution for marine ecological monitoring and conservation, highlighting its scientific value and practical applicability.",
        "arxiv_id": "2509.22930",
        "ARXIVID": "2509.22930",
        "COMMENT": "Does not directly match any specific criteria but is relevant to multimodal learning and few-shot learning, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23915": {
        "authors": [
            "Yihang Guo",
            "Tianyuan Yu",
            "Liang Bai",
            "Yanming Guo",
            "Yirun Ruan",
            "William Li",
            "Weishi Zheng"
        ],
        "title": "Revisit the Imbalance Optimization in Multi-task Learning: An Experimental Analysis",
        "abstract": "arXiv:2509.23915v1 Announce Type: new  Abstract: Multi-task learning (MTL) aims to build general-purpose vision systems by training a single network to perform multiple tasks jointly. While promising, its potential is often hindered by \"unbalanced optimization\", where task interference leads to subpar performance compared to single-task models. To facilitate research in MTL, this paper presents a systematic experimental analysis to dissect the factors contributing to this persistent problem. Our investigation confirms that the performance of existing optimization methods varies inconsistently across datasets, and advanced architectures still rely on costly grid-searched loss weights. Furthermore, we show that while powerful Vision Foundation Models (VFMs) provide strong initialization, they do not inherently resolve the optimization imbalance, and merely increasing data quantity offers limited benefits. A crucial finding emerges from our analysis: a strong correlation exists between the optimization imbalance and the norm of task-specific gradients. We demonstrate that this insight is directly applicable, showing that a straightforward strategy of scaling task losses according to their gradient norms can achieve performance comparable to that of an extensive and computationally expensive grid search. Our comprehensive analysis suggests that understanding and controlling gradient dynamics is a more direct path to stable MTL than developing increasingly complex methods.",
        "arxiv_id": "2509.23915",
        "ARXIVID": "2509.23915",
        "COMMENT": "Does not match any specific criteria but discusses multi-task learning optimization, which is tangentially related to general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23640": {
        "authors": [
            "Chengying She",
            "Ben Wang",
            "Xinran Zhang",
            "Dongjie Fan",
            "Jialu Zhang",
            "Chengwei Chen",
            "Lizhuang Liu"
        ],
        "title": "EfficientMIL: Efficient Linear-Complexity MIL Method for WSI Classification",
        "abstract": "arXiv:2509.23640v1 Announce Type: new  Abstract: Whole slide images (WSIs) classification represents a fundamental challenge in computational pathology, where multiple instance learning (MIL) has emerged as the dominant paradigm. Current state-of-the-art (SOTA) MIL methods rely on attention mechanisms, achieving good performance but requiring substantial computational resources due to quadratic complexity when processing hundreds of thousands of patches. To address this computational bottleneck, we introduce EfficientMIL, a novel linear-complexity MIL approach for WSIs classification with the patches selection module Adaptive Patch Selector (APS) that we designed, replacing the quadratic-complexity self-attention mechanisms in Transformer-based MIL methods with efficient sequence models including RNN-based GRU, LSTM, and State Space Model (SSM) Mamba. EfficientMIL achieves significant computational efficiency improvements while outperforming other MIL methods across multiple histopathology datasets. On TCGA-Lung dataset, EfficientMIL-Mamba achieved AUC of 0.976 and accuracy of 0.933, while on CAMELYON16 dataset, EfficientMIL-GRU achieved AUC of 0.990 and accuracy of 0.975, surpassing previous state-of-the-art methods. Extensive experiments demonstrate that APS is also more effective for patches selection than conventional selection strategies.",
        "arxiv_id": "2509.23640",
        "ARXIVID": "2509.23640",
        "COMMENT": "Does not match any specific criteria but is related to efficient computational methods, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24878": {
        "authors": [
            "Jiuhong Xiao",
            "Roshan Nayak",
            "Ning Zhang",
            "Daniel Tortei",
            "Giuseppe Loianno"
        ],
        "title": "ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation",
        "abstract": "arXiv:2509.24878v1 Announce Type: new  Abstract: Paired RGB-thermal data is crucial for visual-thermal sensor fusion and cross-modality tasks, including important applications such as multi-modal image alignment and retrieval. However, the scarcity of synchronized and calibrated RGB-thermal image pairs presents a major obstacle to progress in these areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image translation has emerged as a promising solution, enabling the synthesis of thermal images from abundant RGB datasets for training purposes. In this study, we propose ThermalGen, an adaptive flow-based generative model for RGB-T image translation, incorporating an RGB image conditioning architecture and a style-disentangled mechanism. To support large-scale training, we curated eight public satellite-aerial, aerial, and ground RGB-T paired datasets, and introduced three new large-scale satellite-aerial RGB-T datasets--DJI-day, Bosonplus-day, and Bosonplus-night--captured across diverse times, sensor types, and geographic regions. Extensive evaluations across multiple RGB-T benchmarks demonstrate that ThermalGen achieves comparable or superior translation performance compared to existing GAN-based and diffusion-based methods. To our knowledge, ThermalGen is the first RGB-T image translation model capable of synthesizing thermal images that reflect significant variations in viewpoints, sensor characteristics, and environmental conditions. Project page: http://xjh19971.github.io/ThermalGen",
        "arxiv_id": "2509.24878",
        "ARXIVID": "2509.24878",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23150": {
        "authors": [
            "Wenxuan Fang",
            "Jiangwei Weng",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "title": "WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning",
        "abstract": "arXiv:2509.23150v1 Announce Type: new  Abstract: Unsupervised image restoration under multi-weather conditions remains a fundamental yet underexplored challenge. While existing methods often rely on task-specific physical priors, their narrow focus limits scalability and generalization to diverse real-world weather scenarios. In this work, we propose \\textbf{WeatherCycle}, a unified unpaired framework that reformulates weather restoration as a bidirectional degradation-content translation cycle, guided by degradation-aware curriculum regularization. At its core, WeatherCycle employs a \\textit{lumina-chroma decomposition} strategy to decouple degradation from content without modeling complex weather, enabling domain conversion between degraded and clean images. To model diverse and complex degradations, we propose a \\textit{Lumina Degradation Guidance Module} (LDGM), which learns luminance degradation priors from a degraded image pool and injects them into clean images via frequency-domain amplitude modulation, enabling controllable and realistic degradation modeling. Additionally, we incorporate a \\textit{Difficulty-Aware Contrastive Regularization (DACR)} module that identifies hard samples via a CLIP-based classifier and enforces contrastive alignment between hard samples and restored features to enhance semantic consistency and robustness. Extensive experiments across serve multi-weather datasets, demonstrate that our method achieves state-of-the-art performance among unsupervised approaches, with strong generalization to complex weather degradations.",
        "arxiv_id": "2509.23150",
        "ARXIVID": "2509.23150",
        "COMMENT": "Does not match any specific criterion but discusses unsupervised image restoration under multi-weather conditions, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22813": {
        "authors": [
            "Sahar Dastani",
            "Ali Bahri",
            "Gustavo Adolfo Vargas Hakim",
            "Moslem Yazdanpanah",
            "Mehrdad Noori",
            "David Osowiechi",
            "Samuel Barbeau",
            "Ismail Ben Ayed",
            "Herve Lombaert",
            "Christian Desrosiers"
        ],
        "title": "TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses",
        "abstract": "arXiv:2509.22813v1 Announce Type: new  Abstract: State Space Models (SSMs) have emerged as efficient alternatives to Vision Transformers (ViTs), with VMamba standing out as a pioneering architecture designed for vision tasks. However, their generalization performance degrades significantly under distribution shifts. To address this limitation, we propose TRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel test-time adaptation (TTA) method that leverages diverse traversal permutations to generate multiple causal perspectives of the input image. Model predictions serve as pseudo-labels to guide updates of the Mamba-specific parameters, and the adapted weights are averaged to integrate the learned information across traversal scans. Altogether, TRUST is the first approach that explicitly leverages the unique architectural properties of SSMs for adaptation. Experiments on seven benchmarks show that TRUST consistently improves robustness and outperforms existing TTA methods.",
        "arxiv_id": "2509.22813",
        "ARXIVID": "2509.22813",
        "COMMENT": "Does not match any specific criterion but discusses test-time adaptation for vision tasks, which is tangentially related to embodied AI and vision models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23097": {
        "authors": [
            "Ziyu Su",
            "Abdul Rehman Akbar",
            "Usama Sajjad",
            "Anil V. Parwani",
            "Muhammad Khalid Khan Niazi"
        ],
        "title": "Streamline pathology foundation model by cross-magnification distillation",
        "abstract": "arXiv:2509.23097v1 Announce Type: new  Abstract: Foundation models (FM) have transformed computational pathology but remain computationally prohibitive for clinical deployment due to their massive parameter counts and high-magnification processing requirements. Here, we introduce XMAG, a lightweight FM developed through corss-magnification distillation that transfers knowledge from state-of-the-art 20x magnification teacher to an efficient 5x magnification student architecture. XMAG employs a compact backbone and operates entirely at 5x, requiring 11.3 times fewer patches per whole slide image (WSI) compared to existing approaches. Our Novel distillation framework incorporates dual-level knowledge transfer, aligning both global image representations and local spatial token mapping. We trained XMAG on 3.49 million images curated from publicly available datasets and evaluated performance across six clinically relevant histopathology analysis tasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within 1% of substantially larger foundation models while delivering 30-fold processing acceleration, reaching 8.8 WSIs per minute processing speed. Our cross-institutional validation confirmed robust generalization. Further, we developed an end-to-end training strategy to further boost our model's performance to approach the larger FMs' performance. These results establish cross-magnification distillation as a viable approach for deploying FM capabilities in resource-constrained clinical environments, potentially enabling real-time pathology AI integration.",
        "arxiv_id": "2509.23097",
        "ARXIVID": "2509.23097",
        "COMMENT": "Does not match any specific criterion but discusses foundation models in pathology, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23392": {
        "authors": [
            "Jinyi Han",
            "Ying Huang",
            "Ying Liao",
            "Zishang Jiang",
            "Xikun Lu",
            "Haiquan Zhao",
            "Xinyi Wang",
            "Guanghao Zhou",
            "Sihang Jiang",
            "Jiaqing Liang",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu",
            "Yanghua Xiao"
        ],
        "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking",
        "abstract": "arXiv:2509.23392v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.",
        "arxiv_id": "2509.23392",
        "ARXIVID": "2509.23392",
        "COMMENT": "Does not match any specific criterion but discusses reasoning efficiency in large reasoning models, which is tangentially related to embodied AI and spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24913": {
        "authors": [
            "Tian Xia",
            "Matthew Sinclair",
            "Andreas Schuh",
            "Fabio De Sousa Ribeiro",
            "Raghav Mehta",
            "Rajat Rasal",
            "Esther Puyol-Ant\\'on",
            "Samuel Gerber",
            "Kersten Petersen",
            "Michiel Schaap",
            "Ben Glocker"
        ],
        "title": "Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis",
        "abstract": "arXiv:2509.24913v1 Announce Type: new  Abstract: Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: https://github.com/biomedia-mira/seg-cft.",
        "arxiv_id": "2509.24913",
        "ARXIVID": "2509.24913",
        "COMMENT": "Does not match any specific criterion but is relevant to counterfactual image generation and data augmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23639": {
        "authors": [
            "Boyu Han",
            "Qianqian Xu",
            "Shilong Bao",
            "Zhiyong Yang",
            "Kangli Zi",
            "Qingming Huang"
        ],
        "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders",
        "abstract": "arXiv:2509.23639v1 Announce Type: new  Abstract: This paper explores a novel lightweight approach LightFair to achieve fair text-to-image diffusion models (T2I DMs) by addressing the adverse effects of the text encoder. Most existing methods either couple different parts of the diffusion model for full-parameter training or rely on auxiliary networks for correction. They incur heavy training or sampling burden and unsatisfactory performance. Since T2I DMs consist of multiple components, with the text encoder being the most fine-tunable and front-end module, this paper focuses on mitigating bias by fine-tuning text embeddings. To validate feasibility, we observe that the text encoder's neutral embedding output shows substantial skewness across image embeddings of various attributes in the CLIP space. More importantly, the noise prediction network further amplifies this imbalance. To finetune the text embedding, we propose a collaborative distance-constrained debiasing strategy that balances embedding distances to improve fairness without auxiliary references. However, mitigating bias can compromise the original generation quality. To address this, we introduce a two-stage text-guided sampling strategy to limit when the debiased text encoder intervenes. Extensive experiments demonstrate that LightFair is effective and efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA debiasing at just $1/4$ of the training burden, with virtually no increase in sampling burden. The code is available at https://github.com/boyuh/LightFair.",
        "arxiv_id": "2509.23639",
        "ARXIVID": "2509.23639",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and fairness in text-to-image diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24416": {
        "authors": [
            "Kai Liu",
            "Shaoqiu Zhang",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "title": "CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers",
        "abstract": "arXiv:2509.24416v1 Announce Type: new  Abstract: Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.",
        "arxiv_id": "2509.24416",
        "ARXIVID": "2509.24416",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of model compression and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24545": {
        "authors": [
            "Yuhao Wang",
            "Zhuoran Zheng",
            "Han Hu",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Chen Lyu"
        ],
        "title": "Foggy Crowd Counting: Combining Physical Priors and KAN-Graph",
        "abstract": "arXiv:2509.24545v1 Announce Type: new  Abstract: Aiming at the key challenges of crowd counting in foggy environments, such as long-range target blurring, local feature degradation, and image contrast attenuation, this paper proposes a crowd-counting method with a physical a priori of atmospheric scattering, which improves crowd counting accuracy under complex meteorological conditions through the synergistic optimization of the physical mechanism and data-driven.Specifically, first, the method introduces a differentiable atmospheric scattering model and employs transmittance dynamic estimation and scattering parameter adaptive calibration techniques to accurately quantify the nonlinear attenuation laws of haze on targets with different depths of field.Secondly, the MSA-KAN was designed based on the Kolmogorov-Arnold Representation Theorem to construct a learnable edge activation function. By integrating a multi-layer progressive architecture with adaptive skip connections, it significantly enhances the model's nonlinear representation capability in feature-degraded regions, effectively suppressing feature confusion under fog interference.Finally, we further propose a weather-aware GCN that dynamically constructs spatial adjacency matrices using deep features extracted by MSA-KAN. Experiments on four public datasets demonstrate that our method achieves a 12.2\\%-27.5\\% reduction in MAE metrics compared to mainstream algorithms in dense fog scenarios.",
        "arxiv_id": "2509.24545",
        "ARXIVID": "2509.24545",
        "COMMENT": "Does not match any specific criteria but is related to general interest in computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23535": {
        "authors": [
            "Ibne Farabi Shihab",
            "Weiheng Chai",
            "Jiyang Wang",
            "Sanjeda Akter",
            "Senem Velipasalar Gursoy",
            "Anuj Sharma"
        ],
        "title": "Calibrated and Resource-Aware Super-Resolution for Reliable Driver Behavior Analysis",
        "abstract": "arXiv:2509.23535v1 Announce Type: new  Abstract: Driver monitoring systems require not just high accuracy but reliable, well-calibrated confidence scores for safety-critical deployment. While direct low-resolution training yields high overall accuracy, it produces poorly calibrated predictions that can be dangerous in safety-critical scenarios. We propose a resource-aware adaptive super-resolution framework that optimizes for model calibration and high precision-recall on critical events. Our approach achieves state-of-the-art performance on safety-centric metrics: best calibration (ECE of 5.8\\% vs 6.2\\% for LR-trained baselines), highest AUPR for drowsiness detection (0.78 vs 0.74), and superior precision-recall for phone use detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters, 5.2ms overhead) provides additional safety by filtering SR-induced hallucinations. While LR-trained video models serve as strong general-purpose baselines, our adaptive framework represents the state-of-the-art solution for safety-critical applications where reliability is paramount.",
        "arxiv_id": "2509.23535",
        "ARXIVID": "2509.23535",
        "COMMENT": "Does not match any specific criteria but is related to general interest in machine learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23629": {
        "authors": [
            "Sihan Hu",
            "Xiansheng Cai",
            "Yuan Huang",
            "Zhiyuan Yao",
            "Linfeng Zhang",
            "Pan Zhang",
            "Youjin Deng",
            "Kun Chen"
        ],
        "title": "How LLMs Learn to Reason: A Complex Network Perspective",
        "abstract": "arXiv:2509.23629v1 Announce Type: new  Abstract: Training large language models with Reinforcement Learning from Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, V-shaped response-length trajectories, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these seemingly disparate phenomena can be explained using a single unifying theory: the model's reasoning process maps to the self-organization of a semantic complex network whose topology remains persistently sparse, with the average degree pinned close to two. This topology imposes a fundamental mechanism for forgetting and learning: it first drives the system into a maximally frustrated state where ``skill islands'' form, slow-learning happens, and forgetting is induced; then it enters a sharp growth phase where the new skills are ``bolted on'', driven by phase-transition-like learning at the web's frontier. Equipped with the theory, we propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an SFT-based ``heating'' step at the point of maximal frustration to resolve the competitive bottleneck and enhance the reasoning capability of the model. Experiments on a 1.5B-parameter model demonstrate that the approach outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks. By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.",
        "arxiv_id": "2509.23629",
        "ARXIVID": "2509.23629",
        "COMMENT": "Does not match any specific criteria but discusses reasoning in large models, which is tangentially related to embodied agents and reasoning tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23045": {
        "authors": [
            "Zonghan Yang",
            "Shengjie Wang",
            "Kelin Fu",
            "Wenyang He",
            "Weimin Xiong",
            "Yibo Liu",
            "Yibo Miao",
            "Bofei Gao",
            "Yejie Wang",
            "Yingwei Ma",
            "Yanhao Li",
            "Yue Liu",
            "Zhenxing Hu",
            "Kaitai Zhang",
            "Shuyi Wang",
            "Huarong Chen",
            "Flood Sung",
            "Yang Liu",
            "Yang Gao",
            "Zhilin Yang",
            "Tianyu Liu"
        ],
        "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents",
        "abstract": "arXiv:2509.23045v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents.",
        "arxiv_id": "2509.23045",
        "ARXIVID": "2509.23045",
        "COMMENT": "Does not match any specific criteria but discusses skill priors and training strategies for software engineering agents, which is tangentially relevant to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23676": {
        "authors": [
            "Jue Zhang",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
        "abstract": "arXiv:2509.23676v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{https://aka.ms/R2A-code}{this URL}.",
        "arxiv_id": "2509.23676",
        "ARXIVID": "2509.23676",
        "COMMENT": "Does not match any specific criteria but discusses reasoning in large models, which is tangentially related to embodied agents and reasoning tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23056": {
        "authors": [
            "Ben Liang",
            "Yuan Liu",
            "Bingwen Qiu",
            "Yihong Wang",
            "Xiubao Sui",
            "Qian Chen"
        ],
        "title": "FMC-DETR: Frequency-Decoupled Multi-Domain Coordination for Aerial-View Object Detection",
        "abstract": "arXiv:2509.23056v1 Announce Type: new  Abstract: Aerial-view object detection is a critical technology for real-world applications such as natural resource monitoring, traffic management, and UAV-based search and rescue. Detecting tiny objects in high-resolution aerial imagery presents a long-standing challenge due to their limited visual cues and the difficulty of modeling global context in complex scenes. Existing methods are often hampered by delayed contextual fusion and inadequate non-linear modeling, failing to effectively use global information to refine shallow features and thus encountering a performance bottleneck. To address these challenges, we propose FMC-DETR, a novel framework with frequency-decoupled fusion for aerial-view object detection. First, we introduce the Wavelet Kolmogorov-Arnold Transformer (WeKat) backbone, which applies cascaded wavelet transforms to enhance global low-frequency context perception in shallow features while preserving fine-grained details, and employs Kolmogorov-Arnold networks to achieve adaptive non-linear modeling of multi-scale dependencies. Next, a lightweight Cross-stage Partial Fusion (CPF) module reduces redundancy and improves multi-scale feature interaction. Finally, we introduce the Multi-Domain Feature Coordination (MDFC) module, which unifies spatial, frequency, and structural priors to to balance detail preservation and global enhancement. Extensive experiments on benchmark aerial-view datasets demonstrate that FMC-DETR achieves state-of-the-art performance with fewer parameters. On the challenging VisDrone dataset, our model achieves improvements of 6.5% AP and 8.2% AP50 over the baseline, highlighting its effectiveness in tiny object detection. The code can be accessed at https://github.com/bloomingvision/FMC-DETR.",
        "arxiv_id": "2509.23056",
        "ARXIVID": "2509.23056",
        "COMMENT": "Does not match any specific criteria but is related to object detection in aerial imagery, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.24156": {
        "authors": [
            "Yuhui Wang",
            "Changjiang Li",
            "Guangke Chen",
            "Jiacheng Liang",
            "Ting Wang"
        ],
        "title": "Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models",
        "abstract": "arXiv:2509.24156v1 Announce Type: new  Abstract: Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively \"hacking\" the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",
        "arxiv_id": "2509.24156",
        "ARXIVID": "2509.24156",
        "COMMENT": "Does not match any specific criteria but discusses reasoning in large models, which is tangentially related to embodied agents and reasoning tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.23100": {
        "authors": [
            "Ajo Babu George",
            "Sadhvik Bathini",
            "Niranjana S R"
        ],
        "title": "Deep Learning for Oral Health: Benchmarking ViT, DeiT, BEiT, ConvNeXt, and Swin Transformer",
        "abstract": "arXiv:2509.23100v1 Announce Type: new  Abstract: Objective: The aim of this study was to systematically evaluate and compare the performance of five state-of-the-art transformer-based architectures - Vision Transformer (ViT), Data-efficient Image Transformer (DeiT), ConvNeXt, Swin Transformer, and Bidirectional Encoder Representation from Image Transformers (BEiT) - for multi-class dental disease classification. The study specifically focused on addressing real-world challenges such as data imbalance, which is often overlooked in existing literature.   Study Design: The Oral Diseases dataset was used to train and validate the selected models. Performance metrics, including validation accuracy, precision, recall, and F1-score, were measured, with special emphasis on how well each architecture managed imbalanced classes.   Results: ConvNeXt achieved the highest validation accuracy at 81.06, followed by BEiT at 80.00 and Swin Transformer at 79.73, all demonstrating strong F1-scores. ViT and DeiT achieved accuracies of 79.37 and 78.79, respectively, but both struggled particularly with Caries-related classes.   Conclusions: ConvNeXt, Swin Transformer, and BEiT showed reliable diagnostic performance, making them promising candidates for clinical application in dental imaging. These findings provide guidance for model selection in future AI-driven oral disease diagnostic tools and highlight the importance of addressing data imbalance in real-world scenarios",
        "arxiv_id": "2509.23100",
        "ARXIVID": "2509.23100",
        "COMMENT": "Does not match any specific criterion but benchmarks vision transformer models, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.23774": {
        "authors": [
            "Qifan Li",
            "Jiale Zou",
            "Jinhua Zhang",
            "Wei Long",
            "Xinyu Zhou",
            "Shuhang Gu"
        ],
        "title": "Texture Vector-Quantization and Reconstruction Aware Prediction for Generative Super-Resolution",
        "abstract": "arXiv:2509.23774v1 Announce Type: new  Abstract: Vector-quantized based models have recently demonstrated strong potential for visual prior modeling. However, existing VQ-based methods simply encode visual features with nearest codebook items and train index predictor with code-level supervision. Due to the richness of visual signal, VQ encoding often leads to large quantization error. Furthermore, training predictor with code-level supervision can not take the final reconstruction errors into consideration, result in sub-optimal prior modeling accuracy. In this paper we address the above two issues and propose a Texture Vector-Quantization and a Reconstruction Aware Prediction strategy. The texture vector-quantization strategy leverages the task character of super-resolution and only introduce codebook to model the prior of missing textures. While the reconstruction aware prediction strategy makes use of the straight-through estimator to directly train index predictor with image-level supervision. Our proposed generative SR model (TVQ&RAP) is able to deliver photo-realistic SR results with small computational cost.",
        "arxiv_id": "2509.23774",
        "ARXIVID": "2509.23774",
        "COMMENT": "Does not match any specific criterion but focuses on generative super-resolution, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.23025": {
        "authors": [
            "Gabriel A. Viana",
            "Luis F. Alves Pereira",
            "Tsang Ing Ren",
            "George D. C. Cavalcanti",
            "Jan Sijbers"
        ],
        "title": "Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement",
        "abstract": "arXiv:2509.23025v1 Announce Type: new  Abstract: Perceptual losses have emerged as powerful tools for training networks to enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to traditional pixel-wise losses such as Mean Squared Error, which often lead to over-smoothed reconstructions and loss of clinically relevant details in LDCT images. The perceptual losses operate in a latent feature space defined by a pretrained encoder and aim to preserve semantic content by comparing high-level features rather than raw pixel values. However, the design of perceptual losses involves critical yet underexplored decisions, including the feature representation level, the dataset used to pretrain the encoder, and the relative importance assigned to the perceptual component during optimization. In this work, we introduce the concept of perceptual influence (a metric that quantifies the relative contribution of the perceptual loss term to the total loss) and propose a principled framework to assess the impact of the loss design choices on the model training performance. Through systematic experimentation, we show that the widely used configurations in the literature to set up a perceptual loss underperform compared to better-designed alternatives. Our findings show that better perceptual loss designs lead to significant improvements in noise reduction and structural fidelity of reconstructed CT images, without requiring any changes to the network architecture. We also provide objective guidelines, supported by statistical analysis, to inform the effective use of perceptual losses in LDCT denoising. Our source code is available at https://github.com/vngabriel/perceptual-influence.",
        "arxiv_id": "2509.23025",
        "ARXIVID": "2509.23025",
        "COMMENT": "Does not match any specific criterion but discusses perceptual loss design for CT enhancement, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.22909": {
        "authors": [
            "Abdulkarim Atrash",
            "Omar Moured",
            "Yufan Chen",
            "Jiaming Zhang",
            "Seyda Ertekin",
            "Omur Ugur"
        ],
        "title": "TY-RIST: Tactical YOLO Tricks for Real-time Infrared Small Target Detection",
        "abstract": "arXiv:2509.22909v1 Announce Type: new  Abstract: Infrared small target detection (IRSTD) is critical for defense and surveillance but remains challenging due to (1) target loss from minimal features, (2) false alarms in cluttered environments, (3) missed detections from low saliency, and (4) high computational costs. To address these issues, we propose TY-RIST, an optimized YOLOv12n architecture that integrates (1) a stride-aware backbone with fine-grained receptive fields, (2) a high-resolution detection head, (3) cascaded coordinate attention blocks, and (4) a branch pruning strategy that reduces computational cost by about 25.5% while marginally improving accuracy and enabling real-time inference. We also incorporate the Normalized Gaussian Wasserstein Distance (NWD) to enhance regression stability. Extensive experiments on four benchmarks and across 20 different models demonstrate state-of-the-art performance, improving mAP at 0.5 IoU by +7.9%, Precision by +3%, and Recall by +10.2%, while achieving up to 123 FPS on a single GPU. Cross-dataset validation on a fifth dataset further confirms strong generalization capability. Additional results and resources are available at https://www.github.com/moured/TY-RIST",
        "arxiv_id": "2509.22909",
        "ARXIVID": "2509.22909",
        "COMMENT": "Does not match any specific criterion but focuses on infrared small target detection, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}