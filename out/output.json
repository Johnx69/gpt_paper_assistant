{
    "2508.05606": {
        "authors": [
            "Luozheng Qin",
            "Jia Gong",
            "Yuqing Sun",
            "Tianjiao Li",
            "Mengping Yang",
            "Xiaomeng Yang",
            "Chao Qu",
            "Zhiyu Tan",
            "Hao Li"
        ],
        "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision",
        "abstract": "arXiv:2508.05606v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/",
        "arxiv_id": "2508.05606",
        "ARXIVID": "2508.05606",
        "COMMENT": "Matches criteria 2 and 5 as it introduces Uni-CoT, a unified framework for chain-of-thought reasoning across text and vision, addressing multimodal reasoning challenges.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.05405": {
        "authors": [
            "Xinrun Xu",
            "Pi Bu",
            "Ye Wang",
            "B\\\"orje F. Karlsson",
            "Ziming Wang",
            "Tengtao Song",
            "Qi Zhu",
            "Jun Song",
            "Zhiming Ding",
            "Bo Zheng"
        ],
        "title": "DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning",
        "abstract": "arXiv:2508.05405v1 Announce Type: new  Abstract: Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control.",
        "arxiv_id": "2508.05405",
        "ARXIVID": "2508.05405",
        "COMMENT": "Matches criteria 1 and 3 closely as it introduces a novel benchmark (DeepPHY) for evaluating spatial reasoning and physical understanding in Vision Language Models (VLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.04852": {
        "authors": [
            "Chenhui Qiang",
            "Zhaoyang Wei",
            "Xumeng Han Zipeng Wang",
            "Siyao Li",
            "Xiangyuan Lan",
            "Jianbin Jiao",
            "Zhenjun Han"
        ],
        "title": "VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence",
        "abstract": "arXiv:2508.04852v1 Announce Type: new  Abstract: With the rapid development of MLLMs, evaluating their visual capabilities has become increasingly crucial. Current benchmarks primarily fall into two main types: basic perception benchmarks, which focus on local details but lack deep reasoning (e.g., \"what is in the image?\"), and mainstream reasoning benchmarks, which concentrate on prominent image elements but may fail to assess subtle clues requiring intricate analysis. However, profound visual understanding and complex reasoning depend more on interpreting subtle, inconspicuous local details than on perceiving salient, macro-level objects. These details, though occupying minimal image area, often contain richer, more critical information for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues, often occupying on average just 0.25% of the image area; 2) integrate these clues with world knowledge for complex reasoning. Comprising 374 carefully designed questions across Geospatial, Temporal, Situational, Intent, System State, and Symbolic reasoning, each question in VER-Bench is accompanied by structured evidence: visual clues and question-related reasoning derived from them. VER-Bench reveals current models' limitations in extracting subtle visual evidence and constructing evidence-based arguments, highlighting the need to enhance models's capabilities in fine-grained visual evidence extraction, integration, and reasoning for genuine visual understanding and human-like analysis. Dataset and additional materials are available https://github.com/verbta/ACMMM-25-Materials.",
        "arxiv_id": "2508.04852",
        "ARXIVID": "2508.04852",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark (VER-Bench) for evaluating MLLMs on reasoning with fine-grained visual evidence.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.05383": {
        "authors": [
            "Xiangxiang Zhang",
            "Jingxuan Wei",
            "Donghong Zhong",
            "Qi Chen",
            "Caijun Jia",
            "Cheng Tan",
            "Jinming Gu",
            "Xiaobo Qin",
            "Zhiping Liu",
            "Liang Hu",
            "Tong Sun",
            "Yuchen Wu",
            "Zewei Sun",
            "Chenwei Lou",
            "Hua Zheng",
            "Tianyang Zhan",
            "Changbao Wang",
            "Shuangzhi Wu",
            "Zefa Lin",
            "Chang Guo",
            "Sihang Yuan",
            "Riwei Chen",
            "Shixiong Zhao",
            "Yingping Zhang",
            "Gaowei Wu",
            "Bihui Yu",
            "Jiahui Wu",
            "Zhehui Zhao",
            "Qianqian Liu",
            "Ruofeng Tang",
            "Xingyue Huang",
            "Bing Zhao",
            "Mengyang Zhang",
            "Youqiang Zhou"
        ],
        "title": "StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models",
        "abstract": "arXiv:2508.05383v1 Announce Type: new  Abstract: Existing Vision-Language Models often struggle with complex, multi-question reasoning tasks where partial correctness is crucial for effective learning. Traditional reward mechanisms, which provide a single binary score for an entire response, are too coarse to guide models through intricate problems with multiple sub-parts. To address this, we introduce StructVRM, a method that aligns multimodal reasoning with Structured and Verifiable Reward Models. At its core is a model-based verifier trained to provide fine-grained, sub-question-level feedback, assessing semantic and mathematical equivalence rather than relying on rigid string matching. This allows for nuanced, partial credit scoring in previously intractable problem formats. Extensive experiments demonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM, achieves state-of-the-art performance on six out of twelve public multimodal benchmarks and our newly curated, high-difficulty STEM-Bench. The success of StructVRM validates that training with structured, verifiable rewards is a highly effective approach for advancing the capabilities of multimodal models in complex, real-world reasoning domains.",
        "arxiv_id": "2508.05383",
        "ARXIVID": "2508.05383",
        "COMMENT": "Matches criteria 2 and 5 as it introduces StructVRM, a structured reward model for multimodal reasoning, enhancing vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05430": {
        "authors": [
            "Hubert Baniecki",
            "Maximilian Muschalik",
            "Fabian Fumagalli",
            "Barbara Hammer",
            "Eyke H\\\"ullermeier",
            "Przemyslaw Biecek"
        ],
        "title": "Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions",
        "abstract": "arXiv:2508.05430v1 Announce Type: new  Abstract: Language-image pre-training (LIP) enables the development of vision-language models capable of zero-shot classification, localization, multimodal retrieval, and semantic understanding. Various explanation methods have been proposed to visualize the importance of input image-text pairs on the model's similarity outputs. However, popular saliency maps are limited by capturing only first-order attributions, overlooking the complex cross-modal interactions intrinsic to such encoders. We introduce faithful interaction explanations of LIP models (FIxLIP) as a unified approach to decomposing the similarity in vision-language encoders. FIxLIP is rooted in game theory, where we analyze how using the weighted Banzhaf interaction index offers greater flexibility and improves computational efficiency over the Shapley interaction quantification framework. From a practical perspective, we propose how to naturally extend explanation evaluation metrics, like the pointing game and area between the insertion/deletion curves, to second-order interaction explanations. Experiments on MS COCO and ImageNet-1k benchmarks validate that second-order methods like FIxLIP outperform first-order attribution methods. Beyond delivering high-quality explanations, we demonstrate the utility of FIxLIP in comparing different models like CLIP vs. SigLIP-2 and ViT-B/32 vs. ViT-L/16.",
        "arxiv_id": "2508.05430",
        "ARXIVID": "2508.05430",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a game-theoretic approach to explain vision-language model interactions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05202": {
        "authors": [
            "Dongchen Si",
            "Di Wang",
            "Erzhong Gao",
            "Xiaolei Qin",
            "Liu Zhao",
            "Jing Zhang",
            "Minqiang Xu",
            "Jianbo Zhan",
            "Jianshe Wang",
            "Lin Liu",
            "Bo Du",
            "Liangpei Zhang"
        ],
        "title": "SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images",
        "abstract": "arXiv:2508.05202v1 Announce Type: new  Abstract: Spectral information has long been recognized as a critical cue in remote sensing observations. Although numerous vision-language models have been developed for pixel-level interpretation, spectral information remains underutilized, resulting in suboptimal performance, particularly in multispectral scenarios. To address this limitation, we construct a vision-language instruction-following dataset named SPIE, which encodes spectral priors of land-cover objects into textual attributes recognizable by large language models (LLMs), based on classical spectral index computations. Leveraging this dataset, we propose SPEX, a multimodal LLM designed for instruction-driven land cover extraction. To this end, we introduce several carefully designed components and training strategies, including multiscale feature aggregation, token context condensation, and multispectral visual pre-training, to achieve precise and flexible pixel-level interpretation. To the best of our knowledge, SPEX is the first multimodal vision-language model dedicated to land cover extraction in spectral remote sensing imagery. Extensive experiments on five public multispectral datasets demonstrate that SPEX consistently outperforms existing state-of-the-art methods in extracting typical land cover categories such as vegetation, buildings, and water bodies. Moreover, SPEX is capable of generating textual explanations for its predictions, thereby enhancing interpretability and user-friendliness. Code will be released at: https://github.com/MiliLab/SPEX.",
        "arxiv_id": "2508.05202",
        "ARXIVID": "2508.05202",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a multimodal LLM for spectral remote sensing imagery.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05369": {
        "authors": [
            "Yongjun Zhang",
            "Mingtao Xiong",
            "Yi Wan",
            "Gui-Song Xia"
        ],
        "title": "Cross-View Localization via Redundant Sliced Observations and A-Contrario Validation",
        "abstract": "arXiv:2508.05369v1 Announce Type: new  Abstract: Cross-view localization (CVL) matches ground-level images with aerial references to determine the geo-position of a camera, enabling smart vehicles to self-localize offline in GNSS-denied environments. However, most CVL methods output only a single observation, the camera pose, and lack the redundant observations required by surveying principles, making it challenging to assess localization reliability through the mutual validation of observational data. To tackle this, we introduce Slice-Loc, a two-stage method featuring an a-contrario reliability validation for CVL. Instead of using the query image as a single input, Slice-Loc divides it into sub-images and estimates the 3-DoF pose for each slice, creating redundant and independent observations. Then, a geometric rigidity formula is proposed to filter out the erroneous 3-DoF poses, and the inliers are merged to generate the final camera pose. Furthermore, we propose a model that quantifies the meaningfulness of localization by estimating the number of false alarms (NFA), according to the distribution of the locations of the sliced images. By eliminating gross errors, Slice-Loc boosts localization accuracy and effectively detects failures. After filtering out mislocalizations, Slice-Loc reduces the proportion of errors exceeding 10 m to under 3\\%. In cross-city tests on the DReSS dataset, Slice-Loc cuts the mean localization error from 4.47 m to 1.86 m and the mean orientation error from $\\mathbf{3.42^{\\circ}}$ to $\\mathbf{1.24^{\\circ}}$, outperforming state-of-the-art methods. Code and dataset will be available at: https://github.com/bnothing/Slice-Loc.",
        "arxiv_id": "2508.05369",
        "ARXIVID": "2508.05369",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (Slice-Loc) for cross-view localization with redundant observations and reliability validation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05585": {
        "authors": [
            "Haijing Liu",
            "Tao Pu",
            "Hefeng Wu",
            "Keze Wang",
            "Liang Lin"
        ],
        "title": "DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition",
        "abstract": "arXiv:2508.05585v1 Announce Type: new  Abstract: Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness.",
        "arxiv_id": "2508.05585",
        "ARXIVID": "2508.05585",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for open-vocabulary multi-label recognition using vision-language pretraining and structured knowledge.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05417": {
        "authors": [
            "Rongzhen Zhao",
            "Wenyan Yang",
            "Juho Kannala",
            "Joni Pajarinen"
        ],
        "title": "Smoothing Slot Attention Iterations and Recurrences",
        "abstract": "arXiv:2508.05417v1 Announce Type: new  Abstract: Slot Attention (SA) and its variants lie at the heart of mainstream Object-Centric Learning (OCL). Objects in an image can be aggregated into respective slot vectors, by \\textit{iteratively} refining cold-start query vectors, typically three times, via SA on image features. For video, such aggregation is \\textit{recurrently} shared across frames, with queries cold-started on the first frame while transitioned from the previous frame's slots on non-first frames. However, the cold-start queries lack sample-specific cues thus hinder precise aggregation on the image or video's first frame; Also, non-first frames' queries are already sample-specific thus require transforms different from the first frame's aggregation. We address these issues for the first time with our \\textit{SmoothSA}: (1) To smooth SA iterations on the image or video's first frame, we \\textit{preheat} the cold-start queries with rich information of input features, via a tiny module self-distilled inside OCL; (2) To smooth SA recurrences across all video frames, we \\textit{differentiate} the homogeneous transforms on the first and non-first frames, by using full and single iterations respectively. Comprehensive experiments on object discovery, recognition and downstream benchmarks validate our method's effectiveness. Further analyses intuitively illuminate how our method smooths SA iterations and recurrences. Our code is available in the supplement.",
        "arxiv_id": "2508.05417",
        "ARXIVID": "2508.05417",
        "COMMENT": "Matches criterion 1 as it presents novel methodological improvements in spatial reasoning for embodied agents through SmoothSA.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05399": {
        "authors": [
            "Wonjun Kang",
            "Byeongkeun Ahn",
            "Minjae Lee",
            "Kevin Galim",
            "Seunghyuk Oh",
            "Hyung Il Koo",
            "Nam Ik Cho"
        ],
        "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation",
        "abstract": "arXiv:2508.05399v1 Announce Type: new  Abstract: Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.",
        "arxiv_id": "2508.05399",
        "ARXIVID": "2508.05399",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a method to improve compositional fidelity in text-to-image generation using attention guidance.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05221": {
        "authors": [
            "Xiao Wang",
            "Liye Jin",
            "Xufeng Lou",
            "Shiao Wang",
            "Lan Chen",
            "Bo Jiang",
            "Zhipeng Zhang"
        ],
        "title": "ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking",
        "abstract": "arXiv:2508.05221v1 Announce Type: new  Abstract: Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on https://github.com/Event-AHU/Open_VLTrack",
        "arxiv_id": "2508.05221",
        "ARXIVID": "2508.05221",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel reasoning-based vision-language tracking framework and a new benchmark dataset for long-term vision-language tracking.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05211": {
        "authors": [
            "Sihan Yang",
            "Runsen Xu",
            "Chenhang Cui",
            "Tai Wang",
            "Dahua Lin",
            "Jiangmiao Pang"
        ],
        "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization",
        "abstract": "arXiv:2508.05211v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging numerous visual tokens for fine-grained visual information, but this token redundancy results in significant computational costs. Previous research aimed at reducing visual tokens during inference typically leverages importance maps derived from attention scores among vision-only tokens or vision-language tokens to prune tokens across one or multiple pruning stages. Despite this progress, pruning frameworks and strategies remain simplistic and insufficiently explored, often resulting in substantial performance degradation. In this paper, we propose VFlowOpt, a token pruning framework that introduces an importance map derivation process and a progressive pruning module with a recycling mechanism. The hyperparameters of its pruning strategy are further optimized by a visual information flow-guided method. Specifically, we compute an importance map for image tokens based on their attention-derived context relevance and patch-level information entropy. We then decide which tokens to retain or prune and aggregate the pruned ones as recycled tokens to avoid potential information loss. Finally, we apply a visual information flow-guided method that regards the last token in the LMM as the most representative signal of text-visual interactions. This method minimizes the discrepancy between token representations in LMMs with and without pruning, thereby enabling superior pruning strategies tailored to different LMMs. Experiments demonstrate that VFlowOpt can prune 90% of visual tokens while maintaining comparable performance, leading to an 89% reduction in KV-Cache memory and 3.8 times faster inference.",
        "arxiv_id": "2508.05211",
        "ARXIVID": "2508.05211",
        "COMMENT": "Matches criteria 2 as it proposes a novel token pruning framework (VFlowOpt) for Large Multimodal Models (LMMs), optimizing visual-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.05602": {
        "authors": [
            "Tao Sun",
            "Oliver Liu",
            "JinJin Li",
            "Lan Ma"
        ],
        "title": "LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model",
        "abstract": "arXiv:2508.05602v1 Announce Type: new  Abstract: Multimodal generative AI usually involves generating image or text responses given inputs in another modality. The evaluation of image-text relevancy is essential for measuring response quality or ranking candidate responses. In particular, binary relevancy evaluation, i.e., ``Relevant'' vs. ``Not Relevant'', is a fundamental problem. However, this is a challenging task considering that texts have diverse formats and the definition of relevancy varies in different scenarios. We find that Multimodal Large Language Models (MLLMs) are an ideal choice to build such evaluators, as they can flexibly handle complex text formats and take in additional task information. In this paper, we present LLaVA-RE, a first attempt for binary image-text relevancy evaluation with MLLM. It follows the LLaVA architecture and adopts detailed task instructions and multimodal in-context samples. In addition, we propose a novel binary relevancy data set that covers various tasks. Experimental results validate the effectiveness of our framework.",
        "arxiv_id": "2508.05602",
        "ARXIVID": "2508.05602",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on binary image-text relevancy evaluation using multimodal LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.05009": {
        "authors": [
            "Bin Han",
            "Robert Wolfe",
            "Anat Caspi",
            "Bill Howe"
        ],
        "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses",
        "abstract": "arXiv:2508.05009v1 Announce Type: new  Abstract: We explore the application of large language models (LLMs) to empower domain experts in integrating large, heterogeneous, and noisy urban spatial datasets. Traditional rule-based integration methods are unable to cover all edge cases, requiring manual verification and repair. Machine learning approaches require collecting and labeling of large numbers of task-specific samples. In this study, we investigate the potential of LLMs for spatial data integration. Our analysis first considers how LLMs reason about environmental spatial relationships mediated by human experience, such as between roads and sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they struggle to connect the macro-scale environment with the relevant computational geometry tasks, often producing logically incoherent responses. But when provided relevant features, thereby reducing dependence on spatial reasoning, LLMs are able to generate high-performing results. We then adapt a review-and-refine method, which proves remarkably effective in correcting erroneous initial responses while preserving accurate responses. We discuss practical implications of employing LLMs for spatial data integration in real-world contexts and outline future research directions, including post-training, multi-modal integration methods, and support for diverse data formats. Our findings position LLMs as a promising and flexible alternative to traditional rule-based heuristics, advancing the capabilities of adaptive spatial data integration.",
        "arxiv_id": "2508.05009",
        "ARXIVID": "2508.05009",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it explores spatial reasoning capabilities of LLMs for spatial data integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.05506": {
        "authors": [
            "Shibo Wang",
            "Haonan He",
            "Maria Parelli",
            "Christoph Gebhardt",
            "Zicong Fan",
            "Jie Song"
        ],
        "title": "MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips",
        "abstract": "arXiv:2508.05506v1 Announce Type: new  Abstract: Most RGB-based hand-object reconstruction methods rely on object templates, while template-free methods typically assume full object visibility. This assumption often breaks in real-world settings, where fixed camera viewpoints and static grips leave parts of the object unobserved, resulting in implausible reconstructions. To overcome this, we present MagicHOI, a method for reconstructing hands and objects from short monocular interaction videos, even under limited viewpoint variation. Our key insight is that, despite the scarcity of paired 3D hand-object data, large-scale novel view synthesis diffusion models offer rich object supervision. This supervision serves as a prior to regularize unseen object regions during hand interactions. Leveraging this insight, we integrate a novel view synthesis model into our hand-object reconstruction framework. We further align hand to object by incorporating visible contact constraints. Our results demonstrate that MagicHOI significantly outperforms existing state-of-the-art hand-object reconstruction methods. We also show that novel view synthesis diffusion priors effectively regularize unseen object regions, enhancing 3D hand-object reconstruction.",
        "arxiv_id": "2508.05506",
        "ARXIVID": "2508.05506",
        "COMMENT": "Matches criterion 1 and 3 as it focuses on embodied hand-object reconstruction using novel priors and methods.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.05065": {
        "authors": [
            "Yifu Guo",
            "Yuquan Lu",
            "Wentao Zhang",
            "Zishan Xu",
            "Dexia Chen",
            "Siyu Zhang",
            "Yizhe Zhang",
            "Ruixuan Wang"
        ],
        "title": "Decoupling Continual Semantic Segmentation",
        "abstract": "arXiv:2508.05065v1 Announce Type: new  Abstract: Continual Semantic Segmentation (CSS) requires learning new classes without forgetting previously acquired knowledge, addressing the fundamental challenge of catastrophic forgetting in dense prediction tasks. However, existing CSS methods typically employ single-stage encoder-decoder architectures where segmentation masks and class labels are tightly coupled, leading to interference between old and new class learning and suboptimal retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage framework for CSS. By decoupling class-aware detection from class-agnostic segmentation, DecoupleCSS enables more effective continual learning, preserving past knowledge while learning new classes. The first stage leverages pre-trained text and image encoders, adapted using LoRA, to encode class-specific information and generate location-aware prompts. In the second stage, the Segment Anything Model (SAM) is employed to produce precise segmentation masks, ensuring that segmentation knowledge is shared across both new and previous classes. This approach improves the balance between retention and adaptability in CSS, achieving state-of-the-art performance across a variety of challenging tasks. Our code is publicly available at: https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.",
        "arxiv_id": "2508.05065",
        "ARXIVID": "2508.05065",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for continual semantic segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04924": {
        "authors": [
            "Zahidul Islam",
            "Sujoy Paul",
            "Mrigank Rochan"
        ],
        "title": "Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations",
        "abstract": "arXiv:2508.04924v1 Announce Type: new  Abstract: Existing video highlight detection methods, although advanced, struggle to generalize well to all test videos. These methods typically employ a generic highlight detection model for each test video, which is suboptimal as it fails to account for the unique characteristics and variations of individual test videos. Such fixed models do not adapt to the diverse content, styles, or audio and visual qualities present in new, unseen test videos, leading to reduced highlight detection performance. In this paper, we propose Highlight-TTA, a test-time adaptation framework for video highlight detection that addresses this limitation by dynamically adapting the model during testing to better align with the specific characteristics of each test video, thereby improving generalization and highlight detection performance. Highlight-TTA is jointly optimized with an auxiliary task, cross-modality hallucinations, alongside the primary highlight detection task. We utilize a meta-auxiliary training scheme to enable effective adaptation through the auxiliary task while enhancing the primary task. During testing, we adapt the trained model using the auxiliary task on the test video to further enhance its highlight detection performance. Extensive experiments with three state-of-the-art highlight detection models and three benchmark datasets show that the introduction of Highlight-TTA to these models improves their performance, yielding superior results.",
        "arxiv_id": "2508.04924",
        "ARXIVID": "2508.04924",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on video highlight detection and novel test-time adaptation methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.05502": {
        "authors": [
            "Yufei Gao",
            "Jiaying Fei",
            "Nuo Chen",
            "Ruirui Chen",
            "Guohang Yan",
            "Yunshi Lan",
            "Botian Shi"
        ],
        "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs",
        "abstract": "arXiv:2508.05502v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce \"thin descriptions\", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing \"thick descriptions\". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
        "arxiv_id": "2508.05502",
        "ARXIVID": "2508.05502",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) with a focus on low-resource languages and cultural groundedness.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.05630": {
        "authors": [
            "Henghui Ding",
            "Kaining Ying",
            "Chang Liu",
            "Shuting He",
            "Xudong Jiang",
            "Yu-Gang Jiang",
            "Philip H. S. Torr",
            "Song Bai"
        ],
        "title": "MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes",
        "abstract": "arXiv:2508.05630v1 Announce Type: new  Abstract: Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.",
        "arxiv_id": "2508.05630",
        "ARXIVID": "2508.05630",
        "COMMENT": "Matches criterion 6 as it introduces a new dataset for video object segmentation in complex scenes.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.05008": {
        "authors": [
            "Xusheng Liang",
            "Lihua Zhou",
            "Nianxin Li",
            "Miao Xu",
            "Ziyang Song",
            "Dong Yi",
            "Jinlin Wu",
            "Hongbin Liu",
            "Jiebo Luo",
            "Zhen Lei"
        ],
        "title": "Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation",
        "abstract": "arXiv:2508.05008v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable zero-shot capabilities in various computer vision tasks. However, their application to medical imaging remains challenging due to the high variability and complexity of medical data. Specifically, medical images often exhibit significant domain shifts caused by various confounders, including equipment differences, procedure artifacts, and imaging modes, which can lead to poor generalization when models are applied to unseen domains. To address this limitation, we propose Multimodal Causal-Driven Representation Learning (MCDRL), a novel framework that integrates causal inference with the VLM to tackle domain generalization in medical image segmentation. MCDRL is implemented in two steps: first, it leverages CLIP's cross-modal capabilities to identify candidate lesion regions and construct a confounder dictionary through text prompts, specifically designed to represent domain-specific variations; second, it trains a causal intervention network that utilizes this dictionary to identify and eliminate the influence of these domain-specific variations while preserving the anatomical structural information critical for segmentation tasks. Extensive experiments demonstrate that MCDRL consistently outperforms competing methods, yielding superior segmentation accuracy and exhibiting robust generalizability.",
        "arxiv_id": "2508.05008",
        "ARXIVID": "2508.05008",
        "COMMENT": "Matches criterion 2 as it integrates vision-language models (VLMs) with causal inference for medical image segmentation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.05213": {
        "authors": [
            "Jianming Liu",
            "Wenlong Qiu",
            "Haitao Wei"
        ],
        "title": "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation",
        "abstract": "arXiv:2508.05213v1 Announce Type: new  Abstract: Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with few labeled samples. However, its performance significantly degrades when domain discrepancies exist between training and deployment. Cross-Domain Few-Shot Segmentation(CD-FSS) is proposed to mitigate such performance degradation. Current CD-FSS methods primarily sought to develop segmentation models on a source domain capable of cross-domain generalization. However, driven by escalating concerns over data privacy and the imperative to minimize data transfer and training expenses, the development of source-free CD-FSS approaches has become essential. In this work, we propose a source-free CD-FSS method that leverages both textual and visual information to facilitate target domain task adaptation without requiring source domain data. Specifically, we first append Task-Specific Attention Adapters (TSAA) to the feature pyramid of a pretrained backbone, which adapt multi-level features extracted from the shared pre-trained backbone to the target task. Then, the parameters of the TSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and a Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes global-local visual features to align image features across different views, while the TVEA module leverages textual priors from pre-aligned multi-modal features (e.g., from CLIP) to guide cross-modal adaptation. By combining the outputs of these modules through dense comparison operations and subsequent fusion via skip connections, our method produces refined prediction masks. Under both 1-shot and 5-shot settings, the proposed approach achieves average segmentation accuracy improvements of 2.18\\% and 4.11\\%, respectively, across four cross-domain datasets, significantly outperforming state-of-the-art CD-FSS methods. Code are available at https://github.com/ljm198134/TVGTANet.",
        "arxiv_id": "2508.05213",
        "ARXIVID": "2508.05213",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it leverages textual and visual information for cross-domain few-shot segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04943": {
        "authors": [
            "Zhu Xu",
            "Ting Lei",
            "Zhimin Li",
            "Guan Wang",
            "Qingchao Chen",
            "Yuxin Peng",
            "Yang liu"
        ],
        "title": "TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring",
        "abstract": "arXiv:2508.04943v1 Announce Type: new  Abstract: Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.",
        "arxiv_id": "2508.04943",
        "ARXIVID": "2508.04943",
        "COMMENT": "Matches criterion 6 as it focuses on dynamic scene graph generation for video understanding.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.04848": {
        "authors": [
            "Chang Tian",
            "Matthew B. Blaschko",
            "Mingzhe Xing",
            "Xiuxing Li",
            "Yinliang Yue",
            "Marie-Francine Moens"
        ],
        "title": "Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning",
        "abstract": "arXiv:2508.04848v1 Announce Type: new  Abstract: Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs. We formally define and evaluate these challenging scenarios. We fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM) using RL with a representative policy-gradient algorithm and then test their performance on eight public datasets. Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities. Although we propose a scenario-specific remediation method, our results suggest current methods leave these reasoning deficits largely unresolved. This work highlights that the reasoning abilities of large models are often overstated and underscores the importance of evaluating models under non-ideal scenarios. The code and data will be released at XXXX.",
        "arxiv_id": "2508.04848",
        "ARXIVID": "2508.04848",
        "COMMENT": "Partially relevant to criterion 2 (Visual and Multimodal Large Language Models) as it evaluates reasoning abilities of a vision-language model under non-ideal conditions.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2508.05227": {
        "authors": [
            "Semanur K\\\"u\\c{c}\\\"uk",
            "Cosimo Della Santina",
            "Angeliki Laskari"
        ],
        "title": "Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2",
        "abstract": "arXiv:2508.05227v1 Announce Type: new  Abstract: Segmenting gas bubbles in multiphase flows is a critical yet unsolved challenge in numerous industrial settings, from metallurgical processing to maritime drag reduction. Traditional approaches-and most recent learning-based methods-assume near-spherical shapes, limiting their effectiveness in regimes where bubbles undergo deformation, coalescence, or breakup. This complexity is particularly evident in air lubrication systems, where coalesced bubbles form amorphous and topologically diverse patches. In this work, we revisit the problem through the lens of modern vision foundation models. We cast the task as a transfer learning problem and demonstrate, for the first time, that a fine-tuned Segment Anything Model SAM v2.1 can accurately segment highly non-convex, irregular bubble structures using as few as 100 annotated images.",
        "arxiv_id": "2508.05227",
        "ARXIVID": "2508.05227",
        "COMMENT": "Matches criterion 4 as it applies a vision foundation model (SAM) to a novel segmentation task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04868": {
        "authors": [
            "Noreen Anwar",
            "Guillaume-Alexandre Bilodeau",
            "Wassim Bouachir"
        ],
        "title": "Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications",
        "abstract": "arXiv:2508.04868v1 Announce Type: new  Abstract: Transformer-based object detectors often struggle with occlusions, fine-grained localization, and computational inefficiency caused by fixed queries and dense attention. We propose DAMM, Dual-stream Attention with Multi-Modal queries, a novel framework introducing both query adaptation and structured cross-attention for improved accuracy and efficiency. DAMM capitalizes on three types of queries: appearance-based queries from vision-language models, positional queries using polygonal embeddings, and random learned queries for general scene coverage. Furthermore, a dual-stream cross-attention module separately refines semantic and spatial features, boosting localization precision in cluttered scenes. We evaluated DAMM on four challenging benchmarks, and it achieved state-of-the-art performance in average precision (AP) and recall, demonstrating the effectiveness of multi-modal query adaptation and dual-stream attention. Source code is at: \\href{https://github.com/DET-LIP/DAMM}{GitHub}.",
        "arxiv_id": "2508.04868",
        "ARXIVID": "2508.04868",
        "COMMENT": "Matches criterion 5 as it discusses multi-modal queries for object detection, integrating vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.05083": {
        "authors": [
            "Dexuan Xu",
            "Jieyi Wang",
            "Zhongyan Chai",
            "Yongzhi Cao",
            "Hanpin Wang",
            "Huamin Zhang",
            "Yu Huang"
        ],
        "title": "MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models",
        "abstract": "arXiv:2508.05083v1 Announce Type: new  Abstract: Recent advances in multimodal large language models (MLLMs) have significantly improved medical AI, enabling it to unify the understanding of visual and textual information. However, as medical knowledge continues to evolve, it is critical to allow these models to efficiently update outdated or incorrect information without retraining from scratch. Although textual knowledge editing has been widely studied, there is still a lack of systematic benchmarks for multimodal medical knowledge editing involving image and text modalities. To fill this gap, we present MedMKEB, the first comprehensive benchmark designed to evaluate the reliability, generality, locality, portability, and robustness of knowledge editing in medical multimodal large language models. MedMKEB is built on a high-quality medical visual question-answering dataset and enriched with carefully constructed editing tasks, including counterfactual correction, semantic generalization, knowledge transfer, and adversarial robustness. We incorporate human expert validation to ensure the accuracy and reliability of the benchmark. Extensive single editing and sequential editing experiments on state-of-the-art general and medical MLLMs demonstrate the limitations of existing knowledge-based editing approaches in medicine, highlighting the need to develop specialized editing strategies. MedMKEB will serve as a standard benchmark to promote the development of trustworthy and efficient medical knowledge editing algorithms.",
        "arxiv_id": "2508.05083",
        "ARXIVID": "2508.05083",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark for medical multimodal large language models (MLLMs), focusing on knowledge editing.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.05237": {
        "authors": [
            "Zane Xu",
            "Jason Sun"
        ],
        "title": "Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models",
        "abstract": "arXiv:2508.05237v1 Announce Type: new  Abstract: This report synthesizes eight seminal papers on the zero-shot adversarial robustness of vision-language models (VLMs) like CLIP. A central challenge in this domain is the inherent trade-off between enhancing adversarial robustness and preserving the model's zero-shot generalization capabilities. We analyze two primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies model parameters, and Training-Free/Test-Time Defenses, which preserve them. We trace the evolution from alignment-preserving methods (TeCoA) to embedding space re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to latent-space purification (CLIPure). Finally, we identify key challenges and future directions including hybrid defense strategies and adversarial pre-training.",
        "arxiv_id": "2508.05237",
        "ARXIVID": "2508.05237",
        "COMMENT": "Matches criterion 2 as it synthesizes defensive strategies for vision-language models (VLLMs) and discusses adversarial robustness.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2508.05599": {
        "authors": [
            "Shaobin Zhuang",
            "Yiwei Guo",
            "Canmiao Fu",
            "Zhipeng Huang",
            "Zeyue Tian",
            "Ying Zhang",
            "Chen Li",
            "Yali Wang"
        ],
        "title": "WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction",
        "abstract": "arXiv:2508.05599v1 Announce Type: new  Abstract: Visual tokenizer is a critical component for vision generation. However, the existing tokenizers often face unsatisfactory trade-off between compression ratios and reconstruction fidelity. To fill this gap, we introduce a powerful and concise WeTok tokenizer, which surpasses the previous leading tokenizers via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We partition the latent features into groups, and perform lookup-free quantization for each group. As a result, GQ can efficiently overcome memory and computation limitations of prior tokenizers, while achieving a reconstruction breakthrough with more scalable codebooks. (2) Generative Decoding (GD). Different from prior tokenizers, we introduce a generative decoder with a prior of extra noise variable. In this case, GD can probabilistically model the distribution of visual data conditioned on discrete tokens, allowing WeTok to reconstruct visual details, especially at high compression ratios. Extensive experiments on mainstream benchmarks show superior performance of our WeTok. On the ImageNet 50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs. FLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression model achieves a zero-shot rFID of 3.49 with a compression ratio of 768, outperforming Cosmos (384) 4.57 which has only 50% compression rate of ours. Code and models are available: https://github.com/zhuangshaobin/WeTok.",
        "arxiv_id": "2508.05599",
        "ARXIVID": "2508.05599",
        "COMMENT": "Does not match any specific criteria but is relevant to visual tokenization and high-fidelity visual reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.05609": {
        "authors": [
            "Yuhan Zhang",
            "Long Zhuo",
            "Ziyang Chu",
            "Tong Wu",
            "Zhibing Li",
            "Liang Pan",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "title": "Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity",
        "abstract": "arXiv:2508.05609v1 Announce Type: new  Abstract: Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.",
        "arxiv_id": "2508.05609",
        "ARXIVID": "2508.05609",
        "COMMENT": "Does not match any specific criteria but is relevant to 3D generation evaluation, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.05414": {
        "authors": [
            "Jiawei Liang",
            "Siyuan Liang",
            "Jianjie Huang",
            "Chenxi Si",
            "Ming Zhang",
            "Xiaochun Cao"
        ],
        "title": "Physical Adversarial Camouflage through Gradient Calibration and Regularization",
        "abstract": "arXiv:2508.05414v1 Announce Type: new  Abstract: The advancement of deep object detectors has greatly affected safety-critical fields like autonomous driving. However, physical adversarial camouflage poses a significant security risk by altering object textures to deceive detectors. Existing techniques struggle with variable physical environments, facing two main challenges: 1) inconsistent sampling point densities across distances hinder the gradient optimization from ensuring local continuity, and 2) updating texture gradients from multiple angles causes conflicts, reducing optimization stability and attack effectiveness. To address these issues, we propose a novel adversarial camouflage framework based on gradient optimization. First, we introduce a gradient calibration strategy, which ensures consistent gradient updates across distances by propagating gradients from sparsely to unsampled texture points. Additionally, we develop a gradient decorrelation method, which prioritizes and orthogonalizes gradients based on loss values, enhancing stability and effectiveness in multi-angle optimization by eliminating redundant or conflicting updates. Extensive experimental results on various detection models, angles and distances show that our method significantly exceeds the state of the art, with an average increase in attack success rate (ASR) of 13.46% across distances and 11.03% across angles. Furthermore, empirical evaluation in real-world scenarios highlights the need for more robust system design.",
        "arxiv_id": "2508.05414",
        "ARXIVID": "2508.05414",
        "COMMENT": "Does not match any specific criteria but discusses adversarial camouflage in object detection, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.05353": {
        "authors": [
            "Kang Liu",
            "Zhuoqi Ma",
            "Zikang Fang",
            "Yunan Li",
            "Kun Xie",
            "Qiguang Miao"
        ],
        "title": "PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation",
        "abstract": "arXiv:2508.05353v1 Announce Type: new  Abstract: Chest X-ray report generation aims to reduce radiologists' workload by automatically producing high-quality preliminary reports. A critical yet underexplored aspect of this task is the effective use of patient-specific prior knowledge -- including clinical context (e.g., symptoms, medical history) and the most recent prior image -- which radiologists routinely rely on for diagnostic reasoning. Most existing methods generate reports from single images, neglecting this essential prior information and thus failing to capture diagnostic intent or disease progression. To bridge this gap, we propose PriorRG, a novel chest X-ray report generation framework that emulates real-world clinical workflows via a two-stage training pipeline. In Stage 1, we introduce a prior-guided contrastive pre-training scheme that leverages clinical context to guide spatiotemporal feature extraction, allowing the model to align more closely with the intrinsic spatiotemporal semantics in radiology reports. In Stage 2, we present a prior-aware coarse-to-fine decoding for report generation that progressively integrates patient-specific prior knowledge with the vision encoder's hidden states. This decoding allows the model to align with diagnostic focus and track disease progression, thereby enhancing the clinical accuracy and fluency of the generated reports. Extensive experiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG outperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score improvement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and checkpoints will be released upon acceptance.",
        "arxiv_id": "2508.05353",
        "ARXIVID": "2508.05353",
        "COMMENT": "Does not match any specific criteria but focuses on medical image report generation, which is tangentially related to vision-language integration.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.05496": {
        "authors": [
            "Shuo Cai",
            "Su Lu",
            "Qi Zhou",
            "Kejing Yang",
            "Zhijie Sang",
            "Congkai Xie",
            "Hongxia Yang"
        ],
        "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities",
        "abstract": "arXiv:2508.05496v1 Announce Type: new  Abstract: Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.",
        "arxiv_id": "2508.05496",
        "ARXIVID": "2508.05496",
        "COMMENT": "Does not match any specific criteria but focuses on improving reasoning capabilities in LLMs, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05622": {
        "authors": [
            "Yu Yuan",
            "Lili Zhao",
            "Wei Chen",
            "Guangting Zheng",
            "Kai Zhang",
            "Mengdi Zhang",
            "Qi Liu"
        ],
        "title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents",
        "abstract": "arXiv:2508.05622v1 Announce Type: new  Abstract: Capturing human learning behavior based on deep learning methods has become a major research focus in both psychology and intelligent systems. Recent approaches rely on controlled experiments or rule-based models to explore cognitive processes. However, they struggle to capture learning dynamics, track progress over time, or provide explainability. To address these challenges, we introduce LearnerAgent, a novel multi-agent framework based on Large Language Models (LLMs) to simulate a realistic teaching environment. To explore human-like learning dynamics, we construct learners with psychologically grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free General Learner to inspect the base LLM's default behavior. Through weekly knowledge acquisition, monthly strategic choices, periodic tests, and peer interaction, we can track the dynamic learning progress of individual learners over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis reveals that only Deep Learner achieves sustained cognitive growth. Our specially designed \"trap questions\" effectively diagnose Surface Learner's shallow knowledge. 2) The behavioral and cognitive patterns of distinct learners align closely with their psychological profiles. 3) Learners' self-concept scores evolve realistically, with the General Learner developing surprisingly high self-efficacy despite its cognitive limitations. 4) Critically, the default profile of base LLM is a \"diligent but brittle Surface Learner\"-an agent that mimics the behaviors of a good student but lacks true, generalizable understanding. Extensive simulation experiments demonstrate that LearnerAgent aligns well with real scenarios, yielding more insightful findings about LLMs' behavior.",
        "arxiv_id": "2508.05622",
        "ARXIVID": "2508.05622",
        "COMMENT": "Does not match any specific criteria but is tangentially related to embodied AI and learning dynamics, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05094": {
        "authors": [
            "Liang Bai",
            "Hong Song",
            "Jinfu Li",
            "Yucong Lin",
            "Jingfan Fan",
            "Tianyu Fu",
            "Danni Ai",
            "Deqiang Xiao",
            "Jian Yang"
        ],
        "title": "Sculpting Margin Penalty: Intra-Task Adapter Merging and Classifier Calibration for Few-Shot Class-Incremental Learning",
        "abstract": "arXiv:2508.05094v1 Announce Type: new  Abstract: Real-world applications often face data privacy constraints and high acquisition costs, making the assumption of sufficient training data in incremental tasks unrealistic and leading to significant performance degradation in class-incremental learning. Forward-compatible learning, which prospectively prepares for future tasks during base task training, has emerged as a promising solution for Few-Shot Class-Incremental Learning (FSCIL). However, existing methods still struggle to balance base-class discriminability and new-class generalization. Moreover, limited access to original data during incremental tasks often results in ambiguous inter-class decision boundaries. To address these challenges, we propose SMP (Sculpting Margin Penalty), a novel FSCIL method that strategically integrates margin penalties at different stages within the parameter-efficient fine-tuning paradigm. Specifically, we introduce the Margin-aware Intra-task Adapter Merging (MIAM) mechanism for base task learning. MIAM trains two sets of low-rank adapters with distinct classification losses: one with a margin penalty to enhance base-class discriminability, and the other without margin constraints to promote generalization to future new classes. These adapters are then adaptively merged to improve forward compatibility. For incremental tasks, we propose a Margin Penalty-based Classifier Calibration (MPCC) strategy to refine decision boundaries by fine-tuning classifiers on all seen classes' embeddings with a margin penalty. Extensive experiments on CIFAR100, ImageNet-R, and CUB200 demonstrate that SMP achieves state-of-the-art performance in FSCIL while maintaining a better balance between base and new classes.",
        "arxiv_id": "2508.05094",
        "ARXIVID": "2508.05094",
        "COMMENT": "Does not match any specific criterion but is tangentially relevant to continual learning and class-incremental learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05254": {
        "authors": [
            "Hyunjoon Lee",
            "Joonkyu Min",
            "Jaesik Park"
        ],
        "title": "CF3: Compact and Fast 3D Feature Fields",
        "abstract": "arXiv:2508.05254v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.",
        "arxiv_id": "2508.05254",
        "ARXIVID": "2508.05254",
        "COMMENT": "Does not match any specific criterion but is tangentially relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05508": {
        "authors": [
            "Roshita Bhonsle",
            "Rishav Dutta",
            "Sneha Vavilapalli",
            "Harsh Seth",
            "Abubakarr Jaye",
            "Yapei Chang",
            "Mukund Rungta",
            "Emmanuel Aboah Boateng",
            "Sadid Hasan",
            "Ehi Nosakhare",
            "Soundar Srinivasan"
        ],
        "title": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation",
        "abstract": "arXiv:2508.05508v1 Announce Type: new  Abstract: The increasing adoption of foundation models as agents across diverse domains necessitates a robust evaluation framework. Current methods, such as LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step reasoning that drives agentic decision-making. Meanwhile, existing Agent-as-a-Judge systems, where one agent evaluates another's task completion, are typically designed for narrow, domain-specific settings. To address this gap, we propose a generalizable, modular framework for evaluating agent task completion independent of the task domain. The framework emulates human-like evaluation by decomposing tasks into sub-tasks and validating each step using available information, such as the agent's output and reasoning. Each module contributes to a specific aspect of the evaluation process, and their outputs are aggregated to produce a final verdict on task completion. We validate our framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA and BigCodeBench. Our Judge Agent predicts task success with closer agreement to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy, respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This demonstrates the potential of our proposed general-purpose evaluation framework.",
        "arxiv_id": "2508.05508",
        "ARXIVID": "2508.05508",
        "COMMENT": "Does not match any specific criteria but is relevant to evaluation frameworks for agentic systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05409": {
        "authors": [
            "Farah Wahida",
            "M. A. P. Chamikara",
            "Yashothara Shanmugarasa",
            "Mohan Baruwal Chhetri",
            "Thilina Ranbaduge",
            "Ibrahim Khalil"
        ],
        "title": "From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization",
        "abstract": "arXiv:2508.05409v1 Announce Type: new  Abstract: Biometric systems, such as face recognition systems powered by deep neural networks (DNNs), rely on large and highly sensitive datasets. Backdoor attacks can subvert these systems by manipulating the training process. By inserting a small trigger, such as a sticker, make-up, or patterned mask, into a few training images, an adversary can later present the same trigger during authentication to be falsely recognized as another individual, thereby gaining unauthorized access. Existing defense mechanisms against backdoor attacks still face challenges in precisely identifying and mitigating poisoned images without compromising data utility, which undermines the overall reliability of the system. We propose a novel and generalizable approach, TrueBiometric: Trustworthy Biometrics, which accurately detects poisoned images using a majority voting mechanism leveraging multiple state-of-the-art large vision language models. Once identified, poisoned samples are corrected using targeted and calibrated corrective noise. Our extensive empirical results demonstrate that TrueBiometric detects and corrects poisoned images with 100\\% accuracy without compromising accuracy on clean images. Compared to existing state-of-the-art approaches, TrueBiometric offers a more practical, accurate, and effective solution for mitigating backdoor attacks in face recognition systems.",
        "arxiv_id": "2508.05409",
        "ARXIVID": "2508.05409",
        "COMMENT": "Does not match any specific criteria but is relevant to vision-language integration in biometric systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04928": {
        "authors": [
            "Suchisrit Gangopadhyay",
            "Jung-Hee Kim",
            "Xien Chen",
            "Patrick Rim",
            "Hyoungseob Park",
            "Alex Wong"
        ],
        "title": "Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens",
        "abstract": "arXiv:2508.04928v1 Announce Type: new  Abstract: We propose a method to extend foundational monocular depth estimators (FMDEs), trained on perspective images, to fisheye images. Despite being trained on tens of millions of images, FMDEs are susceptible to the covariate shift introduced by changes in camera calibration (intrinsic, distortion) parameters, leading to erroneous depth estimates. Our method aligns the distribution of latent embeddings encoding fisheye images to those of perspective images, enabling the reuse of FMDEs for fisheye cameras without retraining or finetuning. To this end, we introduce a set of Calibration Tokens as a light-weight adaptation mechanism that modulates the latent embeddings for alignment. By exploiting the already expressive latent space of FMDEs, we posit that modulating their embeddings avoids the negative impact of artifacts and loss introduced in conventional recalibration or map projection to a canonical reference frame in the image space. Our method is self-supervised and does not require fisheye images but leverages publicly available large-scale perspective image datasets. This is done by recalibrating perspective images to fisheye images, and enforcing consistency between their estimates during training. We evaluate our approach with several FMDEs, on both indoors and outdoors, where we consistently improve over state-of-the-art methods using a single set of tokens for both. Code available at: https://github.com/JungHeeKim29/calibration-token.",
        "arxiv_id": "2508.04928",
        "ARXIVID": "2508.04928",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision applications in depth estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05521": {
        "authors": [
            "Shaowu Chen",
            "Wei Ma",
            "Binhua Huang",
            "Qingyuan Wang",
            "Guoxin Wang",
            "Weize Sun",
            "Lei Huang",
            "Deepu John"
        ],
        "title": "Optimal Brain Connection: Towards Efficient Structural Pruning",
        "abstract": "arXiv:2508.05521v1 Announce Type: new  Abstract: Structural pruning has been widely studied for its effectiveness in compressing neural networks. However, existing methods often neglect the interconnections among parameters. To address this limitation, this paper proposes a structural pruning framework termed Optimal Brain Connection. First, we introduce the Jacobian Criterion, a first-order metric for evaluating the saliency of structural parameters. Unlike existing first-order methods that assess parameters in isolation, our criterion explicitly captures both intra-component interactions and inter-layer dependencies. Second, we propose the Equivalent Pruning mechanism, which utilizes autoencoders to retain the contributions of all original connection--including pruned ones--during fine-tuning. Experimental results demonstrate that the Jacobian Criterion outperforms several popular metrics in preserving model performance, while the Equivalent Pruning mechanism effectively mitigates performance degradation after fine-tuning. Code: https://github.com/ShaowuChen/Optimal_Brain_Connection",
        "arxiv_id": "2508.05521",
        "ARXIVID": "2508.05521",
        "COMMENT": "Does not match any specific criteria but discusses structural pruning in neural networks, which is tangentially related to general machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04720": {
        "authors": [
            "Yingjie Zhou",
            "Jiezhang Cao",
            "Farong Wen",
            "Li Xu",
            "Yanwei Jiang",
            "Jun Jia",
            "Ronghui Li",
            "Xiaohong Liu",
            "Yu Zhou",
            "Xiongkuo Min",
            "Jie Guo",
            "Zicheng Zhang",
            "Guangtao Zhai"
        ],
        "title": "Who is a Better Player: LLM against LLM",
        "abstract": "arXiv:2508.04720v1 Announce Type: new  Abstract: Adversarial board games, as a paradigmatic domain of strategic reasoning and intelligence, have long served as both a popular competitive activity and a benchmark for evaluating artificial intelligence (AI) systems. Building on this foundation, we propose an adversarial benchmarking framework to assess the comprehensive performance of Large Language Models (LLMs) through board games competition, compensating the limitation of data dependency of the mainstream Question-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a specialized evaluation platform that supports 5 widely played games and involves 20 LLM-driven players. The platform employs both the Elo rating system and a novel Performance Loop Graph (PLG) to quantitatively evaluate the technical capabilities of LLMs, while also capturing Positive Sentiment Score (PSS) throughout gameplay to assess mental fitness. The evaluation is structured as a round-robin tournament, enabling systematic comparison across players. Experimental results indicate that, despite technical differences, most LLMs remain optimistic about winning and losing, demonstrating greater adaptability to high-stress adversarial environments than humans. On the other hand, the complex relationship between cyclic wins and losses in PLGs exposes the instability of LLMs' skill play during games, warranting further explanation and exploration.",
        "arxiv_id": "2508.04720",
        "ARXIVID": "2508.04720",
        "COMMENT": "Does not match any specific criteria but discusses benchmarking LLMs in adversarial board games.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04818": {
        "authors": [
            "Mehrdad Moradi",
            "Marco Grasso",
            "Bianca Maria Colosimo",
            "Kamran Paynabar"
        ],
        "title": "Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models",
        "abstract": "arXiv:2508.04818v1 Announce Type: new  Abstract: Generative models have demonstrated significant success in anomaly detection and segmentation over the past decade. Recently, diffusion models have emerged as a powerful alternative, outperforming previous approaches such as GANs and VAEs. In typical diffusion-based anomaly detection, a model is trained on normal data, and during inference, anomalous images are perturbed to a predefined intermediate step in the forward diffusion process. The corresponding normal image is then reconstructed through iterative reverse sampling.   However, reconstruction-based approaches present three major challenges: (1) the reconstruction process is computationally expensive due to multiple sampling steps, making real-time applications impractical; (2) for complex or subtle patterns, the reconstructed image may correspond to a different normal pattern rather than the original input; and (3) Choosing an appropriate intermediate noise level is challenging because it is application-dependent and often assumes prior knowledge of anomalies, an assumption that does not hold in unsupervised settings.   We introduce Reconstruction-free Anomaly Detection with Attention-based diffusion models in Real-time (RADAR), which overcomes the limitations of reconstruction-based anomaly detection. Unlike current SOTA methods that reconstruct the input image, RADAR directly produces anomaly maps from the diffusion model, improving both detection accuracy and computational efficiency. We evaluate RADAR on real-world 3D-printed material and the MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and statistical machine learning models across all key metrics, including accuracy, precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on MVTec-AD and 13% on the 3D-printed material dataset compared to the next best model.   Code available at: https://github.com/mehrdadmoradi124/RADAR",
        "arxiv_id": "2508.04818",
        "ARXIVID": "2508.04818",
        "COMMENT": "Does not match any specific criteria. Focuses on anomaly detection using diffusion models, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05264": {
        "authors": [
            "Xiaoyang Zhang",
            "Zhen Hua",
            "Yakun Ju",
            "Wei Zhou",
            "Jun Liu",
            "Alex C. Kot"
        ],
        "title": "SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion",
        "abstract": "arXiv:2508.05264v1 Announce Type: new  Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.",
        "arxiv_id": "2508.05264",
        "ARXIVID": "2508.05264",
        "COMMENT": "Does not match any specific criteria. Focuses on infrared and visible image fusion using diffusion models, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04979": {
        "authors": [
            "Zheng Chen",
            "Mingde Zhou",
            "Jinpei Guo",
            "Jiale Yuan",
            "Yifei Ji",
            "Yulun Zhang"
        ],
        "title": "Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression",
        "abstract": "arXiv:2508.04979v1 Announce Type: new  Abstract: Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20$\\times$. Code is released at: https://github.com/zhengchen1999/SODEC.",
        "arxiv_id": "2508.04979",
        "ARXIVID": "2508.04979",
        "COMMENT": "Does not match any specific criteria. Focuses on image compression using diffusion models, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05271": {
        "authors": [
            "Xiaoyang Zhang",
            "Guodong Fan",
            "Guang-Yong Chen",
            "Zhen Hua",
            "Jinjiang Li",
            "Min Gan",
            "C. L. Philip Chen"
        ],
        "title": "Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection",
        "abstract": "arXiv:2508.05271v1 Announce Type: new  Abstract: Change detection in remote sensing imagery plays a vital role in various engineering applications, such as natural disaster monitoring, urban expansion tracking, and infrastructure management. Despite the remarkable progress of deep learning in recent years, most existing methods still rely on spatial-domain modeling, where the limited diversity of feature representations hinders the detection of subtle change regions. We observe that frequency-domain feature modeling particularly in the wavelet domain an amplify fine-grained differences in frequency components, enhancing the perception of edge changes that are challenging to capture in the spatial domain. Thus, we propose a method called Wavelet-Guided Dual-Frequency Encoding (WGDF). Specifically, we first apply Discrete Wavelet Transform (DWT) to decompose the input images into high-frequency and low-frequency components, which are used to model local details and global structures, respectively. In the high-frequency branch, we design a Dual-Frequency Feature Enhancement (DFFE) module to strengthen edge detail representation and introduce a Frequency-Domain Interactive Difference (FDID) module to enhance the modeling of fine-grained changes. In the low-frequency branch, we exploit Transformers to capture global semantic relationships and employ a Progressive Contextual Difference Module (PCDM) to progressively refine change regions, enabling precise structural semantic characterization. Finally, the high- and low-frequency features are synergistically fused to unify local sensitivity with global discriminability. Extensive experiments on multiple remote sensing datasets demonstrate that WGDF significantly alleviates edge ambiguity and achieves superior detection accuracy and robustness compared to state-of-the-art methods. The code will be available at https://github.com/boshizhang123/WGDF.",
        "arxiv_id": "2508.05271",
        "ARXIVID": "2508.05271",
        "COMMENT": "Does not match any specific criterion. Focuses on change detection in remote sensing imagery, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05465": {
        "authors": [
            "Lumin Chen",
            "Zhiying Wu",
            "Tianye Lei",
            "Xuexue Bai",
            "Ming Feng",
            "Yuxi Wang",
            "Gaofeng Meng",
            "Zhen Lei",
            "Hongbin Liu"
        ],
        "title": "F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery",
        "abstract": "arXiv:2508.05465v1 Announce Type: new  Abstract: Pituitary tumors often cause deformation or encapsulation of adjacent vital structures. Anatomical structure segmentation can provide surgeons with early warnings of regions that pose surgical risks, thereby enhancing the safety of pituitary surgery. However, pixel-level annotated video stream datasets for pituitary surgeries are extremely rare. To address this challenge, we introduce a new dataset for Pituitary Anatomy Segmentation (PAS). PAS comprises 7,845 time-coherent images extracted from 120 videos. To mitigate class imbalance, we apply data augmentation techniques that simulate the presence of surgical instruments in the training data. One major challenge in pituitary anatomy segmentation is the inconsistency in feature representation due to occlusions, camera motion, and surgical bleeding. By incorporating a Feature Fusion module, F2PASeg is proposed to refine anatomical structure segmentation by leveraging both high-resolution image features and deep semantic embeddings, enhancing robustness against intraoperative variations. Experimental results demonstrate that F2PASeg consistently segments critical anatomical structures in real time, providing a reliable solution for intraoperative pituitary surgery planning. Code: https://github.com/paulili08/F2PASeg.",
        "arxiv_id": "2508.05465",
        "ARXIVID": "2508.05465",
        "COMMENT": "Does not match any specific criterion. Focuses on anatomical segmentation in pituitary surgery, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05474": {
        "authors": [
            "Burak Can Kaplan",
            "Hugo Cesar De Castro Carneiro",
            "Stefan Wermter"
        ],
        "title": "Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?",
        "abstract": "arXiv:2508.05474v1 Announce Type: new  Abstract: Emotion recognition in conversations (ERC) focuses on identifying emotion shifts within interactions, representing a significant step toward advancing machine intelligence. However, ERC data remains scarce, and existing datasets face numerous challenges due to their highly biased sources and the inherent subjectivity of soft labels. Even though Large Language Models (LLMs) have demonstrated their quality in many affective tasks, they are typically expensive to train, and their application to ERC tasks--particularly in data generation--remains limited. To address these challenges, we employ a small, resource-efficient, and general-purpose LLM to synthesize ERC datasets with diverse properties, supplementing the three most widely used ERC benchmarks. We generate six novel datasets, with two tailored to enhance each benchmark. We evaluate the utility of these datasets to (1) supplement existing datasets for ERC classification, and (2) analyze the effects of label imbalance in ERC. Our experimental results indicate that ERC classifier models trained on the generated datasets exhibit strong robustness and consistently achieve statistically significant performance improvements on existing ERC benchmarks.",
        "arxiv_id": "2508.05474",
        "ARXIVID": "2508.05474",
        "COMMENT": "Does not match any specific criterion. Focuses on emotion recognition in conversations and dataset generation, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04846": {
        "authors": [
            "Mahdi Nazari Ashani",
            "Ali Asghar Alesheikh",
            "Saba Kazemi",
            "Kimya Kheirkhah",
            "Yasin Mohammadi",
            "Fatemeh Rezaie",
            "Amir Mahdi Manafi",
            "Hedieh Zarkesh"
        ],
        "title": "Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)",
        "abstract": "arXiv:2508.04846v1 Announce Type: new  Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to perform geospatial operations from natural language input, providing intuitive, intelligent, and hands-free interaction. However, most current solutions rely on cloud-based large language models (LLMs), which require continuous internet access and raise users' privacy and scalability issues due to centralized server processing. This study compares three approaches to enabling AWebGIS: (1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2) a semi-automated offline method using classical machine learning classifiers such as support vector machine and random forest; and (3) a fully autonomous offline (client-side) method based on a fine-tuned small language model (SLM), specifically T5-small model, executed in the client's web browser. The third approach, which leverages SLMs, achieved the highest accuracy among all methods, with an exact matching accuracy of 0.93, Levenshtein similarity of 0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L scores of 0.98. Crucially, this client-side computation strategy reduces the load on backend servers by offloading processing to the user's device, eliminating the need for server-based inference. These results highlight the feasibility of browser-executable models for AWebGIS solutions.",
        "arxiv_id": "2508.04846",
        "ARXIVID": "2508.04846",
        "COMMENT": "Does not match any specific criterion. Focuses on small language models for geographical information systems, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04998": {
        "authors": [
            "Rui Zhi",
            "Zhen Yang",
            "Haiyang Zhang"
        ],
        "title": "Attribute Guidance With Inherent Pseudo-label For Occluded Person Re-identification",
        "abstract": "arXiv:2508.04998v1 Announce Type: new  Abstract: Person re-identification (Re-ID) aims to match person images across different camera views, with occluded Re-ID addressing scenarios where pedestrians are partially visible. While pre-trained vision-language models have shown effectiveness in Re-ID tasks, they face significant challenges in occluded scenarios by focusing on holistic image semantics while neglecting fine-grained attribute information. This limitation becomes particularly evident when dealing with partially occluded pedestrians or when distinguishing between individuals with subtle appearance differences. To address this limitation, we propose Attribute-Guide ReID (AG-ReID), a novel framework that leverages pre-trained models' inherent capabilities to extract fine-grained semantic attributes without additional data or annotations. Our framework operates through a two-stage process: first generating attribute pseudo-labels that capture subtle visual characteristics, then introducing a dual-guidance mechanism that combines holistic and fine-grained attribute information to enhance image feature extraction. Extensive experiments demonstrate that AG-ReID achieves state-of-the-art results on multiple widely-used Re-ID datasets, showing significant improvements in handling occlusions and subtle attribute differences while maintaining competitive performance on standard Re-ID scenarios.",
        "arxiv_id": "2508.04998",
        "ARXIVID": "2508.04998",
        "COMMENT": "Does not match any specific criterion. Focuses on person re-identification with occlusions, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05636": {
        "authors": [
            "Mohammed Talha Alam",
            "Fahad Shamshad",
            "Fakhri Karray",
            "Karthik Nandakumar"
        ],
        "title": "FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space Mixing",
        "abstract": "arXiv:2508.05636v1 Announce Type: new  Abstract: Advancements in face recognition (FR) technologies have amplified privacy concerns, necessitating methods that protect identity while maintaining recognition utility. Existing face anonymization methods typically focus on obscuring identity but fail to meet the requirements of biometric template protection, including revocability, unlinkability, and irreversibility. We propose FaceAnonyMixer, a cancelable face generation framework that leverages the latent space of a pre-trained generative model to synthesize privacy-preserving face images. The core idea of FaceAnonyMixer is to irreversibly mix the latent code of a real face image with a synthetic code derived from a revocable key. The mixed latent code is further refined through a carefully designed multi-objective loss to satisfy all cancelable biometric requirements. FaceAnonyMixer is capable of generating high-quality cancelable faces that can be directly matched using existing FR systems without requiring any modifications. Extensive experiments on benchmark datasets demonstrate that FaceAnonyMixer delivers superior recognition accuracy while providing significantly stronger privacy protection, achieving over an 11% gain on commercial API compared to recent cancelable biometric methods. Code is available at: https://github.com/talha-alam/faceanonymixer.",
        "arxiv_id": "2508.05636",
        "ARXIVID": "2508.05636",
        "COMMENT": "Does not match any specific criterion. Focuses on privacy-preserving face generation, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05084": {
        "authors": [
            "Yuxiang Xiao",
            "Yang Hu",
            "Bin Li",
            "Tianyang Zhang",
            "Zexi Li",
            "Huazhu Fu",
            "Jens Rittscher",
            "Kaixiang Yang"
        ],
        "title": "AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models",
        "abstract": "arXiv:2508.05084v1 Announce Type: new  Abstract: Pathology foundation models (PFMs) have demonstrated strong representational capabilities through self-supervised pre-training on large-scale, unannotated histopathology image datasets. However, their diverse yet opaque pretraining contexts, shaped by both data-related and structural/training factors, introduce latent biases that hinder generalisability and transparency in downstream applications. In this paper, we propose AdaFusion, a novel prompt-guided inference framework that, to our knowledge, is among the very first to dynamically integrate complementary knowledge from multiple PFMs. Our method compresses and aligns tile-level features from diverse models and employs a lightweight attention mechanism to adaptively fuse them based on tissue phenotype context. We evaluate AdaFusion on three real-world benchmarks spanning treatment response prediction, tumour grading, and spatial gene expression inference. Our approach consistently surpasses individual PFMs across both classification and regression tasks, while offering interpretable insights into each model's biosemantic specialisation. These results highlight AdaFusion's ability to bridge heterogeneous PFMs, achieving both enhanced performance and interpretability of model-specific inductive biases.",
        "arxiv_id": "2508.05084",
        "ARXIVID": "2508.05084",
        "COMMENT": "Does not match any specific criterion. Focuses on pathology foundation models and adaptive fusion, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05432": {
        "authors": [
            "Krzysztof Janowicz",
            "Zilong Liu",
            "Gengchen Mai",
            "Zhangyu Wang",
            "Ivan Majic",
            "Alexandra Fortacz",
            "Grant McKenzie",
            "Song Gao"
        ],
        "title": "Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI",
        "abstract": "arXiv:2508.05432v1 Announce Type: new  Abstract: AI (super) alignment describes the challenge of ensuring (future) AI systems behave in accordance with societal norms and goals. While a quickly evolving literature is addressing biases and inequalities, the geographic variability of alignment remains underexplored. Simply put, what is considered appropriate, truthful, or legal can differ widely across regions due to cultural norms, political realities, and legislation. Alignment measures applied to AI/ML workflows can sometimes produce outcomes that diverge from statistical realities, such as text-to-image models depicting balanced gender ratios in company leadership despite existing imbalances. Crucially, some model outputs are globally acceptable, while others, e.g., questions about Kashmir, depend on knowing the user's location and their context. This geographic sensitivity is not new. For instance, Google Maps renders Kashmir's borders differently based on user location. What is new is the unprecedented scale and automation with which AI now mediates knowledge, expresses opinions, and represents geographic reality to millions of users worldwide, often with little transparency about how context is managed. As we approach Agentic AI, the need for spatio-temporally aware alignment, rather than one-size-fits-all approaches, is increasingly urgent. This paper reviews key geographic research problems, suggests topics for future work, and outlines methods for assessing alignment sensitivity.",
        "arxiv_id": "2508.05432",
        "ARXIVID": "2508.05432",
        "COMMENT": "Does not match any specific criterion but discusses geographic alignment in AI, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.05016": {
        "authors": [
            "Shushi Wang",
            "Chunyi Li",
            "Zicheng Zhang",
            "Han Zhou",
            "Wei Dong",
            "Jun Chen",
            "Guangtao Zhai",
            "Xiaohong Liu"
        ],
        "title": "AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content",
        "abstract": "arXiv:2508.05016v1 Announce Type: new  Abstract: AI-based image enhancement techniques have been widely adopted in various visual applications, significantly improving the perceptual quality of user-generated content (UGC). However, the lack of specialized quality assessment models has become a significant limiting factor in this field, limiting user experience and hindering the advancement of enhancement methods. While perceptual quality assessment methods have shown strong performance on UGC and AIGC individually, their effectiveness on AI-enhanced UGC (AI-UGC) which blends features from both, remains largely unexplored. To address this gap, we construct AU-IQA, a benchmark dataset comprising 4,800 AI-UGC images produced by three representative enhancement types which include super-resolution, low-light enhancement, and denoising. On this dataset, we further evaluate a range of existing quality assessment models, including traditional IQA methods and large multimodal models. Finally, we provide a comprehensive analysis of how well current approaches perform in assessing the perceptual quality of AI-UGC. The access link to the AU-IQA is https://github.com/WNNGGU/AU-IQA-Dataset.",
        "arxiv_id": "2508.05016",
        "ARXIVID": "2508.05016",
        "COMMENT": "Does not match any specific criteria. Focuses on perceptual quality assessment of AI-enhanced user-generated content, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}