{
    "2506.09985": {
        "authors": [
            "Mido Assran",
            "Adrien Bardes",
            "David Fan",
            "Quentin Garrido",
            "Russell Howes",
            "Mojtaba",
            "Komeili",
            "Matthew Muckley",
            "Ammar Rizvi",
            "Claire Roberts",
            "Koustuv Sinha",
            "Artem Zholus",
            "Sergio Arnaud",
            "Abha Gejji",
            "Ada Martin",
            "Francois Robert Hogan",
            "Daniel Dugas",
            "Piotr Bojanowski",
            "Vasil Khalidov",
            "Patrick Labatut",
            "Francisco Massa",
            "Marc Szafraniec",
            "Kapil Krishnakumar",
            "Yong Li",
            "Xiaodong Ma",
            "Sarath Chandar",
            "Franziska Meier",
            "Yann LeCun",
            "Michael Rabbat",
            "Nicolas Ballas"
        ],
        "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",
        "abstract": "arXiv:2506.09985v1 Announce Type: new  Abstract: A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.",
        "arxiv_id": "2506.09985",
        "ARXIVID": "2506.09985",
        "COMMENT": "Matches criteria 2 and 6 as it explores video understanding and planning with a self-supervised video model.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.09935": {
        "authors": [
            "Jiangyong Huang",
            "Xiaojian Ma",
            "Xiongkun Linghu",
            "Yue Fan",
            "Junchao He",
            "Wenxin Tan",
            "Qing Li",
            "Song-Chun Zhu",
            "Yixin Chen",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation",
        "abstract": "arXiv:2506.09935v1 Announce Type: new  Abstract: Developing 3D-VL generalists capable of understanding 3D scenes and following natural language instructions to perform a wide range of tasks has been a long-standing goal in the 3D-VL community. Despite recent progress, 3D-VL models still lag behind their 2D counterparts in capability and robustness, falling short of the generalist standard. A key obstacle to developing 3D-VL generalists lies in data scalability, hindered by the lack of an efficient scene representation. We propose LEO-VL, a 3D-VL model built upon condensed feature grid (CFG), an efficient scene representation that bridges 2D perception and 3D spatial structure while significantly reducing token overhead. This efficiency unlocks large-scale training towards 3D-VL generalist, for which we curate over 700k high-quality 3D-VL data spanning four domains of real-world indoor scenes and five tasks such as captioning and dialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA benchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the efficiency of our representation, the importance of task and scene diversity, and the validity of our data curation principle. Furthermore, we introduce SceneDPO, a novel post-training objective that enhances the robustness of 3D-VL models. We hope our findings contribute to the advancement of scalable and robust 3D-VL generalists.",
        "arxiv_id": "2506.09935",
        "ARXIVID": "2506.09935",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on 3D vision-language models, introduces a new representation (CFG), and scales training for 3D-VL tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09965": {
        "authors": [
            "Junfei Wu",
            "Jian Guan",
            "Kaituo Feng",
            "Qiang Liu",
            "Shu Wu",
            "Liang Wang",
            "Wei Wu",
            "Tieniu Tan"
        ],
        "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing",
        "abstract": "arXiv:2506.09965v1 Announce Type: new  Abstract: As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%.",
        "arxiv_id": "2506.09965",
        "ARXIVID": "2506.09965",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning in vision-language models with novel visual drawing techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09839": {
        "authors": [
            "Chen Gao",
            "Liankai Jin",
            "Xingyu Peng",
            "Jiazhao Zhang",
            "Yue Deng",
            "Annan Li",
            "He Wang",
            "Si Liu"
        ],
        "title": "OctoNav: Towards Generalist Embodied Navigation",
        "abstract": "arXiv:2506.09839v1 Announce Type: new  Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods.",
        "arxiv_id": "2506.09839",
        "ARXIVID": "2506.09839",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a generalist navigation benchmark and method for embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09079": {
        "authors": [
            "Xinlong Chen",
            "Yuanxing Zhang",
            "Yushuo Guan",
            "Bohan Zeng",
            "Yang Shi",
            "Sihan Yang",
            "Pengfei Wan",
            "Qiang Liu",
            "Liang Wang",
            "Tieniu Tan"
        ],
        "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks",
        "abstract": "arXiv:2506.09079v1 Announce Type: new  Abstract: Recent advancements in multimodal large language models have successfully extended the Reason-Then-Respond paradigm to image-based reasoning, yet video-based reasoning remains an underdeveloped frontier, primarily due to the scarcity of high-quality reasoning-oriented data and effective training methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA, two novel datasets specifically designed to stimulate the model's advanced video understanding and reasoning abilities. DarkEventinfer presents videos with masked event segments, requiring models to infer the obscured content based on contextual video cues. MixVidQA, on the other hand, presents interleaved video sequences composed of two distinct clips, challenging models to isolate and reason about one while disregarding the other. Leveraging these carefully curated training samples together with reinforcement learning guided by diverse reward functions, we develop VersaVid-R1, the first versatile video understanding and reasoning model under the Reason-Then-Respond paradigm capable of handling multiple-choice and open-ended question answering, as well as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1 significantly outperforms existing models across a broad spectrum of benchmarks, covering video general understanding, cognitive reasoning, and captioning tasks.",
        "arxiv_id": "2506.09079",
        "ARXIVID": "2506.09079",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces datasets and a model for advanced video reasoning and understanding tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09995": {
        "authors": [
            "Yuanpeng Tu",
            "Hao Luo",
            "Xi Chen",
            "Xiang Bai",
            "Fan Wang",
            "Hengshuang Zhao"
        ],
        "title": "PlayerOne: Egocentric World Simulator",
        "abstract": "arXiv:2506.09995v1 Announce Type: new  Abstract: We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.",
        "arxiv_id": "2506.09995",
        "ARXIVID": "2506.09995",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces an egocentric world simulator for immersive exploration, relevant to embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09344": {
        "authors": [
            "Inclusion AI",
            "Biao Gong",
            "Cheng Zou",
            "Chuanyang Zheng",
            "Chunluan Zhou",
            "Canxiang Yan",
            "Chunxiang Jin",
            "Chunjie Shen",
            "Dandan Zheng",
            "Fudong Wang",
            "Furong Xu",
            "GuangMing Yao",
            "Jun Zhou",
            "Jingdong Chen",
            "Jianxin Sun",
            "Jiajia Liu",
            "Jianjiang Zhu",
            "Jun Peng",
            "Kaixiang Ji",
            "Kaiyou Song",
            "Kaimeng Ren",
            "Libin Wang",
            "Lixiang Ru",
            "Lele Xie",
            "Longhua Tan",
            "Lyuxin Xue",
            "Lan Wang",
            "Mochen Bai",
            "Ning Gao",
            "Pei Chen",
            "Qingpei Guo",
            "Qinglong Zhang",
            "Qiang Xu",
            "Rui Liu",
            "Ruijie Xiong",
            "Sirui Gao",
            "Tinghao Liu",
            "Taisong Li",
            "Weilong Chai",
            "Xinyu Xiao",
            "Xiaomei Wang",
            "Xiaoxue Chen",
            "Xiao Lu",
            "Xiaoyu Li",
            "Xingning Dong",
            "Xuzheng Yu",
            "Yi Yuan",
            "Yuting Gao",
            "Yunxiao Sun",
            "Yipeng Chen",
            "Yifei Wu",
            "Yongjie Lyu",
            "Ziping Ma",
            "Zipeng Feng",
            "Zhijiang Fang",
            "Zhihao Qiu",
            "Ziyuan Huang",
            "Zhengyu He"
        ],
        "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
        "abstract": "arXiv:2506.09344v1 Announce Type: new  Abstract: We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.",
        "arxiv_id": "2506.09344",
        "ARXIVID": "2506.09344",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it introduces a unified multimodal model for perception and generation across multiple modalities.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09952": {
        "authors": [
            "Ziyi Wang",
            "Yanran Zhang",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting",
        "abstract": "arXiv:2506.09952v1 Announce Type: new  Abstract: The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
        "arxiv_id": "2506.09952",
        "ARXIVID": "2506.09952",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a unified pre-training method for 3D point cloud models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09082": {
        "authors": [
            "Zheda Mai",
            "Arpita Chowdhury",
            "Zihe Wang",
            "Sooyoung Jeon",
            "Lemeng Wang",
            "Jiacheng Hou",
            "Jihyung Kil",
            "Wei-Lun Chao"
        ],
        "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models",
        "abstract": "arXiv:2506.09082v1 Announce Type: new  Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation. A common approach pairs VFMs with large language models (LLMs) as general-purpose heads, followed by evaluation on broad Visual Question Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i) the instruction tuning data may not align with VQA test distributions, meaning a wrong prediction can stem from such data mismatch rather than a VFM' visual shortcomings; (ii) VQA benchmarks often require multiple visual abilities, making it hard to tell whether errors stem from lacking all required abilities or just a single critical one. To address these gaps, we introduce AVA-Bench, the first benchmark that explicitly disentangles 14 Atomic Visual Abilities (AVAs) -- foundational skills like localization, depth estimation, and spatial understanding that collectively support complex visual reasoning tasks. By decoupling AVAs and matching training and test distributions within each, AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench to leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM selection from educated guesswork into principled engineering. Notably, we find that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours by 8x, enabling more efficient evaluation. By offering a comprehensive and transparent benchmark, we hope AVA-Bench lays the foundation for the next generation of VFMs.",
        "arxiv_id": "2506.09082",
        "ARXIVID": "2506.09082",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces AVA-Bench, a benchmark for evaluating vision foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09518": {
        "authors": [
            "Jianing Chen",
            "Zehao Li",
            "Yujun Cai",
            "Hao Jiang",
            "Chengxuan Qian",
            "Juyuan Kang",
            "Shuqin Gao",
            "Honglong Zhao",
            "Tianlu Mao",
            "Yucheng Zhang"
        ],
        "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene",
        "abstract": "arXiv:2506.09518v1 Announce Type: new  Abstract: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppresses redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency.",
        "arxiv_id": "2506.09518",
        "ARXIVID": "2506.09518",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for dynamic 3D scene reconstruction, addressing underexplored challenges in motion representation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.09229": {
        "authors": [
            "Sungwon Hwang",
            "Hyojin Jang",
            "Kinam Kim",
            "Minho Park",
            "Jaegul choo"
        ],
        "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models",
        "abstract": "arXiv:2506.09229v1 Announce Type: new  Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate videos that reflect specific attributes of training data presents notable challenges, yet remains underexplored despite its practical importance. Meanwhile, recent work such as Representation Alignment (REPA) has shown promise in improving the convergence and quality of DiT-based image diffusion models by aligning, or assimilating, its internal hidden states with external pretrained visual features, suggesting its potential for VDM fine-tuning. In this work, we first propose a straightforward adaptation of REPA for VDMs and empirically show that, while effective for convergence, it is suboptimal in preserving semantic consistency across frames. To address this limitation, we introduce Cross-frame Representation Alignment (CREPA), a novel regularization technique that aligns hidden states of a frame with external features from neighboring frames. Empirical evaluations on large-scale VDMs, including CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA. We further validate CREPA across diverse datasets with varying attributes, confirming its broad applicability. Project page: https://crepavideo.github.io",
        "arxiv_id": "2506.09229",
        "ARXIVID": "2506.09229",
        "COMMENT": "Matches criterion 6 as it introduces a novel method for video understanding through cross-frame representation alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.09943": {
        "authors": [
            "Aaron Foss",
            "Chloe Evans",
            "Sasha Mitts",
            "Koustuv Sinha",
            "Ammar Rizvi",
            "Justine T. Kao"
        ],
        "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models",
        "abstract": "arXiv:2506.09943v1 Announce Type: new  Abstract: We introduce CausalVQA, a benchmark dataset for video question answering (VQA) composed of question-answer pairs that probe models' understanding of causality in the physical world. Existing VQA benchmarks either tend to focus on surface perceptual understanding of real-world videos, or on narrow physical reasoning questions created using simulation environments. CausalVQA fills an important gap by presenting challenging questions that are grounded in real-world scenarios, while focusing on models' ability to predict the likely outcomes of different actions and events through five question types: counterfactual, hypothetical, anticipation, planning and descriptive. We designed quality control mechanisms that prevent models from exploiting trivial shortcuts, requiring models to base their answers on deep visual understanding instead of linguistic cues. We find that current frontier multimodal models fall substantially below human performance on the benchmark, especially on anticipation and hypothetical questions. This highlights a challenge for current systems to leverage spatial-temporal reasoning, understanding of physical principles, and comprehension of possible alternatives to make accurate predictions in real-world settings.",
        "arxiv_id": "2506.09943",
        "ARXIVID": "2506.09943",
        "COMMENT": "Matches criterion 6 as it introduces a new benchmark (CausalVQA) for video question answering with a focus on causality and spatial-temporal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.09385": {
        "authors": [
            "Jialong Zuo",
            "Yongtai Deng",
            "Mengdan Tan",
            "Rui Jin",
            "Dongyue Wu",
            "Nong Sang",
            "Liang Pan",
            "Changxin Gao"
        ],
        "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model",
        "abstract": "arXiv:2506.09385v1 Announce Type: new  Abstract: In real-word scenarios, person re-identification (ReID) expects to identify a person-of-interest via the descriptive query, regardless of whether the query is a single modality or a combination of multiple modalities. However, existing methods and datasets remain constrained to limited modalities, failing to meet this requirement. Therefore, we investigate a new challenging problem called Omni Multi-modal Person Re-identification (OM-ReID), which aims to achieve effective retrieval with varying multi-modal queries. To address dataset scarcity, we construct ORBench, the first high-quality multi-modal dataset comprising 1,000 unique identities across five modalities: RGB, infrared, color pencil, sketch, and textual description. This dataset also has significant superiority in terms of diversity, such as the painting perspectives and textual information. It could serve as an ideal platform for follow-up investigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal learning framework for person ReID. It enables synergistic fusion and cross-modal alignment of arbitrary modality combinations in a single model, with a unified encoding and multi-expert routing mechanism proposed. Extensive experiments verify the advancement and practicality of our ORBench. A wide range of possible models have been evaluated and compared on it, and our proposed ReID5o model gives the best performance. The dataset and code will be made publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.",
        "arxiv_id": "2506.09385",
        "ARXIVID": "2506.09385",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a multi-modal person re-identification framework (ReID5o) and a new dataset (ORBench) for combining multiple modalities, including textual descriptions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09650": {
        "authors": [
            "Kunyu Peng",
            "Junchao Huang",
            "Xiangsheng Huang",
            "Di Wen",
            "Junwei Zheng",
            "Yufan Chen",
            "Kailun Yang",
            "Jiamin Wu",
            "Chongqing Hao",
            "Rainer Stiefelhagen"
        ],
        "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios",
        "abstract": "arXiv:2506.09650v1 Announce Type: new  Abstract: Action segmentation is a core challenge in high-level video understanding, aiming to partition untrimmed videos into segments and assign each a label from a predefined action set. Existing methods primarily address single-person activities with fixed action sequences, overlooking multi-person scenarios. In this work, we pioneer textual reference-guided human action segmentation in multi-person settings, where a textual description specifies the target person for segmentation. We introduce the first dataset for Referring Human Action Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137 fine-grained actions with 33h video data, together with textual descriptions for this new task. Benchmarking existing action recognition methods on RHAS133 using VLM-based feature extractors reveals limited performance and poor aggregation of visual cues for the target person. To address this, we propose a holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF, leveraging a novel cross-input gate attentional xLSTM to enhance holistic-partial long-range reasoning and a novel Fourier condition to introduce more fine-grained control to improve the action segmentation generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse evaluation settings. The code is available at https://github.com/KPeng9510/HopaDIFF.git.",
        "arxiv_id": "2506.09650",
        "ARXIVID": "2506.09650",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel task (Referring Human Action Segmentation) and a new dataset (RHAS133) for video-based tasks, along with a method (HopaDIFF) for action segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09984": {
        "authors": [
            "Zhenzhi Wang",
            "Jiaqi Yang",
            "Jianwen Jiang",
            "Chao Liang",
            "Gaojie Lin",
            "Zerong Zheng",
            "Ceyuan Yang",
            "Dahua Lin"
        ],
        "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions",
        "abstract": "arXiv:2506.09984v1 Announce Type: new  Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.",
        "arxiv_id": "2506.09984",
        "ARXIVID": "2506.09984",
        "COMMENT": "Matches criterion 5 as it integrates image, video, and audio modalities for multi-concept human animation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09981": {
        "authors": [
            "Jiazhi Yang",
            "Kashyap Chitta",
            "Shenyuan Gao",
            "Long Chen",
            "Yuqian Shao",
            "Xiaosong Jia",
            "Hongyang Li",
            "Andreas Geiger",
            "Xiangyu Yue",
            "Li Chen"
        ],
        "title": "ReSim: Reliable World Simulation for Autonomous Driving",
        "abstract": "arXiv:2506.09981v1 Announce Type: new  Abstract: How can we reliably simulate future driving scenarios under a wide range of ego driving behaviors? Recent driving world models, developed exclusively on real-world driving data composed mainly of safe expert trajectories, struggle to follow hazardous or non-expert behaviors, which are rare in such data. This limitation restricts their applicability to tasks such as policy evaluation. In this work, we address this challenge by enriching real-world human demonstrations with diverse non-expert data collected from a driving simulator (e.g., CARLA), and building a controllable world model trained on this heterogeneous corpus. Starting with a video generator featuring a diffusion transformer architecture, we devise several strategies to effectively integrate conditioning signals and improve prediction controllability and fidelity. The resulting model, ReSim, enables Reliable Simulation of diverse open-world driving scenarios under various actions, including hazardous non-expert ones. To close the gap between high-fidelity simulation and applications that require reward signals to judge different actions, we introduce a Video2Reward module that estimates a reward from ReSim's simulated future. Our ReSim paradigm achieves up to 44% higher visual fidelity, improves controllability for both expert and non-expert actions by over 50%, and boosts planning and policy selection performance on NAVSIM by 2% and 25%, respectively.",
        "arxiv_id": "2506.09981",
        "ARXIVID": "2506.09981",
        "COMMENT": "Matches criterion 3 as it introduces a new simulator (ReSim) for embodied AI with novel methods for controllable world modeling and policy evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09883": {
        "authors": [
            "Seonho Lee",
            "Jiho Choi",
            "Inha Kang",
            "Jiwook Kim",
            "Junsung Park",
            "Hyunjung Shim"
        ],
        "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation",
        "abstract": "arXiv:2506.09883v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse visual and linguistic tasks, yet they remain fundamentally limited in their understanding of 3D spatial structures. We propose Geometric Distillation, a lightweight, annotation-free fine-tuning framework that injects human-inspired geometric cues into pretrained VLMs without modifying their architecture. By distilling (1) sparse correspondences, (2) relative depth relations, and (3) dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R, VGGT), our method shapes representations to be geometry-aware while remaining compatible with natural image-text inputs. Through extensive evaluations on 3D vision-language reasoning and 3D perception benchmarks, our method consistently outperforms prior approaches, achieving improved 3D spatial reasoning with significantly lower computational cost. Our work demonstrates a scalable and efficient path to bridge 2D-trained VLMs with 3D understanding, opening up wider use in spatially grounded multimodal tasks.",
        "arxiv_id": "2506.09883",
        "ARXIVID": "2506.09883",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on fine-tuning vision-language models with geometric distillation for 3D spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09350": {
        "authors": [
            "Shanchuan Lin",
            "Ceyuan Yang",
            "Hao He",
            "Jianwen Jiang",
            "Yuxi Ren",
            "Xin Xia",
            "Yang Zhao",
            "Xuefeng Xiao",
            "Lu Jiang"
        ],
        "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation",
        "abstract": "arXiv:2506.09350v1 Announce Type: new  Abstract: Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2",
        "arxiv_id": "2506.09350",
        "ARXIVID": "2506.09350",
        "COMMENT": "Matches criterion 6 (Video Understanding) and criterion 5 (Integration of Image/Video and Large Language Models) due to its focus on real-time interactive video generation and novel training strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09427": {
        "authors": [
            "Yukang Feng",
            "Jianwen Sun",
            "Chuanhao Li",
            "Zizhen Li",
            "Jiaxin Ai",
            "Fanrui Zhang",
            "Yifan Chang",
            "Sizhuo Zhou",
            "Shenglin Zhang",
            "Yu Dai",
            "Kaipeng Zhang"
        ],
        "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation",
        "abstract": "arXiv:2506.09427v1 Announce Type: new  Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.   Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.   Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.",
        "arxiv_id": "2506.09427",
        "ARXIVID": "2506.09427",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 5 (Integration of Image/Video and Large Language Models) due to the focus on interleaved image-text generation and evaluation for multimodal systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09748": {
        "authors": [
            "Xiangkai Zhang",
            "Xiang Zhou",
            "Mao Chen",
            "Yuchen Lu",
            "Xu Yang",
            "Zhiyong Liu"
        ],
        "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints",
        "abstract": "arXiv:2506.09748v1 Announce Type: new  Abstract: Absolute localization, aiming to determine an agent's location with respect to a global reference, is crucial for unmanned aerial vehicles (UAVs) in various applications, but it becomes challenging when global navigation satellite system (GNSS) signals are unavailable. Vision-based absolute localization methods, which locate the current view of the UAV in a reference satellite map to estimate its position, have become popular in GNSS-denied scenarios. However, existing methods mostly rely on traditional and low-level image matching, suffering from difficulties due to significant differences introduced by cross-source discrepancies and temporal variations. To overcome these limitations, in this paper, we introduce a hierarchical cross-source image matching method designed for UAV absolute localization, which integrates a semantic-aware and structure-constrained coarse matching module with a lightweight fine-grained matching module. Specifically, in the coarse matching module, semantic features derived from a vision foundation model first establish region-level correspondences under semantic and structural constraints. Then, the fine-grained matching module is applied to extract fine features and establish pixel-level correspondences. Building upon this, a UAV absolute visual localization pipeline is constructed without any reliance on relative localization techniques, mainly by employing an image retrieval module before the proposed hierarchical image matching modules. Experimental evaluations on public benchmark datasets and a newly introduced CS-UAV dataset demonstrate superior accuracy and robustness of the proposed method under various challenging conditions, confirming its effectiveness.",
        "arxiv_id": "2506.09748",
        "ARXIVID": "2506.09748",
        "COMMENT": "Matches criteria 1 (Spatial Intelligence and Embodied Agents) as it focuses on hierarchical image matching for UAV localization, leveraging semantic and structural constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09445": {
        "authors": [
            "Ayush Gupta",
            "Anirban Roy",
            "Rama Chellappa",
            "Nathaniel D. Bastian",
            "Alvaro Velasquez",
            "Susmit Jha"
        ],
        "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision",
        "abstract": "arXiv:2506.09445v1 Announce Type: new  Abstract: We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks.",
        "arxiv_id": "2506.09445",
        "ARXIVID": "2506.09445",
        "COMMENT": "Matches criteria 6 (Video Understanding) as it addresses video question answering with temporal grounding, focusing on weak supervision and open-ended QA.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09987": {
        "authors": [
            "Benno Krojer",
            "Mojtaba Komeili",
            "Candace Ross",
            "Quentin Garrido",
            "Koustuv Sinha",
            "Nicolas Ballas",
            "Mahmoud Assran"
        ],
        "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs",
        "abstract": "arXiv:2506.09987v1 Announce Type: new  Abstract: Existing benchmarks for assessing the spatio-temporal understanding and reasoning abilities of video language models are susceptible to score inflation due to the presence of shortcut solutions based on superficial visual or textual cues. This paper mitigates the challenges in accurately assessing model performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple shortcut-aware video QA benchmark for assessing the physical understanding of video language models. The benchmark is comprised of 55K high-quality multiple-choice video QA examples focusing on physical world understanding. Examples are curated from nine video data sources, spanning first-person egocentric and exocentric videos, robotic interaction data, and cognitive science intuitive physics benchmarks. To mitigate shortcut solutions that rely on superficial visual or textual cues and biases, each sample in MVP has a minimal-change pair -- a visually similar video accompanied by an identical question but an opposing answer. To answer a question correctly, a model must provide correct answers for both examples in the minimal-change pair; as such, models that solely rely on visual or textual biases would achieve below random performance. Human performance on MVP is 92.9\\%, while the best open-source state-of-the-art video-language model achieves 40.2\\% compared to random performance at 25\\%.",
        "arxiv_id": "2506.09987",
        "ARXIVID": "2506.09987",
        "COMMENT": "Matches criteria 6 (Video Understanding) as it introduces a shortcut-aware video QA benchmark for physical understanding, focusing on spatio-temporal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09885": {
        "authors": [
            "Haoru Wang",
            "Kai Ye",
            "Yangyan Li",
            "Wenzheng Chen",
            "Baoquan Chen"
        ],
        "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge",
        "abstract": "arXiv:2506.09885v1 Announce Type: new  Abstract: We consider the problem of generalizable novel view synthesis (NVS), which aims to generate photorealistic novel views from sparse or even unposed 2D images without per-scene optimization. This task remains fundamentally challenging, as it requires inferring 3D structure from incomplete and ambiguous 2D observations. Early approaches typically rely on strong 3D knowledge, including architectural 3D inductive biases (e.g., embedding explicit 3D representations, such as NeRF or 3DGS, into network design) and ground-truth camera poses for both input and target views. While recent efforts have sought to reduce the 3D inductive bias or the dependence on known camera poses of input views, critical questions regarding the role of 3D knowledge and the necessity of circumventing its use remain under-explored. In this work, we conduct a systematic analysis on the 3D knowledge and uncover a critical trend: the performance of methods that requires less 3D knowledge accelerates more as data scales, eventually achieving performance on par with their 3D knowledge-driven counterparts, which highlights the increasing importance of reducing dependence on 3D knowledge in the era of large-scale data. Motivated by and following this trend, we propose a novel NVS framework that minimizes 3D inductive bias and pose dependence for both input and target views. By eliminating this 3D knowledge, our method fully leverages data scaling and learns implicit 3D awareness directly from sparse 2D images, without any 3D inductive bias or pose annotation during training. Extensive experiments demonstrate that our model generates photorealistic and 3D-consistent novel views, achieving even comparable performance with methods that rely on posed inputs, thereby validating the feasibility and effectiveness of our data-centric paradigm. Project page: https://pku-vcl-geometry.github.io/Less3Depend/ .",
        "arxiv_id": "2506.09885",
        "ARXIVID": "2506.09885",
        "COMMENT": "Matches criteria 6 (Video Understanding) as it focuses on novel view synthesis from sparse images, which involves understanding 3D structure and video-based tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2506.09881": {
        "authors": [
            "Siyu Chen",
            "Ting Han",
            "Chengzheng Fu",
            "Changshe Zhang",
            "Chaolei Wang",
            "Jinhe Su",
            "Guorong Cai",
            "Meiliu Wu"
        ],
        "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation",
        "abstract": "arXiv:2506.09881v1 Announce Type: new  Abstract: Open-Vocabulary semantic segmentation (OVSS) and domain generalization in semantic segmentation (DGSS) highlight a subtle complementarity that motivates Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS aims to generate pixel-level masks for unseen categories while maintaining robustness across unseen domains, a critical capability for real-world scenarios such as autonomous driving in adverse conditions. We introduce Vireo, a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS and DGSS for the first time. Vireo builds upon the frozen Visual Foundation Models (VFMs) and incorporates scene geometry via Depth VFMs to extract domain-invariant structural features. To bridge the gap between visual and textual modalities under domain shift, we propose three key components: (1) GeoText Prompts, which align geometric features with language cues and progressively refine VFM encoder representations; (2) Coarse Mask Prior Embedding (CMPE) for enhancing gradient flow for faster convergence and stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH), which fuses refined structural and semantic features for robust prediction. Comprehensive evaluation on these components demonstrates the effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art performance and surpasses existing methods by a large margin in both domain generalization and open-vocabulary recognition, offering a unified and scalable solution for robust visual understanding in diverse and dynamic environments. Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.",
        "arxiv_id": "2506.09881",
        "ARXIVID": "2506.09881",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) as it builds upon Visual Foundation Models and integrates depth for domain-generalized semantic segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09565": {
        "authors": [
            "Qijing Li",
            "Jingxiang Sun",
            "Liang An",
            "Zhaoqi Su",
            "Hongwen Zhang",
            "Yebin Liu"
        ],
        "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields",
        "abstract": "arXiv:2506.09565v1 Announce Type: new  Abstract: Holistic 3D scene understanding, which jointly models geometry, appearance, and semantics, is crucial for applications like augmented reality and robotic interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM) are limited to extracting language-based semantics from scenes, failing to achieve holistic scene comprehension. Additionally, they suffer from low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene optimization methods rely on dense input views, which reduces practicality and increases complexity during deployment. In this paper, we propose SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which unifies 3D Gaussians with latent semantic attributes for joint geometry-appearance-semantics modeling. To predict the semantic anisotropic Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a cost volume representation that stores cross-view feature similarities, enhancing coherent and accurate scene comprehension. Leveraging a two-stage distillation framework, SemanticSplat reconstructs a holistic multi-modal semantic feature field from sparse-view images. Experiments demonstrate the effectiveness of our method for 3D scene understanding tasks like promptable and open-vocabulary segmentation. Video results are available at https://semanticsplat.github.io.",
        "arxiv_id": "2506.09565",
        "ARXIVID": "2506.09565",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on holistic 3D scene understanding with semantic and spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09736": {
        "authors": [
            "Yuting Li",
            "Lai Wei",
            "Kaipeng Zheng",
            "Jingyuan Huang",
            "Linghe Kong",
            "Lichao Sun",
            "Weiran Huang"
        ],
        "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning",
        "abstract": "arXiv:2506.09736v1 Announce Type: new  Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters.",
        "arxiv_id": "2506.09736",
        "ARXIVID": "2506.09736",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores visual perturbations to improve multimodal reasoning in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09541": {
        "authors": [
            "Yi Zhang",
            "Yi Wang",
            "Yawen Cui",
            "Lap-Pui Chau"
        ],
        "title": "3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection",
        "abstract": "arXiv:2506.09541v1 Announce Type: new  Abstract: This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection approach that effectively handles single- and multi-view RGB images in indoor and outdoor environments, showcasing its general-purpose applicability. The key challenge for image-based 3D object detection tasks is the lack of 3D geometric cues, which leads to ambiguity in establishing correspondences between images and 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D geometric representations in both explicit and implicit manners based on predicted depth information. Specifically, we utilize the predicted depth to learn voxel occupancy and optimize the voxelized 3D feature volume explicitly through the proposed voxel occupancy attention. To further enhance 3D awareness, the feature volume is integrated with an implicit 3D representation, the truncated signed distance function (TSDF). Without requiring supervision from 3D signals, we significantly improve the model's comprehension of 3D geometry by leveraging intermediate 3D representations and achieve end-to-end training. Our approach surpasses the performance of state-of-the-art image-based methods on both single- and multi-view benchmark datasets across diverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D dataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19 AP3D@0.7 improvement on the KITTI dataset. The project page is available at: https://cindy0725.github.io/3DGeoDet/.",
        "arxiv_id": "2506.09541",
        "ARXIVID": "2506.09541",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on geometry-aware 3D object detection and spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.09612": {
        "authors": [
            "Mingxiao LI",
            "mang ning",
            "Marie-Francine Moens"
        ],
        "title": "Consistent Story Generation with Asymmetry Zigzag Sampling",
        "abstract": "arXiv:2506.09612v1 Announce Type: new  Abstract: Text-to-image generation models have made significant progress in producing high-quality images from textual descriptions, yet they continue to struggle with maintaining subject consistency across multiple images, a fundamental requirement for visual storytelling. Existing methods attempt to address this by either fine-tuning models on large-scale story visualization datasets, which is resource-intensive, or by using training-free techniques that share information across generations, which still yield limited success. In this paper, we introduce a novel training-free sampling strategy called Zigzag Sampling with Asymmetric Prompts and Visual Sharing to enhance subject consistency in visual story generation. Our approach proposes a zigzag sampling mechanism that alternates between asymmetric prompting to retain subject characteristics, while a visual sharing module transfers visual cues across generated images to %further enforce consistency. Experimental results, based on both quantitative metrics and qualitative evaluations, demonstrate that our method significantly outperforms previous approaches in generating coherent and consistent visual stories. The code is available at https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.",
        "arxiv_id": "2506.09612",
        "ARXIVID": "2506.09612",
        "COMMENT": "Matches criterion 6 as it focuses on visual storytelling and consistency in video generation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.09417": {
        "authors": [
            "Yunxiao Shi",
            "Yinhao Zhu",
            "Shizhong Han",
            "Jisoo Jeong",
            "Amin Ansari",
            "Hong Cai",
            "Fatih Porikli"
        ],
        "title": "ODG: Occupancy Prediction Using Dual Gaussians",
        "abstract": "arXiv:2506.09417v1 Announce Type: new  Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene understanding which is critical for autonomous driving. Most existing methods, however, carry high compute costs, requiring dense 3D feature volume and cross-attention to effectively aggregate information. More recent works have adopted Bird's Eye View (BEV) or sparse points as scene representation with much reduced cost, but still suffer from their respective shortcomings. More concretely, BEV struggles with small objects that often experience significant information loss after being projected to the ground plane. On the other hand, points can flexibly model little objects in 3D, but is inefficient at capturing flat surfaces or large objects. To address these challenges, in this paper, we present a novel 3D occupancy prediction approach, ODG, which combines BEV and sparse points based representations. We propose a dual-branch design: a query-based sparse points branch and a BEV branch. The 3D information learned in the sparse points branch is shared with the BEV stream via cross-attention, which enriches the weakened signals of difficult objects on the BEV plane. The outputs of both branches are finally fused to generate predicted 3D occupancy. We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo benchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG also delivers competitive inference speed when compared to the latest efficient approaches.",
        "arxiv_id": "2506.09417",
        "ARXIVID": "2506.09417",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (ODG) for 3D occupancy prediction, addressing challenges in autonomous driving and scene understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.09740": {
        "authors": [
            "Qin Zhou",
            "Zhiyang Zhang",
            "Jinglong Wang",
            "Xiaobin Li",
            "Jing Zhang",
            "Qian Yu",
            "Lu Sheng",
            "Dong Xu"
        ],
        "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models",
        "abstract": "arXiv:2506.09740v1 Announce Type: new  Abstract: Diffusion models excel at image generation. Recent studies have shown that these models not only generate high-quality images but also encode text-image alignment information through attention maps or loss functions. This information is valuable for various downstream tasks, including segmentation, text-guided image editing, and compositional image generation. However, current methods heavily rely on the assumption of perfect text-image alignment in diffusion models, which is not the case. In this paper, we propose using zero-shot referring image segmentation as a proxy task to evaluate the pixel-level image and class-level text alignment of popular diffusion models. We conduct an in-depth analysis of pixel-text misalignment in diffusion models from the perspective of training data bias. We find that misalignment occurs in images with small sized, occluded, or rare object classes. Therefore, we propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text alignment in diffusion models based on the evidence lower bound (ELBO) of likelihood. Our method is training-free and generic, eliminating the need to identify the specific cause of misalignment and works well across various diffusion model architectures. Extensive experiments on commonly used benchmark datasets on image segmentation and generation have verified the effectiveness of our proposed calibration approach.",
        "arxiv_id": "2506.09740",
        "ARXIVID": "2506.09740",
        "COMMENT": "Matches criterion 5 as it focuses on calibrating pixel-level text-image alignment in diffusion models, which integrates image understanding and large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.09745": {
        "authors": [
            "Yangrui Zhu",
            "Junhua Bao",
            "Yipan Wei",
            "Yapeng Li",
            "Bo Du"
        ],
        "title": "Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets",
        "abstract": "arXiv:2506.09745v1 Announce Type: new  Abstract: Existing multimodal methods typically assume that different modalities share the same category set. However, in real-world applications, the category distributions in multimodal data exhibit inconsistencies, which can hinder the model's ability to effectively utilize cross-modal information for recognizing all categories. In this work, we propose the practical setting termed Multi-Modal Heterogeneous Category-set Learning (MMHCL), where models are trained in heterogeneous category sets of multi-modal data and aim to recognize complete classes set of all modalities during test. To effectively address this task, we propose a Class Similarity-based Cross-modal Fusion model (CSCF). Specifically, CSCF aligns modality-specific features to a shared semantic space to enable knowledge transfer between seen and unseen classes. It then selects the most discriminative modality for decision fusion through uncertainty estimation. Finally, it integrates cross-modal information based on class similarity, where the auxiliary modality refines the prediction of the dominant one. Experimental results show that our method significantly outperforms existing state-of-the-art (SOTA) approaches on multiple benchmark datasets, effectively addressing the MMHCL task.",
        "arxiv_id": "2506.09745",
        "ARXIVID": "2506.09745",
        "COMMENT": "Matches criterion 2 as it addresses multimodal classification with heterogeneous category sets, relevant to vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.09460": {
        "authors": [
            "Amirreza Khoshbakht",
            "Erchan Aptoula"
        ],
        "title": "Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization",
        "abstract": "arXiv:2506.09460v1 Announce Type: new  Abstract: Open-set domain generalization(OSDG) for hyperspectral image classification presents significant challenges due to the presence of unknown classes in target domains and the need for models to generalize across multiple unseen domains without target-specific adaptation. Existing domain adaptation methods assume access to target domain data during training and fail to address the fundamental issue of domain shift when unknown classes are present, leading to negative transfer and reduced classification performance. To address these limitations, we propose a novel open-set domain generalization framework that combines four key components: Spectrum-Invariant Frequency Disentanglement (SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network (DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning (EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty Disentanglement (SSUD) for reliable open-set classification. The SIFD module extracts domain-invariant spectral features in the frequency domain through attention-weighted frequency analysis and domain-agnostic regularization, while DCRN captures complementary spectral and spatial information via parallel pathways with adaptive fusion. EDL provides principled uncertainty estimation using Dirichlet distributions, enabling the SSUD module to make reliable open-set decisions through uncertainty-aware pathway weighting and adaptive rejection thresholding. Experimental results on three cross-scene hyperspectral classification tasks show that our approach achieves performance comparable to state-of-the-art domain adaptation methods while requiring no access to the target domain during training. The implementation will be made available at https://github.com/amir-khb/SSUDOSDG upon acceptance.",
        "arxiv_id": "2506.09460",
        "ARXIVID": "2506.09460",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and uncertainty disentanglement in hyperspectral domain generalization.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.09113": {
        "authors": [
            "Yu Gao",
            "Haoyuan Guo",
            "Tuyen Hoang",
            "Weilin Huang",
            "Lu Jiang",
            "Fangyuan Kong",
            "Huixia Li",
            "Jiashi Li",
            "Liang Li",
            "Xiaojie Li",
            "Xunsong Li",
            "Yifu Li",
            "Shanchuan Lin",
            "Zhijie Lin",
            "Jiawei Liu",
            "Shu Liu",
            "Xiaonan Nie",
            "Zhiwu Qing",
            "Yuxi Ren",
            "Li Sun",
            "Zhi Tian",
            "Rui Wang",
            "Sen Wang",
            "Guoqiang Wei",
            "Guohong Wu",
            "Jie Wu",
            "Ruiqi Xia",
            "Fei Xiao",
            "Xuefeng Xiao",
            "Jiangqiao Yan",
            "Ceyuan Yang",
            "Jianchao Yang",
            "Runkai Yang",
            "Tao Yang",
            "Yihang Yang",
            "Zilyu Ye",
            "Xuejiao Zeng",
            "Yan Zeng",
            "Heng Zhang",
            "Yang Zhao",
            "Xiaozheng Zheng",
            "Peihao Zhu",
            "Jiaxin Zou",
            "Feilong Zuo"
        ],
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "abstract": "arXiv:2506.09113v1 Announce Type: new  Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.",
        "arxiv_id": "2506.09113",
        "ARXIVID": "2506.09113",
        "COMMENT": "Matches criterion 6 as it focuses on video generation and addresses challenges in spatiotemporal fluidity and visual quality, which are key aspects of video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.09081": {
        "authors": [
            "Zheqi He",
            "Yesheng Liu",
            "Jing-shu Zheng",
            "Xuejing Li",
            "Richeng Xuan",
            "Jin-Ge Yao",
            "Xi Yang"
        ],
        "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation",
        "abstract": "arXiv:2506.09081v1 Announce Type: new  Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.",
        "arxiv_id": "2506.09081",
        "ARXIVID": "2506.09081",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive evaluation framework for multimodal models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.09849": {
        "authors": [
            "Florian Bordes",
            "Quentin Garrido",
            "Justine T Kao",
            "Adina Williams",
            "Michael Rabbat",
            "Emmanuel Dupoux"
        ],
        "title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments",
        "abstract": "arXiv:2506.09849v1 Announce Type: new  Abstract: We present IntPhys 2, a video benchmark designed to evaluate the intuitive physics understanding of deep learning models. Building on the original IntPhys benchmark, IntPhys 2 focuses on four core principles related to macroscopic objects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity. These conditions are inspired by research into intuitive physical understanding emerging during early childhood. IntPhys 2 offers a comprehensive suite of tests, based on the violation of expectation framework, that challenge models to differentiate between possible and impossible events within controlled and diverse virtual environments. Alongside the benchmark, we provide performance evaluations of several state-of-the-art models. Our findings indicate that while these models demonstrate basic visual understanding, they face significant challenges in grasping intuitive physics across the four principles in complex scenes, with most models performing at chance levels (50%), in stark contrast to human performance, which achieves near-perfect accuracy. This underscores the gap between current models and human-like intuitive physics understanding, highlighting the need for advancements in model architectures and training methodologies.",
        "arxiv_id": "2506.09849",
        "ARXIVID": "2506.09849",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark (IntPhys 2) for intuitive physics understanding in complex environments.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.09473": {
        "authors": [
            "Cheng Chen",
            "Yunpeng Zhai",
            "Yifan Zhao",
            "Jinyang Gao",
            "Bolin Ding",
            "Jia Li"
        ],
        "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning",
        "abstract": "arXiv:2506.09473v1 Announce Type: new  Abstract: In-context learning (ICL), a predominant trend in instruction learning, aims at enhancing the performance of large language models by providing clear task guidance and examples, improving their capability in task understanding and execution. This paper investigates ICL on Large Vision-Language Models (LVLMs) and explores the policies of multi-modal demonstration selection. Existing research efforts in ICL face significant challenges: First, they rely on pre-defined demonstrations or heuristic selecting strategies based on human intuition, which are usually inadequate for covering diverse task requirements, leading to sub-optimal solutions; Second, individually selecting each demonstration fails in modeling the interactions between them, resulting in information redundancy. Unlike these prevailing efforts, we propose a new exploration-exploitation reinforcement learning framework, which explores policies to fuse multi-modal information and adaptively select adequate demonstrations as an integrated whole. The framework allows LVLMs to optimize themselves by continually refining their demonstrations through self-exploration, enabling the ability to autonomously identify and generate the most effective selection policies for in-context learning. Experimental results verify the superior performance of our approach on four Visual Question-Answering (VQA) datasets, demonstrating its effectiveness in enhancing the generalization capability of few-shot LVLMs.",
        "arxiv_id": "2506.09473",
        "ARXIVID": "2506.09473",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores in-context learning for large vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.09634": {
        "authors": [
            "Yanzhao Shi",
            "Xiaodan Zhang",
            "Junzhong Ji",
            "Haoning Jiang",
            "Chengxin Zheng",
            "Yinong Wang",
            "Liangqiong Qu"
        ],
        "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding",
        "abstract": "arXiv:2506.09634v1 Announce Type: new  Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based decisions by enhancing diagnostic accuracy and workflow efficiency. While multimodal large language models (MLLMs) exhibit promising performance in visual-language understanding, existing methods mainly focus on 2D medical images, which fundamentally limits their ability to capture complex 3D anatomical structures. This limitation often leads to misinterpretation of subtle pathologies and causes diagnostic hallucinations. In this paper, we present Hybrid Spatial Encoding Network (HSENet), a framework that exploits enriched 3D medical visual cues by effective visual perception and projection for accurate and robust vision-language understanding. Specifically, HSENet employs dual-3D vision encoders to perceive both global volumetric contexts and fine-grained anatomical details, which are pre-trained by dual-stage alignment with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient multimodal projector that condenses high-resolution 3D spatial regions into a compact set of informative visual tokens via centroid-based compression. By assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly perceive and transfer hybrid visual representations to LLM's semantic space, facilitating accurate diagnostic text generation. Experimental results demonstrate that our method achieves state-of-the-art performance in 3D language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering (73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness. Our code is available at https://github.com/YanzhaoShi/HSENet.",
        "arxiv_id": "2506.09634",
        "ARXIVID": "2506.09634",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a framework for 3D medical vision-language understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.09278": {
        "authors": [
            "Yuchen Zhang",
            "Nikhil Keetha",
            "Chenwei Lyu",
            "Bhuvan Jhamb",
            "Yutian Chen",
            "Yuheng Qiu",
            "Jay Karhade",
            "Shreyas Jha",
            "Yaoyu Hu",
            "Deva Ramanan",
            "Sebastian Scherer",
            "Wenshan Wang"
        ],
        "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow",
        "abstract": "arXiv:2506.09278v1 Announce Type: new  Abstract: Dense image correspondence is central to many applications, such as visual odometry, 3D reconstruction, object association, and re-identification. Historically, dense correspondence has been tackled separately for wide-baseline scenarios and optical flow estimation, despite the common goal of matching content between two images. In this paper, we develop a Unified Flow & Matching model (UFM), which is trained on unified data for pixels that are co-visible in both source and target images. UFM uses a simple, generic transformer architecture that directly regresses the (u,v) flow. It is easier to train and more accurate for large flows compared to the typical coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than state-of-the-art flow methods (Unimatch), while also having 62% less error and 6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to demonstrate that unified training can outperform specialized approaches across both domains. This result enables fast, general-purpose correspondence and opens new directions for multi-modal, long-range, and real-time correspondence tasks.",
        "arxiv_id": "2506.09278",
        "ARXIVID": "2506.09278",
        "COMMENT": "Matches criterion 3 as it introduces a unified model for dense correspondence, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.09982": {
        "authors": [
            "Zijie Wu",
            "Chaohui Yu",
            "Fan Wang",
            "Xiang Bai"
        ],
        "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation",
        "abstract": "arXiv:2506.09982v1 Announce Type: new  Abstract: Recent advances in 4D content generation have attracted increasing attention, yet creating high-quality animated 3D models remains challenging due to the complexity of modeling spatio-temporal distributions and the scarcity of 4D training data. In this paper, we present AnimateAnyMesh, the first feed-forward framework that enables efficient text-driven animation of arbitrary 3D meshes. Our approach leverages a novel DyMeshVAE architecture that effectively compresses and reconstructs dynamic mesh sequences by disentangling spatial and temporal features while preserving local topological structures. To enable high-quality text-conditional generation, we employ a Rectified Flow-based training strategy in the compressed latent space. Additionally, we contribute the DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text annotations. Experimental results demonstrate that our method generates semantically accurate and temporally coherent mesh animations in a few seconds, significantly outperforming existing approaches in both quality and efficiency. Our work marks a substantial step forward in making 4D content creation more accessible and practical. All the data, code, and models will be open-released.",
        "arxiv_id": "2506.09982",
        "ARXIVID": "2506.09982",
        "COMMENT": "Matches criterion 5 as it presents a framework for text-driven animation of 3D meshes, combining image understanding and generation with language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.09663": {
        "authors": [
            "Haowen Wang",
            "Xiaoping Yuan",
            "Zhao Jin",
            "Zhen Zhao",
            "Zhengping Che",
            "Yousong Xue",
            "Jin Tian",
            "Yakun Huang",
            "Jian Tang"
        ],
        "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation",
        "abstract": "arXiv:2506.09663v1 Announce Type: new  Abstract: Articulated objects are ubiquitous in everyday life, and accurate 3D representations of their geometry and motion are critical for numerous applications. However, in the absence of human annotation, existing approaches still struggle to build a unified representation for objects that contain multiple movable parts. We introduce DeGSS, a unified framework that encodes articulated objects as deformable 3D Gaussian fields, embedding geometry, appearance, and motion in one compact representation. Each interaction state is modeled as a smooth deformation of a shared field, and the resulting deformation trajectories guide a progressive coarse-to-fine part segmentation that identifies distinct rigid components, all in an unsupervised manner. The refined field provides a spatially continuous, fully decoupled description of every part, supporting part-level reconstruction and precise modeling of their kinematic relationships. To evaluate generalization and realism, we enlarge the synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset that pairs RGB captures with accurately reverse-engineered 3D models. Extensive experiments demonstrate that our method outperforms existing methods in both accuracy and stability.",
        "arxiv_id": "2506.09663",
        "ARXIVID": "2506.09663",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for modeling articulated objects, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.09482": {
        "authors": [
            "Dingcheng Zhen",
            "Qian Qiao",
            "Tan Yu",
            "Kangxi Wu",
            "Ziwei Zhang",
            "Siyuan Liu",
            "Shunshun Yin",
            "Ming Tao"
        ],
        "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression",
        "abstract": "arXiv:2506.09482v1 Announce Type: new  Abstract: We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs a diffusion model to estimate the distribution of image samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster inference latency compared to state-of-the-art methods based on AR Transformer and x112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce a novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up a new frontier in the field of image generation.",
        "arxiv_id": "2506.09482",
        "ARXIVID": "2506.09482",
        "COMMENT": "Matches criterion 4 as it introduces a novel image generation model combining AR Transformer and diffusion models, which is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.09989": {
        "authors": [
            "Yiming Dou",
            "Wonseok Oh",
            "Yuqing Luo",
            "Antonio Loquercio",
            "Andrew Owens"
        ],
        "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes",
        "abstract": "arXiv:2506.09989v1 Announce Type: new  Abstract: We study the problem of making 3D scene reconstructions interactive by asking the following question: can we predict the sounds of human hands physically interacting with a scene? First, we record a video of a human manipulating objects within a 3D scene using their hands. We then use these action-sound pairs to train a rectified flow model to map 3D hand trajectories to their corresponding audio. At test time, a user can query the model for other actions, parameterized as sequences of hand poses, to estimate their corresponding sounds. In our experiments, we find that our generated sounds accurately convey material properties and actions, and that they are often indistinguishable to human observers from real sounds. Project page: https://www.yimingdou.com/hearing_hands/",
        "arxiv_id": "2506.09989",
        "ARXIVID": "2506.09989",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it explores generating sounds from physical interactions in 3D scenes, which is relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.09988": {
        "authors": [
            "Ron Yosef",
            "Moran Yanuka",
            "Yonatan Bitton",
            "Dani Lischinski"
        ],
        "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits",
        "abstract": "arXiv:2506.09988v1 Announce Type: new  Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is becoming increasingly widespread. This trend highlights the need for a comprehensive framework to verify text-guided edits and assess their quality. To address this need, we introduce EditInspector, a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models in assessing edits across various dimensions, including accuracy, artifact detection, visual quality, seamless integration with the image scene, adherence to common sense, and the ability to describe edit-induced changes. Our findings indicate that current models struggle to evaluate edits comprehensively and frequently hallucinate when describing the changes. To address these challenges, we propose two novel methods that outperform SoTA models in both artifact detection and difference caption generation.",
        "arxiv_id": "2506.09988",
        "ARXIVID": "2506.09988",
        "COMMENT": "Matches criterion 7 as it introduces a benchmark for evaluating text-guided image edits, which is a survey-like contribution.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.09993": {
        "authors": [
            "Jaewon Min",
            "Jin Hyeon Kim",
            "Paul Hyunbin Cho",
            "Jaeeun Lee",
            "Jihye Park",
            "Minkyu Park",
            "Sangpil Kim",
            "Hyunhee Park",
            "Seungryong Kim"
        ],
        "title": "Text-Aware Image Restoration with Diffusion Models",
        "abstract": "arXiv:2506.09993v1 Announce Type: new  Abstract: Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
        "arxiv_id": "2506.09993",
        "ARXIVID": "2506.09993",
        "COMMENT": "Matches criterion 5 as it focuses on text-aware image restoration using diffusion models, combining image understanding and language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.09958": {
        "authors": [
            "Sushant Gautam",
            "Michael A. Riegler",
            "P{\\aa}l Halvorsen"
        ],
        "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy",
        "abstract": "arXiv:2506.09958v1 Announce Type: new  Abstract: Medical Visual Question Answering (MedVQA) is a promising field for developing clinical decision support systems, yet progress is often limited by the available datasets, which can lack clinical complexity and visual diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new, large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs that are designed to test deeper clinical reasoning. We developed a systematic method using large language models to generate these questions, which are stratified by complexity to better assess a model's inference capabilities. To ensure our dataset prepares models for real-world clinical scenarios, we have also introduced a variety of visual augmentations that mimic common imaging artifacts. The dataset is structured to support two main evaluation tracks: one for standard VQA performance and another to test model robustness against these visual perturbations. By providing a more challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for use in clinical settings. The dataset is fully accessible and adheres to FAIR data principles, making it a valuable resource for the wider research community. Code and data: https://github.com/Simula/Kvasir-VQA-x1 and https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1",
        "arxiv_id": "2506.09958",
        "ARXIVID": "2506.09958",
        "COMMENT": "Matches criterion 6 as it introduces a multimodal dataset for medical visual question answering, which is a video understanding task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.09109": {
        "authors": [
            "Arnav Yayavaram",
            "Siddharth Yayavaram",
            "Simran Khanuja",
            "Michael Saxon",
            "Graham Neubig"
        ],
        "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation",
        "abstract": "arXiv:2506.09109v1 Announce Type: new  Abstract: As text-to-image models become increasingly prevalent, ensuring their equitable performance across diverse cultural contexts is critical. Efforts to mitigate cross-cultural biases have been hampered by trade-offs, including a loss in performance, factual inaccuracies, or offensive outputs. Despite widespread recognition of these challenges, an inability to reliably measure these biases has stalled progress. To address this gap, we introduce CAIRe, a novel evaluation metric that assesses the degree of cultural relevance of an image, given a user-defined set of labels. Our framework grounds entities and concepts in the image to a knowledge base and uses factual information to give independent graded judgments for each culture label. On a manually curated dataset of culturally salient but rare items built using language models, CAIRe surpasses all baselines by 28% F1 points. Additionally, we construct two datasets for culturally universal concept, one comprising of T2I-generated outputs and another retrieved from naturally occurring data. CAIRe achieves Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based on a 5-point Likert scale of cultural relevance. This demonstrates its strong alignment with human judgment across diverse image sources.",
        "arxiv_id": "2506.09109",
        "ARXIVID": "2506.09109",
        "COMMENT": "Matches criterion 2 as it discusses cultural relevance evaluation for text-to-image models, which aligns with vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.09345": {
        "authors": [
            "Songping Wang",
            "Xiantao Hu",
            "Yueming Lyu",
            "Caifeng Shan"
        ],
        "title": "An Effective End-to-End Solution for Multimodal Action Recognition",
        "abstract": "arXiv:2506.09345v1 Announce Type: new  Abstract: Recently, multimodal tasks have strongly advanced the field of action recognition with their rich multimodal information. However, due to the scarcity of tri-modal data, research on tri-modal action recognition tasks faces many challenges. To this end, we have proposed a comprehensive multimodal action recognition solution that effectively utilizes multimodal information. First, the existing data are transformed and expanded by optimizing data enhancement techniques to enlarge the training scale. At the same time, more RGB datasets are used to pre-train the backbone network, which is better adapted to the new task by means of transfer learning. Secondly, multimodal spatial features are extracted with the help of 2D CNNs and combined with the Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature extraction comparable to 3D CNNs and improve the computational efficiency. In addition, common prediction enhancement methods, such as Stochastic Weight Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to integrate the knowledge of models from different training periods of the same architecture and different architectures, so as to predict the actions from different perspectives and fully exploit the target information. Ultimately, we achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the competition leaderboard, demonstrating the superiority of our solution.",
        "arxiv_id": "2506.09345",
        "ARXIVID": "2506.09345",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on multimodal action recognition, which involves video-based tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.09416": {
        "authors": [
            "Xinyu Peng",
            "Ziyang Zheng",
            "Yaoming Wang",
            "Han Li",
            "Nuowen Kan",
            "Wenrui Dai",
            "Chenglin Li",
            "Junni Zou",
            "Hongkai Xiong"
        ],
        "title": "Noise Conditional Variational Score Distillation",
        "abstract": "arXiv:2506.09416v1 Announce Type: new  Abstract: We propose Noise Conditional Variational Score Distillation (NCVSD), a novel method for distilling pretrained diffusion models into generative denoisers. We achieve this by revealing that the unconditional score function implicitly characterizes the score function of denoising posterior distributions. By integrating this insight into the Variational Score Distillation (VSD) framework, we enable scalable learning of generative denoisers capable of approximating samples from the denoising posterior distribution across a wide range of noise levels. The proposed generative denoisers exhibit desirable properties that allow fast generation while preserve the benefit of iterative refinement: (1) fast one-step generation through sampling from pure Gaussian noise at high noise levels; (2) improved sample quality by scaling the test-time compute with multi-step sampling; and (3) zero-shot probabilistic inference for flexible and controllable sampling. We evaluate NCVSD through extensive experiments, including class-conditional image generation and inverse problem solving. By scaling the test-time compute, our method outperforms teacher diffusion models and is on par with consistency models of larger sizes. Additionally, with significantly fewer NFEs than diffusion-based methods, we achieve record-breaking LPIPS on inverse problems.",
        "arxiv_id": "2506.09416",
        "ARXIVID": "2506.09416",
        "COMMENT": "Does not closely match any specific criteria but is relevant to generative modeling due to its novel method for distilling pretrained diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2506.09836": {
        "authors": [
            "Junli Deng",
            "Ping Shi",
            "Qipei Li",
            "Jinyang Guo"
        ],
        "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction",
        "abstract": "arXiv:2506.09836v1 Announce Type: new  Abstract: Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction.",
        "arxiv_id": "2506.09836",
        "ARXIVID": "2506.09836",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and scene reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.09534": {
        "authors": [
            "Tao Wang",
            "Mengyu Li",
            "Geduo Zeng",
            "Cheng Meng",
            "Qiong Zhang"
        ],
        "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS",
        "abstract": "arXiv:2506.09534v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering.",
        "arxiv_id": "2506.09534",
        "ARXIVID": "2506.09534",
        "COMMENT": "Does not match any specific criterion but is relevant to neural rendering and optimization techniques in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.09954": {
        "authors": [
            "Ziyi Wang",
            "Yongming Rao",
            "Shuofeng Sun",
            "Xinrun Liu",
            "Yi Wei",
            "Xumin Yu",
            "Zuyan Liu",
            "Yanbo Wang",
            "Hongmin Liu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Vision Generalist Model: A Survey",
        "abstract": "arXiv:2506.09954v1 Announce Type: new  Abstract: Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process various downstream tasks simultaneously. Encouraged by their impressive performance, an increasing number of researchers are venturing into the realm of applying these models to computer vision tasks. However, the inputs and outputs of vision tasks are more diverse, and it is difficult to summarize them as a unified representation. In this paper, we provide a comprehensive overview of the vision generalist models, delving into their characteristics and capabilities within the field. First, we review the background, including the datasets, tasks, and benchmarks. Then, we dig into the design of frameworks that have been proposed in existing research, while also introducing the techniques employed to enhance their performance. To better help the researchers comprehend the area, we take a brief excursion into related domains, shedding light on their interconnections and potential synergies. To conclude, we provide some real-world application scenarios, undertake a thorough examination of the persistent challenges, and offer insights into possible directions for future research endeavors.",
        "arxiv_id": "2506.09954",
        "ARXIVID": "2506.09954",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive survey on vision generalist models.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.09476": {
        "authors": [
            "Tianxiang Hao",
            "Lixian Zhang",
            "Yingjia Zhang",
            "Mengxuan Chen",
            "Jinxiao Zhang",
            "Haohuan Fu"
        ],
        "title": "Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries",
        "abstract": "arXiv:2506.09476v1 Announce Type: new  Abstract: Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data, offers rare insights into understanding early urban development and long-term transformation. However, severe quality degradation (e.g., distortion, misalignment, and spectral scarcity) and annotation absence have long hindered semantic segmentation on such historical RS imagery. To bridge this gap and enhance understanding of urban development, we introduce $\\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on historical satellite imagery with the earliest observation time among all existing segmentation datasets, along with a benchmark framework for unsupervised segmentation tasks, $\\textbf{Urban1960SatUSM}$. First, $\\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic segmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering 1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the earliest segmentation dataset of its kind, it provides a pioneering benchmark for historical urban understanding. Second, $\\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel unsupervised semantic segmentation framework for historical RS imagery. It employs a confidence-aware alignment mechanism and focal-confidence loss based on a self-supervised learning architecture, which generates robust pseudo-labels and adaptively prioritizes prediction difficulty and label reliability to improve unsupervised segmentation on noisy historical data without manual supervision. Experiments show Urban1960SatUSM significantly outperforms existing unsupervised segmentation methods on Urban1960SatSeg for segmenting historical urban scenes, promising in paving the way for quantitative studies of long-term urban change using modern computer vision. Our benchmark and supplementary material are available at https://github.com/Tianxiang-Hao/Urban1960SatSeg.",
        "arxiv_id": "2506.09476",
        "ARXIVID": "2506.09476",
        "COMMENT": "Does not closely match any specific criteria but is relevant to computer vision and machine learning due to its focus on unsupervised segmentation and historical satellite imagery.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.09446": {
        "authors": [
            "Yuhe Ding",
            "Jian Liang",
            "Bo Jiang",
            "Zi Wang",
            "Aihua Zheng",
            "Bin Luo"
        ],
        "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization",
        "abstract": "arXiv:2506.09446v1 Announce Type: new  Abstract: CLIP-based domain generalization aims to improve model generalization to unseen domains by leveraging the powerful zero-shot classification capabilities of CLIP and multiple source datasets. Existing methods typically train a single model across multiple source domains to capture domain-shared information. However, this paradigm inherently suffers from two types of conflicts: 1) sample conflicts, arising from noisy samples and extreme domain shifts among sources; and 2) optimization conflicts, stemming from competition and trade-offs during multi-source training. Both hinder the generalization and lead to suboptimal solutions. Recent studies have shown that model merging can effectively mitigate the competition of multi-objective optimization and improve generalization performance. Inspired by these findings, we propose Harmonizing and Merging (HAM), a novel source model merging framework for CLIP-based domain generalization. During the training process of the source models, HAM enriches the source samples without conflicting samples, and harmonizes the update directions of all models. Then, a redundancy-aware historical model merging method is introduced to effectively integrate knowledge across all source models. HAM comprehensively consolidates source domain information while enabling mutual enhancement among source models, ultimately yielding a final model with optimal generalization capabilities. Extensive experiments on five widely used benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance.",
        "arxiv_id": "2506.09446",
        "ARXIVID": "2506.09446",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to domain generalization and CLIP-based models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.09363": {
        "authors": [
            "Hongguang Zhu",
            "Yunchao Wei",
            "Mengyu Wang",
            "Siyu Jiao",
            "Yan Fang",
            "Jiannan Huang",
            "Yao Zhao"
        ],
        "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing",
        "abstract": "arXiv:2506.09363v1 Announce Type: new  Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in ``word concept abyss'', which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at https://github.com/KevinLight831/SAGE.",
        "arxiv_id": "2506.09363",
        "ARXIVID": "2506.09363",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and safety in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.09066": {
        "authors": [
            "Maoyu Wang",
            "Yao Lu",
            "Jiaqi Nie",
            "Zeyu Wang",
            "Yun Lin",
            "Qi Xuan",
            "Guan Gui"
        ],
        "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices",
        "abstract": "arXiv:2506.09066v1 Announce Type: new  Abstract: With the rapid development of deep learning, a growing number of pre-trained models have been publicly available. However, deploying these fixed models in real-world IoT applications is challenging because different devices possess heterogeneous computational and memory resources, making it impossible to deploy a single model across all platforms. Although traditional compression methods, such as pruning, quantization, and knowledge distillation, can improve efficiency, they become inflexible once applied and cannot adapt to changing resource constraints. To address these issues, we propose ReStNet, a Reusable and Stitchable Network that dynamically constructs a hybrid network by stitching two pre-trained models together. Implementing ReStNet requires addressing several key challenges, including how to select the optimal stitching points, determine the stitching order of the two pre-trained models, and choose an effective fine-tuning strategy. To systematically address these challenges and adapt to varying resource constraints, ReStNet determines the stitching point by calculating layer-wise similarity via Centered Kernel Alignment (CKA). It then constructs the hybrid model by retaining early layers from a larger-capacity model and appending deeper layers from a smaller one. To facilitate efficient deployment, only the stitching layer is fine-tuned. This design enables rapid adaptation to changing budgets while fully leveraging available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN, Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching, allowing to combine different model families flexibly. Extensive experiments on multiple benchmarks demonstrate that ReStNet achieve flexible accuracy-efficiency trade-offs at runtime while significantly reducing training cost.",
        "arxiv_id": "2506.09066",
        "ARXIVID": "2506.09066",
        "COMMENT": "Does not closely match any specific criterion but is generally related to machine learning and model adaptation, which might be of general interest to your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09668": {
        "authors": [
            "Maik Dannecker",
            "Vasiliki Sideri-Lampretsa",
            "Sophie Starck",
            "Angeline Mihailov",
            "Mathieu Milh",
            "Nadine Girard",
            "Guillaume Auzias",
            "Daniel Rueckert"
        ],
        "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain",
        "abstract": "arXiv:2506.09668v1 Announce Type: new  Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid neurodevelopment marked by substantial anatomical changes unfolding within days. Studying this critical stage of the developing human brain, therefore, requires accurate brain models-referred to as atlases-of high spatial and temporal resolution. To meet these demands, established traditional atlases and recently proposed deep learning-based methods rely on large and comprehensive datasets. This poses a major challenge for studying brains in the presence of pathologies for which data remains scarce. We address this limitation with CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for creating high-resolution, spatio-temporal, multimodal brain atlases, suitable for low-data settings. Unlike established methods, CINeMA operates in latent space, avoiding compute-intensive image registration and reducing atlas construction times from days to minutes. Furthermore, it enables flexible conditioning on anatomical features including GA, birth age, and pathologies like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA supports downstream tasks such as tissue segmentation and age prediction whereas its generative properties enable synthetic data creation and anatomically informed data augmentation. Surpassing state-of-the-art methods in accuracy, efficiency, and versatility, CINeMA represents a powerful tool for advancing brain research. We release the code and atlases at https://github.com/m-dannecker/CINeMA.",
        "arxiv_id": "2506.09668",
        "ARXIVID": "2506.09668",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09479": {
        "authors": [
            "Zetian Song",
            "Jiaye Fu",
            "Jiaqi Zhang",
            "Xiaohan Lu",
            "Chuanmin Jia",
            "Siwei Ma",
            "Wen Gao"
        ],
        "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation",
        "abstract": "arXiv:2506.09479v1 Announce Type: new  Abstract: The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a training-free compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time.",
        "arxiv_id": "2506.09479",
        "ARXIVID": "2506.09479",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of 3D scene representation and compression methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09644": {
        "authors": [
            "Dongxu Liu",
            "Yuang Peng",
            "Haomiao Tang",
            "Yuwei Chen",
            "Chunrui Han",
            "Zheng Ge",
            "Daxin Jiang",
            "Mingxue Liao"
        ],
        "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning",
        "abstract": "arXiv:2506.09644v1 Announce Type: new  Abstract: Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization. Although recent advances have alleviated the performance degradation of autoencoders under high compression ratios, addressing the training instability caused by GAN remains an open challenge. While improving spatial compression, we also aim to minimize the latent space dimensionality, enabling more efficient and compact representations. To tackle these challenges, we focus on improving the decoder's expressiveness. Concretely, we propose DGAE, which employs a diffusion model to guide the decoder in recovering informative signals that are not fully decoded from the latent representation. With this design, DGAE effectively mitigates the performance degradation under high spatial compression rates. At the same time, DGAE achieves state-of-the-art performance with a 2x smaller latent space. When integrated with Diffusion Models, DGAE demonstrates competitive performance on image generation for ImageNet-1K and shows that this compact latent representation facilitates faster convergence of the diffusion model.",
        "arxiv_id": "2506.09644",
        "ARXIVID": "2506.09644",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of generative modeling and latent representation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09378": {
        "authors": [
            "Qijian Tian",
            "Xin Tan",
            "Jingyu Gong",
            "Yuan Xie",
            "Lizhuang Ma"
        ],
        "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images",
        "abstract": "arXiv:2506.09378v1 Announce Type: new  Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction.",
        "arxiv_id": "2506.09378",
        "ARXIVID": "2506.09378",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of computer vision and 3D scene reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09538": {
        "authors": [
            "Wenjun Ji",
            "Yuxiang Fu",
            "Luyang Ying",
            "Deng-Ping Fan",
            "Yuyi Wang",
            "Ming-Ming Cheng",
            "Ivor Tsang",
            "Qing Guo"
        ],
        "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches",
        "abstract": "arXiv:2506.09538v1 Announce Type: new  Abstract: Cutting-edge works have demonstrated that text-to-image (T2I) diffusion models can generate adversarial patches that mislead state-of-the-art object detectors in the physical world, revealing detectors' vulnerabilities and risks. However, these methods neglect the T2I patches' attack effectiveness when observed from different views in the physical world (i.e., angle robustness of the T2I adversarial patches). In this paper, we study the angle robustness of T2I adversarial patches comprehensively, revealing their angle-robust issues, demonstrating that texts affect the angle robustness of generated patches significantly, and task-specific linguistic instructions fail to enhance the angle robustness. Motivated by the studies, we introduce Angle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that learns a generalizable concept (i.e., text embeddings in implementation) representing the capability of generating angle-robust patches. The learned concept can be incorporated into textual prompts and guides T2I models to generate patches with their attack effectiveness inherently resistant to viewpoint variations. Through extensive simulation and physical-world experiments on five SOTA detectors across multiple views, we demonstrate that AngleRoCL significantly enhances the angle robustness of T2I adversarial patches compared to baseline methods. Our patches maintain high attack success rates even under challenging viewing conditions, with over 50% average relative improvement in attack effectiveness across multiple angles. This research advances the understanding of physically angle-robust patches and provides insights into the relationship between textual concepts and physical properties in T2I-generated contents.",
        "arxiv_id": "2506.09538",
        "ARXIVID": "2506.09538",
        "COMMENT": "Does not match any specific criterion but is relevant to adversarial robustness in text-to-image models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09919": {
        "authors": [
            "He Zhang",
            "Chentao Song",
            "Hongwen Zhang",
            "Tao Yu"
        ],
        "title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images",
        "abstract": "arXiv:2506.09919v1 Announce Type: new  Abstract: We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric human mesh recovery with accurate global translation from monocular images. In contrast to existing HMR methods that suffer from severe scale and depth ambiguity, MetricHMR is able to produce geometrically reasonable body shape and global translation in the reconstruction results. To this end, we first systematically analyze previous HMR methods on camera models to emphasize the critical role of the standard perspective projection model in enabling metric-scale HMR. We then validate the acceptable ambiguity range of metric HMR under the standard perspective projection model. Finally, we contribute a novel approach that introduces a ray map based on the standard perspective projection to jointly encode bounding-box information, camera parameters, and geometric cues for End2End metric HMR without any additional metric-regularization modules. Extensive experiments demonstrate that our method achieves state-of-the-art performance, even compared with sequential HMR methods, in metric pose, shape, and global translation estimation across both indoor and in-the-wild scenarios.",
        "arxiv_id": "2506.09919",
        "ARXIVID": "2506.09919",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09067": {
        "authors": [
            "Zhiyu Xue",
            "Reza Abbasi-Asl",
            "Ramtin Pedarsani"
        ],
        "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations",
        "abstract": "arXiv:2506.09067v1 Announce Type: new  Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed to generate complex textual information~(e.g., diagnostic reports) from multimodal inputs including vision modality~(e.g., medical images) and language modality~(e.g., clinical queries). However, their security vulnerabilities remain underexplored. Med-VLMs should be capable of rejecting harmful queries, such as \\textit{Provide detailed instructions for using this CT scan for insurance fraud}. At the same time, addressing security concerns introduces the risk of over-defense, where safety-enhancing mechanisms may degrade general performance, causing Med-VLMs to reject benign clinical queries. In this paper, we propose a novel inference-time defense strategy to mitigate harmful queries, enabling defense against visual and textual jailbreak attacks. Using diverse medical imaging datasets collected from nine modalities, we demonstrate that our defense strategy based on synthetic clinical demonstrations enhances model safety without significantly compromising performance. Additionally, we find that increasing the demonstration budget alleviates the over-defense issue. We then introduce a mixed demonstration strategy as a trade-off solution for balancing security and performance under few-shot demonstration budget constraints.",
        "arxiv_id": "2506.09067",
        "ARXIVID": "2506.09067",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of vision-language models and safety in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09677": {
        "authors": [
            "Bin Zhu",
            "Hailong Yin",
            "Jingjing Chen",
            "Yu-Gang Jiang"
        ],
        "title": "Reasoning Models Are More Easily Gaslighted Than You Think",
        "abstract": "arXiv:2506.09677v1 Announce Type: new  Abstract: Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand misleading user input remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation prompts, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation prompt. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings reveal fundamental limitations in the robustness of reasoning models, highlighting the gap between step-by-step reasoning and belief persistence.",
        "arxiv_id": "2506.09677",
        "ARXIVID": "2506.09677",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of reasoning models and robustness in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09469": {
        "authors": [
            "Maria Damanaki",
            "Nikos Piperigkos",
            "Alexandros Gkillas",
            "Aris S. Lalos"
        ],
        "title": "Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing",
        "abstract": "arXiv:2506.09469v1 Announce Type: new  Abstract: Multi-Object Tracking (MOT) plays a crucial role in autonomous driving systems, as it lays the foundations for advanced perception and precise path planning modules. Nonetheless, single agent based MOT lacks in sensing surroundings due to occlusions, sensors failures, etc. Hence, the integration of multiagent information is essential for comprehensive understanding of the environment. This paper proposes a novel Cooperative MOT framework for tracking objects in 3D LiDAR scene by formulating and solving a graph topology-aware optimization problem so as to fuse information coming from multiple vehicles. By exploiting a fully connected graph topology defined by the detected bounding boxes, we employ the Graph Laplacian processing optimization technique to smooth the position error of bounding boxes and effectively combine them. In that manner, we reveal and leverage inherent coherences of diverse multi-agent detections, and associate the refined bounding boxes to tracked objects at two stages, optimizing localization and tracking accuracies. An extensive evaluation study has been conducted, using the real-world V2V4Real dataset, where the proposed method significantly outperforms the baseline frameworks, including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various testing sequences.",
        "arxiv_id": "2506.09469",
        "ARXIVID": "2506.09469",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09327": {
        "authors": [
            "Tong Wang",
            "Guanzhou Chen",
            "Xiaodong Zhang",
            "Chenxi Liu",
            "Jiaqi Wang",
            "Xiaoliang Tan",
            "Wenchao Guo",
            "Qingyuan Yang",
            "Kaiqi Zhang"
        ],
        "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning",
        "abstract": "arXiv:2506.09327v1 Announce Type: new  Abstract: Remote sensing image interpretation plays a critical role in environmental monitoring, urban planning, and disaster assessment. However, acquiring high-quality labeled data is often costly and time-consuming. To address this challenge, we proposes a multi-modal self-supervised learning framework that leverages high-resolution RGB images, multi-spectral data, and digital surface models (DSM) for pre-training. By designing an information-aware adaptive masking strategy, cross-modal masking mechanism, and multi-task self-supervised objectives, the framework effectively captures both the correlations across different modalities and the unique feature structures within each modality. We evaluated the proposed method on multiple downstream tasks, covering typical remote sensing applications such as scene classification, semantic segmentation, change detection, object detection, and depth estimation. Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks. The results demonstrate that the proposed method outperforms existing pretraining approaches in most tasks. Specifically, on the Potsdam and Vaihingen semantic segmentation tasks, our method achieved mIoU scores of 78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation task, the RMSE error is reduced to 0.182, and for the binary change detection task in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.",
        "arxiv_id": "2506.09327",
        "ARXIVID": "2506.09327",
        "COMMENT": "Does not match any specific criteria. Focuses on remote sensing image learning, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09659": {
        "authors": [
            "Eltayeb Ahmed",
            "Uljad Berdica",
            "Martha Elliott",
            "Danijela Horak",
            "Jakob N. Foerster"
        ],
        "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model",
        "abstract": "arXiv:2506.09659v1 Announce Type: new  Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt remains an open challenge. Current methods for increasing diversity often only operate at the token-level, paraphrasing the same response. This is problematic because it leads to poor exploration on reasoning problems and to unengaging, repetitive conversational agents. To address this we propose Intent Factored Generation (IFG), factorising the sampling process into two stages. First, we sample a semantically dense intent, e.g., a summary or keywords. Second, we sample the final response conditioning on both the original prompt and the intent from the first stage. This allows us to use a higher temperature during the intent step to promote conceptual diversity, and a lower temperature during the final generation to ensure the outputs are coherent and self-consistent. Additionally, we find that prompting the model to explicitly state its intent for each step of the chain-of-thought before generating the step is beneficial for reasoning tasks. We demonstrate our method's effectiveness across a diverse set of tasks. We show this method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. For instruction-tuning, we combine IFG with Direct Preference Optimisation to increase conversational diversity without sacrificing reward. Finally, we achieve higher diversity while maintaining the quality of generations on a general language modelling task, using a new dataset of reader comments and news articles that we collect and open-source. In summary, we present a simple method of increasing the sample diversity of LLMs while maintaining performance. This method can be implemented by changing the prompt and varying the temperature during generation, making it easy to integrate into many algorithms for gains across various applications.",
        "arxiv_id": "2506.09659",
        "ARXIVID": "2506.09659",
        "COMMENT": "Does not match any specific criteria. Focuses on improving diversity in language model outputs, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09429": {
        "authors": [
            "Swadhin Das",
            "Divyansh Mundra",
            "Priyanshu Dayal",
            "Raksha Sharma"
        ],
        "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning",
        "abstract": "arXiv:2506.09429v1 Announce Type: new  Abstract: Transformer-based models have achieved strong performance in remote sensing image captioning by capturing long-range dependencies and contextual information. However, their practical deployment is hindered by high computational costs, especially in multi-modal frameworks that employ separate transformer-based encoders and decoders. In addition, existing remote sensing image captioning models primarily focus on high-level semantic extraction while often overlooking fine-grained structural features such as edges, contours, and object boundaries. To address these challenges, a lightweight transformer architecture is proposed by reducing the dimensionality of the encoder layers and employing a distilled version of GPT-2 as the decoder. A knowledge distillation strategy is used to transfer knowledge from a more complex teacher model to improve the performance of the lightweight network. Furthermore, an edge-aware enhancement strategy is incorporated to enhance image representation and object boundary understanding, enabling the model to capture fine-grained spatial details in remote sensing images. Experimental results demonstrate that the proposed approach significantly improves caption quality compared to state-of-the-art methods.",
        "arxiv_id": "2506.09429",
        "ARXIVID": "2506.09429",
        "COMMENT": "Does not closely match any specific criteria but is relevant to computer vision and generative modeling due to its lightweight transformer for remote sensing image captioning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09916": {
        "authors": [
            "Tilemachos Aravanis (School of Electrical & Computer Engineering",
            "National Technical University of Athens",
            "Greece)",
            "Panagiotis Filntisis (Robotics Institute",
            "Athena Research Center",
            "Maroussi",
            "Greece",
            "HERON - Center of Excellence in Robotics",
            "Athens",
            "Greece)",
            "Petros Maragos (School of Electrical & Computer Engineering",
            "National Technical University of Athens",
            "Greece",
            "Robotics Institute",
            "Athena Research Center",
            "Maroussi",
            "Greece",
            "HERON - Center of Excellence in Robotics",
            "Athens",
            "Greece)",
            "George Retsinas (Robotics Institute",
            "Athena Research Center",
            "Maroussi",
            "Greece",
            "HERON - Center of Excellence in Robotics",
            "Athens",
            "Greece)"
        ],
        "title": "Only-Style: Stylistic Consistency in Image Generation without Content Leakage",
        "abstract": "arXiv:2506.09916v1 Announce Type: new  Abstract: Generating images in a consistent reference visual style remains a challenging computer vision task. State-of-the-art methods aiming for style-consistent generation struggle to effectively separate semantic content from stylistic elements, leading to content leakage from the image provided as a reference to the targets. To address this challenge, we propose Only-Style: a method designed to mitigate content leakage in a semantically coherent manner while preserving stylistic consistency. Only-Style works by localizing content leakage during inference, allowing the adaptive tuning of a parameter that controls the style alignment process, specifically within the image patches containing the subject in the reference image. This adaptive process best balances stylistic consistency with leakage elimination. Moreover, the localization of content leakage can function as a standalone component, given a reference-target image pair, allowing the adaptive tuning of any method-specific parameter that provides control over the impact of the stylistic reference. In addition, we propose a novel evaluation framework to quantify the success of style-consistent generations in avoiding undesired content leakage. Our approach demonstrates a significant improvement over state-of-the-art methods through extensive evaluation across diverse instances, consistently achieving robust stylistic consistency without undesired content leakage.",
        "arxiv_id": "2506.09916",
        "ARXIVID": "2506.09916",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to style-consistent image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09724": {
        "authors": [
            "Ye Zhang",
            "Yu Zhou",
            "Yifeng Wang",
            "Jun Xiao",
            "Ziyue Wang",
            "Yongbing Zhang",
            "Jianxu Chen"
        ],
        "title": "The Four Color Theorem for Cell Instance Segmentation",
        "abstract": "arXiv:2506.09724v1 Announce Type: new  Abstract: Cell instance segmentation is critical to analyzing biomedical images, yet accurately distinguishing tightly touching cells remains a persistent challenge. Existing instance segmentation frameworks, including detection-based, contour-based, and distance mapping-based approaches, have made significant progress, but balancing model performance with computational efficiency remains an open problem. In this paper, we propose a novel cell instance segmentation method inspired by the four-color theorem. By conceptualizing cells as countries and tissues as oceans, we introduce a four-color encoding scheme that ensures adjacent instances receive distinct labels. This reformulation transforms instance segmentation into a constrained semantic segmentation problem with only four predicted classes, substantially simplifying the instance differentiation process. To solve the training instability caused by the non-uniqueness of four-color encoding, we design an asymptotic training strategy and encoding transformation method. Extensive experiments on various modes demonstrate our approach achieves state-of-the-art performance. The code is available at https://github.com/zhangye-zoe/FCIS.",
        "arxiv_id": "2506.09724",
        "ARXIVID": "2506.09724",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.09106": {
        "authors": [
            "Xiaofeng Zhang",
            "Michelle Lin",
            "Simon Lacoste-Julien",
            "Aaron Courville",
            "Yash Goyal"
        ],
        "title": "Bias Analysis in Unconditional Image Generative Models",
        "abstract": "arXiv:2506.09106v1 Announce Type: new  Abstract: The widespread adoption of generative AI models has raised growing concerns about representational harm and potential discriminatory outcomes. Yet, despite growing literature on this topic, the mechanisms by which bias emerges - especially in unconditional generation - remain disentangled. We define the bias of an attribute as the difference between the probability of its presence in the observed distribution and its expected proportion in an ideal reference distribution. In our analysis, we train a set of unconditional image generative models and adopt a commonly used bias evaluation framework to study bias shift between training and generated distributions. Our experiments reveal that the detected attribute shifts are small. We find that the attribute shifts are sensitive to the attribute classifier used to label generated images in the evaluation framework, particularly when its decision boundaries fall in high-density regions. Our empirical analysis indicates that this classifier sensitivity is often observed in attributes values that lie on a spectrum, as opposed to exhibiting a binary nature. This highlights the need for more representative labeling practices, understanding the shortcomings through greater scrutiny of evaluation frameworks, and recognizing the socially complex nature of attributes when evaluating bias.",
        "arxiv_id": "2506.09106",
        "ARXIVID": "2506.09106",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and bias analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.09920": {
        "authors": [
            "Jianhan Qi",
            "Yuheng Jia",
            "Hui Liu",
            "Junhui Hou"
        ],
        "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering",
        "abstract": "arXiv:2506.09920v1 Announce Type: new  Abstract: Hyperspectral image (HSI) clustering assigns similar pixels to the same class without any annotations, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.",
        "arxiv_id": "2506.09920",
        "ARXIVID": "2506.09920",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and clustering methods.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}