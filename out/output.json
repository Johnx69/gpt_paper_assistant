{
    "2509.13161": {
        "authors": [
            "Zhihao He",
            "Tianyao He",
            "Tieyuan Chen",
            "Yun Xu",
            "Huabin Liu",
            "Chaofan Gan",
            "Gui Zou",
            "Weiyao Lin"
        ],
        "title": "Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)",
        "abstract": "arXiv:2509.13161v1 Announce Type: new  Abstract: Despite the prosperity of the video language model, the current pursuit of comprehensive video reasoning is thwarted by the inherent spatio-temporal incompleteness within individual videos, resulting in hallucinations and inaccuracies. A promising solution is to augment the reasoning performance with multiple related videos. However, video tokens are numerous and contain redundant information, so directly feeding the relevant video data into a large language model to enhance responses could be counterproductive. To address this challenge, we propose a multi-video collaborative framework for video language models. For efficient and flexible video representation, we establish a Video Structuring Module to represent the video's knowledge as a spatio-temporal graph. Based on the structured video representation, we design the Graph Fusion Module to fuse the structured knowledge and valuable information from related videos into the augmented graph node tokens. Finally, we construct an elaborate multi-video structured prompt to integrate the graph, visual, and textual tokens as the input to the large language model. Extensive experiments substantiate the effectiveness of our framework, showcasing its potential as a promising avenue for advancing video language models.",
        "arxiv_id": "2509.13161",
        "ARXIVID": "2509.13161",
        "COMMENT": "Matches criteria 6 and 5 as it enhances video understanding and integrates multi-video reasoning with large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.13317": {
        "authors": [
            "An-Chieh Cheng",
            "Yang Fu",
            "Yukang Chen",
            "Zhijian Liu",
            "Xiaolong Li",
            "Subhashree Radhakrishnan",
            "Song Han",
            "Yao Lu",
            "Jan Kautz",
            "Pavlo Molchanov",
            "Hongxu Yin",
            "Xiaolong Wang",
            "Sifei Liu"
        ],
        "title": "3D Aware Region Prompted Vision Language Model",
        "abstract": "arXiv:2509.13317v1 Announce Type: new  Abstract: We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.",
        "arxiv_id": "2509.13317",
        "ARXIVID": "2509.13317",
        "COMMENT": "Matches criteria 1 and 2 closely as it presents a novel method for spatial reasoning in embodied agents and integrates vision-language models with 3D spatial reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.12897": {
        "authors": [
            "Jianfei Zhao",
            "Feng Zhang",
            "Xin Sun",
            "Lingxing Kong",
            "Zhixing Tan",
            "Chong Feng"
        ],
        "title": "Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models",
        "abstract": "arXiv:2509.12897v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in images, yet their attention to these objects tends to be very brief. Motivated by the hypothesis that sustained focus on key objects can improve LVLMs' visual capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of CLVS is to incorporate a vision memory that smooths the attention distribution across layers. Specifically, we initialize this vision memory with position-unbiased visual attention in the first layer. In subsequent layers, the model's visual attention jointly considers the vision memory from previous layers, while the memory is updated iteratively, thereby maintaining smooth attention on key objects. Given that visual understanding primarily occurs in the early and middle layers of the model, we use uncertainty as an indicator of completed visual understanding and terminate the smoothing process accordingly. Experiments on four benchmarks across three LVLMs confirm the effectiveness and generalizability of our method. CLVS achieves state-of-the-art performance on a variety of visual understanding tasks, with particularly significant improvements in relation and attribute understanding.",
        "arxiv_id": "2509.12897",
        "ARXIVID": "2509.12897",
        "COMMENT": "Matches criterion 2 as it focuses on improving visual understanding in Large Vision-Language Models (LVLMs) through a novel attention mechanism.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.12883": {
        "authors": [
            "Qifei Jia",
            "Yu Liu",
            "Yajie Chai",
            "Xintong Yao",
            "Qiming Lu",
            "Yasen Zhang",
            "Runyu Shi",
            "Ying Huang",
            "Guoquan Zhang"
        ],
        "title": "Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder",
        "abstract": "arXiv:2509.12883v1 Announce Type: new  Abstract: Instruction-based image editing has garnered significant attention due to its direct interaction with users. However, real-world user instructions are immensely diverse, and existing methods often fail to generalize effectively to instructions outside their training domain, limiting their practical application. To address this, we propose Lego-Edit, which leverages the generalization capability of Multi-modal Large Language Model (MLLM) to organize a suite of model-level editing tools to tackle this challenge. Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising diverse models efficiently trained on limited data and several image manipulation functions, enabling fine-grained composition of editing actions by the MLLM; and (2) a three-stage progressive reinforcement learning approach that uses feedback on unannotated, open-domain instructions to train the MLLM, equipping it with generalized reasoning capabilities for handling real-world instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art performance on GEdit-Bench and ImgBench. It exhibits robust reasoning capabilities for open-domain instructions and can utilize newly introduced editing tools without additional fine-tuning.   Code is available: https://github.com/xiaomi-research/lego-edit.",
        "arxiv_id": "2509.12883",
        "ARXIVID": "2509.12883",
        "COMMENT": "Matches criterion 2 as it focuses on Multi-modal Large Language Models (MLLMs) for image editing.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.13067": {
        "authors": [
            "Xu Li",
            "Yuxuan Liang",
            "Xiaolei Chen",
            "Yi Zheng",
            "Haotian Chen",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models",
        "abstract": "arXiv:2509.13067v1 Announce Type: new  Abstract: By cropping high-resolution images into local tiles and encoding them independently, High-Resolution Large Vision-Language Models (HR-LVLMs) have demonstrated remarkable fine-grained visual understanding capabilities. However, this divide-and-conquer paradigm significantly increases the number of visual tokens, resulting in substantial computational and memory overhead. To better understand and address this challenge, we empirically investigate visual token utilization in HR-LVLMs and uncover three key findings: (1) the local tiles have varying importance, jointly determined by visual saliency and task relevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage attention pattern across layers, with each stage attending to different types of visual tokens; (3) the visual tokens emphasized at different stages encode information at varying levels of granularity, playing complementary roles within LVLMs. Building on these insights, we propose HERO, a High-resolution visual token early dropping framework that integrates content-adaptive token budget allocation with function-aware token selection. By accurately estimating tile-level importance and selectively retaining visual tokens with complementary roles, HERO achieves superior efficiency-accuracy trade-offs across diverse benchmarks and model scales, all in a training-free manner. This study provides both empirical insights and practical solutions toward efficient inference in HR-LVLMs.",
        "arxiv_id": "2509.13067",
        "ARXIVID": "2509.13067",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on improving efficiency in high-resolution vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.12894": {
        "authors": [
            "Leekyeung Han",
            "Hyunji Min",
            "Gyeom Hwangbo",
            "Jonghyun Choi",
            "Paul Hongsuck Seo"
        ],
        "title": "DialNav: Multi-turn Dialog Navigation with a Remote Guide",
        "abstract": "arXiv:2509.12894v1 Announce Type: new  Abstract: We introduce DialNav, a novel collaborative embodied dialog task, where a navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn dialog to reach a goal location. Unlike prior work, DialNav aims for holistic evaluation and requires the Guide to infer the Navigator's location, making communication essential for task success. To support this task, we collect and release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog paired with navigation trajectories in photorealistic environments. We design a comprehensive benchmark to evaluate both navigation and dialog, and conduct extensive experiments analyzing the impact of different Navigator and Guide models. We highlight key challenges and publicly release the dataset, code, and evaluation framework to foster future research in embodied dialog.",
        "arxiv_id": "2509.12894",
        "ARXIVID": "2509.12894",
        "COMMENT": "Matches criterion 3 as it introduces a novel embodied dialog task and dataset for navigation in photorealistic environments.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.12718": {
        "authors": [
            "Pukun Zhao",
            "Longxiang Wang",
            "Miaowei Wang",
            "Chen Chen",
            "Fanqing Zhou",
            "Haojian Huang"
        ],
        "title": "EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer",
        "abstract": "arXiv:2509.12718v1 Announce Type: new  Abstract: Most existing spatial reasoning benchmarks focus on static or globally observable environments, failing to capture the challenges of long-horizon reasoning and memory utilization under partial observability and dynamic changes. We introduce two dynamic spatial benchmarks, locally observable maze navigation and match-2 elimination that systematically evaluate models' abilities in spatial understanding and adaptive planning when local perception, environment feedback, and global objectives are tightly coupled. Each action triggers structural changes in the environment, requiring continuous update of cognition and strategy. We further propose a subjective experience-based memory mechanism for cross-task experience transfer and validation. Experiments show that our benchmarks reveal key limitations of mainstream models in dynamic spatial reasoning and long-term memory, providing a comprehensive platform for future methodological advances. Our code and data are available at https://anonymous.4open.science/r/EvoEmpirBench-143C/.",
        "arxiv_id": "2509.12718",
        "ARXIVID": "2509.12718",
        "COMMENT": "Matches criterion 1 as it introduces benchmarks for dynamic spatial reasoning and memory utilization in partially observable environments.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.12757": {
        "authors": [
            "Xiaohan Zhang",
            "Si-Yuan Cao",
            "Xiaokai Bai",
            "Yiming Li",
            "Zhangkai Shen",
            "Zhe Wu",
            "Xiaoxi Hu",
            "Hui-liang Shen"
        ],
        "title": "Recurrent Cross-View Object Geo-Localization",
        "abstract": "arXiv:2509.12757v1 Announce Type: new  Abstract: Cross-view object geo-localization (CVOGL) aims to determine the location of a specific object in high-resolution satellite imagery given a query image with a point prompt. Existing approaches treat CVOGL as a one-shot detection task, directly regressing object locations from cross-view information aggregation, but they are vulnerable to feature noise and lack mechanisms for error correction. In this paper, we propose ReCOT, a Recurrent Cross-view Object geo-localization Transformer, which reformulates CVOGL as a recurrent localization task. ReCOT introduces a set of learnable tokens that encode task-specific intent from the query image and prompt embeddings, and iteratively attend to the reference features to refine the predicted location. To enhance this recurrent process, we incorporate two complementary modules: (1) a SAM-based knowledge distillation strategy that transfers segmentation priors from the Segment Anything Model (SAM) to provide clearer semantic guidance without additional inference cost, and (2) a Reference Feature Enhancement Module (RFEM) that introduces a hierarchical attention to emphasize object-relevant regions in the reference features. Extensive experiments on standard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art (SOTA) performance while reducing parameters by 60% compared to previous SOTA approaches.",
        "arxiv_id": "2509.12757",
        "ARXIVID": "2509.12757",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel recurrent localization method for cross-view object geo-localization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.13255": {
        "authors": [
            "Mattia Soldan",
            "Fabian Caba Heilbron",
            "Bernard Ghanem",
            "Josef Sivic",
            "Bryan Russell"
        ],
        "title": "ResidualViT for Efficient Temporally Dense Video Encoding",
        "abstract": "arXiv:2509.13255v1 Announce Type: new  Abstract: Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require \"temporally dense\" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.",
        "arxiv_id": "2509.13255",
        "ARXIVID": "2509.13255",
        "COMMENT": "Matches criterion 6 as it introduces a novel architecture for temporally dense video understanding tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12989": {
        "authors": [
            "Xu Zheng",
            "Chenfei Liao",
            "Ziqiao Weng",
            "Kaiyu Lei",
            "Zihao Dongfang",
            "Haocong He",
            "Yuanhuiyi Lyu",
            "Lutao Jiang",
            "Lu Qi",
            "Li Chen",
            "Danda Pani Paudel",
            "Kailun Yang",
            "Linfeng Zhang",
            "Luc Van Gool",
            "Xuming Hu"
        ],
        "title": "PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era",
        "abstract": "arXiv:2509.12989v1 Announce Type: new  Abstract: Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.",
        "arxiv_id": "2509.12989",
        "ARXIVID": "2509.12989",
        "COMMENT": "Matches criterion 7 as it provides a comprehensive survey on omnidirectional vision in the embodied AI era.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2509.12250": {
        "authors": [
            "Yihong Ji",
            "Yunze Liu",
            "Yiyao Zhuo",
            "Weijiang Yu",
            "Fei Ma",
            "Joshua Huang",
            "Fei Yu"
        ],
        "title": "OnlineHOI: Towards Online Human-Object Interaction Generation and Perception",
        "abstract": "arXiv:2509.12250v1 Announce Type: new  Abstract: The perception and generation of Human-Object Interaction (HOI) are crucial for fields such as robotics, AR/VR, and human behavior understanding. However, current approaches model this task in an offline setting, where information at each time step can be drawn from the entire interaction sequence. In contrast, in real-world scenarios, the information available at each time step comes only from the current moment and historical data, i.e., an online setting. We find that offline methods perform poorly in an online context. Based on this observation, we propose two new tasks: Online HOI Generation and Perception. To address this task, we introduce the OnlineHOI framework, a network architecture based on the Mamba framework that employs a memory mechanism. By leveraging Mamba's powerful modeling capabilities for streaming data and the Memory mechanism's efficient integration of historical information, we achieve state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as well as the online HOI4D perception task.",
        "arxiv_id": "2509.12250",
        "ARXIVID": "2509.12250",
        "COMMENT": "Matches criterion 3 as it introduces a framework for online human-object interaction generation and perception, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12715": {
        "authors": [
            "Heng Zhang",
            "Haichuan Hu",
            "Yaomin Shen",
            "Weihao Yu",
            "Yilei Yuan",
            "Haochen You",
            "Guo Cheng",
            "Zijian Zhang",
            "Lubin Gan",
            "Huihui Wei",
            "Hao Zhang",
            "Jin Huang"
        ],
        "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models",
        "abstract": "arXiv:2509.12715v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.",
        "arxiv_id": "2509.12715",
        "ARXIVID": "2509.12715",
        "COMMENT": "Matches criterion 2 as it explores a novel architecture for large vision-language models (LVLMs) with a focus on modality-specific processing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12474": {
        "authors": [
            "Kai Qiu",
            "Xiang Li",
            "Hao Chen",
            "Jason Kuen",
            "Xiaohao Xu",
            "Jiuxiang Gu",
            "Yinyi Luo",
            "Bhiksha Raj",
            "Zhe Lin",
            "Marios Savvides"
        ],
        "title": "Image Tokenizer Needs Post-Training",
        "abstract": "arXiv:2509.12474v1 Announce Type: new  Abstract: Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \\ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \\ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.",
        "arxiv_id": "2509.12474",
        "ARXIVID": "2509.12474",
        "COMMENT": "Matches criterion 5 as it discusses techniques combining image understanding tasks and generative tasks with tokenizers, which are relevant to image and LLM integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12437": {
        "authors": [
            "Dingrui Wang",
            "Zhexiao Sun",
            "Zhouheng Li",
            "Cheng Wang",
            "Youlun Peng",
            "Hongyuan Ye",
            "Baha Zarrouki",
            "Wei Li",
            "Mattia Piccinini",
            "Lei Xie",
            "Johannes Betz"
        ],
        "title": "Enhancing Physical Consistency in Lightweight World Models",
        "abstract": "arXiv:2509.12437v1 Announce Type: new  Abstract: A major challenge in deploying world models is the trade-off between size and performance. Large world models can capture rich physical dynamics but require massive computing resources, making them impractical for edge devices. Small world models are easier to deploy but often struggle to learn accurate physics, leading to poor predictions. We propose the Physics-Informed BEV World Model (PIWM), a compact model designed to efficiently capture physical interactions in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training to improve dynamic object modeling and future prediction. We also introduce a simple yet effective technique, Warm Start, for inference to enhance prediction quality with a zero-shot model. Experiments show that at the same parameter scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score. Moreover, even when compared with the largest baseline model (400M), the smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score with a 28% faster inference speed.",
        "arxiv_id": "2509.12437",
        "ARXIVID": "2509.12437",
        "COMMENT": "This paper aligns with criterion 3 as it introduces a novel lightweight world model (PIWM) for embodied AI with improved physical consistency and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.12995": {
        "authors": [
            "Yue Zhou",
            "Xinan He",
            "Kaiqing Lin",
            "Bing Fan",
            "Feng Ding",
            "Jinhua Zeng",
            "Bin Li"
        ],
        "title": "Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection",
        "abstract": "arXiv:2509.12995v1 Announce Type: new  Abstract: While specialized detectors for AI-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on a modern Vision Foundation Model (VFM). Trained on identical data, this baseline decisively `outguns' bespoke detectors, boosting in-the-wild accuracy by a striking margin of over 20\\%.   Our analysis pinpoints the source of the VFM's `firepower': First, by probing text-image similarities, we find that recent VLMs (e.g., Perception Encoder, Meta CLIP2) have learned to align synthetic images with forgery-related concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate that this is due to data exposure, as both this alignment and overall accuracy plummet on a novel dataset scraped after the VFM's pre-training cut-off date, ensuring it was unseen during pre-training. Our findings yield two critical conclusions: 1) For the real-world `gunfight' of AI-generated image detection, the raw `firepower' of an updated VFM is far more effective than the `craftsmanship' of a static detector. 2) True generalization evaluation requires test data to be independent of the model's entire training history, including pre-training.",
        "arxiv_id": "2509.12995",
        "ARXIVID": "2509.12995",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it evaluates Vision Foundation Models for AI-generated image detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.12273": {
        "authors": [
            "Liangqi Yuan",
            "Dong-Jun Han",
            "Christopher G. Brinton",
            "Sabine Brunswicker"
        ],
        "title": "LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences",
        "abstract": "arXiv:2509.12273v1 Announce Type: new  Abstract: The rise of large language models (LLMs) has made natural language-driven route planning an emerging research area that encompasses rich user objectives. Current research exhibits two distinct approaches: direct route planning using LLM-as-Agent and graph-based searching strategies. However, LLMs in the former approach struggle to handle extensive map data, while the latter shows limited capability in understanding natural language preferences. Additionally, a more critical challenge arises from the highly heterogeneous and unpredictable spatio-temporal distribution of users across the globe. In this paper, we introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an LLM-as-Parser to comprehend natural language, identify tasks, and extract user preferences and recognize task dependencies, coupled with a Multi-Step Graph construction with iterative Search (MSGS) algorithm as the underlying solver for optimal route finding. Our multi-objective optimization approach adaptively tunes objective weights to maximize points of interest (POI) quality and task completion rate while minimizing route distance, subject to three key constraints: user time limits, POI opening hours, and task dependencies. We conduct extensive experiments using 1,000 routing prompts sampled with varying complexity across 14 countries and 27 cities worldwide. The results demonstrate that our approach achieves superior performance with guarantees across multiple constraints.",
        "arxiv_id": "2509.12273",
        "ARXIVID": "2509.12273",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on multi-objective route planning with spatio-temporal constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.12278": {
        "authors": [
            "Wanru Zhuang",
            "Wenbo Li",
            "Zhibin Lan",
            "Xu Han",
            "Peng Li",
            "Jinsong Su"
        ],
        "title": "PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models",
        "abstract": "arXiv:2509.12278v1 Announce Type: new  Abstract: Text Image Machine Translation (TIMT) aims to translate texts embedded within an image into another language. Current TIMT studies primarily focus on providing translations for all the text within an image, while neglecting to provide bounding boxes and covering limited scenarios. In this work, we extend traditional TIMT into position-aware TIMT (PATIMT), aiming to support fine-grained and layoutpreserving translation, which holds great practical value but remains largely unexplored. This task comprises two key sub-tasks: regionspecific translation and full-image translation with grounding. To support existing models on PATIMT and conduct fair evaluation, we construct the PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world scenarios. Specifically, we introduce an Adaptive Image OCR Refinement Pipeline, which adaptively selects appropriate OCR tools based on scenario and refines the results of text-rich images. To ensure evaluation reliability, we further construct a test set, which contains 1,200 high-quality instances manually annotated and reviewed by human experts. After fine-tuning on our data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art performance on both sub-tasks. Experimental results also highlight the scalability and generalizability of our training data",
        "arxiv_id": "2509.12278",
        "ARXIVID": "2509.12278",
        "COMMENT": "Matches criterion 5 as it focuses on integrating image understanding tasks (text image machine translation) with large vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.13070": {
        "authors": [
            "Qianqi Lu",
            "Yuxiang Xie",
            "Jing Zhang",
            "Shiwei Zou",
            "Yan Chen",
            "Xidao Luan"
        ],
        "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation",
        "abstract": "arXiv:2509.13070v1 Announce Type: new  Abstract: Referring Image Segmentation (RIS) is a task that segments image regions based on language expressions, requiring fine-grained alignment between two modalities. However, existing methods often struggle with multimodal misalignment and language semantic loss, especially in complex scenes containing multiple visually similar objects, where uniquely described targets are frequently mislocalized or incompletely segmented. To tackle these challenges, this paper proposes TFANet, a Three-stage Image-Text Feature Alignment Network that systematically enhances multimodal alignment through a hierarchical framework comprising three stages: Knowledge Plus Stage (KPS), Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the first stage, we design the Multiscale Linear Cross-Attention Module (MLAM), which facilitates bidirectional semantic exchange between visual features and textual representations across multiple scales. This establishes rich and efficient alignment between image regions and different granularities of linguistic descriptions. Subsequently, the KFS further strengthens feature alignment through the Cross-modal Feature Scanning Module (CFSM), which applies multimodal selective scanning to capture long-range dependencies and construct a unified multimodal representation. This is essential for modeling long-range cross-modal dependencies and enhancing alignment accuracy in complex scenes. Finally, in the KIS, we propose the Word-level Linguistic Feature-guided Semantic Deepening Module (WFDM) to compensate for semantic degradation introduced in earlier stages.",
        "arxiv_id": "2509.13070",
        "ARXIVID": "2509.13070",
        "COMMENT": "Matches criterion 5 as it focuses on techniques for aligning image and text features for referring image segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.12546": {
        "authors": [
            "Yingxin Lai",
            "Zitong Yu",
            "Jun Wang",
            "Linlin Shen",
            "Yong Xu",
            "Xiaochun Cao"
        ],
        "title": "Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery Detection",
        "abstract": "arXiv:2509.12546v1 Announce Type: new  Abstract: Face forgery detection faces a critical challenge: a persistent gap between offline benchmarks and real-world efficacy,which we attribute to the ecological invalidity of training data.This work introduces Agent4FaceForgery to address two fundamental problems: (1) how to capture the diverse intents and iterative processes of human forgery creation, and (2) how to model the complex, often adversarial, text-image interactions that accompany forgeries in social media. To solve this,we propose a multi-agent framework where LLM-poweredagents, equipped with profile and memory modules, simulate the forgery creation process. Crucially, these agents interact in a simulated social environment to generate samples labeled for nuanced text-image consistency, moving beyond simple binary classification. An Adaptive Rejection Sampling (ARS) mechanism ensures data quality and diversity. Extensive experiments validate that the data generated by our simulationdriven approach brings significant performance gains to detectors of multiple architectures, fully demonstrating the effectiveness and value of our framework.",
        "arxiv_id": "2509.12546",
        "ARXIVID": "2509.12546",
        "COMMENT": "Matches criterion 5 as it integrates image understanding tasks with Large Language Models for face forgery detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.13210": {
        "authors": [
            "Ligang Chang",
            "Shengkai Xu",
            "Liangchang Shen",
            "Binhan Xu",
            "Junqiao Wang",
            "Tianyu Shi",
            "Yanhui Du"
        ],
        "title": "Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance",
        "abstract": "arXiv:2509.13210v1 Announce Type: new  Abstract: Violence detection in public surveillance is critical for public safety. This study addresses challenges such as small-scale targets, complex environments, and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal framework that integrates an enhanced YOLOv8 with a Temporal Segment Network (TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as a lightweight backbone, an exponential moving average (EMA) attention mechanism, and pruning to reduce computational cost while maintaining accuracy. YOLOv8 and TSN are trained separately on pedestrian and violence datasets, where YOLOv8 extracts human regions and TSN performs binary classification of violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming existing methods in both accuracy and efficiency, demonstrating its effectiveness for public safety surveillance. Code is available at https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.",
        "arxiv_id": "2509.13210",
        "ARXIVID": "2509.13210",
        "COMMENT": "Matches criterion 6 as it proposes a spatial-temporal framework for violence detection in video surveillance.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.13301": {
        "authors": [
            "Zefan Qu",
            "Zhenwei Wang",
            "Haoyuan Wang",
            "Ke Xu",
            "Gerhard Hancke",
            "Rynson W. H. Lau"
        ],
        "title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance",
        "abstract": "arXiv:2509.13301v1 Announce Type: new  Abstract: Creating 3D assets that follow the texture and geometry style of existing ones is often desirable or even inevitable in practical applications like video gaming and virtual reality. While impressive progress has been made in generating 3D objects from text or images, creating style-controllable 3D assets remains a complex and challenging problem. In this work, we propose StyleSculptor, a novel training-free approach for generating style-guided 3D assets from a content image and one or more style images. Unlike previous works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner, enabling fine-grained 3D style control that captures the texture, geometry, or both styles of user-provided style images. At the core of StyleSculptor is a novel Style Disentangled Attention (SD-Attn) module, which establishes a dynamic interaction between the input content image and style image for style-guided 3D asset generation via a cross-3D attention mechanism, enabling stable feature fusion and effective style-guided generation. To alleviate semantic content leakage, we also introduce a style-disentangled feature selection strategy within the SD-Attn module, which leverages the variance of 3D feature patches to disentangle style- and content-significant channels, allowing selective feature injection within the attention framework. With SD-Attn, the network can dynamically compute texture-, geometry-, or both-guided features to steer the 3D generation process. Built upon this, we further propose the Style Guided Control (SGC) mechanism, which enables exclusive geometry- or texture-only stylization, as well as adjustable style intensity control. Extensive experiments demonstrate that StyleSculptor outperforms existing baseline methods in producing high-fidelity 3D assets.",
        "arxiv_id": "2509.13301",
        "ARXIVID": "2509.13301",
        "COMMENT": "Matches criterion 4 as it focuses on style-controllable 3D asset generation, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.12931": {
        "authors": [
            "Xiao Tang",
            "Guirong Zhuo",
            "Cong Wang",
            "Boyuan Zheng",
            "Minqing Huang",
            "Lianqing Zheng",
            "Long Chen",
            "Shouyi Lu"
        ],
        "title": "4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar",
        "abstract": "arXiv:2509.12931v1 Announce Type: new  Abstract: 3D reconstruction and novel view synthesis are critical for validating autonomous driving systems and training advanced perception models. Recent self-supervised methods have gained significant attention due to their cost-effectiveness and enhanced generalization in scenarios where annotated bounding boxes are unavailable. However, existing approaches, which often rely on frequency-domain decoupling or optical flow, struggle to accurately reconstruct dynamic objects due to imprecise motion estimation and weak temporal consistency, resulting in incomplete or distorted representations of dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a 4D Radar-augmented self-supervised 3D reconstruction framework tailored for dynamic driving scenes. Specifically, we first present a 4D Radar-assisted Gaussian initialization scheme that leverages 4D Radar's velocity and spatial information to segment dynamic objects and recover monocular depth scale, generating accurate Gaussian point representations. In addition, we propose a Velocity-guided PointTrack (VGPT) model, which is jointly trained with the reconstruction pipeline under scene flow supervision, to track fine-grained dynamic trajectories and construct temporally consistent representations. Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art performance in dynamic driving scene 3D reconstruction.",
        "arxiv_id": "2509.12931",
        "ARXIVID": "2509.12931",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D reconstruction in dynamic driving scenes, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.13149": {
        "authors": [
            "Minqing Huang",
            "Shouyi Lu",
            "Boyuan Zheng",
            "Ziyao Li",
            "Xiao Tang",
            "Guirong Zhuo"
        ],
        "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation",
        "abstract": "arXiv:2509.13149v1 Announce Type: new  Abstract: 4D radar super-resolution, which aims to reconstruct sparse and noisy point clouds into dense and geometrically consistent representations, is a foundational problem in autonomous perception. However, existing methods often suffer from high training cost or rely on complex diffusion-based sampling, resulting in high inference latency and poor generalization, making it difficult to balance accuracy and efficiency. To address these limitations, we propose MSDNet, a multi-stage distillation framework that efficiently transfers dense LiDAR priors to 4D radar features to achieve both high reconstruction quality and computational efficiency. The first stage performs reconstruction-guided feature distillation, aligning and densifying the student's features through feature reconstruction. In the second stage, we propose diffusion-guided feature distillation, which treats the stage-one distilled features as a noisy version of the teacher's representations and refines them via a lightweight diffusion network. Furthermore, we introduce a noise adapter that adaptively aligns the noise level of the feature with a predefined diffusion timestep, enabling a more precise denoising. Extensive experiments on the VoD and in-house datasets demonstrate that MSDNet achieves both high-fidelity reconstruction and low-latency inference in the task of 4D radar point cloud super-resolution, and consistently improves performance on downstream tasks. The code will be publicly available upon publication.",
        "arxiv_id": "2509.13149",
        "ARXIVID": "2509.13149",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 4D radar super-resolution, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.12815": {
        "authors": [
            "Biwen Lei",
            "Yang Li",
            "Xinhai Liu",
            "Shuhui Yang",
            "Lixin Xu",
            "Jingwei Huang",
            "Ruining Tang",
            "Haohan Weng",
            "Jian Liu",
            "Jing Xu",
            "Zhen Zhou",
            "Yiling Zhu",
            "Jiankai Xing",
            "Jiachen Xu",
            "Changfeng Ma",
            "Xinhao Yan",
            "Yunhan Yang",
            "Chunshi Wang",
            "Duoteng Xu",
            "Xueqi Ma",
            "Yuguang Chen",
            "Jing Li",
            "Mingxin Yang",
            "Sheng Zhang",
            "Yifei Feng",
            "Xin Huang",
            "Di Luo",
            "Zebin He",
            "Puhua Jiang",
            "Changrong Hu",
            "Zihan Qin",
            "Shiwei Miao",
            "Haolin Liu",
            "Yunfei Zhao",
            "Zeqiang Lai",
            "Qingxiang Lin",
            "Zibo Zhao",
            "Kunhong Li",
            "Xianghui Yang",
            "Huiwen Shi",
            "Xin Yang",
            "Yuxuan Wang",
            "Zebin Yao",
            "Yihang Lian",
            "Sicong Liu",
            "Xintong Han",
            "Wangchen Qin",
            "Caisheng Ouyang",
            "Jianyin Liu",
            "Tianwen Yuan",
            "Shuai Jiang",
            "Hong Duan",
            "Yanqi Niu",
            "Wencong Lin",
            "Yifu Sun",
            "Shirui Huang",
            "Lin Niu",
            "Gu Gong",
            "Guojian Xiao",
            "Bojian Zheng",
            "Xiang Yuan",
            "Qi Chen",
            "Jie Xiao",
            "Dongyang Zheng",
            "Xiaofeng Yang",
            "Kai Liu",
            "Jianchen Zhu",
            "Lifu Wang",
            "Qinglin Lu",
            "Jie Liu",
            "Liang Dong",
            "Fan Jiang",
            "Ruibin Chen",
            "Lei Wang",
            "Chao Zhang",
            "Jiaxin Lin",
            "Hao Zhang",
            "Zheng Ye",
            "Peng He",
            "Runzhou Wu",
            "Yinhe Wu",
            "Jiayao Du",
            "Jupeng Chen",
            "Xinyue Mao",
            "Dongyuan Guo",
            "Yixuan Tang",
            "Yulin Tsai",
            "Yonghao Tan",
            "Jiaao Yu",
            "Junlin Yu",
            "Keren Zhang",
            "Yifan Li",
            "Peng Chen",
            "Tian Liu",
            "Di Wang",
            "Yuhong Liu",
            "Linus",
            "Jie Jiang",
            "Zhuo Chen",
            "Chunchao Guo"
        ],
        "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation",
        "abstract": "arXiv:2509.12815v1 Announce Type: new  Abstract: The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.",
        "arxiv_id": "2509.12815",
        "ARXIVID": "2509.12815",
        "COMMENT": "Matches criterion 3 as it introduces an AI-powered pipeline for 3D asset generation, which could be relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.12452": {
        "authors": [
            "Zhenxin Zhang",
            "Zhihua Xu",
            "Yuwei Cao",
            "Ningli Xu",
            "Shuye Wang",
            "Shen'ao Cui",
            "Zhen Li",
            "Rongjun Qin"
        ],
        "title": "Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications",
        "abstract": "arXiv:2509.12452v1 Announce Type: new  Abstract: Point cloud processing as a fundamental task in the field of geomatics and computer vision, has been supporting tasks and applications at different scales from air to ground, including mapping, environmental monitoring, urban/tree structure modeling, automated driving, robotics, disaster responses etc. Due to the rapid development of deep learning, point cloud processing algorithms have nowadays been almost explicitly dominated by learning-based approaches, most of which are yet transitioned into real-world practices. Existing surveys primarily focus on the ever-updating network architecture to accommodate unordered point clouds, largely ignoring their practical values in typical point cloud processing applications, in which extra-large volume of data, diverse scene contents, varying point density, data modality need to be considered. In this paper, we provide a meta review on deep learning approaches and datasets that cover a selection of critical tasks of point cloud processing in use such as scene completion, registration, semantic segmentation, and modeling. By reviewing a broad range of urban and environmental applications these tasks can support, we identify gaps to be closed as these methods transformed into applications and draw concluding remarks in both the algorithmic and practical aspects of the surveyed methods.",
        "arxiv_id": "2509.12452",
        "ARXIVID": "2509.12452",
        "COMMENT": "Matches criterion 7 as it is a survey paper on deep learning for 3D point cloud processing, synthesizing state-of-the-art methods and applications.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2509.12683": {
        "authors": [
            "Xianda Guo",
            "Chenming Zhang",
            "Ruilin Wang",
            "Youmin Zhang",
            "Wenzhao Zheng",
            "Matteo Poggi",
            "Hao Zhao",
            "Qin Zou",
            "Long Chen"
        ],
        "title": "StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo",
        "abstract": "arXiv:2509.12683v1 Announce Type: new  Abstract: Stereo matching plays a crucial role in enabling depth perception for autonomous driving and robotics. While recent years have witnessed remarkable progress in stereo matching algorithms, largely driven by learning-based methods and synthetic datasets, the generalization performance of these models remains constrained by the limited diversity of existing training data. To address these challenges, we present StereoCarla, a high-fidelity synthetic stereo dataset specifically designed for autonomous driving scenarios. Built on the CARLA simulator, StereoCarla incorporates a wide range of camera configurations, including diverse baselines, viewpoints, and sensor placements as well as varied environmental conditions such as lighting changes, weather effects, and road geometries. We conduct comprehensive cross-domain experiments across four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury, ETH3D) and demonstrate that models trained on StereoCarla outperform those trained on 11 existing stereo datasets in terms of generalization accuracy across multiple benchmarks. Furthermore, when integrated into multi-dataset training, StereoCarla contributes substantial improvements to generalization accuracy, highlighting its compatibility and scalability. This dataset provides a valuable benchmark for developing and evaluating stereo algorithms under realistic, diverse, and controllable settings, facilitating more robust depth perception systems for autonomous vehicles. Code can be available at https://github.com/XiandaGuo/OpenStereo, and data can be available at https://xiandaguo.net/StereoCarla.",
        "arxiv_id": "2509.12683",
        "ARXIVID": "2509.12683",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (StereoCarla) for stereo vision in autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.12810": {
        "authors": [
            "Shicheng Ye",
            "Chao Yu",
            "Kaiqiang Ke",
            "Chengdong Xu",
            "Yinqi Wei"
        ],
        "title": "H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents",
        "abstract": "arXiv:2509.12810v1 Announce Type: new  Abstract: Large language model (LLM)-based agents have shown strong potential in multi-task scenarios, owing to their ability to transfer knowledge across diverse tasks. However, existing approaches often treat prior experiences and knowledge as monolithic units, leading to inefficient and coarse-grained knowledge transfer. In this work, we propose a novel hierarchical memory architecture that enables fine-grained knowledge transfer by decoupling high-level planning memory from low-level execution memory. To construct and refine these hierarchical memories, we introduce Hierarchical Hindsight Reflection (H$^2$R), a mechanism that distills reusable and hierarchical knowledge from past agent-environment interactions. At test time, H$^2$R performs retrievals of high-level and low-level memories separately, allowing LLM-based agents to efficiently access and utilize task-relevant knowledge for new tasks.Experimental results across two benchmarks demonstrate that H$^2$R can improve generalization and decision-making performance, outperforming prior baselines such as Expel.",
        "arxiv_id": "2509.12810",
        "ARXIVID": "2509.12810",
        "COMMENT": "Matches criteria 3 as it proposes a novel hierarchical memory architecture for multi-task LLM agents, which could be relevant for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.13172": {
        "authors": [
            "Ruifei Ding",
            "Zhe Chen",
            "Wen Fan",
            "Chen Long",
            "Huijuan Xiao",
            "Yelu Zeng",
            "Zhen Dong",
            "Bisheng Yang"
        ],
        "title": "WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory",
        "abstract": "arXiv:2509.13172v1 Announce Type: new  Abstract: Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasks--tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: https://github.com/WHU-USI3DV/WHU-STree.",
        "arxiv_id": "2509.13172",
        "ARXIVID": "2509.13172",
        "COMMENT": "Matches criteria 3 as it introduces a new multi-modal benchmark dataset for urban street tree inventory, which could be relevant for embodied AI applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.13288": {
        "authors": [
            "Marjorie McShane",
            "Sergei Nirenburg",
            "Sanjay Oruganti",
            "Jesse English"
        ],
        "title": "Shapes of Cognition for Computational Cognitive Modeling",
        "abstract": "arXiv:2509.13288v1 Announce Type: new  Abstract: Shapes of cognition is a new conceptual paradigm for the computational cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are remembered constellations of sensory, linguistic, conceptual, episodic, and procedural knowledge that allow agents to cut through the complexity of real life the same way as people do: by expecting things to be typical, recognizing patterns, acting by habit, reasoning by analogy, satisficing, and generally minimizing cognitive load to the degree situations permit. Atypical outcomes are treated using shapes-based recovery methods, such as learning on the fly, asking a human partner for help, or seeking an actionable, even if imperfect, situational understanding. Although shapes is an umbrella term, it is not vague: shapes-based modeling involves particular objectives, hypotheses, modeling strategies, knowledge bases, and actual models of wide-ranging phenomena, all implemented within a particular cognitive architecture. Such specificity is needed both to vet our hypotheses and to achieve our practical aims of building useful agent systems that are explainable, extensible, and worthy of our trust, even in critical domains. However, although the LEIA example of shapes-based modeling is specific, the principles can be applied more broadly, giving new life to knowledge-based and hybrid AI.",
        "arxiv_id": "2509.13288",
        "ARXIVID": "2509.13288",
        "COMMENT": "Matches criterion 1 as it discusses computational cognitive modeling for embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.12482": {
        "authors": [
            "Tianshu Huang",
            "Akarsh Prabhakara",
            "Chuhan Chen",
            "Jay Karhade",
            "Deva Ramanan",
            "Matthew O'Toole",
            "Anthony Rowe"
        ],
        "title": "Towards Foundational Models for Single-Chip Radar",
        "abstract": "arXiv:2509.12482v1 Announce Type: new  Abstract: mmWave radars are compact, inexpensive, and durable sensors that are robust to occlusions and work regardless of environmental conditions, such as weather and darkness. However, this comes at the cost of poor angular resolution, especially for inexpensive single-chip radars, which are typically used in automotive and indoor sensing applications. Although many have proposed learning-based methods to mitigate this weakness, no standardized foundational models or large datasets for the mmWave radar have emerged, and practitioners have largely trained task-specific models from scratch using relatively small datasets.   In this paper, we collect (to our knowledge) the largest available raw radar dataset with 1M samples (29 hours) and train a foundational model for 4D single-chip radar, which can predict 3D occupancy and semantic segmentation with quality that is typically only possible with much higher resolution sensors. We demonstrate that our Generalizable Radar Transformer (GRT) generalizes across diverse settings, can be fine-tuned for different tasks, and shows logarithmic data scaling of 20\\% per $10\\times$ data. We also run extensive ablations on common design decisions, and find that using raw radar data significantly outperforms widely-used lossy representations, equivalent to a $10\\times$ increase in training data. Finally, we roughly estimate that $\\approx$100M samples (3000 hours) of data are required to fully exploit the potential of GRT.",
        "arxiv_id": "2509.12482",
        "ARXIVID": "2509.12482",
        "COMMENT": "Matches criterion 4 as it focuses on foundational models for radar, which can be considered a vision foundation model in a specific domain.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.13116": {
        "authors": [
            "Ruibo Li",
            "Hanyu Shi",
            "Zhe Wang",
            "Guosheng Lin"
        ],
        "title": "Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving",
        "abstract": "arXiv:2509.13116v1 Announce Type: new  Abstract: Understanding motion in dynamic environments is critical for autonomous driving, thereby motivating research on class-agnostic motion prediction. In this work, we investigate weakly and self-supervised class-agnostic motion prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile foregrounds and static backgrounds, allowing motion understanding to be associated with scene parsing. Based on this observation, we propose a novel weakly supervised paradigm that replaces motion annotations with fully or partially annotated (1%, 0.1%) foreground/background masks for supervision. To this end, we develop a weakly supervised approach utilizing foreground/background cues to guide the self-supervised learning of motion prediction models. Since foreground motion generally occurs in non-ground regions, non-ground/ground masks can serve as an alternative to foreground/background masks, further reducing annotation effort. Leveraging non-ground/ground cues, we propose two additional approaches: a weakly supervised method requiring fewer (0.01%) foreground/background annotations, and a self-supervised method without annotations. Furthermore, we design a Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame information and robust penalty functions to suppress outliers in self-supervised learning. Experiments show that our weakly and self-supervised models outperform existing self-supervised counterparts, and our weakly supervised models even rival some supervised ones. This demonstrates that our approaches effectively balance annotation effort and performance.",
        "arxiv_id": "2509.13116",
        "ARXIVID": "2509.13116",
        "COMMENT": "Matches criterion 3 as it proposes weakly and self-supervised methods for motion prediction in autonomous driving, addressing challenges in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.12248": {
        "authors": [
            "Yuriel Ryan",
            "Rui Yang Tan",
            "Kenny Tsu Wei Choo",
            "Roy Ka-Wei Lee"
        ],
        "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics",
        "abstract": "arXiv:2509.12248v1 Announce Type: new  Abstract: Understanding humor is a core aspect of social intelligence, yet it remains a significant challenge for Large Multimodal Models (LMMs). We introduce PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed to evaluate LMMs' ability to interpret multimodal humor and recognize narrative sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for instance, top models achieve only 61% accuracy in panel sequencing, far below human performance. This underscores critical limitations in current models' integration of visual and textual cues for coherent narrative and humor understanding. By providing a rigorous framework for evaluating multimodal contextual and narrative reasoning, PixelHumor aims to drive the development of LMMs that better engage in natural, socially aware interactions.",
        "arxiv_id": "2509.12248",
        "ARXIVID": "2509.12248",
        "COMMENT": "Matches criterion 2 as it benchmarks large multimodal models (LMMs) for understanding online comics, focusing on vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.12893": {
        "authors": [
            "Yiyi Zhang",
            "Yuchen Yuan",
            "Ying Zheng",
            "Jialun Pei",
            "Jinpeng Li",
            "Zheng Li",
            "Pheng-Ann Heng"
        ],
        "title": "MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization",
        "abstract": "arXiv:2509.12893v1 Announce Type: new  Abstract: Surgical triplet recognition, which involves identifying instrument, verb, target, and their combinations, is a complex surgical scene understanding challenge plagued by long-tailed data distribution. The mainstream multi-task learning paradigm benefiting from cross-task collaborative promotion has shown promising performance in identifying triples, but two key challenges remain: 1) inter-task optimization conflicts caused by entangling task-generic and task-specific representations; 2) intra-task optimization conflicts due to class-imbalanced training data. To overcome these difficulties, we propose the MLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and intra-task optimization for surgical triplet recognition. For inter-task optimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning scheme that decomposes representations into task-shared and task-specific components. To enhance task-shared representations, we construct a Multimodal Large Language Model (MLLM) powered probabilistic prompt pool to dynamically augment visual features with expert-level semantic cues. Additionally, comprehensive task-specific cues are modeled via distinct task prompts covering the temporal-spatial dimensions, effectively mitigating inter-task ambiguities. To tackle intra-task optimization conflicts, we develop a Coordinated Gradient Learning (CGL) strategy, which dissects and rebalances the positive-negative gradients originating from head and tail classes for more coordinated learning behaviors. Extensive experiments on the CholecT45 and CholecT50 datasets demonstrate the superiority of our proposed framework, validating its effectiveness in handling optimization conflicts.",
        "arxiv_id": "2509.12893",
        "ARXIVID": "2509.12893",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) for surgical scene understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.12818": {
        "authors": [
            "Maximilian Ilse",
            "Harshita Sharma",
            "Anton Schwaighofer",
            "Sam Bond-Taylor",
            "Fernando P\\'erez-Garc\\'ia",
            "Olesya Melnichenko",
            "Anne-Marie G. Sykes",
            "Kelly K. Horst",
            "Ashish Khandelwal",
            "Maxwell Reynolds",
            "Maria T. Wetscherek",
            "Noel C. F. Codella",
            "Javier Alvarez-Valle",
            "Korfiatis Panagiotis",
            "Valentina Salvatelli"
        ],
        "title": "Data Scaling Laws for Radiology Foundation Models",
        "abstract": "arXiv:2509.12818v1 Announce Type: new  Abstract: Foundation vision encoders such as CLIP and DINOv2, trained on web-scale data, exhibit strong transfer performance across tasks and datasets. However, medical imaging foundation models remain constrained by smaller datasets, limiting our understanding of how data scale and pretraining paradigms affect performance in this setting. In this work, we systematically study continual pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M chest x-rays from a single institution, holding compute and evaluation protocols constant. We evaluate on classification (radiology findings, lines and tubes), segmentation (lines and tubes), and radiology report generation. While prior work has primarily focused on tasks related to radiology findings, we include lines and tubes tasks to counterbalance this bias and evaluate a model's ability to extract features that preserve continuity along elongated structures. Our experiments show that MI2 scales more effectively for finding-related tasks, while RAD-DINO is stronger on tube-related tasks. Surprisingly, continually pretraining MI2 with both reports and structured labels using UniCL improves performance, underscoring the value of structured supervision at scale. We further show that for some tasks, as few as 30k in-domain samples are sufficient to surpass open-weights foundation models. These results highlight the utility of center-specific continual pretraining, enabling medical institutions to derive significant performance gains by utilizing in-domain data.",
        "arxiv_id": "2509.12818",
        "ARXIVID": "2509.12818",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their applications in medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.12701": {
        "authors": [
            "Wenzhuo Jin",
            "Qianfeng Yang",
            "Xianhao Wu",
            "Hongming Chen",
            "Pengpeng Li",
            "Xiang Chen"
        ],
        "title": "SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes",
        "abstract": "arXiv:2509.12701v1 Announce Type: new  Abstract: Early-stage fire scenes (0-15 minutes after ignition) represent a crucial temporal window for emergency interventions. During this stage, the smoke produced by combustion significantly reduces the visibility of surveillance systems, severely impairing situational awareness and hindering effective emergency response and rescue operations. Consequently, there is an urgent need to remove smoke from images to obtain clear scene information. However, the development of smoke removal algorithms remains limited due to the lack of large-scale, real-world datasets comprising paired smoke-free and smoke-degraded images. To address these limitations, we present a real-world surveillance image desmoking benchmark dataset named SmokeBench, which contains image pairs captured under diverse scenes setup and smoke concentration. The curated dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of desmoking methods on our dataset. Our dataset provides a valuable foundation for advancing robust and practical image desmoking in real-world fire scenes. This dataset has been released to the public and can be downloaded from https://github.com/ncfjd/SmokeBench.",
        "arxiv_id": "2509.12701",
        "ARXIVID": "2509.12701",
        "COMMENT": "Does not match any specific criteria but introduces a new dataset for image desmoking, which is relevant to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12501": {
        "authors": [
            "Yao He",
            "Youngjoong Kwon",
            "Wenxiao Cai",
            "Ehsan Adeli"
        ],
        "title": "Artist-Created Mesh Generation from Raw Observation",
        "abstract": "arXiv:2509.12501v1 Announce Type: new  Abstract: We present an end-to-end framework for generating artist-style meshes from noisy or incomplete point clouds, such as those captured by real-world sensors like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for commercial graphics pipelines due to their compatibility with animation and texturing tools and their efficiency in rendering. However, existing approaches often assume clean, complete inputs or rely on complex multi-stage pipelines, limiting their applicability in real-world scenarios. To address this, we propose an end-to-end method that refines the input point cloud and directly produces high-quality, artist-style meshes. At the core of our approach is a novel reformulation of 3D point cloud refinement as a 2D inpainting task, enabling the use of powerful generative models. Preliminary results on the ShapeNet dataset demonstrate the promise of our framework in producing clean, complete meshes.",
        "arxiv_id": "2509.12501",
        "ARXIVID": "2509.12501",
        "COMMENT": "Does not match any specific criterion but is related to general interest in computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12878": {
        "authors": [
            "Qianguang Zhao",
            "Dongli Wang",
            "Yan Zhou",
            "Jianxun Li",
            "Richard Irampa"
        ],
        "title": "Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation",
        "abstract": "arXiv:2509.12878v1 Announce Type: new  Abstract: Few-shot 3D point cloud semantic segmentation aims to segment novel categories using a minimal number of annotated support samples. While existing prototype-based methods have shown promise, they are constrained by two critical challenges: (1) Intra-class Diversity, where a prototype's limited representational capacity fails to cover a class's full variations, and (2) Inter-set Inconsistency, where prototypes derived from the support set are misaligned with the query feature space. Motivated by the powerful generative capability of diffusion model, we re-purpose its pre-trained conditional encoder to provide a novel source of generalizable features for expanding the prototype's representational range. Under this setup, we introduce the Prototype Expansion Network (PENet), a framework that constructs big-capacity prototypes from two complementary feature sources. PENet employs a dual-stream learner architecture: it retains a conventional fully supervised Intrinsic Learner (IL) to distill representative features, while introducing a novel Diffusion Learner (DL) to provide rich generalizable features. The resulting dual prototypes are then processed by a Prototype Assimilation Module (PAM), which adopts a novel push-pull cross-guidance attention block to iteratively align the prototypes with the query space. Furthermore, a Prototype Calibration Mechanism (PCM) regularizes the final big capacity prototype to prevent semantic drift. Extensive experiments on the S3DIS and ScanNet datasets demonstrate that PENet significantly outperforms state-of-the-art methods across various few-shot settings.",
        "arxiv_id": "2509.12878",
        "ARXIVID": "2509.12878",
        "COMMENT": "Does not match any specific criterion but is related to general interest in machine learning and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12434": {
        "authors": [
            "Jiahao Yu",
            "Zelei Cheng",
            "Xian Wu",
            "Xinyu Xing"
        ],
        "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization",
        "abstract": "arXiv:2509.12434v1 Announce Type: new  Abstract: Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues.   A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs.   While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS.   Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents.   To bridge this gap, we introduce \\sys, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting.   \\sys augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses.   We validate \\sys by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).   To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches.   On the \\swebench leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with \\sys ranks 1st on \\lite and 4th on \\verified on the open-weight leaderboard, surpassed only by models with over 10x more parameters(\\eg$>$350B).",
        "arxiv_id": "2509.12434",
        "ARXIVID": "2509.12434",
        "COMMENT": "Does not match any specific criterion but is related to general interest in machine learning and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12787": {
        "authors": [
            "Linchun Wu",
            "Qin Zou",
            "Xianbiao Qi",
            "Bo Du",
            "Zhongyuan Wang",
            "Qingquan Li"
        ],
        "title": "Double Helix Diffusion for Cross-Domain Anomaly Image Generation",
        "abstract": "arXiv:2509.12787v1 Announce Type: new  Abstract: Visual anomaly inspection is critical in manufacturing, yet hampered by the scarcity of real anomaly samples for training robust detectors. Synthetic data generation presents a viable strategy for data augmentation; however, current methods remain constrained by two principal limitations: 1) the generation of anomalies that are structurally inconsistent with the normal background, and 2) the presence of undesirable feature entanglement between synthesized images and their corresponding annotation masks, which undermines the perceptual realism of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel cross-domain generative framework designed to simultaneously synthesize high-fidelity anomaly images and their pixel-level annotation masks, explicitly addressing these challenges. DH-Diff employs a unique architecture inspired by a double helix, cycling through distinct modules for feature separation, connection, and merging. Specifically, a domain-decoupled attention mechanism mitigates feature entanglement by enhancing image and annotation features independently, and meanwhile a semantic score map alignment module ensures structural authenticity by coherently integrating anomaly foregrounds. DH-Diff offers flexible control via text prompts and optional graphical guidance. Extensive experiments demonstrate that DH-Diff significantly outperforms state-of-the-art methods in diversity and authenticity, leading to significant improvements in downstream anomaly detection performance.",
        "arxiv_id": "2509.12787",
        "ARXIVID": "2509.12787",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and anomaly detection.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.13235": {
        "authors": [
            "Linyue Cai",
            "Yuyang Cheng",
            "Xiaoding Shao",
            "Huiming Wang",
            "Yong Zhao",
            "Wei Zhang",
            "Kang Li"
        ],
        "title": "A Scenario-Driven Cognitive Approach to Next-Generation AI Memory",
        "abstract": "arXiv:2509.13235v1 Announce Type: new  Abstract: As artificial intelligence advances toward artificial general intelligence (AGI), the need for robust and human-like memory systems has become increasingly evident. Current memory architectures often suffer from limited adaptability, insufficient multimodal integration, and an inability to support continuous learning. To address these limitations, we propose a scenario-driven methodology that extracts essential functional requirements from representative cognitive scenarios, leading to a unified set of design principles for next-generation AI memory systems. Based on this approach, we introduce the \\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that integrates cognitive scenarios, memory processes, and storage mechanisms into a cohesive design. COLMA provides a structured foundation for developing AI systems capable of lifelong learning and human-like reasoning, thereby contributing to the pragmatic development of AGI.",
        "arxiv_id": "2509.13235",
        "ARXIVID": "2509.13235",
        "COMMENT": "Does not closely match any specific criterion but discusses a novel memory architecture for AGI, which may have indirect relevance to embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12711": {
        "authors": [
            "Haozhe Zhang",
            "Chenchen Jing",
            "Mingyu Liu",
            "Qingsheng Wang",
            "Hao Chen"
        ],
        "title": "Learning by Imagining: Debiased Feature Augmentation for Compositional Zero-Shot Learning",
        "abstract": "arXiv:2509.12711v1 Announce Type: new  Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute-object compositions by learning prior knowledge of seen primitives, \\textit{i.e.}, attributes and objects. Learning generalizable compositional representations in CZSL remains challenging due to the entangled nature of attributes and objects as well as the prevalence of long-tailed distributions in real-world data. Inspired by neuroscientific findings that imagination and perception share similar neural processes, we propose a novel approach called Debiased Feature Augmentation (DeFA) to address these challenges. The proposed DeFA integrates a disentangle-and-reconstruct framework for feature augmentation with a debiasing strategy. DeFA explicitly leverages the prior knowledge of seen attributes and objects by synthesizing high-fidelity composition features to support compositional generalization. Extensive experiments on three widely used datasets demonstrate that DeFA achieves state-of-the-art performance in both \\textit{closed-world} and \\textit{open-world} settings.",
        "arxiv_id": "2509.12711",
        "ARXIVID": "2509.12711",
        "COMMENT": "Does not match any specific criteria but is related to compositional zero-shot learning, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12743": {
        "authors": [
            "Hanqing Li",
            "Kiran Sheena Jyothi",
            "Henry Liang",
            "Sharika Mahadevan",
            "Diego Klabjan"
        ],
        "title": "Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs",
        "abstract": "arXiv:2509.12743v1 Announce Type: new  Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval Augmented Framework (GRRAF), that harnesses retrieval-augmented generation (RAG) alongside the code-generation capabilities of large language models (LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target graph is stored in a graph database, and the LLM is prompted to generate executable code queries that retrieve the necessary information. This approach circumvents the limitations of existing methods that require extensive finetuning or depend on predefined algorithms, and it incorporates an error feedback loop with a time-out mechanism to ensure both correctness and efficiency. Experimental evaluations on the GraphInstruct dataset reveal that GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle detection, bipartite graph checks, shortest path computation, and maximum flow, while maintaining consistent token costs regardless of graph sizes. Imperfect but still very high performance is observed on subgraph matching. Notably, GRRAF scales effectively to large graphs with up to 10,000 nodes.",
        "arxiv_id": "2509.12743",
        "ARXIVID": "2509.12743",
        "COMMENT": "Does not match any specific criteria but is tangentially related to reasoning tasks, which could be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12987": {
        "authors": [
            "Yarin Benyamin",
            "Argaman Mordoch",
            "Shahaf S. Shperberg",
            "Roni Stern"
        ],
        "title": "Toward PDDL Planning Copilot",
        "abstract": "arXiv:2509.12987v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents capable of performing complicated tasks. However, they lack the ability to perform reliable long-horizon planning on their own. This paper bridges this gap by introducing the Planning Copilot, a chatbot that integrates multiple planning tools and allows users to invoke them through instructions in natural language. The Planning Copilot leverages the Model Context Protocol (MCP), a recently developed standard for connecting LLMs with external tools and systems. This approach allows using any LLM that supports MCP without domain-specific fine-tuning. Our Planning Copilot supports common planning tasks such as checking the syntax of planning problems, selecting an appropriate planner, calling it, validating the plan it generates, and simulating their execution. We empirically evaluate the ability of our Planning Copilot to perform these tasks using three open-source LLMs. The results show that the Planning Copilot highly outperforms using the same LLMs without the planning tools. We also conducted a limited qualitative comparison of our tool against Chat GPT-5, a very recent commercial LLM. Our results shows that our Planning Copilot significantly outperforms GPT-5 despite relying on a much smaller LLM. This suggests dedicated planning tools may be an effective way to enable LLMs to perform planning tasks.",
        "arxiv_id": "2509.12987",
        "ARXIVID": "2509.12987",
        "COMMENT": "Does not match any specific criteria but is tangentially related to embodied AI through planning tools.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.13084": {
        "authors": [
            "Yunyao Lu",
            "Yihang Wu",
            "Ahmad Chaddad",
            "Tareef Daqqaq",
            "Reem Kateb"
        ],
        "title": "Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling",
        "abstract": "arXiv:2509.13084v1 Announce Type: new  Abstract: Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\\% Dice score on left Atrial with 10\\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments.",
        "arxiv_id": "2509.13084",
        "ARXIVID": "2509.13084",
        "COMMENT": "This paper does not match any specific criteria but is related to semi-supervised learning and medical image segmentation, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12633": {
        "authors": [
            "Liming Lu",
            "Shuchao Pang",
            "Xu Zheng",
            "Xiang Gu",
            "Anan Du",
            "Yunhuai Liu",
            "Yongbin Zhou"
        ],
        "title": "CIARD: Cyclic Iterative Adversarial Robustness Distillation",
        "abstract": "arXiv:2509.12633v1 Announce Type: new  Abstract: Adversarial robustness distillation (ARD) aims to transfer both performance and robustness from teacher model to lightweight student model, enabling resilient performance on resource-constrained scenarios. Though existing ARD approaches enhance student model's robustness, the inevitable by-product leads to the degraded performance on clean examples. We summarize the causes of this problem inherent in existing methods with dual-teacher framework as: 1. The divergent optimization objectives of dual-teacher models, i.e., the clean and robust teachers, impede effective knowledge transfer to the student model, and 2. The iteratively generated adversarial examples during training lead to performance deterioration of the robust teacher model. To address these challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key innovations: a. A multi-teacher framework with contrastive push-loss alignment to resolve conflicts in dual-teacher optimization objectives, and b. Continuous adversarial retraining to maintain dynamic teacher robustness against performance degradation from the varying adversarial examples. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD achieves remarkable performance with an average 3.53 improvement in adversarial defense rates across various attack scenarios and a 5.87 increase in clean sample accuracy, establishing a new benchmark for balancing model robustness and generalization. Our code is available at https://github.com/eminentgu/CIARD",
        "arxiv_id": "2509.12633",
        "ARXIVID": "2509.12633",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.13281": {
        "authors": [
            "Vincent Siu",
            "Nathan W. Henry",
            "Nicholas Crispino",
            "Yang Liu",
            "Dawn Song",
            "Chenguang Wang"
        ],
        "title": "RepIt: Representing Isolated Targets to Steer Language Models",
        "abstract": "arXiv:2509.13281v1 Announce Type: new  Abstract: While activation steering in large language models (LLMs) is a growing area of research, methods can often incur broader effects than desired. This motivates isolation of purer concept vectors to enable targeted interventions and understand LLM behavior at a more granular level. We present RepIt, a simple and data-efficient framework for isolating concept-specific representations. Across five frontier LLMs, RepIt enables precise interventions: it selectively suppresses refusal on targeted concepts while preserving refusal elsewhere, producing models that answer WMD-related questions while still scoring as safe on standard benchmarks. We further show that the corrective signal localizes to just 100-200 neurons and that robust target representations can be extracted from as few as a dozen examples on a single A6000. This efficiency raises a dual concern: manipulations can be performed with modest compute and data to extend to underrepresented data-scarce topics while evading existing benchmarks. By disentangling refusal vectors with RepIt, this work demonstrates that targeted interventions can counteract overgeneralization, laying the foundation for more granular control of model behavior.",
        "arxiv_id": "2509.13281",
        "ARXIVID": "2509.13281",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12905": {
        "authors": [
            "Branko Mitic",
            "Philipp Seeb\\\"ock",
            "Helmut Prosch",
            "Georg Langs"
        ],
        "title": "AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring",
        "abstract": "arXiv:2509.12905v1 Announce Type: new  Abstract: Early detection of newly emerging diseases, lesion severity assessment, differentiation of medical conditions and automated screening are examples for the wide applicability and importance of anomaly detection (AD) and unsupervised segmentation in medicine. Normal fine-grained tissue variability such as present in pulmonary anatomy is a major challenge for existing generative AD methods. Here, we propose a novel generative AD approach addressing this issue. It consists of an image-to-image translation for anomaly-free reconstruction and a subsequent patch similarity scoring between observed and generated image-pairs for precise anomaly localization. We validate the new method on chest computed tomography (CT) scans for the detection and segmentation of infectious disease lesions. To assess generalizability, we evaluate the method on an ischemic stroke lesion segmentation task in T1-weighted brain MRI. Results show improved pixel-level anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score improvements of +1.9% and +4.4%, respectively, compared to other state-of-the-art reconstruction-based methods.",
        "arxiv_id": "2509.12905",
        "ARXIVID": "2509.12905",
        "COMMENT": "Does not match any specific criteria but discusses anomaly detection in medical imaging, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12464": {
        "authors": [
            "Ryan Lucas",
            "Kayhan Behdin",
            "Zhipeng Wang",
            "Qingquan Song",
            "Shao Tang",
            "Rahul Mazumder"
        ],
        "title": "Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction",
        "abstract": "arXiv:2509.12464v1 Announce Type: new  Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought traces during inference time which make them costly to deploy at scale. We show that using compression techniques such as neural network pruning produces greater performance loss than in typical language modeling tasks, and in some cases can make the model slower since they cause the model to produce more thinking tokens but with worse performance. We show that this is partly due to the fact that standard LLM pruning methods often focus on input reconstruction, whereas reasoning is a decode-dominated task. We introduce a simple, drop-in fix: during pruning we jointly reconstruct activations from the input and the model's on-policy chain-of-thought traces. This \"Reasoning-Aware Compression\" (RAC) integrates seamlessly into existing pruning workflows such as SparseGPT, and boosts their performance significantly. Code reproducing the results in the paper can be found at: https://github.com/RyanLucas3/RAC",
        "arxiv_id": "2509.12464",
        "ARXIVID": "2509.12464",
        "COMMENT": "Does not match any specific criteria but discusses reasoning-aware compression for language models, which is tangentially related to spatial reasoning or embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12958": {
        "authors": [
            "Bihao Zhan",
            "Jie Zhou",
            "Junsong Li",
            "Yutao Yang",
            "Shilian Chen",
            "Qianjun Pan",
            "Xin Li",
            "Wen Wu",
            "Xingjiao Wu",
            "Qin Chen",
            "Hang Yan",
            "Liang He"
        ],
        "title": "Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning",
        "abstract": "arXiv:2509.12958v1 Announce Type: new  Abstract: Continual Learning (CL) models, while adept at sequential knowledge acquisition, face significant and often overlooked privacy challenges due to accumulating diverse information. Traditional privacy methods, like a uniform Differential Privacy (DP) budget, indiscriminately protect all data, leading to substantial model utility degradation and hindering CL deployment in privacy-sensitive areas. To overcome this, we propose a privacy-enhanced continual learning (PeCL) framework that forgets what's sensitive and remembers what matters. Our approach first introduces a token-level dynamic Differential Privacy strategy that adaptively allocates privacy budgets based on the semantic sensitivity of individual tokens. This ensures robust protection for private entities while minimizing noise injection for non-sensitive, general knowledge. Second, we integrate a privacy-guided memory sculpting module. This module leverages the sensitivity analysis from our dynamic DP mechanism to intelligently forget sensitive information from the model's memory and parameters, while explicitly preserving the task-invariant historical knowledge crucial for mitigating catastrophic forgetting. Extensive experiments show that PeCL achieves a superior balance between privacy preserving and model utility, outperforming baseline models by maintaining high accuracy on previous tasks while ensuring robust privacy.",
        "arxiv_id": "2509.12958",
        "ARXIVID": "2509.12958",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and privacy in continual learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.13229": {
        "authors": [
            "Hugo Carlesso",
            "Josiane Mothe",
            "Radu Tudor Ionescu"
        ],
        "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation",
        "abstract": "arXiv:2509.13229v1 Announce Type: new  Abstract: Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at https://github.com/hugocarlesso/CMTSSL.",
        "arxiv_id": "2509.13229",
        "ARXIVID": "2509.13229",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of machine learning and lightweight architectures.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.13107": {
        "authors": [
            "Kohou Wang",
            "Huan Hu",
            "Xiang Liu",
            "Zezhou Chen",
            "Ping Chen",
            "Zhaoxiang Liu",
            "Shiguo Lian"
        ],
        "title": "Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection - The 2024 Global Deepfake Image Detection Challenge",
        "abstract": "arXiv:2509.13107v1 Announce Type: new  Abstract: The proliferation of sophisticated deepfake technology poses significant challenges to digital security and authenticity. Detecting these forgeries, especially across a wide spectrum of manipulation techniques, requires robust and generalized models. This paper introduces the Hierarchical Deep Fusion Framework (HDFF), an ensemble-based deep learning architecture designed for high-performance facial forgery detection. Our framework integrates four diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT, which are meticulously fine-tuned through a multi-stage process on the MultiFFDI dataset. By concatenating the feature representations from these specialized models and training a final classifier layer, HDFF effectively leverages their collective strengths. This approach achieved a final score of 0.96852 on the competition's private leaderboard, securing the 20th position out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex image classification tasks.",
        "arxiv_id": "2509.13107",
        "ARXIVID": "2509.13107",
        "COMMENT": "Does not match any specific criterion but is related to general interest in computer vision and detection tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.13214": {
        "authors": [
            "Fei Wang",
            "Xuecheng Wu",
            "Zheng Zhang",
            "Danlei Huang",
            "Yuheng Huang",
            "BoWang"
        ],
        "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection",
        "abstract": "arXiv:2509.13214v1 Announce Type: new  Abstract: The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.",
        "arxiv_id": "2509.13214",
        "ARXIVID": "2509.13214",
        "COMMENT": "Does not match any specific criterion but is related to general interest in generative modeling and detection tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12763": {
        "authors": [
            "Yican Zhao",
            "Ce Wang",
            "You Hao",
            "Lei Li",
            "Tianli Liao"
        ],
        "title": "DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation",
        "abstract": "arXiv:2509.12763v1 Announce Type: new  Abstract: Medical image segmentation grapples with challenges including multi-scale lesion variability, ill-defined tissue boundaries, and computationally intensive processing demands. This paper proposes the DyGLNet, which achieves efficient and accurate segmentation by fusing global and local features with a dynamic upsampling mechanism. The model innovatively designs a hybrid feature extraction module (SHDCBlock), combining single-head self-attention and multi-scale dilated convolutions to model local details and global context collaboratively. We further introduce a dynamic adaptive upsampling module (DyFusionUp) to realize high-fidelity reconstruction of feature maps based on learnable offsets. Then, a lightweight design is adopted to reduce computational overhead. Experiments on seven public datasets demonstrate that DyGLNet outperforms existing methods, particularly excelling in boundary accuracy and small-object segmentation. Meanwhile, it exhibits lower computation complexity, enabling an efficient and reliable solution for clinical medical image analysis. The code will be made available soon.",
        "arxiv_id": "2509.12763",
        "ARXIVID": "2509.12763",
        "COMMENT": "Does not match any specific criterion but is related to general interest in computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12279": {
        "authors": [
            "He Gao",
            "Baoxiang Huang",
            "Milena Radenkovic",
            "Borui Li",
            "Ge Chen"
        ],
        "title": "Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance",
        "abstract": "arXiv:2509.12279v1 Announce Type: new  Abstract: Synthetic Aperture Radar (SAR), with its all- weather and wide-area observation capabilities, serves as a crucial tool for wake detection. However, due to its complex imaging mechanism, wake features in SAR images often appear abstract and noisy, posing challenges for accurate annotation. In contrast, optical images provide more distinct visual cues, but models trained on optical data suffer from performance degradation when applied to SAR images due to domain shift. To address this cross-modal domain adaptation challenge, we propose a Similarity-Guided and Memory-Guided Domain Adap- tation (termed SimMemDA) framework for unsupervised domain adaptive ship wake detection via instance-level feature similarity filtering and feature memory guidance. Specifically, to alleviate the visual discrepancy between optical and SAR images, we first utilize WakeGAN to perform style transfer on optical images, generating pseudo-images close to the SAR style. Then, instance-level feature similarity filtering mechanism is designed to identify and prioritize source samples with target-like dis- tributions, minimizing negative transfer. Meanwhile, a Feature- Confidence Memory Bank combined with a K-nearest neighbor confidence-weighted fusion strategy is introduced to dynamically calibrate pseudo-labels in the target domain, improving the reliability and stability of pseudo-labels. Finally, the framework further enhances generalization through region-mixed training, strategically combining source annotations with calibrated tar- get pseudo-labels. Experimental results demonstrate that the proposed SimMemDA method can improve the accuracy and robustness of cross-modal ship wake detection tasks, validating the effectiveness and feasibility of the proposed method.",
        "arxiv_id": "2509.12279",
        "ARXIVID": "2509.12279",
        "COMMENT": "Does not match any specific criteria but focuses on domain adaptation for SAR wake detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12768": {
        "authors": [
            "Mohammed Al-Habib",
            "Zuping Zhang",
            "Abdulrahman Noman"
        ],
        "title": "BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers",
        "abstract": "arXiv:2509.12768v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have shown significant promise in computer vision applications. However, their performance in few-shot learning is limited by challenges in refining token-level interactions, struggling with limited training data, and developing a strong inductive bias. Existing methods often depend on inflexible token matching or basic similarity measures, which limit the effective incorporation of global context and localized feature refinement. To address these challenges, we propose Bi-Level Adaptive Token Refinement for Few-Shot Transformers (BATR-FST), a two-stage approach that progressively improves token representations and maintains a robust inductive bias for few-shot classification. During the pre-training phase, Masked Image Modeling (MIM) provides Vision Transformers (ViTs) with transferable patch-level representations by recreating masked image regions, providing a robust basis for subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates a Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to capture localized interactions, Uncertainty-Aware Token Weighting to prioritize dependable features, and a Bi-Level Attention mechanism to balance intra-cluster and inter-cluster relationships, thereby facilitating thorough token refinement. Furthermore, Graph Token Propagation ensures semantic consistency between support and query instances, while a Class Separation Penalty preserves different class borders, enhancing discriminative capability. Extensive experiments on three benchmark few-shot datasets demonstrate that BATR-FST achieves superior results in both 1-shot and 5-shot scenarios and improves the few-shot classification via transformers.",
        "arxiv_id": "2509.12768",
        "ARXIVID": "2509.12768",
        "COMMENT": "Does not match any specific criteria but focuses on few-shot learning with Vision Transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.13083": {
        "authors": [
            "Yan Xingyang",
            "Huang Xiaohong",
            "Zhang Zhao",
            "You Tian",
            "Xu Ziheng"
        ],
        "title": "Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement",
        "abstract": "arXiv:2509.13083v1 Announce Type: new  Abstract: In the Fourier domain, luminance information is primarily encoded in the amplitude spectrum, while spatial structures are captured in the phase components. The traditional Fourier Frequency information fitting employs pixel-wise loss functions, which tend to focus excessively on local information and may lead to global information loss. In this paper, we present LLFDisc, a U-shaped deep enhancement network that integrates cross-attention and gating mechanisms tailored for frequency-aware enhancement. We propose a novel distribution-aware loss that directly fits the Fourier-domain information and minimizes their divergence using a closed-form KL-Divergence objective. This enables the model to align Fourier-domain information more robustly than with conventional MSE-based losses. Furthermore, we enhance the perceptual loss based on VGG by embedding KL-Divergence on extracted deep features, enabling better structural fidelity. Extensive experiments across multiple benchmarks demonstrate that LLFDisc achieves state-of-the-art performance in both qualitative and quantitative evaluations. Our code will be released at: https://github.com/YanXY000/LLFDisc",
        "arxiv_id": "2509.13083",
        "ARXIVID": "2509.13083",
        "COMMENT": "Does not match any specific criteria but focuses on low-light image enhancement using novel loss functions.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12496": {
        "authors": [
            "Ali Torabi",
            "Sanjog Gaihre",
            "MD Mahbubur Rahman",
            "Yaqoob Majeed"
        ],
        "title": "Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation",
        "abstract": "arXiv:2509.12496v1 Announce Type: new  Abstract: Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of training segmentation models using only image-level annotations, eliminating the need for expensive pixel-level labeling. While existing methods struggle with precise object boundary localization and often focus only on the most discriminative regions, we propose IG-CAM (Instance-Guided Class Activation Mapping), a novel approach that leverages instance-level cues and influence functions to generate high-quality, boundary-aware localization maps. Our method introduces three key innovations: (1) Instance-Guided Refinement that uses ground truth segmentation masks to guide CAM generation, ensuring complete object coverage rather than just discriminative parts; (2) Influence Function Integration that captures the relationship between training samples and model predictions, leading to more robust feature representations; and (3) Multi-Scale Boundary Enhancement that employs progressive refinement strategies to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before post-processing, which further improves to 86.6% after applying Conditional Random Field (CRF) refinement, significantly outperforming previous WSSS methods. Our approach demonstrates superior localization accuracy, with complete object coverage and precise boundary delineation, while maintaining computational efficiency. Extensive ablation studies validate the contribution of each component, and qualitative comparisons across 600 diverse images showcase the method's robustness and generalization capability. The results establish IG-CAM as a new benchmark for weakly supervised semantic segmentation, offering a practical solution for scenarios where pixel-level annotations are unavailable or prohibitively expensive.",
        "arxiv_id": "2509.12496",
        "ARXIVID": "2509.12496",
        "COMMENT": "Does not match any specific criteria but is related to weakly supervised learning and segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12423": {
        "authors": [
            "Danielle Cohen",
            "Yoni Halpern",
            "Noam Kahlon",
            "Joel Oren",
            "Omri Berkovitch",
            "Sapir Caduri",
            "Ido Dagan",
            "Anatoly Efros"
        ],
        "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition",
        "abstract": "arXiv:2509.12423v1 Announce Type: new  Abstract: Understanding user intents from UI interaction trajectories remains a challenging, yet crucial, frontier in intelligent agent development. While massive, datacenter-based, multi-modal large language models (MLLMs) possess greater capacity to handle the complexities of such sequences, smaller models which can run on-device to provide a privacy-preserving, low-cost, and low-latency user experience, struggle with accurate intent inference. We address these limitations by introducing a novel decomposed approach: first, we perform structured interaction summarization, capturing key information from each user action. Second, we perform intent extraction using a fine-tuned model operating on the aggregated summaries. This method improves intent understanding in resource-constrained models, even surpassing the base performance of large MLLMs.",
        "arxiv_id": "2509.12423",
        "ARXIVID": "2509.12423",
        "COMMENT": "Does not closely match any specific criterion but discusses intent extraction from UI interaction trajectories, which may have indirect relevance to embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12817": {
        "authors": [
            "Yuan Cao",
            "Dong Wang"
        ],
        "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention",
        "abstract": "arXiv:2509.12817v1 Announce Type: new  Abstract: While Transformer architecture excel at modeling long-range dependencies contributing to its widespread adoption in vision tasks the quadratic complexity of softmax-based attention mechanisms imposes a major bottleneck, particularly when processing high-resolution images. Linear attention presents a promising alternative by reformulating the attention computation from $(QK)V$ to $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$ while preserving the global receptive field. However, most existing methods compress historical key-value (KV) information uniformly, which can lead to feature redundancy and the loss of directional alignment with the query (Q). This uniform compression results in low-rank $KV$ feature maps, contributing to a performance gap compared to softmax attention. To mitigate this limitation, we propose \\textbf{S}elective \\textbf{A}daptive \\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which introduces input-adaptive learnable gates to selectively modulate information aggregation into the $KV$ feature map. These gates enhance semantic diversity and alleviate the low-rank constraint inherent in conventional linear attention. Additionally, we propose an efficient Hadamard-product decomposition method for gate computation, which introduces no additional memory overhead. Experiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in throughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at a resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up to 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency and model effectiveness.",
        "arxiv_id": "2509.12817",
        "ARXIVID": "2509.12817",
        "COMMENT": "Does not closely match any specific criterion but discusses improvements in linear attention mechanisms, which may have indirect relevance to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.12258": {
        "authors": [
            "Li Kun",
            "Milena Radenkovic"
        ],
        "title": "EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces",
        "abstract": "arXiv:2509.12258v1 Announce Type: new  Abstract: Currently, deep learning has been utilised to tackle several difficulties in our everyday lives. It not only exhibits progress in computer vision but also constitutes the foundation for several revolutionary technologies. Nonetheless, similar to all phenomena, the use of deep learning in diverse domains has produced a multifaceted interaction of advantages and disadvantages for human society. Deepfake technology has advanced, significantly impacting social life. However, developments in this technology can affect privacy, the reputations of prominent personalities, and national security via software development. It can produce indistinguishable counterfeit photographs and films, potentially impairing the functionality of facial recognition systems, so presenting a significant risk.   The improper application of deepfake technology produces several detrimental effects on society. Face-swapping programs mislead users by altering persons' appearances or expressions to fulfil particular aims or to appropriate personal information. Deepfake technology permeates daily life through such techniques. Certain individuals endeavour to sabotage election campaigns or subvert prominent political figures by creating deceptive pictures to influence public perception, causing significant harm to a nation's political and economic structure.",
        "arxiv_id": "2509.12258",
        "ARXIVID": "2509.12258",
        "COMMENT": "This paper does not match any specific criteria but discusses deepfake detection, which is tangentially related to computer vision and societal impacts of AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.13089": {
        "authors": [
            "Jonas Werheid",
            "Shengjie He",
            "Aymen Gannouni",
            "Anas Abdelrazeq",
            "Robert H. Schmitt"
        ],
        "title": "A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control",
        "abstract": "arXiv:2509.13089v1 Announce Type: new  Abstract: Quality control of assembly processes is essential in manufacturing to ensure not only the quality of individual components but also their proper integration into the final product. To assist in this matter, automated assembly control using computer vision methods has been widely implemented. However, the costs associated with image acquisition, annotation, and training of computer vision algorithms pose challenges for integration, especially for small- and medium-sized enterprises (SMEs), which often lack the resources for extensive training, data collection, and manual image annotation. Synthetic data offers the potential to reduce manual data collection and labeling. Nevertheless, its practical application in the context of assembly quality remains limited. In this work, we present a novel approach for easily integrable and data-efficient visual assembly control. Our approach leverages simulated scene generation based on computer-aided design (CAD) data and object detection algorithms. The results demonstrate a time-saving pipeline for generating image data in manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95) up to 99,5% for correctly identifying instances of synthetic planetary gear system components within our simulated training data, and up to 93% when transferred to real-world camera-captured testing data. This research highlights the effectiveness of synthetic data generation within an adaptable pipeline and underscores its potential to support SMEs in implementing resource-efficient visual assembly control solutions.",
        "arxiv_id": "2509.13089",
        "ARXIVID": "2509.13089",
        "COMMENT": "This paper does not match any specific criteria but is tangentially related to computer vision and synthetic data generation, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12400": {
        "authors": [
            "Rongkun Zhu",
            "Kangning Cui",
            "Wei Tang",
            "Rui-Feng Wang",
            "Sarra Alqahtani",
            "David Lutz",
            "Fan Yang",
            "Paul Fine",
            "Jordan Karubian",
            "Robert Plemmons",
            "Jean-Michel Morel",
            "Victor Pauca",
            "Miles Silman"
        ],
        "title": "From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization",
        "abstract": "arXiv:2509.12400v1 Announce Type: new  Abstract: Accurate mapping of individual trees is essential for ecological monitoring and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs) is widely used, but stitching artifacts and heavy preprocessing limit its suitability for field deployment. This study explores the use of raw UAV imagery for palm detection and crown-center localization in tropical forests. Two research questions are addressed: (1) how detection performance varies across orthomosaic and raw imagery, including within-domain and cross-domain transfer, and (2) to what extent crown-center annotations improve localization accuracy beyond bounding-box centroids. Using state-of-the-art detectors and keypoint models, we show that raw imagery yields superior performance in deployment-relevant scenarios, while orthomosaics retain value for robust cross-domain generalization. Incorporating crown-center annotations in training further improves localization and provides precise tree positions for downstream ecological analyses. These findings offer practical guidance for UAV-based biodiversity and conservation monitoring.",
        "arxiv_id": "2509.12400",
        "ARXIVID": "2509.12400",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12265": {
        "authors": [
            "Xiaoguang Chang",
            "Teng Wang",
            "Changyin Sun"
        ],
        "title": "A Modern Look at Simplicity Bias in Image Classification Tasks",
        "abstract": "arXiv:2509.12265v1 Announce Type: new  Abstract: The simplicity Bias (SB) of neural networks, i.e.\\ their tendency to represent simple functions, is a key factor in their generalization capabilities. Recent studies show that an excessive SB may harm performance on complex tasks, and the need for this bias varies across tasks. Many of these studies focus on simple models or synthetic tasks. It remains challenging to measure the SB in large models and little is known about the relevance of the SB to various image classification tasks.   In this paper, we investigate the relationship between the SB in CLIP models and their performance across image classification tasks. First, we theoretically analyze the potential limitation of existing measures of complexity that have been used to characterize small models. To address this, we propose a frequency-aware measure capturing finer-grained SB differences. We validate this measure on CLIP models subjected to two recent SB-modulation methods, demonstrating that it is more informative and consistent than previous measures. Second, we examine the relation between the SB of those models and their performance across a range of image classification tasks, including zero-shot and fine-tuning settings. These experiments reveal a range of behaviors. For example, a stronger SB correlates with a better performance on OOD generalization than on adversarial robustness. These results highlight the benefits of aligning a model's inductive biases with the characteristics of the target task.",
        "arxiv_id": "2509.12265",
        "ARXIVID": "2509.12265",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12759": {
        "authors": [
            "Yiwei Xu",
            "Xiang Wang",
            "Yifei Yu",
            "Wentian Gan",
            "Luca Morelli",
            "Giulio Perda",
            "Xiongwu Xiao",
            "Zongqian Zhan",
            "Xin Wang",
            "Fabio Remondino"
        ],
        "title": "A-TDOM: Active TDOM via On-the-Fly 3DGS",
        "abstract": "arXiv:2509.12759v1 Announce Type: new  Abstract: True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in various fields such as urban management, city planning, land surveying, etc. However, traditional TDOM generation methods generally rely on a complex offline photogrammetric pipeline, resulting in delays that hinder real-time applications. Moreover, the quality of TDOM may degrade due to various challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and scene occlusions. To address these challenges, this work introduces A-TDOM, a near real-time TDOM generation method based on On-the-Fly 3DGS optimization. As each image is acquired, its pose and sparse point cloud are computed via On-the-Fly SfM. Then new Gaussians are integrated and optimized into previously unseen or coarsely reconstructed regions. By integrating with orthogonal splatting, A-TDOM can render just after each update of a new 3DGS field. Initial experiments on multiple benchmarks show that the proposed A-TDOM is capable of actively rendering TDOM in near real-time, with 3DGS optimization for each new image in seconds while maintaining acceptable rendering quality and TDOM geometric accuracy.",
        "arxiv_id": "2509.12759",
        "ARXIVID": "2509.12759",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12592": {
        "authors": [
            "Aaron Baughman",
            "Gozde Akay",
            "Eduardo Morales",
            "Rahul Agarwal",
            "Preetika Srivastava"
        ],
        "title": "Match Chat: Real Time Generative AI and Generative Computing for Tennis",
        "abstract": "arXiv:2509.12592v1 Announce Type: new  Abstract: We present Match Chat, a real-time, agent-driven assistant designed to enhance the tennis fan experience by delivering instant, accurate responses to match-related queries. Match Chat integrates Generative Artificial Intelligence (GenAI) with Generative Computing (GenComp) techniques to synthesize key insights during live tennis singles matches. The system debuted at the 2025 Wimbledon Championships and the 2025 US Open, where it provided about 1 million users with seamless access to streaming and static data through natural language queries. The architecture is grounded in an Agent-Oriented Architecture (AOA) combining rule engines, predictive models, and agents to pre-process and optimize user queries before passing them to GenAI components. The Match Chat system had an answer accuracy of 92.83% with an average response time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over 96.08% of all queries were guided using interactive prompt design, contributing to a user experience that prioritized clarity, responsiveness, and minimal effort. The system was designed to mask architectural complexity, offering a frictionless and intuitive interface that required no onboarding or technical familiarity. Across both Grand Slam deployments, Match Chat maintained 100% uptime and supported nearly 1 million unique users, underscoring the scalability and reliability of the platform. This work introduces key design patterns for real-time, consumer-facing AI systems that emphasize speed, precision, and usability that highlights a practical path for deploying performant agentic systems in dynamic environments.",
        "arxiv_id": "2509.12592",
        "ARXIVID": "2509.12592",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative AI and real-time systems.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12543": {
        "authors": [
            "Harshit Rajgarhia",
            "Shivali Dalmia",
            "Mengyang Zhao",
            "Mukherji Abhishek",
            "Kiran Ganesh"
        ],
        "title": "Human + AI for Accelerating Ad Localization Evaluation",
        "abstract": "arXiv:2509.12543v1 Announce Type: new  Abstract: Adapting advertisements for multilingual audiences requires more than simple text translation; it demands preservation of visual consistency, spatial alignment, and stylistic integrity across diverse languages and formats. We introduce a structured framework that combines automated components with human oversight to address the complexities of advertisement localization. To the best of our knowledge, this is the first work to integrate scene text detection, inpainting, machine translation (MT), and text reimposition specifically for accelerating ad localization evaluation workflows. Qualitative results across six locales demonstrate that our approach produces semantically accurate and visually coherent localized advertisements, suitable for deployment in real-world workflows.",
        "arxiv_id": "2509.12543",
        "ARXIVID": "2509.12543",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of multimodal AI and advertisement localization.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12595": {
        "authors": [
            "Yizhen Lao",
            "Yu Zhang",
            "Ziting Wang",
            "Chengbo Wang",
            "Yifei Xue",
            "Wanpeng Shao"
        ],
        "title": "DisorientLiDAR: Physical Attacks on LiDAR-based Localization",
        "abstract": "arXiv:2509.12595v1 Announce Type: new  Abstract: Deep learning models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Even this poses a serious security challenge for the localization of self-driving cars, there has been very little exploration of attack on it, as most of adversarial attacks have been applied to 3D perception. In this work, we propose a novel adversarial attack framework called DisorientLiDAR targeting LiDAR-based localization. By reverse-engineering localization models (e.g., feature extraction networks), adversaries can identify critical keypoints and strategically remove them, thereby disrupting LiDAR-based localization. Our proposal is first evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and GeoTransformer) using the KITTI dataset. Experimental results demonstrate that removing regions containing Top-K keypoints significantly degrades their registration accuracy. We further validate the attack's impact on the Autoware autonomous driving platform, where hiding merely a few critical regions induces noticeable localization drift. Finally, we extended our attacks to the physical world by hiding critical regions with near-infrared absorptive materials, thereby successfully replicate the attack effects observed in KITTI data. This step has been closer toward the realistic physical-world attack that demonstrate the veracity and generality of our proposal.",
        "arxiv_id": "2509.12595",
        "ARXIVID": "2509.12595",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of adversarial attacks in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}