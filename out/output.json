{
    "2509.22281": {
        "authors": [
            "Jinkun Hao",
            "Naifu Liang",
            "Zhen Luo",
            "Xudong Xu",
            "Weipeng Zhong",
            "Ran Yi",
            "Yichen Jin",
            "Zhaoyang Lyu",
            "Feng Zheng",
            "Lizhuang Ma",
            "Jiangmiao Pang"
        ],
        "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
        "abstract": "arXiv:2509.22281v1 Announce Type: new  Abstract: The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Project page is at https://mesatask.github.io/",
        "arxiv_id": "2509.22281",
        "ARXIVID": "2509.22281",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on 3D spatial reasoning and task-driven scene generation for embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.21760": {
        "authors": [
            "Lan Chen",
            "Yuchao Gu",
            "Qi Mao"
        ],
        "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
        "abstract": "arXiv:2509.21760v1 Announce Type: new  Abstract: Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.",
        "arxiv_id": "2509.21760",
        "ARXIVID": "2509.21760",
        "COMMENT": "Matches criterion 6 as it explores video-based tasks using a pre-trained video generation model, pushing the boundaries of video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.21574": {
        "authors": [
            "You Xie",
            "Tianpei Gu",
            "Zenan Li",
            "Chenxu Zhang",
            "Guoxian Song",
            "Xiaochen Zhao",
            "Chao Liang",
            "Jianwen Jiang",
            "Hongyi Xu",
            "Linjie Luo"
        ],
        "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
        "abstract": "arXiv:2509.21574v1 Announce Type: new  Abstract: We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.",
        "arxiv_id": "2509.21574",
        "ARXIVID": "2509.21574",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a unified multimodal framework for audiovisual interaction, integrating vision and language.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.22460": {
        "authors": [
            "Shichao Weng",
            "Zhiqiang Wang",
            "Yuhua Zhou",
            "Rui Lu",
            "Ting Liu",
            "Zhiyang Teng",
            "Xiaozhang Liu",
            "Hanmeng Liu"
        ],
        "title": "GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation",
        "abstract": "arXiv:2509.22460v1 Announce Type: new  Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large Language Models (MLLMs), requiring not only the joint interpretation of text and diagrams but also iterative visuospatial reasoning. While existing approaches process diagrams as static images, they lack the capacity for dynamic manipulation - a core aspect of human geometric reasoning involving auxiliary line construction and affine transformations. We present GeoSketch, a neural-symbolic framework that recasts geometric reasoning as an interactive perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning module that applies geometric theorems to decide the next deductive step, and (3) a Sketch Action module that executes operations such as drawing auxiliary lines or applying transformations, thereby updating the diagram in a closed loop. To train this agent, we develop a two-stage pipeline: supervised fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement learning with dense, symbolic rewards to enhance robustness and strategic exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a high-quality set of 390 geometry problems requiring auxiliary construction or affine transformations. Experiments on strong MLLM baselines demonstrate that GeoSketch significantly improves stepwise reasoning accuracy and problem-solving success over static perception methods. By unifying hierarchical decision-making, executable visual actions, and symbolic verification, GeoSketch advances multimodal reasoning from static interpretation to dynamic, verifiable interaction, establishing a new foundation for solving complex visuospatial problems.",
        "arxiv_id": "2509.22460",
        "ARXIVID": "2509.22460",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel framework for geometric multimodal reasoning with spatial intelligence and embodied interaction.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.22548": {
        "authors": [
            "Shuang Zeng",
            "Dekang Qi",
            "Xinyuan Chang",
            "Feng Xiong",
            "Shichao Xie",
            "Xiaolong Wu",
            "Shiyi Liang",
            "Mu Xu",
            "Xing Wei"
        ],
        "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
        "abstract": "arXiv:2509.22548v1 Announce Type: new  Abstract: Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
        "arxiv_id": "2509.22548",
        "ARXIVID": "2509.22548",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel framework (JanusVLN) for Vision-Language Navigation with dual implicit memory, enhancing spatial reasoning and embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.21976": {
        "authors": [
            "Zilun Zhang",
            "Zian Guan",
            "Tiancheng Zhao",
            "Haozhan Shen",
            "Tianyu Li",
            "Yuxiang Cai",
            "Zhonggen Su",
            "Zhaojun Liu",
            "Jianwei Yin",
            "Xiang Li"
        ],
        "title": "Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning",
        "abstract": "arXiv:2509.21976v1 Announce Type: new  Abstract: Referring expression understanding in remote sensing poses unique challenges, as it requires reasoning over complex object-context relationships. While supervised fine-tuning (SFT) on multimodal large language models achieves strong performance with massive labeled datasets, they struggle in data-scarce scenarios, leading to poor generalization. To address this limitation, we propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm for few-shot geospatial referring. Geo-R1 enforces the model to first generate explicit, interpretable reasoning chains that decompose referring expressions, and then leverage these rationales to localize target objects. This \"reason first, then act\" process enables the model to make more effective use of limited annotations, enhances generalization, and provides interpretability. We validate Geo-R1 on three carefully designed few-shot geospatial referring benchmarks, where our model consistently and substantially outperforms SFT baselines. It also demonstrates strong cross-dataset generalization, highlighting its robustness. Code and data will be released at http://geo-r1.github.io.",
        "arxiv_id": "2509.21976",
        "ARXIVID": "2509.21976",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and embodied agents with a novel reinforcement fine-tuning paradigm for geospatial referring.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.22225": {
        "authors": [
            "Jiayu Ding",
            "Xinpeng Liu",
            "Zhiyi Pan",
            "Shiqiang Long",
            "Ge Li"
        ],
        "title": "Polysemous Language Gaussian Splatting via Matching-based Mask Lifting",
        "abstract": "arXiv:2509.22225v1 Announce Type: new  Abstract: Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.",
        "arxiv_id": "2509.22225",
        "ARXIVID": "2509.22225",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on lifting 2D open-vocabulary understanding into 3D Gaussian Splatting using vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.22151": {
        "authors": [
            "Jonas Belouadi",
            "Tamy Boubekeur",
            "Adrien Kaiser"
        ],
        "title": "MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models",
        "abstract": "arXiv:2509.22151v1 Announce Type: new  Abstract: Material node graphs are programs that generate the 2D channels of procedural materials, including geometry such as roughness and displacement maps, and reflectance such as albedo and conductivity maps. They are essential in computer graphics for representing the appearance of virtual 3D objects parametrically and at arbitrary resolution. In particular, their directed acyclic graph structures and intermediate states provide an intuitive understanding and workflow for interactive appearance modeling. Creating such graphs is a challenging task and typically requires professional training. While recent neural program synthesis approaches attempt to simplify this process, they solely represent graphs as textual programs, failing to capture the inherently visual-spatial nature of node graphs that makes them accessible to humans. To address this gap, we present MultiMat, a multimodal program synthesis framework that leverages large multimodal models to process both visual and textual graph representations for improved generation of procedural material graphs. We train our models on a new dataset of production-quality procedural materials and combine them with a constrained tree search inference algorithm that ensures syntactic validity while efficiently navigating the program space. Our experimental results show that our multimodal program synthesis method is more efficient in both unconditional and conditional graph synthesis with higher visual quality and fidelity than text-only baselines, establishing new state-of-the-art performance.",
        "arxiv_id": "2509.22151",
        "ARXIVID": "2509.22151",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on multimodal program synthesis using large multimodal models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.21991": {
        "authors": [
            "Jewon Lee",
            "Wooksu Shin",
            "Seungmin Yang",
            "Ki-Ung Song",
            "DongUk Lim",
            "Jaeyeon Kim",
            "Tae-Ho Kim",
            "Bo-Kyeong Kim"
        ],
        "title": "ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models",
        "abstract": "arXiv:2509.21991v1 Announce Type: new  Abstract: Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of \"thinking with images\" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage \"coarse-to-fine\" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: https://github.com/nota-github/ERGO.",
        "arxiv_id": "2509.21991",
        "ARXIVID": "2509.21991",
        "COMMENT": "Matches criterion 2. Proposes a novel architecture (ERGO) for efficient high-resolution visual understanding in vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.21377": {
        "authors": [
            "Yinfeng Yu",
            "Hailong Zhang",
            "Meiling Zhu"
        ],
        "title": "Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation",
        "abstract": "arXiv:2509.21377v1 Announce Type: new  Abstract: Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at   https://github.com/zzzmmm-svg/DMTF.",
        "arxiv_id": "2509.21377",
        "ARXIVID": "2509.21377",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for audio-visual embodied navigation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.22407": {
        "authors": [
            "Zhehao Dong",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Yirui Wang",
            "Yang Wang",
            "Yukun Zhou",
            "Boyuan Wang",
            "Chaojun Ni",
            "Runqi Ouyang",
            "Wenkang Qin",
            "Xinze Chen",
            "Yun Ye",
            "Guan Huang"
        ],
        "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
        "abstract": "arXiv:2509.22407v1 Announce Type: new  Abstract: Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.",
        "arxiv_id": "2509.22407",
        "ARXIVID": "2509.22407",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a generative framework for robotic manipulation and improves generalization in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2509.22010": {
        "authors": [
            "Xinyu Zhang",
            "Yuxuan Dong",
            "Lingling Zhang",
            "Chengyou Jia",
            "Zhuohang Dang",
            "Basura Fernando",
            "Jun Liu",
            "Mike Zheng Shou"
        ],
        "title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models",
        "abstract": "arXiv:2509.22010v1 Announce Type: new  Abstract: Despite significant advances in Vision Language Models (VLMs), they remain constrained by the complexity and redundancy of visual input. When images contain large amounts of irrelevant information, VLMs are susceptible to interference, thus generating excessive task-irrelevant reasoning processes or even hallucinations. This limitation stems from their inability to discover and process the required regions during reasoning precisely. To address this limitation, we present the Chain of Foresight-Focus Thought (CoFFT), a novel training-free approach that enhances VLMs' visual reasoning by emulating human visual cognition. Each Foresight-Focus Thought consists of three stages: (1) Diverse Sample Generation: generates diverse reasoning samples to explore potential reasoning paths, where each sample contains several reasoning steps; (2) Dual Foresight Decoding: rigorously evaluates these samples based on both visual focus and reasoning progression, adding the first step of optimal sample to the reasoning process; (3) Visual Focus Adjustment: precisely adjust visual focus toward regions most beneficial for future reasoning, before returning to stage (1) to generate subsequent reasoning samples until reaching the final answer. These stages function iteratively, creating an interdependent cycle where reasoning guides visual focus and visual focus informs subsequent reasoning. Empirical results across multiple benchmarks using Qwen2.5-VL, InternVL-2.5, and Llava-Next demonstrate consistent performance improvements of 3.1-5.8\\% with controllable increasing computational overhead.",
        "arxiv_id": "2509.22010",
        "ARXIVID": "2509.22010",
        "COMMENT": "Matches criteria 2 and 5 as it presents a novel method (CoFFT) to enhance visual reasoning in Vision Language Models (VLMs) and integrates image understanding with reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.21917": {
        "authors": [
            "Xianghao Kong",
            "Hansheng Chen",
            "Yuwei Guo",
            "Lvmin Zhang",
            "Gordon Wetzstein",
            "Maneesh Agrawala",
            "Anyi Rao"
        ],
        "title": "Taming Flow-based I2V Models for Creative Video Editing",
        "abstract": "arXiv:2509.21917v1 Announce Type: new  Abstract: Although image editing techniques have advanced significantly, video editing, which aims to manipulate videos according to user intent, remains an emerging challenge. Most existing image-conditioned video editing methods either require inversion with model-specific design or need extensive optimization, limiting their capability of leveraging up-to-date image-to-video (I2V) models to transfer the editing capability of image editing models to the video domain. To this end, we propose IF-V2V, an Inversion-Free method that can adapt off-the-shelf flow-matching-based I2V models for video editing without significant computational overhead. To circumvent inversion, we devise Vector Field Rectification with Sample Deviation to incorporate information from the source video into the denoising process by introducing a deviation term into the denoising vector field. To further ensure consistency with the source video in a model-agnostic way, we introduce Structure-and-Motion-Preserving Initialization to generate motion-aware temporally correlated noise with structural information embedded. We also present a Deviation Caching mechanism to minimize the additional computational cost for denoising vector rectification without significantly impacting editing quality. Evaluations demonstrate that our method achieves superior editing quality and consistency over existing approaches, offering a lightweight plug-and-play solution to realize visual creativity.",
        "arxiv_id": "2509.21917",
        "ARXIVID": "2509.21917",
        "COMMENT": "Matches criterion 5 as it focuses on integrating image-to-video models with creative video editing tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21388": {
        "authors": [
            "Anton Konushin",
            "Nikita Drozdov",
            "Bulat Gabdullin",
            "Alexey Zakharov",
            "Anna Vorontsova",
            "Danila Rukhovich",
            "Maksim Kolodiazhnyi"
        ],
        "title": "TUN3D: Towards Real-World Scene Understanding from Unposed Images",
        "abstract": "arXiv:2509.21388v1 Announce Type: new  Abstract: Layout estimation and 3D object detection are two fundamental tasks in indoor scene understanding. When combined, they enable the creation of a compact yet semantically rich spatial representation of a scene. Existing approaches typically rely on point cloud input, which poses a major limitation since most consumer cameras lack depth sensors and visual-only data remains far more common. We address this issue with TUN3D, the first method that tackles joint layout estimation and 3D object detection in real scans, given multi-view images as input, and does not require ground-truth camera poses or depth supervision. Our approach builds on a lightweight sparse-convolutional backbone and employs two dedicated heads: one for 3D object detection and one for layout estimation, leveraging a novel and effective parametric wall representation. Extensive experiments show that TUN3D achieves state-of-the-art performance across three challenging scene understanding benchmarks: (i) using ground-truth point clouds, (ii) using posed images, and (iii) using unposed images. While performing on par with specialized 3D object detection methods, TUN3D significantly advances layout estimation, setting a new benchmark in holistic indoor scene understanding. Code is available at https://github.com/col14m/tun3d .",
        "arxiv_id": "2509.21388",
        "ARXIVID": "2509.21388",
        "COMMENT": "Matches criterion 1 as it focuses on spatial intelligence and scene understanding with a novel method for layout estimation and 3D object detection.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22262": {
        "authors": [
            "Yujian Yuan",
            "Changjie Wu",
            "Xinyuan Chang",
            "Sijin Wang",
            "Hang Zhang",
            "Shiyi Liang",
            "Shuang Zeng",
            "Mu Xu"
        ],
        "title": "UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data",
        "abstract": "arXiv:2509.22262v1 Announce Type: new  Abstract: Large-scale map construction is foundational for critical applications such as autonomous driving and navigation systems. Traditional large-scale map construction approaches mainly rely on costly and inefficient special data collection vehicles and labor-intensive annotation processes. While existing satellite-based methods have demonstrated promising potential in enhancing the efficiency and coverage of map construction, they exhibit two major limitations: (1) inherent drawbacks of satellite data (e.g., occlusions, outdatedness) and (2) inefficient vectorization from perception-based methods, resulting in discontinuous and rough roads that require extensive post-processing. This paper presents a novel generative framework, UniMapGen, for large-scale map construction, offering three key innovations: (1) representing lane lines as \\textbf{discrete sequence} and establishing an iterative strategy to generate more complete and smooth map vectors than traditional perception-based methods. (2) proposing a flexible architecture that supports \\textbf{multi-modal} inputs, enabling dynamic selection among BEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3) developing a \\textbf{state update} strategy for global continuity and consistency of the constructed large-scale map. UniMapGen achieves state-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen can infer occluded roads and predict roads missing from dataset annotations. Our code will be released.",
        "arxiv_id": "2509.22262",
        "ARXIVID": "2509.22262",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for large-scale map construction, which is relevant to embodied/robotic AI benchmarks and methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21593": {
        "authors": [
            "Peng Luo",
            "Xiayin Lou",
            "Yu Zheng",
            "Zhuo Zheng",
            "Stefano Ermon"
        ],
        "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models",
        "abstract": "arXiv:2509.21593v1 Announce Type: new  Abstract: Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.",
        "arxiv_id": "2509.21593",
        "ARXIVID": "2509.21593",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on geospatial modeling and spatial reasoning improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21766": {
        "authors": [
            "Haotian Luo",
            "Huaisong Zhang",
            "Xuelin Zhang",
            "Haoyu Wang",
            "Zeyu Qin",
            "Wenjie Lu",
            "Guozheng Ma",
            "Haiying He",
            "Yingsha Xie",
            "Qiyang Zhou",
            "Zixuan Hu",
            "Hongze Mi",
            "Yibo Wang",
            "Naiqiang Tan",
            "Hong Chen",
            "Yi R. Fung",
            "Chun Yuan",
            "Li Shen"
        ],
        "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios",
        "abstract": "arXiv:2509.21766v1 Announce Type: new  Abstract: Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool calls, whereas in standard configurations they still exceed \\textbf{35k} tokens and involve more than \\textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}",
        "arxiv_id": "2509.21766",
        "ARXIVID": "2509.21766",
        "COMMENT": "Matches criterion 3. Introduces UltraHorizon, a benchmark for long-horizon agent capabilities, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22496": {
        "authors": [
            "Ruoyu Chen",
            "Xiaoqing Guo",
            "Kangwei Liu",
            "Siyuan Liang",
            "Shiming Liu",
            "Qunli Zhang",
            "Hua Zhang",
            "Xiaochun Cao"
        ],
        "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation",
        "abstract": "arXiv:2509.22496v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE.",
        "arxiv_id": "2509.22496",
        "ARXIVID": "2509.22496",
        "COMMENT": "Matches criterion 2. Proposes EAGLE, a framework for explaining token generation in multimodal large language models, advancing interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22258": {
        "authors": [
            "Miao Jing",
            "Mengting Jia",
            "Junling Lin",
            "Zhongxia Shen",
            "Lijun Wang",
            "Yuanyuan Peng",
            "Huan Gao",
            "Mingkun Xu",
            "Shangyang Li"
        ],
        "title": "Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks",
        "abstract": "arXiv:2509.22258v1 Announce Type: new  Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.",
        "arxiv_id": "2509.22258",
        "ARXIVID": "2509.22258",
        "COMMENT": "Matches criteria 7 as it introduces a benchmark for evaluating clinical reasoning in vision-language models, highlighting open challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21451": {
        "authors": [
            "Abdul Waheed",
            "Zhen Wu",
            "Dareen Alharthi",
            "Seungone Kim",
            "Bhiksha Raj"
        ],
        "title": "VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding",
        "abstract": "arXiv:2509.21451v1 Announce Type: new  Abstract: Precisely evaluating video understanding models remains challenging: commonly used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of human judgment, while obtaining such judgments through manual evaluation is costly. Recent work has explored using large language models (LLMs) or multimodal LLMs (MLLMs) as evaluators, but their extension to video understanding remains relatively unexplored. In this work, we introduce VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from video understanding models (\\textit{i.e.}, text responses conditioned on videos). To train VideoJudge, our recipe builds on the interplay between a generator and an evaluator: the generator is prompted to produce responses conditioned on a target rating, and responses not matching the evaluator's rating are discarded. Across three out of four meta-evaluation benchmarks, VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve performance, indicating that providing video inputs is crucial for evaluation of video understanding tasks.",
        "arxiv_id": "2509.21451",
        "ARXIVID": "2509.21451",
        "COMMENT": "Matches criteria 6 as it focuses on video understanding and introduces a novel evaluation framework for video-based tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21552": {
        "authors": [
            "Yu Zhao",
            "Wei-Ning Chen",
            "Huseyin Atahan Inan",
            "Samuel Kessler",
            "Lu Wang",
            "Lukas Wutschitz",
            "Fangkai Yang",
            "Chaoyun Zhang",
            "Pasquale Minervini",
            "Saravan Rajmohan",
            "Robert Sim"
        ],
        "title": "Learning GUI Grounding with Spatial Reasoning from Visual Feedback",
        "abstract": "arXiv:2509.21552v1 Announce Type: new  Abstract: Graphical User Interface (GUI) grounding is commonly framed as a coordinate prediction task -- given a natural language instruction, generate on-screen coordinates for actions such as clicks and keystrokes. However, recent Vision Language Models (VLMs) often fail to predict accurate numeric coordinates when processing high-resolution GUI images with complex layouts. To address this issue, we reframe GUI grounding as an \\emph{interactive search task}, where the VLM generates actions to move a cursor in the GUI to locate UI elements. At each step, the model determines the target object, evaluates the spatial relations between the cursor and the target, and moves the cursor closer to the target conditioned on the movement history. In this interactive process, the rendered cursor provides visual feedback to help the model align its predictions with the corresponding on-screen locations. We train our GUI grounding model, GUI-Cursor, using multi-step online reinforcement learning with a dense trajectory-based reward function. Our experimental results show that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\\% \\rightarrow 93.9\\%$) and ScreenSpot-Pro ($26.8\\% \\rightarrow 56.5\\%$). Moreover, we observe that GUI-Cursor learns to solve the problem within two steps for 95\\% of instances and can adaptively conduct more steps on more difficult examples.",
        "arxiv_id": "2509.21552",
        "ARXIVID": "2509.21552",
        "COMMENT": "Matches criteria 1 as it explores spatial reasoning in GUI grounding tasks using interactive search and reinforcement learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22624": {
        "authors": [
            "Ziyu Liu",
            "Yuhang Zang",
            "Shengyuan Ding",
            "Yuhang Cao",
            "Xiaoyi Dong",
            "Haodong Duan",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
        "abstract": "arXiv:2509.22624v1 Announce Type: new  Abstract: Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.",
        "arxiv_id": "2509.22624",
        "ARXIVID": "2509.22624",
        "COMMENT": "Matches criteria 2 and 5 as it discusses novel reinforcement learning methods for improving Large Vision-Language Models (LVLMs) and their integration with generative reward models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22186": {
        "authors": [
            "Junbo Niu",
            "Zheng Liu",
            "Zhuangcheng Gu",
            "Bin Wang",
            "Linke Ouyang",
            "Zhiyuan Zhao",
            "Tao Chu",
            "Tianyao He",
            "Fan Wu",
            "Qintong Zhang",
            "Zhenjiang Jin",
            "Guang Liang",
            "Rui Zhang",
            "Wenzheng Zhang",
            "Yuan Qu",
            "Zhifei Ren",
            "Yuefeng Sun",
            "Yuanhong Zheng",
            "Dongsheng Ma",
            "Zirui Tang",
            "Boyu Niu",
            "Ziyang Miao",
            "Hejun Dong",
            "Siyi Qian",
            "Junyuan Zhang",
            "Jingzhou Chen",
            "Fangdong Wang",
            "Xiaomeng Zhao",
            "Liqun Wei",
            "Wei Li",
            "Shasha Wang",
            "Ruiliang Xu",
            "Yuanyuan Cao",
            "Lu Chen",
            "Qianqian Wu",
            "Huaiyu Gu",
            "Lindong Lu",
            "Keming Wang",
            "Dechen Lin",
            "Guanlin Shen",
            "Xuanhe Zhou",
            "Linfeng Zhang",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Jiaqi Wang",
            "Bo Zhang",
            "Lei Bai",
            "Pei Chu",
            "Weijia Li",
            "Jiang Wu",
            "Lijun Wu",
            "Zhenxiang Li",
            "Guangyu Wang",
            "Zhongying Tu",
            "Chao Xu",
            "Kai Chen",
            "Yu Qiao",
            "Bowen Zhou",
            "Dahua Lin",
            "Wentao Zhang",
            "Conghui He"
        ],
        "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing",
        "abstract": "arXiv:2509.22186v1 Announce Type: new  Abstract: We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.",
        "arxiv_id": "2509.22186",
        "ARXIVID": "2509.22186",
        "COMMENT": "Matches criterion 5 as it focuses on combining image understanding tasks with large language models for document parsing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22650": {
        "authors": [
            "Anna Kukleva",
            "Enis Simsar",
            "Alessio Tonioni",
            "Muhammad Ferjad Naeem",
            "Federico Tombari",
            "Jan Eric Lenssen",
            "Bernt Schiele"
        ],
        "title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
        "abstract": "arXiv:2509.22650v1 Announce Type: new  Abstract: Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components.",
        "arxiv_id": "2509.22650",
        "ARXIVID": "2509.22650",
        "COMMENT": "Matches criterion 2 as it explores vision-language grounding tasks using diffusion transformers.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22570": {
        "authors": [
            "Qi Mao",
            "Tinghan Yang",
            "Jiahao Li",
            "Bin Li",
            "Libiao Jin",
            "Yan Lu"
        ],
        "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration",
        "abstract": "arXiv:2509.22570v1 Announce Type: new  Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.",
        "arxiv_id": "2509.22570",
        "ARXIVID": "2509.22570",
        "COMMENT": "Matches criteria 2 and 5 as it proposes a token-based multimodal interactive coding framework for human-AI collaboration, integrating multimodal tasks with LMMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.21797": {
        "authors": [
            "Yu Shang",
            "Yangcheng Yu",
            "Xin Zhang",
            "Xin Jin",
            "Haisheng Su",
            "Wei Wu",
            "Yong Li"
        ],
        "title": "MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation",
        "abstract": "arXiv:2509.21797v1 Announce Type: new  Abstract: Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.",
        "arxiv_id": "2509.21797",
        "ARXIVID": "2509.21797",
        "COMMENT": "Matches criteria 3 as it introduces a novel mixture-of-world-models framework for embodied action planning, addressing challenges in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.22292": {
        "authors": [
            "Wonjun Lee",
            "Haon Park",
            "Doehyeon Lee",
            "Bumsub Ham",
            "Suhyun Kim"
        ],
        "title": "Jailbreaking on Text-to-Video Models via Scene Splitting Strategy",
        "abstract": "arXiv:2509.22292v1 Announce Type: new  Abstract: Along with the rapid advancement of numerous Text-to-Video (T2V) models, growing concerns have emerged regarding their safety risks. While recent studies have explored vulnerabilities in models like LLMs, VLMs, and Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely unexplored, leaving a significant safety gap. To address this gap, we introduce SceneSplit, a novel black-box jailbreak method that works by fragmenting a harmful narrative into multiple scenes, each individually benign. This approach manipulates the generative output space, the abstract set of all potential video outputs for a given prompt, using the combination of scenes as a powerful constraint to guide the final outcome. While each scene individually corresponds to a wide and safe space where most outcomes are benign, their sequential combination collectively restricts this space, narrowing it to an unsafe region and significantly increasing the likelihood of generating a harmful video. This core mechanism is further enhanced through iterative scene manipulation, which bypasses the safety filter within this constrained unsafe region. Additionally, a strategy library that reuses successful attack patterns further improves the attack's overall effectiveness and robustness. To validate our method, we evaluate SceneSplit across 11 safety categories on T2V models. Our results show that it achieves a high average Attack Success Rate (ASR) of 77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly outperforming the existing baseline. Through this work, we demonstrate that current T2V safety mechanisms are vulnerable to attacks that exploit narrative structure, providing new insights for understanding and improving the safety of T2V models.",
        "arxiv_id": "2509.22292",
        "ARXIVID": "2509.22292",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it explores vulnerabilities in text-to-video models and proposes a novel jailbreak method.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.21997": {
        "authors": [
            "Youxu Shi",
            "Suorong Yang",
            "Dong Liu"
        ],
        "title": "Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors",
        "abstract": "arXiv:2509.21997v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet they remain highly susceptible to hallucinations, producing content that is fluent but inconsistent with visual evidence. Such hallucinations, spanning objects, attributes, and relations, persist even in larger models, while existing mitigation approaches often require additional finetuning, handcrafted priors, or trade-offs that compromise informativeness and scalability. To address this limitation, we propose a training-free, self-supervised method for hallucination mitigation. Our approach introduces a novel hallucination amplification mechanism: a caption is projected into the visual space via a text-to-image model to reveal implicit hallucination signals, serving as a negative anchor, while the original image provides a positive anchor. Leveraging these dual anchors, we edit decoder hidden states by pulling representations toward faithful semantics and pushing them away from hallucination directions. This correction requires no human priors or additional training costs, ensuring both effectiveness and efficiency. Extensive experiments across multiple benchmarks show that our method significantly reduces hallucinations at the object, attribute, and relation levels while largely preserving recall and caption richness, e.g., achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR. Furthermore, results on diverse architectures, including LLaVA-NEXT-7B, Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture generalization. More importantly, when applied to hallucination-free captions, our method introduces almost no side effects, underscoring its robustness and practical plug-and-play applicability. The implementation will be publicly available.",
        "arxiv_id": "2509.21997",
        "ARXIVID": "2509.21997",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it addresses hallucination mitigation in multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.21979": {
        "authors": [
            "Zikun Guo",
            "Xinyue Xu",
            "Pei Xiang",
            "Shu Yang",
            "Xin Han",
            "Di Wang",
            "Lijie Hu"
        ],
        "title": "Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models",
        "abstract": "arXiv:2509.21979v1 Announce Type: new  Abstract: Vision language models(VLMs) are increasingly integrated into clinical workflows, but they often exhibit sycophantic behavior prioritizing alignment with user phrasing social cues or perceived authority over evidence based reasoning. This study evaluate clinical sycophancy in medical visual question answering through a novel clinically grounded benchmark. We propose a medical sycophancy dataset construct from PathVQA, SLAKE, and VQA-RAD stratified by different type organ system and modality. Using psychologically motivated pressure templates including various sycophancy. In our adversarial experiments on various VLMs, we found that these models are generally vulnerable, exhibiting significant variations in the occurrence of adversarial responses, with weak correlations to the model accuracy or size. Imitation and expert provided corrections were found to be the most effective triggers, suggesting that the models possess a bias mechanism independent of visual evidence. To address this, we propose Visual Information Purification for Evidence based Response (VIPER) a lightweight mitigation strategy that filters non evidentiary content for example social pressures and then generates constrained evidence first answers. This framework reduces sycophancy by an average amount outperforming baselines while maintaining interpretability. Our benchmark analysis and mitigation framework lay the groundwork for robust deployment of medical VLMs in real world clinician interactions emphasizing the need for evidence anchored defenses.",
        "arxiv_id": "2509.21979",
        "ARXIVID": "2509.21979",
        "COMMENT": "Matches criterion 2. Focuses on mitigating sycophancy in medical vision-language models, which is a novel application of VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.22228": {
        "authors": [
            "Jun He",
            "Yi Lin",
            "Zilong Huang",
            "Jiacong Yin",
            "Junyan Ye",
            "Yuchuan Zhou",
            "Weijia Li",
            "Xiang Zhang"
        ],
        "title": "UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective",
        "abstract": "arXiv:2509.22228v1 Announce Type: new  Abstract: Urban development impacts over half of the global population, making human-centered understanding of its structural and perceptual changes essential for sustainable development. While Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various domains, existing benchmarks that explore their performance in urban environments remain limited, lacking systematic exploration of temporal evolution and subjective perception of urban environment that aligns with human perception. To address these limitations, we propose UrbanFeel, a comprehensive benchmark designed to evaluate the performance of MLLMs in urban development understanding and subjective environmental perception. UrbanFeel comprises 14.3K carefully constructed visual questions spanning three cognitively progressive dimensions: Static Scene Perception, Temporal Change Understanding, and Subjective Environmental Perception. We collect multi-temporal single-view and panoramic street-view images from 11 representative cities worldwide, and generate high-quality question-answer pairs through a hybrid pipeline of spatial clustering, rule-based generation, model-assisted prompting, and manual annotation. Through extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5 Pro achieves the best overall performance, with its accuracy approaching human expert levels and narrowing the average gap to just 1.5\\%. Most models perform well on tasks grounded in scene understanding. In particular, some models even surpass human annotators in pixel-level change detection. However, performance drops notably in tasks requiring temporal reasoning over urban development. Additionally, in the subjective perception dimension, several models reach human-level or even higher consistency in evaluating dimension such as beautiful and safety.",
        "arxiv_id": "2509.22228",
        "ARXIVID": "2509.22228",
        "COMMENT": "Matches criterion 7 as it provides a comprehensive benchmark for urban scene understanding using multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.21733": {
        "authors": [
            "Jiannan Xiang",
            "Yun Zhu",
            "Lei Shu",
            "Maria Wang",
            "Lijun Yu",
            "Gabriel Barcik",
            "James Lyon",
            "Srinivas Sunkara",
            "Jindong Chen"
        ],
        "title": "UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments",
        "abstract": "arXiv:2509.21733v1 Announce Type: new  Abstract: Developing and testing user interfaces (UIs) and training AI agents to interact with them are challenging due to the dynamic and diverse nature of real-world mobile environments. Existing methods often rely on cumbersome physical devices or limited static analysis of screenshots, which hinders scalable testing and the development of intelligent UI agents. We introduce UISim, a novel image-based UI simulator that offers a dynamic and interactive platform for exploring mobile phone environments purely from screen images. Our system employs a two-stage method: given an initial phone screen image and a user action, it first predicts the abstract layout of the next UI state, then synthesizes a new, visually consistent image based on this predicted layout. This approach enables the realistic simulation of UI transitions. UISim provides immediate practical benefits for UI testing, rapid prototyping, and synthetic data generation. Furthermore, its interactive capabilities pave the way for advanced applications, such as UI navigation task planning for AI agents. Our experimental results show that UISim outperforms end-to-end UI generation baselines in generating realistic and coherent subsequent UI states, highlighting its fidelity and potential to streamline UI development and enhance AI agent training.",
        "arxiv_id": "2509.21733",
        "ARXIVID": "2509.21733",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel UI simulator for training AI agents in dynamic environments.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.21790": {
        "authors": [
            "Yu Shang",
            "Lei Jin",
            "Yiding Ma",
            "Xin Zhang",
            "Chen Gao",
            "Wei Wu",
            "Yong Li"
        ],
        "title": "LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE",
        "abstract": "arXiv:2509.21790v1 Announce Type: new  Abstract: Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.",
        "arxiv_id": "2509.21790",
        "ARXIVID": "2509.21790",
        "COMMENT": "Matches criterion 3. Introduces a novel method for long-horizon embodied world models, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.21980": {
        "authors": [
            "Zeyu Wang",
            "Baiyu Chen",
            "Kun Yan",
            "Hongjing Piao",
            "Hao Xue",
            "Flora D. Salim",
            "Yuanchun Shi",
            "Yuntao Wang"
        ],
        "title": "Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm",
        "abstract": "arXiv:2509.21980v1 Announce Type: new  Abstract: With the rise in popularity of smart glasses, users' attention has been integrated into Vision-Language Models (VLMs) to streamline multi-modal querying in daily scenarios. However, leveraging gaze data to model users' attention may introduce ambiguity challenges: (1) users' verbal questions become ambiguous by using pronouns or skipping context, (2) humans' gaze patterns can be noisy and exhibit complex spatiotemporal relationships with their spoken questions. Previous works only consider single image as visual modality input, failing to capture the dynamic nature of the user's attention. In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal gaze information to enhance the model's effectiveness in real-world applications. Initially, we analyzed hundreds of querying samples with the gaze modality to demonstrate the noisy nature of users' gaze patterns. We then utilized GPT-4o to design an automatic data synthesis pipeline to generate the GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process to handle noisy gaze patterns. Finally, we designed a heatmap module to incorporate gaze information into cutting-edge VLMs while preserving their pretrained knowledge. We evaluated GLARIFY using a hold-out test set. Experiments demonstrate that GLARIFY significantly outperforms baselines. By robustly aligning VLMs with human attention, GLARIFY paves the way for a usable and intuitive interaction paradigm with a visual assistant.",
        "arxiv_id": "2509.21980",
        "ARXIVID": "2509.21980",
        "COMMENT": "Matches criteria 2 and 5 as it explores gaze-facilitated interaction in Vision-Language Models (VLMs) and integrates spatiotemporal gaze information with visual reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.21839": {
        "authors": [
            "Cheng Lei",
            "Jiayu Zhang",
            "Yue Ma",
            "Xinyu Wang",
            "Long Chen",
            "Liang Tang",
            "Yiqiang Yan",
            "Fei Su",
            "Zhicheng Zhao"
        ],
        "title": "DiTraj: training-free trajectory control for video diffusion transformer",
        "abstract": "arXiv:2509.21839v1 Announce Type: new  Abstract: Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object's trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability.",
        "arxiv_id": "2509.21839",
        "ARXIVID": "2509.21839",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on trajectory control in video generation using diffusion transformers.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.21365": {
        "authors": [
            "Zhicheng Du",
            "Qingyang Shi",
            "Jiasheng Lu",
            "Yingshan Liang",
            "Xinyu Zhang",
            "Yiran Wang",
            "Peiwu Qin"
        ],
        "title": "MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation",
        "abstract": "arXiv:2509.21365v1 Announce Type: new  Abstract: The multimodal relevance metric is usually borrowed from the embedding ability of pretrained contrastive learning models for bimodal data, which is used to evaluate the correlation between cross-modal data (e.g., CLIP). However, the commonly used evaluation metrics are only suitable for the associated analysis between two modalities, which greatly limits the evaluation of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation metric for the relevance of multiple modalities (N modalities, N>=3) via multimodal joint representation for the first time. The ability of multimodal joint representation to integrate multiple modalities into the same latent space can accurately represent different modalities at one scale, providing support for fair relevance scoring. Extensive experiments have shown that MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by 13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves as a more reliable metric for evaluating similarity on large-scale multimodal datasets and multimodal model performance evaluation.",
        "arxiv_id": "2509.21365",
        "ARXIVID": "2509.21365",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a novel metric for evaluating multimodal relevance, which is central to vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.21982": {
        "authors": [
            "Renqi Chen",
            "Zeyin Tao",
            "Jianming Guo",
            "Jingzhe Zhu",
            "Yiheng Peng",
            "Qingqing Sun",
            "Tianyi Zhang",
            "Shuai Chen"
        ],
        "title": "RISK: A Framework for GUI Agents in E-commerce Risk Management",
        "abstract": "arXiv:2509.21982v1 Announce Type: new  Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web data through multi-step, stateful interactions, which traditional scraping methods and most existing Graphical User Interface (GUI) agents cannot handle. These agents are typically limited to single-step tasks and lack the ability to manage dynamic, interactive content critical for effective risk assessment. To address this challenge, we introduce RISK, a novel framework designed to build and deploy GUI agents for this domain. RISK integrates three components: (1) RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction trajectories, collected through a high-fidelity browser framework and a meticulous data curation process; (2) RISK-Bench, a benchmark with 802 single-step and 320 multi-step trajectories across three difficulty levels for standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning framework considering four aspects: (i) Output Format: Updated format reward to enhance output syntactic correctness and task comprehension, (ii) Single-step Level: Stepwise accuracy reward to provide granular feedback during early training stages, (iii) Multi-step Level: Process reweight to emphasize critical later steps in interaction sequences, and (iv) Task Level: Level reweight to focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms existing baselines, achieving a 6.8% improvement in offline single-step and an 8.8% improvement in offline multi-step. Moreover, it attains a top task success rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific solution for automating complex web interactions, advancing the state of the art in e-commerce risk management.",
        "arxiv_id": "2509.21982",
        "ARXIVID": "2509.21982",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark (RISK-Bench) and methods for GUI agents in e-commerce risk management, which aligns with embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.22518": {
        "authors": [
            "Bo Li",
            "Guanzhi Deng",
            "Ronghao Chen",
            "Junrong Yue",
            "Shuo Zhang",
            "Qinghua Zhao",
            "Linqi Song",
            "Lijie Wen"
        ],
        "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model",
        "abstract": "arXiv:2509.22518v1 Announce Type: new  Abstract: Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.",
        "arxiv_id": "2509.22518",
        "ARXIVID": "2509.22518",
        "COMMENT": "Matches criteria 2 as it provides insights into reasoning mechanisms in Large Language Models, including multimodal models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.21783": {
        "authors": [
            "Tianci Wu",
            "Guangming Zhu",
            "Jiang Lu",
            "Siyuan Wang",
            "Ning Wang",
            "Nuoye Xiong",
            "Zhang Liang"
        ],
        "title": "Prompt-guided Representation Disentanglement for Action Recognition",
        "abstract": "arXiv:2509.21783v1 Announce Type: new  Abstract: Action recognition is a fundamental task in video understanding. Existing methods typically extract unified features to process all actions in one video, which makes it challenging to model the interactions between different objects in multi-action scenarios. To alleviate this issue, we explore disentangling any specified actions from complex scenes as an effective solution. In this paper, we propose Prompt-guided Disentangled Representation for Action Recognition (ProDA), a novel framework that disentangles any specified actions from a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs) and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing Neural Network (GPNN) in generating action-specific representations. Furthermore, we design a video-adapted GPNN that aggregates information using dynamic weights. Experiments in video action recognition demonstrate the effectiveness of our approach when compared with the state-of-the-art methods. Our code can be found in https://github.com/iamsnaping/ProDA.git",
        "arxiv_id": "2509.21783",
        "ARXIVID": "2509.21783",
        "COMMENT": "Matches criteria 6 as it focuses on video understanding through disentangled representations for action recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.22415": {
        "authors": [
            "Jiawei Liang",
            "Ruoyu Chen",
            "Xianghao Jiao",
            "Siyuan Liang",
            "Shiming Liu",
            "Qunli Zhang",
            "Zheng Hu",
            "Xiaochun Cao"
        ],
        "title": "Explaining multimodal LLMs via intra-modal token interactions",
        "abstract": "arXiv:2509.22415v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose \\textit{Activation Ranking Correlation} (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior.",
        "arxiv_id": "2509.22415",
        "ARXIVID": "2509.22415",
        "COMMENT": "Matches criterion 2 as it focuses on interpretability of multimodal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.22014": {
        "authors": [
            "Saurav Jha",
            "Stefan K. Ehrlich"
        ],
        "title": "Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics",
        "abstract": "arXiv:2509.22014v1 Announce Type: new  Abstract: Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.",
        "arxiv_id": "2509.22014",
        "ARXIVID": "2509.22014",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores a multimodal framework for clinical scene understanding with vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.21953": {
        "authors": [
            "Tao Wu",
            "Yibo Jiang",
            "Yehao Lu",
            "Zhizhong Wang",
            "Zeyi Huang",
            "Zequn Qin",
            "Xi Li"
        ],
        "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning",
        "abstract": "arXiv:2509.21953v1 Announce Type: new  Abstract: Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better.",
        "arxiv_id": "2509.21953",
        "ARXIVID": "2509.21953",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on multi-subject image generation with attention mechanisms and reinforcement learning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.21894": {
        "authors": [
            "Yixiao Liu (College of Computer Science",
            "Sichuan University",
            "China)",
            "Yizhou Yang (College of Computer Science",
            "Sichuan University",
            "China)",
            "Jinwen Li (School of Computer Science and Technology",
            "Xinjiang University",
            "China)",
            "Jun Tao (College of Computer Science",
            "Sichuan University",
            "China)",
            "Ruoyu Li (College of Computer Science",
            "Sichuan University",
            "China)",
            "Xiangkun Wang (College of Computer Science",
            "Sichuan University",
            "China)",
            "Min Zhu (College of Computer Science",
            "Sichuan University",
            "China)",
            "Junlong Cheng (College of Computer Science",
            "Sichuan University",
            "China)"
        ],
        "title": "LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation",
        "abstract": "arXiv:2509.21894v1 Announce Type: new  Abstract: Remote Sensing Change Detection (RSCD) typically identifies changes in land cover or surface conditions by analyzing multi-temporal images. Currently, most deep learning-based methods primarily focus on learning unimodal visual information, while neglecting the rich semantic information provided by multimodal data such as text. To address this limitation, we propose a novel Language-Guided Change Detection model (LG-CD). This model leverages natural language prompts to direct the network's attention to regions of interest, significantly improving the accuracy and robustness of change detection. Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature extractor to capture multi-scale pyramid features from high-resolution to low-resolution across bi-temporal remote sensing images. Subsequently, multi-layer adapters are employed to fine-tune the model for downstream tasks, ensuring its effectiveness in remote sensing change detection. Additionally, we design a Text Fusion Attention Module (TFAM) to align visual and textual information, enabling the model to focus on target change regions using text prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented, which deeply integrates visual and semantic information through a cross-attention mechanism to produce highly accurate change detection masks. Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate that LG-CD consistently outperforms state-of-the-art change detection methods. Furthermore, our approach provides new insights into achieving generalized change detection by leveraging multimodal information.",
        "arxiv_id": "2509.21894",
        "ARXIVID": "2509.21894",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates visual and textual information for change detection tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.21995": {
        "authors": [
            "Muxi Chen",
            "Zhaohua Zhang",
            "Chenchen Zhao",
            "Mingyang Chen",
            "Wenyu Jiang",
            "Tianwen Jiang",
            "Jianhuan Zhuo",
            "Yu Tang",
            "Qiuyong Xiao",
            "Jihong Zhang",
            "Qiang Xu"
        ],
        "title": "FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration",
        "abstract": "arXiv:2509.21995v1 Announce Type: new  Abstract: Static benchmarks have provided a valuable foundation for comparing Text-to-Image (T2I) models. However, their passive design offers limited diagnostic power, struggling to uncover the full landscape of systematic failures or isolate their root causes. We argue for a complementary paradigm: active exploration. We introduce FailureAtlas, the first framework designed to autonomously explore and map the vast failure landscape of T2I models at scale. FailureAtlas frames error discovery as a structured search for minimal, failure-inducing concepts. While it is a computationally explosive problem, we make it tractable with novel acceleration techniques. When applied to Stable Diffusion models, our method uncovers hundreds of thousands of previously unknown error slices (over 247,000 in SD1.5 alone) and provides the first large-scale evidence linking these failures to data scarcity in the training set. By providing a principled and scalable engine for deep model auditing, FailureAtlas establishes a new, diagnostic-first methodology to guide the development of more robust generative AI. The code is available at https://github.com/cure-lab/FailureAtlas",
        "arxiv_id": "2509.21995",
        "ARXIVID": "2509.21995",
        "COMMENT": "Matches criterion 3. Introduces a framework for mapping failure landscapes in text-to-image models, which aligns with new benchmarks or methods in embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.22628": {
        "authors": [
            "Hongyu Chen",
            "Guangrun Wang"
        ],
        "title": "UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning",
        "abstract": "arXiv:2509.22628v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism.",
        "arxiv_id": "2509.22628",
        "ARXIVID": "2509.22628",
        "COMMENT": "Matches criterion 3. Proposes a structured reasoning and planning framework for robotic room cleaning, which aligns with embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.21394": {
        "authors": [
            "Qiyu Ma",
            "Wanli Ni",
            "Zhijin Qin"
        ],
        "title": "Large AI Model-Enabled Generative Semantic Communications for Image Transmission",
        "abstract": "arXiv:2509.21394v1 Announce Type: new  Abstract: The rapid development of generative artificial intelligence (AI) has introduced significant opportunities for enhancing the efficiency and accuracy of image transmission within semantic communication systems. Despite these advancements, existing methodologies often neglect the difference in importance of different regions of the image, potentially compromising the reconstruction quality of visually critical content. To address this issue, we introduce an innovative generative semantic communication system that refines semantic granularity by segmenting images into key and non-key regions. Key regions, which contain essential visual information, are processed using an image oriented semantic encoder, while non-key regions are efficiently compressed through an image-to-text modeling approach. Additionally, to mitigate the substantial storage and computational demands posed by large AI models, the proposed system employs a lightweight deployment strategy incorporating model quantization and low-rank adaptation fine-tuning techniques, significantly boosting resource utilization without sacrificing performance. Simulation results demonstrate that the proposed system outperforms traditional methods in terms of both semantic fidelity and visual quality, thereby affirming its effectiveness for image transmission tasks.",
        "arxiv_id": "2509.21394",
        "ARXIVID": "2509.21394",
        "COMMENT": "Matches criteria 5 as it integrates generative AI with semantic communication for image transmission, focusing on image understanding and generation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.21715": {
        "authors": [
            "Xu Yang",
            "Gady Agam"
        ],
        "title": "Motion-Aware Transformer for Multi-Object Tracking",
        "abstract": "arXiv:2509.21715v1 Announce Type: new  Abstract: Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.",
        "arxiv_id": "2509.21715",
        "ARXIVID": "2509.21715",
        "COMMENT": "Matches criteria 6 as it introduces a novel motion-aware transformer for video-based multi-object tracking, advancing video understanding tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.22085": {
        "authors": [
            "Hadar Peer",
            "Eyal Weiss",
            "Ron Alterovitz",
            "Oren Salzman"
        ],
        "title": "Generalizing Multi-Objective Search via Objective-Aggregation Functions",
        "abstract": "arXiv:2509.22085v1 Announce Type: new  Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world robotic systems need to simultaneously balance multiple, often conflicting objectives. Recent works explore complex interactions between objectives, leading to problem formulations that do not allow the usage of out-of-the-box state-of-the-art MOS algorithms. In this paper, we suggest a generalized problem formulation that optimizes solution objectives via aggregation functions of hidden (search) objectives. We show that our formulation supports the application of standard MOS algorithms, necessitating only to properly extend several core operations to reflect the specific aggregation functions employed. We demonstrate our approach in several diverse robotics planning problems, spanning motion-planning for navigation, manipulation and planning fr medical systems under obstacle uncertainty as well as inspection planning, and route planning with different road types. We solve the problems using state-of-the-art MOS algorithms after properly extending their core operations, and provide empirical evidence that they outperform by orders of magnitude the vanilla versions of the algorithms applied to the same problems but without objective aggregation.",
        "arxiv_id": "2509.22085",
        "ARXIVID": "2509.22085",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it proposes a generalized multi-objective search framework for robotics planning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.21592": {
        "authors": [
            "Gabrijel Boduljak",
            "Laurynas Karazija",
            "Iro Laina",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "title": "What Happens Next? Anticipating Future Motion by Generating Point Trajectories",
        "abstract": "arXiv:2509.21592v1 Announce Type: new  Abstract: We consider the problem of forecasting motion from a single image, i.e., predicting how objects in the world are likely to move, without the ability to observe other parameters such as the object velocities or the forces applied to them. We formulate this task as conditional generation of dense trajectory grids with a model that closely follows the architecture of modern video generators but outputs motion trajectories instead of pixels. This approach captures scene-wide dynamics and uncertainty, yielding more accurate and diverse predictions than prior regressors and generators. We extensively evaluate our method on simulated data, demonstrate its effectiveness on downstream applications such as robotics, and show promising accuracy on real-world intuitive physics datasets. Although recent state-of-the-art video generators are often regarded as world models, we show that they struggle with forecasting motion from a single image, even in simple physical scenarios such as falling blocks or mechanical object interactions, despite fine-tuning on such data. We show that this limitation arises from the overhead of generating pixels rather than directly modeling motion.",
        "arxiv_id": "2509.21592",
        "ARXIVID": "2509.21592",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on forecasting motion trajectories, a video-based task, with novel methodologies.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.21922": {
        "authors": [
            "Vahid Mirjalili",
            "Ramin Giahi",
            "Sriram Kollipara",
            "Akshay Kekuda",
            "Kehui Yao",
            "Kai Zhao",
            "Jianpeng Xu",
            "Kaushiki Nag",
            "Sinduja Subramaniam",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "title": "Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding",
        "abstract": "arXiv:2509.21922v1 Announce Type: new  Abstract: Spatial understanding is a critical capability for vision foundation models. While recent advances in large vision models or vision-language models (VLMs) have expanded recognition capabilities, most benchmarks emphasize localization accuracy rather than whether models capture how objects are arranged and related within a scene. This gap is consequential; effective scene understanding requires not only identifying objects, but reasoning about their relative positions, groupings, and depth. In this paper, we present a systematic benchmark for object-centric spatial reasoning in foundation models. Using a controlled synthetic dataset, we evaluate state-of-the-art vision models (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL, LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, and downstream retrieval tasks. We find a stable trade-off: detectors such as GroundingDINO and OWLv2 deliver precise boxes with limited relational reasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues and fluent captions but struggle with fine-grained spatial context. Our study highlights the gap between localization and true spatial understanding, and pointing toward the need for spatially-aware foundation models in the community.",
        "arxiv_id": "2509.21922",
        "ARXIVID": "2509.21922",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it benchmarks spatial reasoning in vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.21990": {
        "authors": [
            "Changli Tang",
            "Qinfan Xiao",
            "Ke Mei",
            "Tianyi Wang",
            "Fengyun Rao",
            "Chao Zhang"
        ],
        "title": "WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM",
        "abstract": "arXiv:2509.21990v1 Announce Type: new  Abstract: While embeddings from multimodal large language models (LLMs) excel as general-purpose representations, their application to dynamic modalities like audio and video remains underexplored. We introduce WAVE (\\textbf{u}nified \\& \\textbf{v}ersatile \\textbf{a}udio-\\textbf{v}isual \\textbf{e}mbeddings), the first LLM-based embedding that creates a unified representation space for text, audio, and video modalities. WAVE employs a novel hierarchical feature fusion strategy and a joint multi-modal, multi-task training approach to enable two key capabilities: any-to-any cross-modal retrieval and the generation of prompt-aware embeddings tailored to user instructions. Experimentally, WAVE sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves superior results in audio and video-to-audio retrieval. Its prompt-aware nature also yields remarkable performance in multimodal question answering, significantly outperforming existing embedding models. Ablation studies validate our joint training strategy, demonstrating improved performance across all modalities. With a newly introduced benchmark for versatile audio-visual learning, WAVE opens up broad possibilities for cross-modal, any-to-any applications. Our code, checkpoints, and data will be released.",
        "arxiv_id": "2509.21990",
        "ARXIVID": "2509.21990",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a unified embedding space for text, audio, and video using multimodal LLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.21799": {
        "authors": [
            "Hongze Mi",
            "Yibo Feng",
            "Wenjie Lu",
            "Yuqi Wang",
            "Jinyuan Li",
            "Song Cao",
            "He Cui",
            "Tengfei Tian",
            "Xuelin Zhang",
            "Haotian Luo",
            "Di Sun",
            "Naiqiang Tan",
            "Gang Pan"
        ],
        "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents",
        "abstract": "arXiv:2509.21799v1 Announce Type: new  Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of human tasks by emulating user interaction. Despite rapid advancements, current approaches are hindered by several critical challenges: data bottleneck in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance. Inspired by the human cognitive loop of Thinking, Alignment, and Reflection, we present D-Artemis -- a novel deliberative framework in this paper. D-Artemis leverages a fine-grained, app-specific tip retrieval mechanism to inform its decision-making process. It also employs a proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC) Check module and Action Correction Agent (ACA) work in concert to mitigate the risk of execution failures. A post-execution Status Reflection Agent (SRA) completes the cognitive loop, enabling strategic learning from experience. Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal large language models (MLLMs) for GUI tasks without the need for training on complex trajectory datasets, demonstrating strong generalization. D-Artemis establishes new state-of-the-art (SOTA) results across both major benchmarks, achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2. Extensive ablation studies further demonstrate the significant contribution of each component to the framework.",
        "arxiv_id": "2509.21799",
        "ARXIVID": "2509.21799",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it proposes a cognitive framework for GUI multi-agents, enhancing embodied AI capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.21981": {
        "authors": [
            "Zhimin Wang",
            "Shaokang He",
            "Duo Wu",
            "Jinghe Wang",
            "Linjia Kang",
            "Jing Yu",
            "Zhi Wang"
        ],
        "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
        "abstract": "arXiv:2509.21981v1 Announce Type: new  Abstract: Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
        "arxiv_id": "2509.21981",
        "ARXIVID": "2509.21981",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for multi-agent collaboration in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.22339": {
        "authors": [
            "Arman Akbari",
            "Jian Gao",
            "Yifei Zou",
            "Mei Yang",
            "Jinru Duan",
            "Dmitrii Torbunov",
            "Yanzhi Wang",
            "Yihui Ren",
            "Xuan Zhang"
        ],
        "title": "CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process",
        "abstract": "arXiv:2509.22339v1 Announce Type: new  Abstract: Engineering design operates through hierarchical abstraction from system specifications to component implementations, requiring visual understanding coupled with mathematical reasoning at each level. While Multi-modal Large Language Models (MLLMs) excel at natural image tasks, their ability to extract mathematical models from technical diagrams remains unexplored. We present \\textbf{CircuitSense}, a comprehensive benchmark evaluating circuit understanding across this hierarchy through 8,006+ problems spanning component-level schematics to system-level block diagrams. Our benchmark uniquely examines the complete engineering workflow: Perception, Analysis, and Design, with a particular emphasis on the critical but underexplored capability of deriving symbolic equations from visual inputs. We introduce a hierarchical synthetic generation pipeline consisting of a grid-based schematic generator and a block diagram generator with auto-derived symbolic equation labels. Comprehensive evaluation of six state-of-the-art MLLMs, including both closed-source and open-source models, reveals fundamental limitations in visual-to-mathematical reasoning. Closed-source models achieve over 85\\% accuracy on perception tasks involving component recognition and topology identification, yet their performance on symbolic derivation and analytical reasoning falls below 19\\%, exposing a critical gap between visual parsing and symbolic reasoning. Models with stronger symbolic reasoning capabilities consistently achieve higher design task accuracy, confirming the fundamental role of mathematical understanding in circuit synthesis and establishing symbolic reasoning as the key metric for engineering competence.",
        "arxiv_id": "2509.22339",
        "ARXIVID": "2509.22339",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for circuit understanding, which involves visual comprehension and symbolic reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.22323": {
        "authors": [
            "Wangbo Zhao",
            "Yizeng Han",
            "Zhiwei Tang",
            "Jiasheng Tang",
            "Pengfei Zhou",
            "Kai Wang",
            "Bohan Zhuang",
            "Zhangyang Wang",
            "Fan Wang",
            "Yang You"
        ],
        "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer",
        "abstract": "arXiv:2509.22323v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model's distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
        "arxiv_id": "2509.22323",
        "ARXIVID": "2509.22323",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on improving the efficiency of Diffusion Transformers, a type of vision foundation model.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.22645": {
        "authors": [
            "Zhen-Hao Wen",
            "Yan Wang",
            "Ji Feng",
            "Han-Jia Ye",
            "De-Chuan Zhan",
            "Da-Wei Zhou"
        ],
        "title": "Hierarchical Representation Matching for CLIP-based Class-Incremental Learning",
        "abstract": "arXiv:2509.22645v1 Announce Type: new  Abstract: Class-Incremental Learning (CIL) aims to endow models with the ability to continuously adapt to evolving data streams. Recent advances in pre-trained vision-language models (e.g., CLIP) provide a powerful foundation for this task. However, existing approaches often rely on simplistic templates, such as \"a photo of a [CLASS]\", which overlook the hierarchical nature of visual concepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained cues, while distinguishing \"cat\" from \"lion\" requires fine-grained details. Similarly, the current feature mapping in CLIP relies solely on the representation from the last layer, neglecting the hierarchical information contained in earlier layers. In this work, we introduce HiErarchical Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages LLMs to recursively generate discriminative textual descriptors, thereby augmenting the semantic space with explicit hierarchical cues. These descriptors are matched to different levels of the semantic hierarchy and adaptively routed based on task-specific requirements, enabling precise discrimination while alleviating catastrophic forgetting in incremental tasks. Extensive experiments on multiple benchmarks demonstrate that our method consistently achieves state-of-the-art performance.",
        "arxiv_id": "2509.22645",
        "ARXIVID": "2509.22645",
        "COMMENT": "Matches criterion 4. Focuses on hierarchical representation matching for CLIP-based models, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.21764": {
        "authors": [
            "Wenyi Gong",
            "Mieszko Lis"
        ],
        "title": "CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones",
        "abstract": "arXiv:2509.21764v1 Announce Type: new  Abstract: Many modern ViT backbones adopt spatial architectural designs, such as window attention, decomposed relative positional embeddings in SAM, and RoPE in DINOv3. Such architectures impose new challenges on token reduction, as the vast majority of existing methods fail to preserve the spatial structure these architectures depend on. In this paper, we introduce a simple yet effective token merging method that maintains spatial integrity, enabling seamless compatibility with spatial architectures. We reconcile two seemingly conflicting requirements: (i)exploiting the uneven information distribution across the spatial layout while (ii)preserving the spatial structure post-merging. Our approach employs (i)a 2D reduction strategy to enforce structured token layouts, (ii)a spatial-aware merging algorithm that maintains relative token positions, and (iii)a novel max-magnitude-per-dimension token representation that preserves salient features. Our method demonstrates strong performance both off-the-shelf and with fine-tuning, achieving state-of-the-art results on spatial and non-spatial architectures across various vision tasks. Specifically, we achieve 1.25x speedup on SAM-H with only 0.7% mIOU drop evaluated on COCO off-the-shelf, and 1.15x speedup on DeiT-B with no top-1 accuracy drop on ImageNet within just one epoch of fine-tuning.",
        "arxiv_id": "2509.21764",
        "ARXIVID": "2509.21764",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a token merging method for spatial-preserving architectures in vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.22063": {
        "authors": [
            "Chao Huang",
            "Susan Liang",
            "Yapeng Tian",
            "Anurag Kumar",
            "Chenliang Xu"
        ],
        "title": "High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling",
        "abstract": "arXiv:2509.22063v1 Announce Type: new  Abstract: We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS circumvents these issues by leveraging potent generative modeling paradigms, specifically Denoising Diffusion Probabilistic Models (DDPM) and the more recent Flow Matching (FM), integrated within a specialized Separation U-Net architecture. Our framework operates by synthesizing the desired separated sound spectrograms directly from a noise distribution, conditioned concurrently on the mixed audio input and associated visual information. The inherent nature of its generative objective makes DAVIS particularly adept at producing high-quality sound separations for diverse sound categories. We present comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching variants, against leading methods on the standard AVE and MUSIC datasets. The results affirm that both variants surpass existing approaches in separation quality, highlighting the efficacy of our generative framework for tackling the audio-visual source separation task.",
        "arxiv_id": "2509.22063",
        "ARXIVID": "2509.22063",
        "COMMENT": "Does not match any specific criteria but is related to audio-visual generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.22034": {
        "authors": [
            "Xiaochong Lan",
            "Yu Zheng",
            "Shiteng Cao",
            "Yong Li"
        ],
        "title": "The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging",
        "abstract": "arXiv:2509.22034v1 Announce Type: new  Abstract: The growing demand for large language models (LLMs) with tunable reasoning capabilities in many real-world applications highlights a critical need for methods that can efficiently produce a spectrum of models balancing reasoning depth and computational cost. Model merging has emerged as a promising, training-free technique to address this challenge by arithmetically combining the weights of a general-purpose model with a specialized reasoning model. While various merging techniques exist, their potential to create a spectrum of models with fine-grained control over reasoning abilities remains largely unexplored. This work presents a large-scale empirical study evaluating a range of model merging techniques across multiple reasoning benchmarks. We systematically vary merging strengths to construct accuracy-efficiency curves, providing the first comprehensive view of the tunable performance landscape. Our findings reveal that model merging offers an effective and controllable method for calibrating the trade-off between reasoning accuracy and token efficiency, even when parent models have highly divergent weight spaces. Crucially, we identify instances of Pareto Improvement, where a merged model achieves both higher accuracy and lower token consumption than one of its parents. Our study provides the first comprehensive analysis of this tunable space, offering practical guidelines for creating LLMs with specific reasoning profiles to meet diverse application demands.",
        "arxiv_id": "2509.22034",
        "ARXIVID": "2509.22034",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to large language models and reasoning capabilities.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22485": {
        "authors": [
            "Guohui Zhang",
            "Hu Yu",
            "Xiaoxiao Ma",
            "JingHao Zhang",
            "Yaning Pan",
            "Mingde Yao",
            "Jie Xiao",
            "Linjiang Huang",
            "Feng Zhao"
        ],
        "title": "Group Critical-token Policy Optimization for Autoregressive Image Generation",
        "abstract": "arXiv:2509.22485v1 Announce Type: new  Abstract: Recent studies have extended Reinforcement Learning with Verifiable Rewards (RLVR) to autoregressive (AR) visual generation and achieved promising progress. However, existing methods typically apply uniform optimization across all image tokens, while the varying contributions of different image tokens for RLVR's training remain unexplored. In fact, the key obstacle lies in how to identify more critical image tokens during AR generation and implement effective token-wise optimization for them. To tackle this challenge, we propose $\\textbf{G}$roup $\\textbf{C}$ritical-token $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{GCPO}$), which facilitates effective policy optimization on critical tokens. We identify the critical tokens in RLVR-based AR generation from three perspectives, specifically: $\\textbf{(1)}$ Causal dependency: early tokens fundamentally determine the later tokens and final image effect due to unidirectional dependency; $\\textbf{(2)}$ Entropy-induced spatial structure: tokens with high entropy gradients correspond to image structure and bridges distinct visual regions; $\\textbf{(3)}$ RLVR-focused token diversity: tokens with low visual similarity across a group of sampled images contribute to richer token-level diversity. For these identified critical tokens, we further introduce a dynamic token-wise advantage weight to encourage exploration, based on confidence divergence between the policy model and reference model. By leveraging 30\\% of the image tokens, GCPO achieves better performance than GRPO with full tokens. Extensive experiments on multiple text-to-image benchmarks for both AR models and unified multimodal models demonstrate the effectiveness of GCPO for AR visual generation.",
        "arxiv_id": "2509.22485",
        "ARXIVID": "2509.22485",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.21853": {
        "authors": [
            "Kaixuan Zhang",
            "Zhipeng Xiong",
            "Minxian Li",
            "Mingwu Ren",
            "Jiankang Deng",
            "Xiatian Zhu"
        ],
        "title": "Dynamic Novel View Synthesis in High Dynamic Range",
        "abstract": "arXiv:2509.21853v1 Announce Type: new  Abstract: High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.",
        "arxiv_id": "2509.21853",
        "ARXIVID": "2509.21853",
        "COMMENT": "Does not match any specific criteria but is related to novel view synthesis and dynamic HDR modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22414": {
        "authors": [
            "Song Fei",
            "Tian Ye",
            "Lujia Wang",
            "Lei Zhu"
        ],
        "title": "LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer",
        "abstract": "arXiv:2509.22414v1 Announce Type: new  Abstract: Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild.",
        "arxiv_id": "2509.22414",
        "ARXIVID": "2509.22414",
        "COMMENT": "Does not match any specific criteria but is related to image restoration and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22635": {
        "authors": [
            "Luc Boudier",
            "Loris Manganelli",
            "Eleftherios Tsonis",
            "Nicolas Dufour",
            "Vicky Kalogeiton"
        ],
        "title": "Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance",
        "abstract": "arXiv:2509.22635v1 Announce Type: new  Abstract: Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks.",
        "arxiv_id": "2509.22635",
        "ARXIVID": "2509.22635",
        "COMMENT": "Does not match any specific criteria but discusses synthetic data generation for few-shot learning, which is tangentially related to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.21993": {
        "authors": [
            "Dong-Kyum Kim",
            "Minsung Kim",
            "Jea Kwon",
            "Nakyeong Yang",
            "Meeyoung Cha"
        ],
        "title": "Bilinear relational structure fixes reversal curse and enables consistent model editing",
        "abstract": "arXiv:2509.21993v1 Announce Type: new  Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.",
        "arxiv_id": "2509.21993",
        "ARXIVID": "2509.21993",
        "COMMENT": "Does not match any specific criteria but discusses logical consistency in language models, which is tangentially related to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22284": {
        "authors": [
            "Aleksandar Terzi\\'c",
            "Nicolas Menet",
            "Michael Hersche",
            "Thomas Hofmann",
            "Abbas Rahimi"
        ],
        "title": "Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models",
        "abstract": "arXiv:2509.22284v1 Announce Type: new  Abstract: Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \\times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model",
        "arxiv_id": "2509.22284",
        "ARXIVID": "2509.22284",
        "COMMENT": "Does not match any specific criterion. Focuses on state-space models and structured sparse transition matrices, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22044": {
        "authors": [
            "Ziqi Wang",
            "Boye Niu",
            "Zhongli Li",
            "Linghui Meng",
            "Jing Liu",
            "Zhi Zheng",
            "Tong Xu",
            "Hua Wu",
            "Haifeng Wang",
            "Enhong Chen"
        ],
        "title": "A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning",
        "abstract": "arXiv:2509.22044v1 Announce Type: new  Abstract: Recent Large Reasoning Models have achieved significant improvements in complex task-solving capabilities by allocating more computation at the inference stage with a \"thinking longer\" paradigm. Even as the foundational reasoning capabilities of models advance rapidly, the persistent gap between a model's performance in a single attempt and its latent potential, often revealed only across multiple solution paths, starkly highlights the disparity between its realized and inherent capabilities. To address this, we present A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge the gap between a model's potential and its actual performance. In this framework, an \"explorer\" model first generates potential solutions in parallel through repeated sampling. Subsequently,a \"synthesizer\" model integrates these references for a more refined, second stage of reasoning. This two-stage process allows computation to be scaled orthogonally to existing sequential methods. Our work makes two key innovations: First, we present A2R as a plug-and-play parallel reasoning framework that explicitly enhances a model's capabilities on complex questions. For example, using our framework, the Qwen3-8B-distill model achieves a 75% performance improvement compared to its self-consistency baseline. Second, through a systematic analysis of the explorer and synthesizer roles, we identify an effective asymmetric scaling paradigm. This insight leads to A2R-Efficient, a \"small-to-big\" variant that combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration surpasses the average performance of a monolithic Qwen3-32B model at a nearly 30% lower cost. Collectively, these results show that A2R is not only a performance-boosting framework but also an efficient and practical solution for real-world applications.",
        "arxiv_id": "2509.22044",
        "ARXIVID": "2509.22044",
        "COMMENT": "Does not match any specific criterion. Focuses on reasoning frameworks for large models, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.21420": {
        "authors": [
            "Jian Liu",
            "Chunshi Wang",
            "Song Guo",
            "Haohan Weng",
            "Zhen Zhou",
            "Zhiqi Li",
            "Jiaao Yu",
            "Yiling Zhu",
            "Jing Xu",
            "Biwen Lei",
            "Zhuo Chen",
            "Chunchao Guo"
        ],
        "title": "QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models",
        "abstract": "arXiv:2509.21420v1 Announce Type: new  Abstract: The generation of quadrilateral-dominant meshes is a cornerstone of professional 3D content creation. However, existing generative models generate quad meshes by first generating triangle meshes and then merging triangles into quadrilaterals with some specific rules, which typically produces quad meshes with poor topology. In this paper, we introduce QuadGPT, the first autoregressive framework for generating quadrilateral meshes in an end-to-end manner. QuadGPT formulates this as a sequence prediction paradigm, distinguished by two key innovations: a unified tokenization method to handle mixed topologies of triangles and quadrilaterals, and a specialized Reinforcement Learning fine-tuning method tDPO for better generation quality. Extensive experiments demonstrate that QuadGPT significantly surpasses previous triangle-to-quad conversion pipelines in both geometric accuracy and topological quality. Our work establishes a new benchmark for native quad-mesh generation and showcases the power of combining large-scale autoregressive models with topology-aware RL refinement for creating structured 3D assets.",
        "arxiv_id": "2509.21420",
        "ARXIVID": "2509.21420",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in 3D content creation, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.22391": {
        "authors": [
            "Jiaqi Shao",
            "Yuxiang Lin",
            "Munish Prasad Lohani",
            "Yufeng Miao",
            "Bing Luo"
        ],
        "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents",
        "abstract": "arXiv:2509.22391v1 Announce Type: new  Abstract: Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.",
        "arxiv_id": "2509.22391",
        "ARXIVID": "2509.22391",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multimodal learning and reasoning due to its focus on epistemic competence in LLM agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21887": {
        "authors": [
            "Liyang Chen",
            "Tianze Zhou",
            "Xu He",
            "Boshi Tang",
            "Zhiyong Wu",
            "Yang Huang",
            "Yang Wu",
            "Zhongqian Sun",
            "Wei Yang",
            "Helen Meng"
        ],
        "title": "StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing",
        "abstract": "arXiv:2509.21887v1 Announce Type: new  Abstract: The visual dubbing task aims to generate mouth movements synchronized with the driving audio, which has seen significant progress in recent years. However, two critical deficiencies hinder their wide application: (1) Audio-only driving paradigms inadequately capture speaker-specific lip habits, which fail to generate lip movements similar to the target avatar; (2) Conventional blind-inpainting approaches frequently produce visual artifacts when handling obstructions (e.g., microphones, hands), limiting practical deployment. In this paper, we propose StableDub, a novel and concise framework integrating lip-habit-aware modeling with occlusion-robust synthesis. Specifically, building upon the Stable-Diffusion backbone, we develop a lip-habit-modulated mechanism that jointly models phonemic audio-visual synchronization and speaker-specific orofacial dynamics. To achieve plausible lip geometries and object appearances under occlusion, we introduce the occlusion-aware training strategy by explicitly exposing the occlusion objects to the inpainting process. By incorporating the proposed designs, the model eliminates the necessity for cost-intensive priors in previous methods, thereby exhibiting superior training efficiency on the computationally intensive diffusion-based backbone. To further optimize training efficiency from the perspective of model architecture, we introduce a hybrid Mamba-Transformer architecture, which demonstrates the enhanced applicability in low-resource research scenarios. Extensive experimental results demonstrate that StableDub achieves superior performance in lip habit resemblance and occlusion robustness. Our method also surpasses other methods in audio-lip sync, video quality, and resolution consistency. We expand the applicability of visual dubbing methods from comprehensive aspects, and demo videos can be found at https://stabledub.github.io.",
        "arxiv_id": "2509.21887",
        "ARXIVID": "2509.21887",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to vision and multimodal learning due to its focus on visual dubbing and occlusion-robust synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21561": {
        "authors": [
            "Joseph Huang",
            "Yichi Zhang",
            "Jingxi Yu",
            "Wei Chen",
            "Seunghyun Hwang",
            "Qiang Qiu",
            "Amy R. Reibman",
            "Edward J. Delp",
            "Fengqing Zhu"
        ],
        "title": "Unsupervised Defect Detection for Surgical Instruments",
        "abstract": "arXiv:2509.21561v1 Announce Type: new  Abstract: Ensuring the safety of surgical instruments requires reliable detection of visual defects. However, manual inspection is prone to error, and existing automated defect detection methods, typically trained on natural/industrial images, fail to transfer effectively to the surgical domain. We demonstrate that simply applying or fine-tuning these approaches leads to issues: false positive detections arising from textured backgrounds, poor sensitivity to small, subtle defects, and inadequate capture of instrument-specific features due to domain shift. To address these challenges, we propose a versatile method that adapts unsupervised defect detection methods specifically for surgical instruments. By integrating background masking, a patch-based analysis strategy, and efficient domain adaptation, our method overcomes these limitations, enabling the reliable detection of fine-grained defects in surgical instrument imagery.",
        "arxiv_id": "2509.21561",
        "ARXIVID": "2509.21561",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and defect detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22527": {
        "authors": [
            "Andrii Litvynchuk",
            "Ivan Livinsky",
            "Anand Ravi",
            "Nima Kalantari",
            "Andrii Tsarov"
        ],
        "title": "EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model",
        "abstract": "arXiv:2509.22527v1 Announce Type: new  Abstract: Monocular depth estimation (MDE) plays a pivotal role in various computer vision applications, such as robotics, augmented reality, and autonomous driving. Despite recent advancements, existing methods often fail to meet key requirements for 3D reconstruction and view synthesis, including geometric consistency, fine details, robustness to real-world challenges like reflective surfaces, and efficiency for edge devices. To address these challenges, we introduce a novel MDE system, called EfficientDepth, which combines a transformer architecture with a lightweight convolutional decoder, as well as a bimodal density head that allows the network to estimate detailed depth maps. We train our model on a combination of labeled synthetic and real images, as well as pseudo-labeled real images, generated using a high-performing MDE method. Furthermore, we employ a multi-stage optimization strategy to improve training efficiency and produce models that emphasize geometric consistency and fine detail. Finally, in addition to commonly used objectives, we introduce a loss function based on LPIPS to encourage the network to produce detailed depth maps. Experimental results demonstrate that EfficientDepth achieves performance comparable to or better than existing state-of-the-art models, with significantly reduced computational resources.",
        "arxiv_id": "2509.22527",
        "ARXIVID": "2509.22527",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and depth estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22631": {
        "authors": [
            "Debargha Ganguly",
            "Sumit Kumar",
            "Ishwar Balappanawar",
            "Weicong Chen",
            "Shashank Kambhatla",
            "Srinivasan Iyengar",
            "Shivkumar Kalyanaraman",
            "Ponnurangam Kumaraguru",
            "Vipin Chaudhary"
        ],
        "title": "LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision",
        "abstract": "arXiv:2509.22631v1 Announce Type: new  Abstract: Curating high-quality, domain-specific datasets is a major bottleneck for deploying robust vision systems, requiring complex trade-offs between data quality, diversity, and cost when researching vast, unlabeled data lakes. We introduce Labeling Copilot, the first data curation deep research agent for computer vision. A central orchestrator agent, powered by a large multimodal language model, uses multi-step reasoning to execute specialized tools across three core capabilities: (1) Calibrated Discovery sources relevant, in-distribution data from large repositories; (2) Controllable Synthesis generates novel data for rare scenarios with robust filtering; and (3) Consensus Annotation produces accurate labels by orchestrating multiple foundation models via a novel consensus mechanism incorporating non-maximum suppression and voting. Our large-scale validation proves the effectiveness of Labeling Copilot's components. The Consensus Annotation module excels at object discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per image-nearly double the 7.4 ground-truth objects-achieving a final annotation mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class imbalance to discover 903 new bounding box categories, expanding its capability to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a 10-million sample scale, features an active learning strategy that is up to 40x more computationally efficient than alternatives with equivalent sample efficiency. These experiments validate that an agentic workflow with optimized, scalable tools provides a robust foundation for curating industrial-scale datasets.",
        "arxiv_id": "2509.22631",
        "ARXIVID": "2509.22631",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and dataset curation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21842": {
        "authors": [
            "Yansong Ning",
            "Rui Liu",
            "Jun Wang",
            "Kai Chen",
            "Wei Li",
            "Jun Fang",
            "Kan Zheng",
            "Naiqiang Tan",
            "Hao Liu"
        ],
        "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents",
        "abstract": "arXiv:2509.21842v1 Announce Type: new  Abstract: Travel planning (TP) agent has recently worked as an emerging building block to interact with external tools and resources for travel itinerary generation, ensuring enjoyable user experience. Despite its benefits, existing studies rely on hand craft prompt and fixed agent workflow, hindering more flexible and autonomous TP agent. This paper proposes DeepTravel, an end to end agentic reinforcement learning framework for building autonomous travel planning agent, capable of autonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi step reasoning. To achieve this, we first construct a robust sandbox environment by caching transportation, accommodation and POI data, facilitating TP agent training without being constrained by real world APIs limitations (e.g., inconsistent outputs). Moreover, we develop a hierarchical reward modeling system, where a trajectory level verifier first checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and then the turn level verifier further validate itinerary detail consistency with tool responses, enabling efficient and precise reward service. Finally, we propose the reply augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity. We deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
        "arxiv_id": "2509.21842",
        "ARXIVID": "2509.21842",
        "COMMENT": "Does not match any specific criterion but is related to reinforcement learning and autonomous agents, which is tangentially relevant to your friend's general interest in embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22393": {
        "authors": [
            "Wenqiang Wang",
            "Siyuan Liang",
            "Xiao Yan",
            "Xiaochun Cao"
        ],
        "title": "Text Adversarial Attacks with Dynamic Outputs",
        "abstract": "arXiv:2509.22393v1 Announce Type: new  Abstract: Text adversarial attack methods are typically designed for static scenarios with fixed numbers of output labels and a predefined label space, relying on extensive querying of the victim model (query-based attacks) or the surrogate model (transfer-based attacks). To address this gap, we introduce the Textual Dynamic Outputs Attack (TDOA) method, which employs a clustering-based surrogate model training approach to convert the dynamic-output scenario into a static single-output scenario. To improve attack effectiveness, we propose the farthest-label targeted attack strategy, which selects adversarial vectors that deviate most from the model's coarse-grained labels, thereby maximizing disruption. We extensively evaluate TDOA on four datasets and eight victim models (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting adversarial examples and its strong potential to compromise large language models with limited access. With a single query per text, TDOA achieves a maximum attack success rate of 50.81\\%. Additionally, we find that TDOA also achieves state-of-the-art performance in conventional static output scenarios, reaching a maximum ASR of 82.68\\%. Meanwhile, by conceptualizing translation tasks as classification problems with unbounded output spaces, we extend the TDOA framework to generative settings, surpassing prior results by up to 0.64 RDBLEU and 0.62 RDchrF.",
        "arxiv_id": "2509.22393",
        "ARXIVID": "2509.22393",
        "COMMENT": "Does not match any specific criterion but is related to adversarial attacks in text models, which is tangentially relevant to your friend's general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22581": {
        "authors": [
            "Jini Yang",
            "Beomseok Oh",
            "Seungryong Kim",
            "Sunok Kim"
        ],
        "title": "SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking Neural Networks",
        "abstract": "arXiv:2509.22581v1 Announce Type: new  Abstract: Spiking neural networks (SNNs) have recently been attracting significant attention for their biological plausibility and energy efficiency, but semi-supervised learning (SSL) methods for SNN-based models remain underexplored compared to those for artificial neural networks (ANNs). In this paper, we introduce SpikeMatch, the first SSL framework for SNNs that leverages the temporal dynamics through the leakage factor of SNNs for diverse pseudo-labeling within a co-training framework. By utilizing agreement among multiple predictions from a single SNN, SpikeMatch generates reliable pseudo-labels from weakly-augmented unlabeled samples to train on strongly-augmented ones, effectively mitigating confirmation bias by capturing discriminative features with limited labels. Experiments show that SpikeMatch outperforms existing SSL methods adapted to SNN backbones across various standard benchmarks.",
        "arxiv_id": "2509.22581",
        "ARXIVID": "2509.22581",
        "COMMENT": "Does not match any specific criteria. Focuses on semi-supervised learning for spiking neural networks, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22244": {
        "authors": [
            "Junyi Wu",
            "Zhiteng Li",
            "Haotong Qin",
            "Xiaohong Liu",
            "Linghe Kong",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing",
        "abstract": "arXiv:2509.22244v1 Announce Type: new  Abstract: Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.",
        "arxiv_id": "2509.22244",
        "ARXIVID": "2509.22244",
        "COMMENT": "Does not match any specific criteria. Focuses on efficient image editing with diffusion models, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21787": {
        "authors": [
            "Dwip Dalal",
            "Gautam Vashishtha",
            "Anku Ranui",
            "Aishwarya Reganti",
            "Parth Patwa",
            "Mohd Sarique",
            "Chandan Gupta",
            "Keshav Nath",
            "Viswanatha Reddy",
            "Vinija Jain",
            "Aman Chadha",
            "Amitava Das",
            "Amit Sheth",
            "Asif Ekbal"
        ],
        "title": "DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images",
        "abstract": "arXiv:2509.21787v1 Announce Type: new  Abstract: The rise in harmful online content not only distorts public discourse but also poses significant challenges to maintaining a healthy digital environment. In response to this, we introduce a multimodal dataset uniquely crafted for identifying hate in digital content. Central to our methodology is the innovative application of watermarked, stability-enhanced, stable diffusion techniques combined with the Digital Attention Analysis Module (DAAM). This combination is instrumental in pinpointing the hateful elements within images, thereby generating detailed hate attention maps, which are used to blur these regions from the image, thereby removing the hateful sections of the image. We release this data set as a part of the dehate shared task. This paper also describes the details of the shared task. Furthermore, we present DeHater, a vision-language model designed for multimodal dehatification tasks. Our approach sets a new standard in AI-driven image hate detection given textual prompts, contributing to the development of more ethical AI applications in social media.",
        "arxiv_id": "2509.21787",
        "ARXIVID": "2509.21787",
        "COMMENT": "Does not match any specific criteria. Focuses on hate speech detection in images using multimodal approaches, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22255": {
        "authors": [
            "Syed Mahbubul Huq",
            "Daniel Brito",
            "Daniel Sikar",
            "Rajesh Mojumder"
        ],
        "title": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing",
        "abstract": "arXiv:2509.22255v1 Announce Type: new  Abstract: This paper presents an evaluation framework for assessing Large Language Models' (LLMs) capabilities in combinatorial optimization, specifically addressing the 2D bin-packing problem. We introduce a systematic methodology that combines LLMs with evolutionary algorithms to generate and refine heuristic solutions iteratively. Through comprehensive experiments comparing LLM generated heuristics against traditional approaches (Finite First-Fit and Hybrid First-Fit), we demonstrate that LLMs can produce more efficient solutions while requiring fewer computational resources. Our evaluation reveals that GPT-4o achieves optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78 to 0.83. This work contributes to understanding LLM evaluation in specialized domains and establishes benchmarks for assessing LLM performance in combinatorial optimization tasks.",
        "arxiv_id": "2509.22255",
        "ARXIVID": "2509.22255",
        "COMMENT": "Does not match any specific criteria. Focuses on combinatorial optimization using LLMs, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22558": {
        "authors": [
            "Chenyu Zhou",
            "Tianyi Xu",
            "Jianghao Lin",
            "Dongdong Ge"
        ],
        "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models",
        "abstract": "arXiv:2509.22558v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs.",
        "arxiv_id": "2509.22558",
        "ARXIVID": "2509.22558",
        "COMMENT": "Does not match any specific criteria but is related to reinforcement learning and generative modeling in a specialized domain (Operations Research).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21360": {
        "authors": [
            "Xingkai Peng",
            "Jun Jiang",
            "Meng Tong",
            "Shuai Li",
            "Weiming Zhang",
            "Nenghai Yu",
            "Kejiang Chen"
        ],
        "title": "Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models",
        "abstract": "arXiv:2509.21360v1 Announce Type: new  Abstract: Text-to-image (T2I) models have been widely applied in generating high-fidelity images across various domains. However, these models may also be abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks. Existing jailbreak methods primarily manipulate the textual prompt, leaving potential vulnerabilities in image-based inputs largely unexplored. Moreover, text-based methods face challenges in bypassing the model's safety filters. In response to these limitations, we propose the Multimodal Prompt Decoupling Attack (MPDA), which utilizes image modality to separate the harmful semantic components of the original unsafe prompt. MPDA follows three core steps: firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe prompts and harmful prompts. The former are seemingly harmless sub-prompts that can bypass filters, while the latter are sub-prompts with unsafe semantics that trigger filters. Subsequently, the LLM rewrites the harmful prompts into natural adversarial prompts to bypass safety filters, which guide the T2I model to modify the base image into an NSFW output. Finally, to ensure semantic consistency between the generated NSFW images and the original unsafe prompts, the visual language model generates image captions, providing a new pathway to guide the LLM in iterative rewriting and refining the generated content.",
        "arxiv_id": "2509.21360",
        "ARXIVID": "2509.21360",
        "COMMENT": "Does not match any specific criteria but is tangentially related to multimodal learning and safety in text-to-image models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21747": {
        "authors": [
            "Qing Zhu",
            "Wangdong Guo",
            "Qirong Mao",
            "Xiaohua Huang",
            "Xiuyan Shao",
            "Wenming Zheng"
        ],
        "title": "Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition",
        "abstract": "arXiv:2509.21747v1 Announce Type: new  Abstract: Group-level emotion recognition (GER) aims to identify holistic emotions within a scene involving multiple individuals. Current existed methods underestimate the importance of visual scene contextual information in modeling individual relationships. Furthermore, they overlook the crucial role of semantic information from emotional labels for complete understanding of emotions. To address this limitation, we propose a novel framework that incorporates visual scene context and label-guided semantic information to improve GER performance. It involves the visual context encoding module that leverages multi-scale scene information to diversely encode individual relationships. Complementarily, the emotion semantic encoding module utilizes group-level emotion labels to prompt a large language model to generate nuanced emotion lexicons. These lexicons, in conjunction with the emotion labels, are then subsequently refined into comprehensive semantic representations through the utilization of a structured emotion tree. Finally, similarity-aware interaction is proposed to align and integrate visual and semantic information, thereby generating enhanced group-level emotion representations and subsequently improving the performance of GER. Experiments on three widely adopted GER datasets demonstrate that our proposed method achieves competitive performance compared to state-of-the-art methods.",
        "arxiv_id": "2509.21747",
        "ARXIVID": "2509.21747",
        "COMMENT": "Does not match any specific criteria but is related to group-level emotion recognition using visual and semantic information.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21352": {
        "authors": [
            "William Saakyan",
            "Matthias Norden",
            "Lola Eversmann",
            "Simon Kirsch",
            "Muyu Lin",
            "Simon Guendelman",
            "Isabel Dziobek",
            "Hanna Drimalla"
        ],
        "title": "Improving Autism Detection with Multimodal Behavioral Analysis",
        "abstract": "arXiv:2509.21352v1 Announce Type: new  Abstract: Due to the complex and resource-intensive nature of diagnosing Autism Spectrum Condition (ASC), several computer-aided diagnostic support methods have been proposed to detect autism by analyzing behavioral cues in patient video data. While these models show promising results on some datasets, they struggle with poor gaze feature performance and lack of real-world generalizability. To tackle these challenges, we analyze a standardized video dataset comprising 168 participants with ASC (46% female) and 157 non-autistic participants (46% female), making it, to our knowledge, the largest and most balanced dataset available. We conduct a multimodal analysis of facial expressions, voice prosody, head motion, heart rate variability (HRV), and gaze behavior. To address the limitations of prior gaze models, we introduce novel statistical descriptors that quantify variability in eye gaze angles, improving gaze-based classification accuracy from 64% to 69% and aligning computational findings with clinical research on gaze aversion in ASC. Using late fusion, we achieve a classification accuracy of 74%, demonstrating the effectiveness of integrating behavioral markers across multiple modalities. Our findings highlight the potential for scalable, video-based screening tools to support autism assessment.",
        "arxiv_id": "2509.21352",
        "ARXIVID": "2509.21352",
        "COMMENT": "Does not match any specific criteria but is related to multimodal behavioral analysis for autism detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22132": {
        "authors": [
            "Jingjing Lu",
            "Huilong Pi",
            "Yunchuan Qin",
            "Zhuo Tang",
            "Ruihui Li"
        ],
        "title": "Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud",
        "abstract": "arXiv:2509.22132v1 Announce Type: new  Abstract: Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model's learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.",
        "arxiv_id": "2509.22132",
        "ARXIVID": "2509.22132",
        "COMMENT": "Does not match any specific criteria but focuses on self-supervised point cloud completion, which is tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22613": {
        "authors": [
            "Siwei Wang",
            "Yifei Shen",
            "Haoran Sun",
            "Shi Feng",
            "Shang-Hua Teng",
            "Li Dong",
            "Yaru Hao",
            "Wei Chen"
        ],
        "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
        "abstract": "arXiv:2509.22613v1 Announce Type: new  Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
        "arxiv_id": "2509.22613",
        "ARXIVID": "2509.22613",
        "COMMENT": "Does not match any specific criterion. Focuses on reinforcement learning for language model planning, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21750": {
        "authors": [
            "Yu Li",
            "Da Chang",
            "Xi Xiao"
        ],
        "title": "KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields",
        "abstract": "arXiv:2509.21750v1 Announce Type: new  Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in image segmentation, its direct application to medical imaging remains hindered by fundamental challenges, including ambiguous boundaries, insufficient modeling of anatomical relationships, and the absence of uncertainty quantification. To address these limitations, we introduce KG-SAM, a knowledge-guided framework that synergistically integrates anatomical priors with boundary refinement and uncertainty estimation. Specifically, KG-SAM incorporates (i) a medical knowledge graph to encode fine-grained anatomical relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce anatomically consistent predictions, and (iii) an uncertainty-aware fusion module to enhance reliability in high-stakes clinical scenarios. Extensive experiments across multi-center medical datasets demonstrate the effectiveness of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate segmentation and delivers substantial gains in abdominal segmentation, reaching 78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and generalizable framework for advancing medical image segmentation.",
        "arxiv_id": "2509.21750",
        "ARXIVID": "2509.21750",
        "COMMENT": "Does not match any specific criterion. Focuses on medical image segmentation using SAM, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.22400": {
        "authors": [
            "Xinhao Zhong",
            "Yimin Zhou",
            "Zhiqi Zhang",
            "Junhao Li",
            "Yi Sun",
            "Bin Chen",
            "Shu-Tao Xia",
            "Ke Xu"
        ],
        "title": "Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models",
        "abstract": "arXiv:2509.22400v1 Announce Type: new  Abstract: The rapid progress of visual autoregressive (VAR) models has brought new opportunities for text-to-image generation, but also heightened safety concerns. Existing concept erasure techniques, primarily designed for diffusion models, fail to generalize to VARs due to their next-scale token prediction paradigm. In this paper, we first propose a novel VAR Erasure framework VARE that enables stable concept erasure in VAR models by leveraging auxiliary visual tokens to reduce fine-tuning intensity. Building upon this, we introduce S-VARE, a novel and effective concept erasure method designed for VAR, which incorporates a filtered cross entropy loss to precisely identify and minimally adjust unsafe visual tokens, along with a preservation loss to maintain semantic fidelity, addressing the issues such as language drift and reduced diversity introduce by na\\\"ive fine-tuning. Extensive experiments demonstrate that our approach achieves surgical concept erasure while preserving generation quality, thereby closing the safety gap in autoregressive text-to-image generation by earlier methods.",
        "arxiv_id": "2509.22400",
        "ARXIVID": "2509.22400",
        "COMMENT": "Does not match any specific criterion. Focuses on concept erasure in visual autoregressive models, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21743": {
        "authors": [
            "Ammar Ahmed",
            "Azal Ahmad Khan",
            "Ayaan Ahmad",
            "Sheng Di",
            "Zirui Liu",
            "Ali Anwar"
        ],
        "title": "Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts",
        "abstract": "arXiv:2509.21743v1 Announce Type: new  Abstract: Large reasoning models improve accuracy by producing long reasoning traces, but this inflates latency and cost, motivating inference-time efficiency. We propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable ``thought\" steps to guide new problems. RoT organizes steps into a thought graph with sequential and semantic edges to enable fast retrieval and flexible recombination. At inference, RoT retrieves query-relevant nodes and applies reward-guided traversal to assemble a problem-specific template that guides generation. This dynamic template reuse reduces redundant exploration and, therefore, reduces output tokens while preserving accuracy. We evaluate RoT on reasoning benchmarks with multiple models, measuring accuracy, token usage, latency, and memory overhead. Findings show small prompt growth but substantial efficiency gains, with RoT reducing output tokens by up to 40%, inference latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a scalable paradigm for efficient LRM reasoning via dynamic template construction through retrieval.",
        "arxiv_id": "2509.21743",
        "ARXIVID": "2509.21743",
        "COMMENT": "Does not match any specific criteria but is relevant to efficient reasoning in large reasoning models, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21718": {
        "authors": [
            "Shehzeen Hussain",
            "Paarth Neekhara",
            "Xuesong Yang",
            "Edresson Casanova",
            "Subhankar Ghosh",
            "Roy Fejgin",
            "Ryan Langman",
            "Mikyas Desta",
            "Leili Tavabi",
            "Jason Li"
        ],
        "title": "Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization",
        "abstract": "arXiv:2509.21718v1 Announce Type: new  Abstract: Developing high-quality text-to-speech (TTS) systems for low-resource languages is challenging due to the scarcity of paired text and speech data. In contrast, automatic speech recognition (ASR) models for such languages are often more accessible, owing to large-scale multilingual pre-training efforts. We propose a framework based on Group Relative Policy Optimization (GRPO) to adapt an autoregressive, multilingual TTS model to new languages. Our method first establishes a language-agnostic foundation for TTS synthesis by training a multilingual baseline with International Phonetic Alphabet (IPA) tokens. Next, we fine-tune this model on limited paired data of the new languages to capture the target language's prosodic features. Finally, we apply GRPO to optimize the model using only unpaired text and speaker prompts, guided by a multi-objective reward from pretrained ASR, speaker verification, and audio quality estimation models. Experiments demonstrate that this pipeline produces intelligible and speaker-consistent speech in low-resource languages, substantially outperforming fine-tuning alone. Furthermore, our GRPO-based framework also improves TTS performance in high-resource languages, surpassing offline alignment methods such as Direct Preference Optimization (DPO) yielding superior intelligibility, speaker similarity, and audio quality.",
        "arxiv_id": "2509.21718",
        "ARXIVID": "2509.21718",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and low-resource language TTS, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.21864": {
        "authors": [
            "Janis Keuper",
            "Margret Keuper"
        ],
        "title": "Deepfakes: we need to re-think the concept of \"real\" images",
        "abstract": "arXiv:2509.21864v1 Announce Type: new  Abstract: The wide availability and low usability barrier of modern image generation models has triggered the reasonable fear of criminal misconduct and negative social implications. The machine learning community has been engaging this problem with an extensive series of publications proposing algorithmic solutions for the detection of \"fake\", e.g. entirely generated or partially manipulated images. While there is undoubtedly some progress towards technical solutions of the problem, we argue that current and prior work is focusing too much on generative algorithms and \"fake\" data-samples, neglecting a clear definition and data collection of \"real\" images. The fundamental question \"what is a real image?\" might appear to be quite philosophical, but our analysis shows that the development and evaluation of basically all current \"fake\"-detection methods is relying on only a few, quite old low-resolution datasets of \"real\" images like ImageNet. However, the technology for the acquisition of \"real\" images, aka taking photos, has drastically evolved over the last decade: Today, over 90% of all photographs are produced by smartphones which typically use algorithms to compute an image from multiple inputs (over time) from multiple sensors. Based on the fact that these image formation algorithms are typically neural network architectures which are closely related to \"fake\"-image generators, we state the position that today, we need to re-think the concept of \"real\" images. The purpose of this position paper is to raise the awareness of the current shortcomings in this active field of research and to trigger an open discussion whether the detection of \"fake\" images is a sound objective at all. At the very least, we need a clear technical definition of \"real\" images and new benchmark datasets.",
        "arxiv_id": "2509.21864",
        "ARXIVID": "2509.21864",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision due to its discussion on the concept of 'real' images and implications for generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.21916": {
        "authors": [
            "Boying Li",
            "Chang Liu",
            "Petter Ky\\\"osti",
            "Mattias \\\"Ohman",
            "Devashish Singha Roy",
            "Sofia Plazzi",
            "Hamam Mokayed",
            "Olle Hagner"
        ],
        "title": "Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning",
        "abstract": "arXiv:2509.21916v1 Announce Type: new  Abstract: Aside from common challenges in remote sensing like small, sparse targets and computation cost limitations, detecting vehicles from UAV images in the Nordic regions faces strong visibility challenges and domain shifts caused by diverse levels of snow coverage. Although annotated data are expensive, unannotated data is cheaper to obtain by simply flying the drones. In this work, we proposed a sideload-CL-adaptation framework that enables the use of unannotated data to improve vehicle detection using lightweight models. Specifically, we propose to train a CNN-based representation extractor through contrastive learning on the unannotated data in the pretraining stage, and then sideload it to a frozen YOLO11n backbone in the fine-tuning stage. To find a robust sideload-CL-adaptation, we conducted extensive experiments to compare various fusion methods and granularity. Our proposed sideload-CL-adaptation model improves the detection performance by 3.8% to 9.5% in terms of mAP50 on the NVD dataset.",
        "arxiv_id": "2509.21916",
        "ARXIVID": "2509.21916",
        "COMMENT": "Does not match any specific criterion. Focuses on vehicle detection under adverse weather conditions, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}