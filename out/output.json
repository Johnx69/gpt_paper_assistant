{
    "2511.10301": {
        "authors": [
            "Dongwan Kim",
            "Viresh Ranjan",
            "Takashi Nagata",
            "Arnab Dhua",
            "Amit Kumar K C"
        ],
        "title": "Rethinking Visual Information Processing in Multimodal LLMs",
        "abstract": "arXiv:2511.10301v1 Announce Type: new  Abstract: Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.",
        "arxiv_id": "2511.10301",
        "ARXIVID": "2511.10301",
        "COMMENT": "Matches criterion 2 as it explores a novel architecture (LLaViT) for vision-language integration in multimodal LLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2511.10035": {
        "authors": [
            "Feiyang Jia",
            "Caiyan Jia",
            "Ailin Liu",
            "Shaoqing Xu",
            "Qiming Xia",
            "Lin Liu",
            "Lei Yang",
            "Yan Gong",
            "Ziying Song"
        ],
        "title": "DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection",
        "abstract": "arXiv:2511.10035v1 Announce Type: new  Abstract: As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\\% mAP, +0.8\\% NDS, and +1.3\\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.",
        "arxiv_id": "2511.10035",
        "ARXIVID": "2511.10035",
        "COMMENT": "Matches criterion 3 as it introduces a novel dual-guided fusion method for multi-modal 3D object detection, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10136": {
        "authors": [
            "Mayank Vatsa",
            "Aparna Bharati",
            "Richa Singh"
        ],
        "title": "Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation",
        "abstract": "arXiv:2511.10136v1 Announce Type: new  Abstract: The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.",
        "arxiv_id": "2511.10136",
        "ARXIVID": "2511.10136",
        "COMMENT": "Matches criterion 5 as it discusses compositional fidelity in text-to-image generation, which involves integrating image understanding and generation with language models.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2511.09675": {
        "authors": [
            "Felix B. Mueller",
            "Jan F. Meier",
            "Timo Lueddecke",
            "Richard Vogg",
            "Roger L. Freixanet",
            "Valentin Hassler",
            "Tiffany Bosshard",
            "Elif Karakoc",
            "William J. O'Hearn",
            "Sofia M. Pereira",
            "Sandro Sehner",
            "Kaja Wierucka",
            "Judith Burkart",
            "Claudia Fichtel",
            "Julia Fischer",
            "Alexander Gail",
            "Catherine Hobaiter",
            "Julia Ostner",
            "Liran Samuni",
            "Oliver Sch\\\"ulke",
            "Neda Shahidi",
            "Erin G. Wessling",
            "Alexander S. Ecker"
        ],
        "title": "PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild",
        "abstract": "arXiv:2511.09675v1 Announce Type: new  Abstract: Non-human primates are our closest living relatives, and analyzing their behavior is central to research in cognition, evolution, and conservation. Computer vision could greatly aid this research, but existing methods often rely on human-centric pretrained models and focus on single datasets, which limits generalization. We address this limitation by shifting from a model-centric to a data-centric approach and introduce PriVi, a large-scale primate-centric video pretraining dataset. PriVi contains 424 hours of curated video, combining 174 hours from behavioral research across 11 settings with 250 hours of diverse web-sourced footage, assembled through a scalable data curation pipeline. We pretrain V-JEPA on PriVi to learn primate-specific representations and evaluate it using a lightweight frozen classifier. Across four benchmark datasets, ChimpACT, BaboonLand, PanAf500, and ChimpBehave, our approach consistently outperforms prior work, including fully finetuned baselines, and scales favorably with fewer labels. These results demonstrate that primate-centric pretraining substantially improves data efficiency and generalization, making it a promising approach for low-label applications. Code, models, and the majority of the dataset will be made available.",
        "arxiv_id": "2511.09675",
        "ARXIVID": "2511.09675",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding for primate behavior and introduces a large-scale dataset and benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10134": {
        "authors": [
            "Mingda Jia",
            "Weiliang Meng",
            "Zenghuang Fu",
            "Yiheng Li",
            "Qi Zeng",
            "Yifan Zhang",
            "Ju Xin",
            "Rongtao Xu",
            "Jiguang Zhang",
            "Xiaopeng Zhang"
        ],
        "title": "Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction",
        "abstract": "arXiv:2511.10134v1 Announce Type: new  Abstract: Dense video captioning jointly localizes and captions salient events in untrimmed videos. Recent methods primarily focus on leveraging additional prior knowledge and advanced multi-task architectures to achieve competitive performance. However, these pipelines rely on implicit modeling that uses frame-level or fragmented video features, failing to capture the temporal coherence across event sequences and comprehensive semantics within visual contexts. To address this, we propose an explicit temporal-semantic modeling framework called Context-Aware Cross-Modal Interaction (CACMI), which leverages both latent temporal characteristics within videos and linguistic semantics from text corpus. Specifically, our model consists of two core components: Cross-modal Frame Aggregation aggregates relevant frames to extract temporally coherent, event-aligned textual features through cross-modal retrieval; and Context-aware Feature Enhancement utilizes query-guided attention to integrate visual dynamics with pseudo-event semantics. Extensive experiments on the ActivityNet Captions and YouCook2 datasets demonstrate that CACMI achieves the state-of-the-art performance on dense video captioning task.",
        "arxiv_id": "2511.10134",
        "ARXIVID": "2511.10134",
        "COMMENT": "Matches criterion 6 closely as it focuses on dense video captioning with novel temporal-semantic modeling techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.09827": {
        "authors": [
            "Aymen Mir",
            "Jian Wang",
            "Riza Alp Guler",
            "Chuan Guo",
            "Gerard Pons-Moll",
            "Bing Zhou"
        ],
        "title": "AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting",
        "abstract": "arXiv:2511.09827v1 Announce Type: new  Abstract: We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.",
        "arxiv_id": "2511.09827",
        "ARXIVID": "2511.09827",
        "COMMENT": "Matches criterion 3 closely as it introduces a novel framework for animating humans in 3D scenes using Gaussian Splatting, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10279": {
        "authors": [
            "Yanbei Jiang",
            "Chao Lei",
            "Yihao Ding",
            "Krista Ehinger",
            "Jey Han Lau"
        ],
        "title": "PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning",
        "abstract": "arXiv:2511.10279v1 Announce Type: new  Abstract: Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.",
        "arxiv_id": "2511.10279",
        "ARXIVID": "2511.10279",
        "COMMENT": "Matches criterion 2 closely as it explores visual reasoning in Vision-Language Models (VLMs) with novel reinforcement learning techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10518": {
        "authors": [
            "Wei Li",
            "Renshan Zhang",
            "Rui Shao",
            "Zhijian Fang",
            "Kaiwen Zhou",
            "Zhuotao Tian",
            "Liqiang Nie"
        ],
        "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
        "abstract": "arXiv:2511.10518v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
        "arxiv_id": "2511.10518",
        "ARXIVID": "2511.10518",
        "COMMENT": "Matches criterion 3 as it focuses on efficient robotic manipulation with a novel vision-language-action framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10560": {
        "authors": [
            "Haosong Peng",
            "Hao Li",
            "Yalun Dai",
            "Yushi Lan",
            "Yihang Luo",
            "Tianyu Qi",
            "Zhengshen Zhang",
            "Yufeng Zhan",
            "Junfei Zhang",
            "Wenchao Xu",
            "Ziwei Liu"
        ],
        "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
        "abstract": "arXiv:2511.10560v1 Announce Type: new  Abstract: General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
        "arxiv_id": "2511.10560",
        "ARXIVID": "2511.10560",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model (OmniVGGT) with applications in 3D tasks and robotic integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.09572": {
        "authors": [
            "Tommaso Castellani",
            "Naimeng Ye",
            "Daksh Mittal",
            "Thomson Yen",
            "Hongseok Namkoong"
        ],
        "title": "SynthTools: A Framework for Scaling Synthetic Tools for Agent Development",
        "abstract": "arXiv:2511.09572v1 Announce Type: new  Abstract: AI agents increasingly rely on external tools to solve complex, long-horizon tasks. Advancing such agents requires reproducible evaluation and large-scale training in controllable, diverse, and realistic tool-use environments. However, real-world APIs are limited in availability, domain coverage, and stability, often requiring access keys and imposing rate limits, which render them impractical for stable evaluation or scalable training. To address these challenges, we introduce SynthTools, a flexible and scalable framework for generating synthetic tool ecosystems. Our framework consists of three core components: Tool Generation for automatic and scalable creation of diverse tools, Tool Simulation to emulate realistic tool behaviors, and Tool Audit to ensure correctness and consistency of tool simulation. To illustrate its scalability, we show that SynthTools can readily produce toolsets that span twice as many domains and twice as many tools per domain as prior work. Furthermore, the tool simulation and tool audit components demonstrate strong reliability, achieving $94\\%$ and $99\\%$ accuracy respectively. Finally, we construct downstream tasks from the generated tools that even state-of-the-art models struggle to complete. By enabling scalable, diverse, and reliable tool ecosystems, SynthTools provides a practical path toward large-scale training and stable evaluation of tool-use agents. Our code is available at https://github.com/namkoong-lab/SynthTools.",
        "arxiv_id": "2511.09572",
        "ARXIVID": "2511.09572",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark framework (SynthTools) for tool-use agents in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.10431": {
        "authors": [
            "Daniele Perlo",
            "Vladimir Despotovic",
            "Selma Boudissa",
            "Sang-Yoon Kim",
            "Petr Nazarov",
            "Yanrong Zhang",
            "Max Wintermark",
            "Olivier Keunen"
        ],
        "title": "RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation",
        "abstract": "arXiv:2511.10431v1 Announce Type: new  Abstract: We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357",
        "arxiv_id": "2511.10431",
        "ARXIVID": "2511.10431",
        "COMMENT": "Matches criterion 6 as it introduces a video dataset and benchmarks for seizure detection in rodents, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.10309": {
        "authors": [
            "Xiaomei Yang",
            "Xizhan Gao",
            "Sijie Niu",
            "Fa Zhu",
            "Guang Feng",
            "Xiaofeng Qu",
            "David Camacho"
        ],
        "title": "CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification",
        "abstract": "arXiv:2511.10309v1 Announce Type: new  Abstract: This paper proposes a novel CLIP-driven modality-shared representation learning network named CLIP4VI-ReID for VI-ReID task, which consists of Text Semantic Generation (TSG), Infrared Feature Embedding (IFE), and High-level Semantic Alignment (HSA). Specifically, considering the huge gap in the physical characteristics between natural images and infrared images, the TSG is designed to generate text semantics only for visible images, thereby enabling preliminary visible-text modality alignment. Then, the IFE is proposed to rectify the feature embeddings of infrared images using the generated text semantics. This process injects id-related semantics into the shared image encoder, enhancing its adaptability to the infrared modality. Besides, with text serving as a bridge, it enables indirect visible-infrared modality alignment. Finally, the HSA is established to refine the high-level semantic alignment. This process ensures that the fine-tuned text semantics only contain id-related information, thereby achieving more accurate cross-modal alignment and enhancing the discriminability of the learned modal-shared representations. Extensive experimental results demonstrate that the proposed CLIP4VI-ReID achieves superior performance than other state-of-the-art methods on some widely used VI-ReID datasets.",
        "arxiv_id": "2511.10309",
        "ARXIVID": "2511.10309",
        "COMMENT": "Matches criterion 5 closely as it integrates image understanding tasks with text-based representations using CLIP for cross-modal alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.09973": {
        "authors": [
            "Satoshi Suzuki",
            "Shin'ya Yamaguchi",
            "Shoichiro Takeda",
            "Taiga Yamane",
            "Naoki Makishima",
            "Naotaka Kawata",
            "Mana Ihori",
            "Tomohiro Tanaka",
            "Shota Orihashi",
            "Ryo Masumura"
        ],
        "title": "Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models",
        "abstract": "arXiv:2511.09973v1 Announce Type: new  Abstract: Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.",
        "arxiv_id": "2511.09973",
        "ARXIVID": "2511.09973",
        "COMMENT": "Matches criterion 5 as it focuses on fine-tuning vision-language models while preserving their generalization capabilities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.10003": {
        "authors": [
            "Xuexun Liu",
            "Xiaoxu Xu",
            "Qiudan Zhang",
            "Lin Ma",
            "Xu Wang"
        ],
        "title": "DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation",
        "abstract": "arXiv:2511.10003v1 Announce Type: new  Abstract: Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \\textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.",
        "arxiv_id": "2511.10003",
        "ARXIVID": "2511.10003",
        "COMMENT": "Matches criterion 3 as it introduces a new method for weakly supervised 3D instance segmentation, relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.10390": {
        "authors": [
            "Jiarui Zhang",
            "Yuliang Liu",
            "Zijun Wu",
            "Guosheng Pang",
            "Zhili Ye",
            "Yupei Zhong",
            "Junteng Ma",
            "Tao Wei",
            "Haiyang Xu",
            "Weikai Chen",
            "Zeen Wang",
            "Qiangjun Ji",
            "Fanxi Zhou",
            "Qi Zhang",
            "Yuanrui Hu",
            "Jiahao Liu",
            "Zhang Li",
            "Ziyang Zhang",
            "Qiang Liu",
            "Xiang Bai"
        ],
        "title": "MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns",
        "abstract": "arXiv:2511.10390v1 Announce Type: new  Abstract: Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.",
        "arxiv_id": "2511.10390",
        "ARXIVID": "2511.10390",
        "COMMENT": "Matches criterion 5 as it focuses on integrating vision and language for document parsing tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.10037": {
        "authors": [
            "Xiaolong Wei",
            "Yuehu Dong",
            "Xingliang Wang",
            "Xingyu Zhang",
            "Zhejun Zhao",
            "Dongdong Shen",
            "Long Xia",
            "Dawei Yin"
        ],
        "title": "Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning",
        "abstract": "arXiv:2511.10037v1 Announce Type: new  Abstract: Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Planner's tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.",
        "arxiv_id": "2511.10037",
        "ARXIVID": "2511.10037",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for tool-augmented reasoning in large language models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.10212": {
        "authors": [
            "Ashutosh Anshul",
            "Shreyas Gopal",
            "Deepu Rajan",
            "Eng Siong Chng"
        ],
        "title": "Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization",
        "abstract": "arXiv:2511.10212v1 Announce Type: new  Abstract: Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.",
        "arxiv_id": "2511.10212",
        "ARXIVID": "2511.10212",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks like temporal localization and classification in multimodal deepfake detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.10177": {
        "authors": [
            "Tishya Chhabra",
            "Manisha Bajpai",
            "Walter Zesk",
            "Skylar Tibbits"
        ],
        "title": "Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands",
        "abstract": "arXiv:2511.10177v1 Announce Type: new  Abstract: We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.",
        "arxiv_id": "2511.10177",
        "ARXIVID": "2511.10177",
        "COMMENT": "Matches criterion 4 as it focuses on a geospatial foundation model and its application in coastal monitoring.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.10211": {
        "authors": [
            "Yueran Zhao",
            "Zhang Zhang",
            "Chao Sun",
            "Tianze Wang",
            "Chao Yue",
            "Nuoran Li"
        ],
        "title": "HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction",
        "abstract": "arXiv:2511.10211v1 Announce Type: new  Abstract: Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.",
        "arxiv_id": "2511.10211",
        "ARXIVID": "2511.10211",
        "COMMENT": "Does not match any specific criteria but is related to collaborative perception in autonomous systems, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.10555": {
        "authors": [
            "Huijie Liu",
            "Shuhao Cui",
            "Haoxiang Cao",
            "Shuai Ma",
            "Kai Wu",
            "Guoliang Kang"
        ],
        "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
        "abstract": "arXiv:2511.10555v1 Announce Type: new  Abstract: Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
        "arxiv_id": "2511.10555",
        "ARXIVID": "2511.10555",
        "COMMENT": "Does not match any specific criterion closely. Focuses on style-based image generation, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.09818": {
        "authors": [
            "Hanzhou Liu",
            "Peng Jiang",
            "Jia Huang",
            "Mi Lu"
        ],
        "title": "Lumos3D: A Single-Forward Framework for Low-Light 3D Scene Restoration",
        "abstract": "arXiv:2511.09818v1 Announce Type: new  Abstract: Restoring 3D scenes captured under low-light con- ditions remains a fundamental yet challenging problem. Most existing approaches depend on precomputed camera poses and scene-specific optimization, which greatly restricts their scala- bility to dynamic real-world environments. To overcome these limitations, we introduce Lumos3D, a generalizable pose-free framework for 3D low-light scene restoration. Trained once on a single dataset, Lumos3D performs inference in a purely feed- forward manner, directly restoring illumination and structure from unposed, low-light multi-view images without any per- scene training or optimization. Built upon a geometry-grounded backbone, Lumos3D reconstructs a normal-light 3D Gaussian representation that restores illumination while faithfully pre- serving structural details. During training, a cross-illumination distillation scheme is employed, where the teacher network is distilled on normal-light ground truth to transfer accurate geometric information, such as depth, to the student model. A dedicated Lumos loss is further introduced to promote photomet- ric consistency within the reconstructed 3D space. Experiments on real-world datasets demonstrate that Lumos3D achieves high- fidelity low-light 3D scene restoration with accurate geometry and strong generalization to unseen cases. Furthermore, the framework naturally extends to handle over-exposure correction, highlighting its versatility for diverse lighting restoration tasks.",
        "arxiv_id": "2511.09818",
        "ARXIVID": "2511.09818",
        "COMMENT": "Does not match any specific criterion closely. Focuses on low-light 3D scene restoration, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.10128": {
        "authors": [
            "Qinfeng Li",
            "Miao Pan",
            "Ke Xiong",
            "Ge Su",
            "Zhiqiang Shen",
            "Yan Liu",
            "Bing Sun",
            "Hao Peng",
            "Xuhong Zhang"
        ],
        "title": "RAGFort: Dual-Path Defense Against Proprietary Knowledge Base Extraction in Retrieval-Augmented Generation",
        "abstract": "arXiv:2511.10128v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) systems deployed over proprietary knowledge bases face growing threats from reconstruction attacks that aggregate model responses to replicate knowledge bases. Such attacks exploit both intra-class and inter-class paths, progressively extracting fine-grained knowledge within topics and diffusing it across semantically related ones, thereby enabling comprehensive extraction of the original knowledge base. However, existing defenses target only one path, leaving the other unprotected. We conduct a systematic exploration to assess the impact of protecting each path independently and find that joint protection is essential for effective defense. Based on this, we propose RAGFort, a structure-aware dual-module defense combining \"contrastive reindexing\" for inter-class isolation and \"constrained cascade generation\" for intra-class protection. Experiments across security, performance, and robustness confirm that RAGFort significantly reduces reconstruction success while preserving answer quality, offering comprehensive defense against knowledge base extraction attacks.",
        "arxiv_id": "2511.10128",
        "ARXIVID": "2511.10128",
        "COMMENT": "Does not match any specific criteria but is related to retrieval-augmented generation and security, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.09791": {
        "authors": [
            "Siddeshwar Raghavan",
            "Jiangpeng He",
            "Fengqing Zhu"
        ],
        "title": "PANDA - Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning",
        "abstract": "arXiv:2511.09791v1 Announce Type: new  Abstract: Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.",
        "arxiv_id": "2511.09791",
        "ARXIVID": "2511.09791",
        "COMMENT": "Does not match any specific criteria but is related to continual learning and pre-trained models, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.10150": {
        "authors": [
            "Feng Ding",
            "Wenhui Yi",
            "Yunpeng Zhou",
            "Xinan He",
            "Hong Rao",
            "Shu Hu"
        ],
        "title": "Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection",
        "abstract": "arXiv:2511.10150v1 Announce Type: new  Abstract: Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.",
        "arxiv_id": "2511.10150",
        "ARXIVID": "2511.10150",
        "COMMENT": "Does not match any specific criterion closely. Focuses on fairness optimization in deepfake detection, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.10300": {
        "authors": [
            "Sumin Lee",
            "Sungwon Park",
            "Jeasurk Yang",
            "Jihee Kim",
            "Meeyoung Cha"
        ],
        "title": "Generalizable Slum Detection from Satellite Imagery with Mixture-of-Experts",
        "abstract": "arXiv:2511.10300v1 Announce Type: new  Abstract: Satellite-based slum segmentation holds significant promise in generating global estimates of urban poverty. However, the morphological heterogeneity of informal settlements presents a major challenge, hindering the ability of models trained on specific regions to generalize effectively to unseen locations. To address this, we introduce a large-scale high-resolution dataset and propose GRAM (Generalized Region-Aware Mixture-of-Experts), a two-phase test-time adaptation framework that enables robust slum segmentation without requiring labeled data from target regions. We compile a million-scale satellite imagery dataset from 12 cities across four continents for source training. Using this dataset, the model employs a Mixture-of-Experts architecture to capture region-specific slum characteristics while learning universal features through a shared backbone. During adaptation, prediction consistency across experts filters out unreliable pseudo-labels, allowing the model to generalize effectively to previously unseen regions. GRAM outperforms state-of-the-art baselines in low-resource settings such as African cities, offering a scalable and label-efficient solution for global slum mapping and data-driven urban planning.",
        "arxiv_id": "2511.10300",
        "ARXIVID": "2511.10300",
        "COMMENT": "Does not match any specific criterion closely. Focuses on slum detection from satellite imagery, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.10166": {
        "authors": [
            "Hu Gao",
            "Xiaoning Lei",
            "Xichen Xu",
            "Depeng Dang",
            "Lizhuang Ma"
        ],
        "title": "Physically Interpretable Multi-Degradation Image Restoration via Deep Unfolding and Explainable Convolution",
        "abstract": "arXiv:2511.10166v1 Announce Type: new  Abstract: Although image restoration has advanced significantly, most existing methods target only a single type of degradation. In real-world scenarios, images often contain multiple degradations simultaneously, such as rain, noise, and haze, requiring models capable of handling diverse degradation types. Moreover, methods that improve performance through module stacking often suffer from limited interpretability. In this paper, we propose a novel interpretability-driven approach for multi-degradation image restoration, built upon a deep unfolding network that maps the iterative process of a mathematical optimization algorithm into a learnable network structure. Specifically, we employ an improved second-order semi-smooth Newton algorithm to ensure that each module maintains clear physical interpretability. To further enhance interpretability and adaptability, we design an explainable convolution module inspired by the human brain's flexible information processing and the intrinsic characteristics of images, allowing the network to flexibly leverage learned knowledge and autonomously adjust parameters for different input. The resulting tightly integrated architecture, named InterIR, demonstrates excellent performance in multi-degradation restoration while remaining highly competitive on single-degradation tasks.",
        "arxiv_id": "2511.10166",
        "ARXIVID": "2511.10166",
        "COMMENT": "Does not match any specific criterion closely. Focuses on image restoration with interpretability, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.09909": {
        "authors": [
            "Zihao Zhang",
            "Yang Li",
            "Aming Wu",
            "Yahong Han"
        ],
        "title": "Simulating Distribution Dynamics: Liquid Temporal Feature Evolution for Single-Domain Generalized Object Detection",
        "abstract": "arXiv:2511.09909v1 Announce Type: new  Abstract: In this paper, we focus on Single-Domain Generalized Object Detection (Single-DGOD), aiming to transfer a detector trained on one source domain to multiple unknown domains. Existing methods for Single-DGOD typically rely on discrete data augmentation or static perturbation methods to expand data diversity, thereby mitigating the lack of access to target domain data. However, in real-world scenarios such as changes in weather or lighting conditions, domain shifts often occur continuously and gradually. Discrete augmentations and static perturbations fail to effectively capture the dynamic variation of feature distributions, thereby limiting the model's ability to perceive fine-grained cross-domain differences. To this end, we propose a new method, Liquid Temporal Feature Evolution, which simulates the progressive evolution of features from the source domain to simulated latent distributions by incorporating temporal modeling and liquid neural network-driven parameter adjustment. Specifically, we introduce controllable Gaussian noise injection and multi-scale Gaussian blurring to simulate initial feature perturbations, followed by temporal modeling and a liquid parameter adjustment mechanism to generate adaptive modulation parameters, enabling a smooth and continuous adaptation across domains. By capturing progressive cross-domain feature evolution and dynamically regulating adaptation paths, our method bridges the source-unknown domain distribution gap, significantly boosting generalization and robustness to unseen shifts. Significant performance improvements on the Diverse Weather dataset and Real-to-Art benchmark demonstrate the superiority of our method. Our code is available at https://github.com/2490o/LTFE.",
        "arxiv_id": "2511.09909",
        "ARXIVID": "2511.09909",
        "COMMENT": "Does not closely match any specific criterion but is relevant to general interest in computer vision and domain generalization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}