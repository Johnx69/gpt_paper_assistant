{
    "2509.08777": {
        "authors": [
            "Eric Slyman",
            "Mehrab Tanjim",
            "Kushal Kafle",
            "Stefan Lee"
        ],
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "abstract": "arXiv:2509.08777v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these \"judge\" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.",
        "arxiv_id": "2509.08777",
        "ARXIVID": "2509.08777",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores multimodal large language models (MLLMs) and introduces a novel Bayesian prompt ensemble method for calibration in text-to-image tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.08016": {
        "authors": [
            "Hyungjin Chung",
            "Hyelin Nam",
            "Jiyeon Kim",
            "Hyojun Go",
            "Byeongjun Park",
            "Junho Kim",
            "Joonseok Lee",
            "Seongsu Ha",
            "Byung-Hoon Kim"
        ],
        "title": "Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs",
        "abstract": "arXiv:2509.08016v1 Announce Type: new  Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck: increasing the number of input frames to capture fine-grained temporal detail leads to prohibitive computational costs and performance degradation from long context lengths. We introduce Video Parallel Scaling (VPS), an inference-time method that expands a model's perceptual bandwidth without increasing its context window. VPS operates by running multiple parallel inference streams, each processing a unique, disjoint subset of the video's frames. By aggregating the output probabilities from these complementary streams, VPS integrates a richer set of visual information than is possible with a single pass. We theoretically show that this approach effectively contracts the Chinchilla scaling law by leveraging uncorrelated visual evidence, thereby improving performance without additional training. Extensive experiments across various model architectures and scales (2B-32B) on benchmarks such as Video-MME and EventHallusion demonstrate that VPS consistently and significantly improves performance. It scales more favorably than other parallel alternatives (e.g. Self-consistency) and is complementary to other decoding strategies, offering a memory-efficient and robust framework for enhancing the temporal reasoning capabilities of VideoLLMs.",
        "arxiv_id": "2509.08016",
        "ARXIVID": "2509.08016",
        "COMMENT": "Matches criterion 2 (Video Large Language Models) due to its focus on improving temporal reasoning in VideoLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.08388": {
        "authors": [
            "Dubing Chen",
            "Huan Zheng",
            "Yucheng Zhou",
            "Xianfei Li",
            "Wenlong Liao",
            "Tao He",
            "Pai Peng",
            "Jianbing Shen"
        ],
        "title": "Semantic Causality-Aware Vision-Based 3D Occupancy Prediction",
        "abstract": "arXiv:2509.08388v1 Announce Type: new  Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision that integrates volumetric 3D reconstruction with semantic understanding. Existing methods, however, often rely on modular pipelines. These modules are typically optimized independently or use pre-configured inputs, leading to cascading errors. In this paper, we address this limitation by designing a novel causal loss that enables holistic, end-to-end supervision of the modular 2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D semantic causality, this loss regulates the gradient flow from 3D voxel representations back to the 2D features. Consequently, it renders the entire pipeline differentiable, unifying the learning process and making previously non-trainable components fully learnable. Building on this principle, we propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises three components guided by our causal loss: Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness against camera perturbations, and Normalized Convolution for effective feature propagation. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Occ3D benchmark, demonstrating significant robustness to camera perturbations and improved 2D-to-3D semantic consistency.",
        "arxiv_id": "2509.08388",
        "ARXIVID": "2509.08388",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on 3D semantic occupancy prediction with a novel causal loss.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08519": {
        "authors": [
            "Liyang Chen",
            "Tianxiang Ma",
            "Jiawei Liu",
            "Bingchuan Li",
            "Zhuowei Chen",
            "Lijie Liu",
            "Xu He",
            "Gen Li",
            "Qian He",
            "Zhiyong Wu"
        ],
        "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning",
        "abstract": "arXiv:2509.08519v1 Announce Type: new  Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.",
        "arxiv_id": "2509.08519",
        "ARXIVID": "2509.08519",
        "COMMENT": "Matches criterion 6. Proposes HuMo, a framework for human-centric video generation with multimodal conditioning, relevant to video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08826": {
        "authors": [
            "Jie Wu",
            "Yu Gao",
            "Zilyu Ye",
            "Ming Li",
            "Liang Li",
            "Hanzhong Guo",
            "Jie Liu",
            "Zeyue Xue",
            "Xiaoxia Hou",
            "Wei Liu",
            "Yan Zeng",
            "Weilin Huang"
        ],
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "abstract": "arXiv:2509.08826v1 Announce Type: new  Abstract: Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.",
        "arxiv_id": "2509.08826",
        "ARXIVID": "2509.08826",
        "COMMENT": "Matches criterion 2. Proposes RewardDance, a novel reward modeling framework for visual generation, addressing challenges in vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.07996": {
        "authors": [
            "Lingdong Kong",
            "Wesley Yang",
            "Jianbiao Mei",
            "Youquan Liu",
            "Ao Liang",
            "Dekai Zhu",
            "Dongyue Lu",
            "Wei Yin",
            "Xiaotao Hu",
            "Mingkai Jia",
            "Junyuan Deng",
            "Kaiwen Zhang",
            "Yang Wu",
            "Tianyi Yan",
            "Shenyuan Gao",
            "Song Wang",
            "Linfeng Li",
            "Liang Pan",
            "Yong Liu",
            "Jianke Zhu",
            "Wei Tsang Ooi",
            "Steven C. H. Hoi",
            "Ziwei Liu"
        ],
        "title": "3D and 4D World Modeling: A Survey",
        "abstract": "arXiv:2509.07996v1 Announce Type: new  Abstract: World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey",
        "arxiv_id": "2509.07996",
        "ARXIVID": "2509.07996",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on 3D and 4D world modeling, providing a structured taxonomy and summarizing datasets, evaluation metrics, and challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.08266": {
        "authors": [
            "Saurav Sengupta",
            "Nazanin Moradinasab",
            "Jiebei Liu",
            "Donald E. Brown"
        ],
        "title": "Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features",
        "abstract": "arXiv:2509.08266v1 Announce Type: new  Abstract: Recent research on Vision Language Models (VLMs) suggests that they rely on inherent biases learned during training to respond to questions about visual properties of an image. These biases are exacerbated when VLMs are asked highly specific questions that require focusing on specific areas of the image. For example, a VLM tasked with counting stars on a modified American flag (e.g., with more than 50 stars) will often disregard the visual evidence and fail to answer accurately. We build upon this research and develop a multi-dimensional examination framework to systematically determine which characteristics of the input data, including both the image and the accompanying prompt, lead to such differences in performance. Using open-source VLMs, we further examine how attention values fluctuate with varying input parameters (e.g., image size, number of objects in the image, background color, prompt specificity). This research aims to learn how the behavior of vision language models changes and to explore methods for characterizing such changes. Our results suggest, among other things, that even minor modifications in image characteristics and prompt specificity can lead to large changes in how a VLM formulates its answer and, subsequently, its overall performance.",
        "arxiv_id": "2509.08266",
        "ARXIVID": "2509.08266",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it examines Vision Language Models and their behavior under various conditions.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.08489": {
        "authors": [
            "Kaleem Ahmad"
        ],
        "title": "Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation",
        "abstract": "arXiv:2509.08489v1 Announce Type: new  Abstract: Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.",
        "arxiv_id": "2509.08489",
        "ARXIVID": "2509.08489",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it discusses a unified pipeline combining image analysis and multimodal generative AI.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.08538": {
        "authors": [
            "Garry Yang",
            "Zizhe Chen",
            "Man Hon Wong",
            "Haoyu Lei",
            "Yongqiang Chen",
            "Zhenguo Li",
            "Kaiwen Zhou",
            "James Cheng"
        ],
        "title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models",
        "abstract": "arXiv:2509.08538v1 Announce Type: new  Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large Language Models (LLMs) and vision modules by integrating temporal information to better understand dynamic video content. Despite their progress, LVMs are prone to hallucinations-producing inaccurate or irrelevant descriptions. Current benchmarks for video hallucination depend heavily on manual categorization of video content, neglecting the perception-based processes through which humans naturally interpret videos. We introduce MESH, a benchmark designed to evaluate hallucinations in LVMs systematically. MESH uses a Question-Answering framework with binary and multi-choice formats incorporating target and trap instances. It follows a bottom-up approach, evaluating basic objects, coarse-to-fine subject features, and subject-action pairs, aligning with human video understanding. We demonstrate that MESH offers an effective and comprehensive approach for identifying hallucinations in videos. Our evaluations show that while LVMs excel at recognizing basic objects and features, their susceptibility to hallucinations increases markedly when handling fine details or aligning multiple actions involving various subjects in longer videos.",
        "arxiv_id": "2509.08538",
        "ARXIVID": "2509.08538",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for evaluating hallucinations in Large Video Models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.08260": {
        "authors": [
            "Chi Zhang",
            "Xiang Zhang",
            "Chenxu Jiang",
            "Gui-Song Xia",
            "Lei Yu"
        ],
        "title": "EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning",
        "abstract": "arXiv:2509.08260v1 Announce Type: new  Abstract: Frame-based cameras with extended exposure times often produce perceptible visual blurring and information loss between frames, significantly degrading video quality. To address this challenge, we introduce EVDI++, a unified self-supervised framework for Event-based Video Deblurring and Interpolation that leverages the high temporal resolution of event cameras to mitigate motion blur and enable intermediate frame prediction. Specifically, the Learnable Double Integral (LDI) network is designed to estimate the mapping relation between reference frames and sharp latent images. Then, we refine the coarse results and optimize overall training efficiency by introducing a learning-based division reconstruction module, enabling images to be converted with varying exposure intervals. We devise an adaptive parameter-free fusion strategy to obtain the final results, utilizing the confidence embedded in the LDI outputs of concurrent events. A self-supervised learning framework is proposed to enable network training with real-world blurry videos and events by exploring the mutual constraints among blurry frames, latent images, and event streams. We further construct a dataset with real-world blurry images and events using a DAVIS346c camera, demonstrating the generalizability of the proposed EVDI++ in real-world scenarios. Extensive experiments on both synthetic and real-world datasets show that our method achieves state-of-the-art performance in video deblurring and interpolation tasks.",
        "arxiv_id": "2509.08260",
        "ARXIVID": "2509.08260",
        "COMMENT": "Matches criterion 6. Proposes EVDI++, a framework for event-based video deblurring and interpolation, advancing video understanding tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.08570": {
        "authors": [
            "Wenjun Yu",
            "Yinchen Zhou",
            "Jia-Xuan Jiang",
            "Shubin Zeng",
            "Yuee Li",
            "Zhong Wang"
        ],
        "title": "Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation",
        "abstract": "arXiv:2509.08570v1 Announce Type: new  Abstract: Multimodal models have achieved remarkable success in natural image segmentation, yet they often underperform when applied to the medical domain. Through extensive study, we attribute this performance gap to the challenges of multimodal fusion, primarily the significant semantic gap between abstract textual prompts and fine-grained medical visual features, as well as the resulting feature dispersion. To address these issues, we revisit the problem from the perspective of semantic aggregation. Specifically, we propose an Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel Decoder. The former mitigates feature dispersion by dynamically clustering features into compact semantic centers to enhance cross-modal correspondence. The latter is designed to bridge the semantic gap by leveraging domain-invariant textual knowledge to effectively guide deep visual representations. The synergy between these two mechanisms significantly improves the model's generalization ability. Extensive experiments on public cardiac and fundus datasets demonstrate that our method consistently outperforms existing SOTA approaches across multiple domain generalization benchmarks.",
        "arxiv_id": "2509.08570",
        "ARXIVID": "2509.08570",
        "COMMENT": "Matches criterion 5. Focuses on vision-language semantic aggregation for medical image segmentation, addressing multimodal challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.08618": {
        "authors": [
            "Zhihao Zhao",
            "Yinzheng Zhao",
            "Junjie Yang",
            "Xiangtong Yao",
            "Quanmin Liang",
            "Shahrooz Faghihroohi",
            "Kai Huang",
            "Nassir Navab",
            "M. Ali Nasseri"
        ],
        "title": "CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging",
        "abstract": "arXiv:2509.08618v1 Announce Type: new  Abstract: Recent advancements in foundation models, such as the Segment Anything Model (SAM), have significantly impacted medical image segmentation, especially in retinal imaging, where precise segmentation is vital for diagnosis. Despite this progress, current methods face critical challenges: 1) modality ambiguity in textual disease descriptions, 2) a continued reliance on manual prompting for SAM-based workflows, and 3) a lack of a unified framework, with most methods being modality- and task-specific. To overcome these hurdles, we propose CLIP-unified Auto-Prompt Segmentation (\\CLAPS), a novel method for unified segmentation across diverse tasks and modalities in retinal imaging. Our approach begins by pre-training a CLIP-based image encoder on a large, multi-modal retinal dataset to handle data scarcity and distribution imbalance. We then leverage GroundingDINO to automatically generate spatial bounding box prompts by detecting local lesions. To unify tasks and resolve ambiguity, we use text prompts enhanced with a unique \"modality signature\" for each imaging modality. Ultimately, these automated textual and spatial prompts guide SAM to execute precise segmentation, creating a fully automated and unified pipeline. Extensive experiments on 12 diverse datasets across 11 critical segmentation categories show that CLAPS achieves performance on par with specialized expert models while surpassing existing benchmarks across most metrics, demonstrating its broad generalizability as a foundation model.",
        "arxiv_id": "2509.08618",
        "ARXIVID": "2509.08618",
        "COMMENT": "Matches criterion 5. Proposes a unified segmentation framework (CLAPS) combining image and text prompts for retinal imaging, integrating vision and language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.08764": {
        "authors": [
            "Lena Wild",
            "Rafael Valencia",
            "Patric Jensfelt"
        ],
        "title": "ArgoTweak: Towards Self-Updating HD Maps through Structured Priors",
        "abstract": "arXiv:2509.08764v1 Announce Type: new  Abstract: Reliable integration of prior information is crucial for self-verifying and self-updating HD maps. However, no public dataset includes the required triplet of prior maps, current maps, and sensor data. As a result, existing methods must rely on synthetic priors, which create inconsistencies and lead to a significant sim2real gap. To address this, we introduce ArgoTweak, the first dataset to complete the triplet with realistic map priors. At its core, ArgoTweak employs a bijective mapping framework, breaking down large-scale modifications into fine-grained atomic changes at the map element level, thus ensuring interpretability. This paradigm shift enables accurate change detection and integration while preserving unchanged elements with high fidelity. Experiments show that training models on ArgoTweak significantly reduces the sim2real gap compared to synthetic priors. Extensive ablations further highlight the impact of structured priors and detailed change annotations. By establishing a benchmark for explainable, prior-aided HD mapping, ArgoTweak advances scalable, self-improving mapping solutions. The dataset, baselines, map modification toolbox, and further resources are available at https://kth-rpl.github.io/ArgoTweak/.",
        "arxiv_id": "2509.08764",
        "ARXIVID": "2509.08764",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark dataset (ArgoTweak) for self-updating HD maps, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.08376": {
        "authors": [
            "Xiao Li",
            "Qi Chen",
            "Xiulian Peng",
            "Kai Yu",
            "Xie Chen",
            "Yan Lu"
        ],
        "title": "Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video",
        "abstract": "arXiv:2509.08376v1 Announce Type: new  Abstract: We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real-world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other types of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation.",
        "arxiv_id": "2509.08376",
        "ARXIVID": "2509.08376",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding by disentangling motion and content in video data.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.08500": {
        "authors": [
            "Kechen Jiao",
            "Zhirui Fang",
            "Jiahao Liu",
            "Bei Li",
            "Qifan Wang",
            "Xinyu Liu",
            "Junhao Ruan",
            "Zhongjian Qiao",
            "Yifan Zhu",
            "Yaxin Xu",
            "Jingang Wang",
            "Xiu Li"
        ],
        "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making",
        "abstract": "arXiv:2509.08500v1 Announce Type: new  Abstract: Using effective generalization capabilities of vision language models (VLMs) in context-specific dynamic tasks for embodied artificial intelligence remains a significant challenge. Although supervised fine-tuned models can better align with the real physical world, they still exhibit sluggish responses and hallucination issues in dynamically changing environments, necessitating further alignment. Existing post-SFT methods, reliant on reinforcement learning and chain-of-thought (CoT) approaches, are constrained by sparse rewards and action-only optimization, resulting in low sample efficiency, poor consistency, and model degradation. To address these issues, this paper proposes Thought-Centric Preference Optimization (TCPO) for effective embodied decision-making. Specifically, TCPO introduces a stepwise preference-based optimization approach, transforming sparse reward signals into richer step sample pairs. It emphasizes the alignment of the model's intermediate reasoning process, mitigating the problem of model degradation. Moreover, by incorporating Action Policy Consistency Constraint (APC), it further imposes consistency constraints on the model output. Experiments in the ALFWorld environment demonstrate an average success rate of 26.67%, achieving a 6% improvement over RL4VLM and validating the effectiveness of our approach in mitigating model degradation after fine-tuning. These results highlight the potential of integrating preference-based learning techniques with CoT processes to enhance the decision-making capabilities of vision-language models in embodied agents.",
        "arxiv_id": "2509.08500",
        "ARXIVID": "2509.08500",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for embodied decision-making in dynamic environments.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08621": {
        "authors": [
            "Xinwei Long",
            "Kai Tian",
            "Peng Xu",
            "Guoli Jia",
            "Jingxuan Li",
            "Sa Yang",
            "Yihua Shao",
            "Kaiyan Zhang",
            "Che Jiang",
            "Hao Xu",
            "Yang Liu",
            "Jiaheng Ma",
            "Bowen Zhou"
        ],
        "title": "AdsQA: Towards Advertisement Video Understanding",
        "abstract": "arXiv:2509.08621v1 Announce Type: new  Abstract: Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domain-specific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videos' traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold: (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 top-tier LLMs on AdsQA, and our \\texttt{ReAd-R}~achieves the state-of-the-art outperforming strong competitors equipped with long-chain reasoning capabilities by a clear margin.",
        "arxiv_id": "2509.08621",
        "ARXIVID": "2509.08621",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark and tasks for video understanding in advertisement videos.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08421": {
        "authors": [
            "Keisuke Toida",
            "Taigo Sakai",
            "Naoki Kato",
            "Kazutoyo Yokota",
            "Takeshi Nakamura",
            "Kazuhiro Hotta"
        ],
        "title": "Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking",
        "abstract": "arXiv:2509.08421v1 Announce Type: new  Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.",
        "arxiv_id": "2509.08421",
        "ARXIVID": "2509.08421",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for multi-view detection and tracking, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08374": {
        "authors": [
            "Zhongyu Xia",
            "Hansong Yang",
            "Yongtao Wang"
        ],
        "title": "InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection",
        "abstract": "arXiv:2509.08374v1 Announce Type: new  Abstract: Three-dimensional Object Detection from multi-view cameras and LiDAR is a crucial component for autonomous driving and smart transportation. However, in the process of basic feature extraction, perspective transformation, and feature fusion, noise and error will gradually accumulate. To address this issue, we propose InsFusion, which can extract proposals from both raw and fused features and utilizes these proposals to query the raw features, thereby mitigating the impact of accumulated errors. Additionally, by incorporating attention mechanisms applied to the raw features, it thereby mitigates the impact of accumulated errors. Experiments on the nuScenes dataset demonstrate that InsFusion is compatible with various advanced baseline methods and delivers new state-of-the-art performance for 3D object detection.",
        "arxiv_id": "2509.08374",
        "ARXIVID": "2509.08374",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for LiDAR-camera fusion in 3D object detection, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08436": {
        "authors": [
            "Xia Yue",
            "Anfeng Liu",
            "Ning Chen",
            "Chenjia Huang",
            "Hui Liu",
            "Zhou Huang",
            "Leyuan Fang"
        ],
        "title": "Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time",
        "abstract": "arXiv:2509.08436v1 Announce Type: new  Abstract: Hyperspectral image (HSI) classification models are highly sensitive to distribution shifts caused by various real-world degradations such as noise, blur, compression, and atmospheric effects. To address this challenge, we propose HyperTTA, a unified framework designed to enhance model robustness under diverse degradation conditions. Specifically, we first construct a multi-degradation hyperspectral dataset that systematically simulates nine representative types of degradations, providing a comprehensive benchmark for robust classification evaluation. Based on this, we design a spectral-spatial transformer classifier (SSTC) enhanced with a multi-level receptive field mechanism and label smoothing regularization to jointly capture multi-scale spatial context and improve generalization. Furthermore, HyperTTA incorporates a lightweight test-time adaptation (TTA) strategy, the confidence-aware entropy-minimized LayerNorm adapter (CELA), which updates only the affine parameters of LayerNorm layers by minimizing prediction entropy on high-confidence unlabeled target samples. This confidence-aware adaptation prevents unreliable updates from noisy predictions, enabling robust and dynamic adaptation without access to source data or target annotations. Extensive experiments on two benchmark datasets demonstrate that HyperTTA outperforms existing baselines across a wide range of degradation scenarios, validating the effectiveness of both its classification backbone and the proposed TTA scheme. Code will be made available publicly.",
        "arxiv_id": "2509.08436",
        "ARXIVID": "2509.08436",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and method for hyperspectral image classification, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.08805": {
        "authors": [
            "Matthieu Vilain",
            "R\\'emi Giraud",
            "Yannick Berthoumieu",
            "Guillaume Bourmaud"
        ],
        "title": "Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching",
        "abstract": "arXiv:2509.08805v1 Announce Type: new  Abstract: Dense image matching aims to find a correspondent for every pixel of a source image in a partially overlapping target image. State-of-the-art methods typically rely on a coarse-to-fine mechanism where a single correspondent hypothesis is produced per source location at each scale. In challenging cases -- such as at depth discontinuities or when the target image is a strong zoom-in of the source image -- the correspondents of neighboring source locations are often widely spread and predicting a single correspondent hypothesis per source location at each scale may lead to erroneous matches. In this paper, we investigate the idea of predicting multiple correspondent hypotheses per source location at each scale instead. We consider a beam search strategy to propagat multiple hypotheses at each scale and propose integrating these multiple hypotheses into cross-attention layers, resulting in a novel dense matching architecture called BEAMER. BEAMER learns to preserve and propagate multiple hypotheses across scales, making it significantly more robust than state-of-the-art methods, especially at depth discontinuities or when the target image is a strong zoom-in of the source image.",
        "arxiv_id": "2509.08805",
        "ARXIVID": "2509.08805",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning due to its novel dense image matching architecture (BEAMER).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.08628": {
        "authors": [
            "Xuqin Wang",
            "Tao Wu",
            "Yanfeng Zhang",
            "Lu Liu",
            "Dong Wang",
            "Mingwei Sun",
            "Yongliang Wang",
            "Niclas Zeller",
            "Daniel Cremers"
        ],
        "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation",
        "abstract": "arXiv:2509.08628v1 Announce Type: new  Abstract: Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.",
        "arxiv_id": "2509.08628",
        "ARXIVID": "2509.08628",
        "COMMENT": "Does not closely match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.08661": {
        "authors": [
            "Liangjin Liu",
            "Haoyang Zheng",
            "Pei Zhou"
        ],
        "title": "Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network",
        "abstract": "arXiv:2509.08661v1 Announce Type: new  Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.",
        "arxiv_id": "2509.08661",
        "ARXIVID": "2509.08661",
        "COMMENT": "Does not closely match any specific criterion but is related to gesture recognition, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08289": {
        "authors": [
            "Yuelin Guo",
            "Haoyu He",
            "Zhiyuan Chen",
            "Zitong Huang",
            "Renhao Lu",
            "Lu Shi",
            "Zejun Wang",
            "Weizhe Zhang"
        ],
        "title": "Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection",
        "abstract": "arXiv:2509.08289v1 Announce Type: new  Abstract: Weakly supervised object detection (WSOD) has attracted significant attention in recent years, as it does not require box-level annotations. State-of-the-art methods generally adopt a multi-module network, which employs WSDDN as the multiple instance detection network module and multiple instance refinement modules to refine performance. However, these approaches suffer from three key limitations. First, existing methods tend to generate pseudo GT boxes that either focus only on discriminative parts, failing to capture the whole object, or cover the entire object but fail to distinguish between adjacent intra-class instances. Second, the foundational WSDDN architecture lacks a crucial background class representation for each proposal and exhibits a large semantic gap between its branches. Third, prior methods discard ignored proposals during optimization, leading to slow convergence. To address these challenges, we first design a heatmap-guided proposal selector (HGPS) algorithm, which utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo GT boxes to both capture the full object extent and distinguish between adjacent intra-class instances. We then present a weakly supervised basic detection network (WSBDN), which augments each proposal with a background class representation and uses heatmaps for pre-supervision to bridge the semantic gap between matrices. At last, we introduce a negative certainty supervision loss on ignored proposals to accelerate convergence. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD methods. Our code is publicly available at https://github.com/gyl2565309278/DTH-CP.",
        "arxiv_id": "2509.08289",
        "ARXIVID": "2509.08289",
        "COMMENT": "Does not closely match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08311": {
        "authors": [
            "Rongsheng Wang",
            "Fenghe Tang",
            "Qingsong Yao",
            "Rui Yan",
            "Xu Zhang",
            "Zhen Huang",
            "Haoran Lai",
            "Zhiyang He",
            "Xiaodong Tao",
            "Zihang Jiang",
            "Shaohua Kevin Zhou"
        ],
        "title": "SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training",
        "abstract": "arXiv:2509.08311v1 Announce Type: new  Abstract: Medical vision-language pre-training shows great potential in learning representative features from massive paired radiographs and reports. However, in computed tomography (CT) scans, the distribution of lesions which contain intricate structures is characterized by spatial sparsity. Besides, the complex and implicit relationships between different pathological descriptions in each sentence of the report and their corresponding sub-regions in radiographs pose additional challenges. In this paper, we propose a Similarity-Driven Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines similarity-driven alignment and cross-granularity fusion to improve radiograph interpretation. We first leverage multi-modal masked modeling to optimize the encoder for understanding precise low-level semantics from radiographs. Then, similarity-driven alignment is designed to pre-train the encoder to adaptively select and align the correct patches corresponding to each sentence in reports. The cross-granularity fusion module integrates multimodal information across instance level and word-patch level, which helps the model better capture key pathology structures in sparse radiographs, resulting in improved performance for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale paired CT-reports dataset and validated on image classification and segmentation tasks across five public datasets. Experimental results demonstrate that SimCroP outperforms both cutting-edge medical self-supervised learning methods and medical vision-language pre-training methods. Codes and models are available at https://github.com/ToniChopp/SimCroP.",
        "arxiv_id": "2509.08311",
        "ARXIVID": "2509.08311",
        "COMMENT": "Does not closely match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08705": {
        "authors": [
            "Shalima Binta Manir",
            "Tim Oates"
        ],
        "title": "One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases",
        "abstract": "arXiv:2509.08705v1 Announce Type: new  Abstract: We introduce a novel Theory of Mind (ToM) framework inspired by dual-process theories from cognitive science, integrating a fast, habitual graph-based reasoning system (System 1), implemented via graph convolutional networks (GCNs), and a slower, context-sensitive meta-adaptive learning system (System 2), driven by meta-learning techniques. Our model dynamically balances intuitive and deliberative reasoning through a learned context gate mechanism. We validate our architecture on canonical false-belief tasks and systematically explore its capacity to replicate hallmark cognitive biases associated with dual-process theory, including anchoring, cognitive-load fatigue, framing effects, and priming effects. Experimental results demonstrate that our dual-process approach closely mirrors human adaptive behavior, achieves robust generalization to unseen contexts, and elucidates cognitive mechanisms underlying reasoning biases. This work bridges artificial intelligence and cognitive theory, paving the way for AI systems exhibiting nuanced, human-like social cognition and adaptive decision-making capabilities.",
        "arxiv_id": "2509.08705",
        "ARXIVID": "2509.08705",
        "COMMENT": "Does not closely match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08583": {
        "authors": [
            "Jinhan Li",
            "Haoyang He",
            "Lei Xie",
            "Jiangning Zhang"
        ],
        "title": "EfficientIML: Efficient High-Resolution Image Manipulation Localization",
        "abstract": "arXiv:2509.08583v1 Announce Type: new  Abstract: With imaging devices delivering ever-higher resolutions and the emerging diffusion-based forgery methods, current detectors trained only on traditional datasets (with splicing, copy-moving and object removal forgeries) lack exposure to this new manipulation type. To address this, we propose a novel high-resolution SIF dataset of 1200+ diffusion-generated manipulations with semantically extracted masks. However, this also imposes a challenge on existing methods, as they face significant computational resource constraints due to their prohibitive computational complexities. Therefore, we propose a novel EfficientIML model with a lightweight, three-stage EfficientRWKV backbone. EfficientRWKV's hybrid state-space and attention network captures global context and local details in parallel, while a multi-scale supervision strategy enforces consistency across hierarchical predictions. Extensive evaluations on our dataset and standard benchmarks demonstrate that our approach outperforms ViT-based and other SOTA lightweight baselines in localization performance, FLOPs and inference speed, underscoring its suitability for real-time forensic applications.",
        "arxiv_id": "2509.08583",
        "ARXIVID": "2509.08583",
        "COMMENT": "Does not match any specific criteria. Focuses on high-resolution image manipulation localization, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08785": {
        "authors": [
            "Anup Tuladhar",
            "Araz Minhas",
            "Adam Kirton",
            "Eli Kinney-Lang"
        ],
        "title": "Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making",
        "abstract": "arXiv:2509.08785v1 Announce Type: new  Abstract: We present a preliminary experimental platform that explores how narrative elements might shape AI decision-making by combining reinforcement learning (RL) with language model reasoning. While AI systems can now both make decisions and engage in narrative reasoning, these capabilities have mostly been studied separately. Our platform attempts to bridge this gap using a dual-system architecture to examine how narrative frameworks could influence reward-based learning. The system comprises a reinforcement learning policy that suggests actions based on past experience, and a language model that processes these suggestions through different narrative frameworks to guide decisions. This setup enables initial experimentation with narrative elements while maintaining consistent environment and reward structures. We implement this architecture in a configurable gridworld environment, where agents receive both policy suggestions and information about their surroundings. The platform's modular design facilitates controlled testing of environmental complexity, narrative parameters, and the interaction between reinforcement learning and narrative-based decisions. Our logging system captures basic decision metrics, from RL policy values to language model reasoning to action selection patterns. While preliminary, this implementation provides a foundation for studying how different narrative frameworks might affect reward-based decisions and exploring potential interactions between optimization-based learning and symbolic reasoning in AI systems.",
        "arxiv_id": "2509.08785",
        "ARXIVID": "2509.08785",
        "COMMENT": "Does not match any specific criteria. Focuses on narrative-guided reinforcement learning, which is tangential to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08243": {
        "authors": [
            "Zheng Yang",
            "Yanteng Zhang",
            "Xupeng Kou",
            "Yang Liu",
            "Chao Ren"
        ],
        "title": "Symmetry Interactive Transformer with CNN Framework for Diagnosis of Alzheimer's Disease Using Structural MRI",
        "abstract": "arXiv:2509.08243v1 Announce Type: new  Abstract: Structural magnetic resonance imaging (sMRI) combined with deep learning has achieved remarkable progress in the prediction and diagnosis of Alzheimer's disease (AD). Existing studies have used CNN and transformer to build a well-performing network, but most of them are based on pretraining or ignoring the asymmetrical character caused by brain disorders. We propose an end-to-end network for the detection of disease-based asymmetric induced by left and right brain atrophy which consist of 3D CNN Encoder and Symmetry Interactive Transformer (SIT). Following the inter-equal grid block fetch operation, the corresponding left and right hemisphere features are aligned and subsequently fed into the SIT for diagnostic analysis. SIT can help the model focus more on the regions of asymmetry caused by structural changes, thus improving diagnostic performance. We evaluated our method based on the ADNI dataset, and the results show that the method achieves better diagnostic accuracy (92.5\\%) compared to several CNN methods and CNNs combined with a general transformer. The visualization results show that our network pays more attention in regions of brain atrophy, especially for the asymmetric pathological characteristics induced by AD, demonstrating the interpretability and effectiveness of the method.",
        "arxiv_id": "2509.08243",
        "ARXIVID": "2509.08243",
        "COMMENT": "Does not closely match any specific criterion but is related to medical imaging and deep learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.08205": {
        "authors": [
            "Jingjing Liu",
            "Yinchao Han",
            "Xianchao Xiu",
            "Jianhua Zhang",
            "Wanquan Liu"
        ],
        "title": "Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection",
        "abstract": "arXiv:2509.08205v1 Announce Type: new  Abstract: Infrared small target detection (ISTD) is one of the key techniques in image processing. Although deep unfolding networks (DUNs) have demonstrated promising performance in ISTD due to their model interpretability and data adaptability, existing methods still face significant challenges in parameter lightweightness and noise robustness. In this regard, we propose a highly lightweight framework based on robust principal component analysis (RPCA) called L-RPCANet. Technically, a hierarchical bottleneck structure is constructed to reduce and increase the channel dimension in the single-channel input infrared image to achieve channel-wise feature refinement, with bottleneck layers designed in each module to extract features. This reduces the number of channels in feature extraction and improves the lightweightness of network parameters. Furthermore, a noise reduction module is embedded to enhance the robustness against complex noise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a channel attention mechanism to focus on the varying importance of different features across channels, thereby achieving excellent performance while maintaining both lightweightness and robustness. Extensive experiments on the ISTD datasets validate the superiority of our proposed method compared with state-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code will be available at https://github.com/xianchaoxiu/L-RPCANet.",
        "arxiv_id": "2509.08205",
        "ARXIVID": "2509.08205",
        "COMMENT": "Does not closely match any specific criterion but is related to lightweight networks for image processing.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.08694": {
        "authors": [
            "Zhen Tian",
            "Christos Anagnostopoulos",
            "Qiyuan Wang",
            "Zhiwei Gao"
        ],
        "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework",
        "abstract": "arXiv:2509.08694v1 Announce Type: new  Abstract: Coastal water segmentation from satellite imagery presents unique challenges due to complex spectral characteristics and irregular boundary patterns. Traditional RGB-based approaches often suffer from training instability and poor generalization in diverse maritime environments. This paper introduces a systematic robust enhancement framework, referred to as Robust U-Net, that leverages HSV color space supervision and multi-modal constraints for improved coastal water segmentation. Our approach integrates five synergistic components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control. Through comprehensive ablation studies, we demonstrate that HSV supervision provides the highest impact (0.85 influence score), while the complete framework achieves superior training stability (84\\% variance reduction) and enhanced segmentation quality. Our method shows consistent improvements across multiple evaluation metrics while maintaining computational efficiency. For reproducibility, our training configurations and code are available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.",
        "arxiv_id": "2509.08694",
        "ARXIVID": "2509.08694",
        "COMMENT": "Does not closely match any specific criterion but is related to segmentation tasks in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.08151": {
        "authors": [
            "Botao Zhu",
            "Jeslyn Wang",
            "Dusit Niyato",
            "Xianbin Wang"
        ],
        "title": "Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI",
        "abstract": "arXiv:2509.08151v1 Announce Type: new  Abstract: Accurate trustworthiness evaluation of potential collaborating devices is essential for the effective execution of complex computing tasks. This evaluation process involves collecting diverse trust-related data from potential collaborators, including historical performance and available resources, for collaborator selection. However, when each task owner independently assesses all collaborators' trustworthiness, frequent data exchange, complex reasoning, and dynamic situation changes can result in significant overhead and deteriorated trust evaluation. To overcome these challenges, we propose a task-specific trust semantics distillation (2TSD) model based on a large AI model (LAM)-driven teacher-student agent architecture. The teacher agent is deployed on a server with powerful computational capabilities and an augmented memory module dedicated to multidimensional trust-related data collection, task-specific trust semantics extraction, and task-collaborator matching analysis. Upon receiving task-specific requests from device-side student agents, the teacher agent transfers the trust semantics of potential collaborators to the student agents, enabling rapid and accurate collaborator selection. Experimental results demonstrate that the proposed 2TSD model can reduce collaborator evaluation time, decrease device resource consumption, and improve the accuracy of collaborator selection.",
        "arxiv_id": "2509.08151",
        "ARXIVID": "2509.08151",
        "COMMENT": "Does not closely match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}