{
    "2508.11058": {
        "authors": [
            "Wentao Mo",
            "Qingchao Chen",
            "Yuxin Peng",
            "Siyuan Huang",
            "Yang Liu"
        ],
        "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset",
        "abstract": "arXiv:2508.11058v1 Announce Type: new  Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several limitations in existing 3D VL datasets: they rarely necessitate reasoning beyond a close range of objects in single viewpoint, and annotations often link instructions to single objects, missing richer contextual alignments between multiple objects. This significantly curtails the development of models capable of deep, multi-view 3D scene understanding over distant objects. To address these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset where 68% of questions explicitly require integrating information from multiple views (compared to less than 7% in existing datasets), thereby rigorously testing multi-view compositional reasoning. To facilitate the training of models for such demanding scenarios, we present TripAlign dataset, a large-scale and low-cost 2D-3D-language pre-training corpus containing 1M  triplets that explicitly aligns groups of contextually related objects with text, providing richer, view-grounded multi-object multimodal alignment signals than previous single-object annotations. We further develop LEGO, a baseline method for the multi-view reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign achieves state-of-the-art performance not only on the proposed MV-ScanQA, but also on existing benchmarks for 3D dense captioning and question answering. Datasets and code are available at https://matthewdm0816.github.io/tripalign-mvscanqa.",
        "arxiv_id": "2508.11058",
        "ARXIVID": "2508.11058",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a new benchmark (MV-ScanQA) for 3D scene understanding and proposes a novel method (LEGO) for multi-view reasoning in embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2508.11433": {
        "authors": [
            "Qian Liang",
            "Yujia Wu",
            "Kuncheng Li",
            "Jiwei Wei",
            "Shiyuan He",
            "Jinyu Guo",
            "Ning Xie"
        ],
        "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation",
        "abstract": "arXiv:2508.11433v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel across a wide range of vision-language tasks, yet aligning them with personalized image generation remains a significant challenge. Existing methods for MLLMs are frequently subject-specific, demanding a data-intensive fine-tuning process for every new subject, which limits their scalability. In this paper, we introduce MM-R1, a framework that integrates a cross-modal Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of unified MLLMs for personalized image generation. Specifically, we structure personalization as an integrated visual reasoning and generation process: (1) grounding subject concepts by interpreting and understanding user-provided images and contextual cues, and (2) generating personalized images conditioned on both the extracted subject representations and user prompts. To further enhance the reasoning capability, we adopt Grouped Reward Proximal Policy Optimization (GRPO) to explicitly align the generation. Experiments demonstrate that MM-R1 unleashes the personalization capability of unified MLLMs to generate images with high subject fidelity and strong text alignment in a zero-shot manner.",
        "arxiv_id": "2508.11433",
        "ARXIVID": "2508.11433",
        "COMMENT": "Matches criteria 2 and 5. Proposes a framework for personalized image generation using Multimodal Large Language Models (MLLMs) with cross-modal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.11428": {
        "authors": [
            "Jingyu Li",
            "Bozhou Zhang",
            "Xin Jin",
            "Jiankang Deng",
            "Xiatian Zhu",
            "Li Zhang"
        ],
        "title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving",
        "abstract": "arXiv:2508.11428v1 Announce Type: new  Abstract: Autonomous driving requires rich contextual comprehension and precise predictive reasoning to navigate dynamic and complex environments safely. Vision-Language Models (VLMs) and Driving World Models (DWMs) have independently emerged as powerful recipes addressing different aspects of this challenge. VLMs provide interpretability and robust action prediction through their ability to understand multi-modal context, while DWMs excel in generating detailed and plausible future driving scenarios essential for proactive planning. Integrating VLMs with DWMs is an intuitive, promising, yet understudied strategy to exploit the complementary strengths of accurate behavioral prediction and realistic scene generation. Nevertheless, this integration presents notable challenges, particularly in effectively connecting action-level decisions with high-fidelity pixel-level predictions and maintaining computational efficiency. In this paper, we propose ImagiDrive, a novel end-to-end autonomous driving framework that integrates a VLM-based driving agent with a DWM-based scene imaginer to form a unified imagination-and-planning loop. The driving agent predicts initial driving trajectories based on multi-modal inputs, guiding the scene imaginer to generate corresponding future scenarios. These imagined scenarios are subsequently utilized to iteratively refine the driving agent's planning decisions. To address efficiency and predictive accuracy challenges inherent in this integration, we introduce an early stopping mechanism and a trajectory selection strategy. Extensive experimental validation on the nuScenes and NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over previous alternatives under both open-loop and closed-loop conditions.",
        "arxiv_id": "2508.11428",
        "ARXIVID": "2508.11428",
        "COMMENT": "Matches criteria 2 and 5. Proposes a novel framework integrating Vision-Language Models (VLMs) and Driving World Models (DWMs) for autonomous driving, focusing on vision-language integration and planning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.11630": {
        "authors": [
            "Yi-Fan Zhang",
            "Xingyu Lu",
            "Shukang Yin",
            "Chaoyou Fu",
            "Wei Chen",
            "Xiao Hu",
            "Bin Wen",
            "Kaiyu Jiang",
            "Changyi Liu",
            "Tianke Zhang",
            "Haonan Fan",
            "Kaibing Chen",
            "Jiankang Chen",
            "Haojie Ding",
            "Kaiyu Tang",
            "Zhang Zhang",
            "Liang Wang",
            "Fan Yang",
            "Tingting Gao",
            "Guorui Zhou"
        ],
        "title": "Thyme: Think Beyond Images",
        "abstract": "arXiv:2508.11630v1 Announce Type: new  Abstract: Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.",
        "arxiv_id": "2508.11630",
        "ARXIVID": "2508.11630",
        "COMMENT": "Matches criterion 2 as it introduces a novel paradigm for Multimodal Large Language Models (MLLMs) with enhanced reasoning and image manipulation capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.11272": {
        "authors": [
            "Jun Li",
            "Kai Li",
            "Shaoguo Liu",
            "Tingting Gao"
        ],
        "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering",
        "abstract": "arXiv:2508.11272v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it requires jointly understanding a reference image and a modified textual instruction to find relevant target images. Some existing methods attempt to use a two-stage approach to further refine retrieval results. However, this often requires additional training of a ranking model. Despite the success of Chain-of-Thought (CoT) techniques in reducing training costs for language models, their application in CIR tasks remains limited -- compressing visual information into text or relying on elaborate prompt designs. Besides, existing works only utilize it for zero-shot CIR, as it is challenging to achieve satisfactory results in supervised CIR with a well-trained model. In this work, we proposed a framework that includes the Pyramid Matching Model with Training-Free Refinement (PMTFR) to address these challenges. Through a simple but effective module called Pyramid Patcher, we enhanced the Pyramid Matching Model's understanding of visual information at different granularities. Inspired by representation engineering, we extracted representations from COT data and injected them into the LVLMs. This approach allowed us to obtain refined retrieval scores in the Training-Free Refinement paradigm without relying on explicit textual reasoning, further enhancing performance. Extensive experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art methods in supervised CIR tasks. The code will be made public.",
        "arxiv_id": "2508.11272",
        "ARXIVID": "2508.11272",
        "COMMENT": "Matches criterion 5 as it discusses techniques combining image understanding and reasoning tasks with Large Language Models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.11538": {
        "authors": [
            "Sitong Gong",
            "Lu Zhang",
            "Yunzhi Zhuge",
            "Xu Jia",
            "Pingping Zhang",
            "Huchuan Lu"
        ],
        "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments",
        "abstract": "arXiv:2508.11538v1 Announce Type: new  Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in videos guided by implicit instructions that encapsulate human intent and temporal logic. Previous approaches leverage large vision language models (LVLMs) to encode object semantics into  tokens for mask prediction. However, this paradigm suffers from limited interpretability during inference and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing inspiration from seminal breakthroughs in reinforcement learning, we introduce Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in segmentation. Veason-R1 is trained through Group Relative Policy Optimization (GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we curate high-quality CoT training data to instill structured reasoning trajectories, bridging video-level semantics and frame-level spatial grounding, yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO fine-tuning encourages efficient exploration of the reasoning space by optimizing reasoning chains. To this end, we incorporate a holistic reward mechanism that synergistically enhances spatial alignment and temporal consistency, bolstering keyframe localization and fine-grained grounding. Comprehensive empirical evaluations demonstrate that Veason-R1 achieves state-of-the-art performance on multiple benchmarks, surpassing prior art by significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS), while exhibiting robustness to hallucinations (+8.8 R). Our code and model weights will be available at Veason-R1.",
        "arxiv_id": "2508.11538",
        "ARXIVID": "2508.11538",
        "COMMENT": "Matches criterion 6 as it focuses on video reasoning segmentation, a video understanding task, with novel methodologies.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.11176": {
        "authors": [
            "Yumiao Zhao",
            "Bo Jiang",
            "Yuhe Ding",
            "Xiao Wang",
            "Jin Tang",
            "Bin Luo"
        ],
        "title": "Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning",
        "abstract": "arXiv:2508.11176v1 Announce Type: new  Abstract: Adapter-based approaches have garnered attention for fine-tuning pre-trained Vision-Language Models (VLMs) on few-shot classification tasks. These methods strive to develop a lightweight module that better aligns visual and (category) textual representations, thereby enhancing performance on downstream few-shot learning tasks. However, existing adapters generally learn/align (category) textual-visual modalities via explicit spatial proximity in the underlying embedding space, which i) fails to capture the inherent one-to-many associations between categories and image samples and ii) struggles to establish accurate associations between the unknown categories and images. To address these issues, inspired by recent works on hyperbolic learning, we develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs on downstream few-shot classification tasks. The core of LatHAdapter is to exploit the latent semantic hierarchy of downstream training data and employ it to provide richer, fine-grained guidance for the adapter learning process. Specifically, LatHAdapter first introduces some learnable `attribute' prompts as the bridge to align categories and images. Then, it projects the categories, attribute prompts, and images within each batch in a hyperbolic space, and employs hierarchical regularization to learn the latent semantic hierarchy of them, thereby fully modeling the inherent one-to-many associations among categories, learnable attributes, and image samples. Extensive experiments on four challenging few-shot tasks show that the proposed LatHAdapter consistently outperforms many other fine-tuning approaches, particularly in adapting known classes and generalizing to unknown classes.",
        "arxiv_id": "2508.11176",
        "ARXIVID": "2508.11176",
        "COMMENT": "Matches criterion 2. Proposes a novel adapter-based fine-tuning method for Vision-Language Models (VLMs), enhancing their performance on few-shot classification tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10956": {
        "authors": [
            "Abhishek Kolari",
            "Mohammadhossein Khojasteh",
            "Yifan Jiang",
            "Floris den Hengst",
            "Filip Ilievski"
        ],
        "title": "ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks",
        "abstract": "arXiv:2508.10956v1 Announce Type: new  Abstract: While vision-language models (VLMs) have made remarkable progress on many popular visual question answering (VQA) benchmarks, it remains unclear whether they abstract and reason over depicted objects. Inspired by human object categorisation, object property reasoning involves identifying and recognising low-level details and higher-level abstractions. While current VQA benchmarks consider a limited set of object property attributes like size, they typically blend perception and reasoning, and lack representativeness in terms of reasoning and image categories. To this end, we introduce a systematic evaluation framework with images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions driven by prior work on commonsense reasoning. We develop a procedure to instantiate this benchmark into ORBIT, a multi-level reasoning VQA benchmark for object properties comprising 360 images paired with a total of 1,080 count-based questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations compared to humans, with the best-performing model only reaching 40\\% accuracy. VLMs struggle particularly with realistic (photographic) images, counterfactual reasoning about physical and functional properties, and higher counts. ORBIT points to the need to develop methods for scalable benchmarking, generalize annotation guidelines, and explore additional reasoning VLMs. We make the ORBIT benchmark and the experimental code available to support such endeavors.",
        "arxiv_id": "2508.10956",
        "ARXIVID": "2508.10956",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark (ORBIT) for visual inference tasks, focusing on object property reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.11317": {
        "authors": [
            "Yuchen Zhou",
            "Jiayu Tang",
            "Shuo Yang",
            "Xiaoyan Xiao",
            "Yuqin Dai",
            "Wenhao Yang",
            "Chao Gou",
            "Xiaobo Xia",
            "Tat-Seng Chua"
        ],
        "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models",
        "abstract": "arXiv:2508.11317v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as foundational for multimodal intelligence. However, their capacity for logical understanding remains significantly underexplored, resulting in critical ''logical blindspots'' that limit their reliability in practical applications. To systematically diagnose this, we introduce LogicBench, a comprehensive benchmark with over 50,000 vision-language pairs across 9 logical categories and 4 diverse scenarios: images, videos, anomaly detection, and medical diagnostics. Our evaluation reveals that existing VLMs, even the state-of-the-art ones, fall at over 40 accuracy points below human performance, particularly in challenging tasks like Causality and Conditionality, highlighting their reliance on surface semantics over critical logical structures. To bridge this gap, we propose LogicCLIP, a novel training framework designed to boost VLMs' logical sensitivity through advancements in both data generation and optimization objectives. LogicCLIP utilizes logic-aware data generation and a contrastive learning strategy that combines coarse-grained alignment, a fine-grained multiple-choice objective, and a novel logical structure-aware objective. Extensive experiments demonstrate LogicCLIP's substantial improvements in logical comprehension across all LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP retains, and often surpasses, competitive performance on general vision-language benchmarks, demonstrating that the enhanced logical understanding does not come at the expense of general alignment. We believe that LogicBench and LogicCLIP will be important resources for advancing VLM logical capabilities.",
        "arxiv_id": "2508.11317",
        "ARXIVID": "2508.11317",
        "COMMENT": "Matches criterion 2 as it explores Vision-Language Models (VLMs) and their logical reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.11350": {
        "authors": [
            "Zhenhao Zhang",
            "Hanqing Wang",
            "Xiangyu Zeng",
            "Ziyu Cheng",
            "Jiaxin Liu",
            "Haoyu Yan",
            "Zhirui Liu",
            "Kaiyang Ji",
            "Tianxiang Gui",
            "Ke Hu",
            "Kangyi Chen",
            "Yahao Fan",
            "Mokai Pan"
        ],
        "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model",
        "abstract": "arXiv:2508.11350v1 Announce Type: new  Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal application in AR/VR and robotics. Recent open-vocabulary HOI detection approaches depend exclusively on large language models for richer textual prompts, neglecting their inherent 3D spatial understanding capabilities. To address this shortcoming, we introduce HOID-R1, the first HOI detection framework that integrates chain-of-thought (CoT) guided supervised fine-tuning (SFT) with group relative policy optimization (GRPO) within a reinforcement learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model with essential reasoning capabilities, forcing the model to articulate its thought process in the output. Subsequently, we integrate GRPO to leverage multi-reward signals for policy optimization, thereby enhancing alignment across diverse modalities. To mitigate hallucinations in the CoT reasoning, we introduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs, further improving generalization. Extensive experiments show that HOID-R1 achieves state-of-the-art performance on HOI detection benchmarks and outperforms existing methods in open-world generalization to novel scenarios.",
        "arxiv_id": "2508.11350",
        "ARXIVID": "2508.11350",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for human-object interaction detection in embodied AI using multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.11569": {
        "authors": [
            "Zheng Wang",
            "Shihao Xu",
            "Wei Shi"
        ],
        "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications",
        "abstract": "arXiv:2508.11569v1 Announce Type: new  Abstract: Sports analytics has received significant attention from both academia and industry in recent years. Despite the growing interest and efforts in this field, several issues remain unresolved, including (1) data unavailability, (2) lack of an effective trajectory-based framework, and (3) requirement for sufficient supervision labels. In this paper, we present TrajSV, a trajectory-based framework that addresses various issues in existing studies. TrajSV comprises three components: data preprocessing, Clip Representation Network (CRNet), and Video Representation Network (VRNet). The data preprocessing module extracts player and ball trajectories from sports broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to learn clip representations based on these trajectories. Additionally, VRNet learns video representations by aggregating clip representations and visual features with an encoder-decoder architecture. Finally, a triple contrastive loss is introduced to optimize both video and clip representations in an unsupervised manner. The experiments are conducted on three broadcast video datasets to verify the effectiveness of TrajSV for three types of sports (i.e., soccer, basketball, and volleyball) with three downstream applications (i.e., sports video retrieval, action spotting, and video captioning). The results demonstrate that TrajSV achieves state-of-the-art performance in sports video retrieval, showcasing a nearly 70% improvement. It outperforms baselines in action spotting, achieving state-of-the-art results in 9 out of 17 action categories, and demonstrates a nearly 20% improvement in video captioning. Additionally, we introduce a deployed system along with the three applications based on TrajSV.",
        "arxiv_id": "2508.11569",
        "ARXIVID": "2508.11569",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding tasks like retrieval, action spotting, and captioning with a novel trajectory-based framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10936": {
        "authors": [
            "Cheng Chen",
            "Hao Huang",
            "Saurabh Bagchi"
        ],
        "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction",
        "abstract": "arXiv:2508.10936v1 Announce Type: new  Abstract: Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets.",
        "arxiv_id": "2508.10936",
        "ARXIVID": "2508.10936",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for collaborative 3D semantic occupancy prediction in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.11106": {
        "authors": [
            "Xinjie Gao",
            "Bi'an Du",
            "Wei Hu"
        ],
        "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing",
        "abstract": "arXiv:2508.11106v1 Announce Type: new  Abstract: 3D content generation remains a fundamental yet challenging task due to the inherent structural complexity of 3D data. While recent octree-based diffusion models offer a promising balance between efficiency and quality through hierarchical generation, they often overlook two key insights: 1) existing methods typically model 3D objects as holistic entities, ignoring their semantic part hierarchies and limiting generalization; and 2) holistic high-resolution modeling is computationally expensive, whereas real-world objects are inherently sparse and hierarchical, making them well-suited for layered generation. Motivated by these observations, we propose HierOctFusion, a part-aware multi-scale octree diffusion model that enhances hierarchical feature interaction for generating fine-grained and sparse object structures. Furthermore, we introduce a cross-attention conditioning mechanism that injects part-level information into the generation process, enabling semantic features to propagate effectively across hierarchical levels from parts to the whole. Additionally, we construct a 3D dataset with part category annotations using a pre-trained segmentation model to facilitate training and evaluation. Experiments demonstrate that HierOctFusion achieves superior shape quality and efficiency compared to prior methods.",
        "arxiv_id": "2508.11106",
        "ARXIVID": "2508.11106",
        "COMMENT": "Matches criteria 3 as it proposes a novel method (HierOctFusion) for 3D shape generation with hierarchical message passing, addressing structural complexity in 3D data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.10922": {
        "authors": [
            "Jianlong Wu",
            "Wei Liu",
            "Ye Liu",
            "Meng Liu",
            "Liqiang Nie",
            "Zhouchen Lin",
            "Chang Wen Chen"
        ],
        "title": "A Survey on Video Temporal Grounding with Multimodal Large Language Model",
        "abstract": "arXiv:2508.10922v1 Announce Type: new  Abstract: The recent advancement in video temporal grounding (VTG) has significantly enhanced fine-grained video understanding, primarily driven by multimodal large language models (MLLMs). With superior multimodal comprehension and reasoning abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing traditional fine-tuned methods. They not only achieve competitive performance but also excel in generalization across zero-shot, multi-task, and multi-domain settings. Despite extensive surveys on general video-language understanding, comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill this gap, this survey systematically examines current research on VTG-MLLMs through a three-dimensional taxonomy: 1) the functional roles of MLLMs, highlighting their architectural significance; 2) training paradigms, analyzing strategies for temporal reasoning and task adaptation; and 3) video feature processing techniques, which determine spatiotemporal representation effectiveness. We further discuss benchmark datasets, evaluation protocols, and summarize empirical findings. Finally, we identify existing limitations and propose promising research directions. For additional resources and details, readers are encouraged to visit our repository at https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.",
        "arxiv_id": "2508.10922",
        "ARXIVID": "2508.10922",
        "COMMENT": "Matches criterion 7. Provides a comprehensive survey on video temporal grounding with Multimodal Large Language Models (MLLMs), synthesizing state-of-the-art progress.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.11354": {
        "authors": [
            "Zhenyi Zhao",
            "Muthu Rama Krishnan Mookiah",
            "Emanuele Trucco"
        ],
        "title": "Leveraging the RETFound foundation model for optic disc segmentation in retinal images",
        "abstract": "arXiv:2508.11354v1 Announce Type: new  Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera and optical coherence tomography images. It has shown promising performance across multiple datasets in diagnosing diseases, both eye-specific and systemic, from retinal images. However, to our best knowledge, it has not been used for other tasks. We present the first adaptation of RETFound for optic disc segmentation, a ubiquitous and foundational task in retinal image analysis. The resulting segmentation system outperforms state-of-the-art, segmentation-specific baseline networks after training a head with only a very modest number of task-specific examples. We report and discuss results with four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private dataset, GoDARTS, achieving about 96% Dice consistently across all datasets. Overall, our method obtains excellent performance in internal verification, domain generalization and domain adaptation, and exceeds most of the state-of-the-art baseline results. We discuss the results in the framework of the debate about FMs as alternatives to task-specific architectures. The code is available at: [link to be added after the paper is accepted]",
        "arxiv_id": "2508.11354",
        "ARXIVID": "2508.11354",
        "COMMENT": "Matches criterion 4 as it focuses on the application of a vision foundation model (RETFound) for optic disc segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.11452": {
        "authors": [
            "Kangyu Wang",
            "Hongliang He",
            "Lin Liu",
            "Ruiqi Liang",
            "Zhenzhong Lan",
            "Jianguo Li"
        ],
        "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps",
        "abstract": "arXiv:2508.11452v1 Announce Type: new  Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at https://doraemon.alipay.com/model-ranking.",
        "arxiv_id": "2508.11452",
        "ARXIVID": "2508.11452",
        "COMMENT": "Matches criterion 2 as it discusses Multimodal Large Language Models (MLLMs) and their evaluation in real-world applications.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.11624": {
        "authors": [
            "Niki Foteinopoulou",
            "Ignas Budvytis",
            "Stephan Liwicki"
        ],
        "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition",
        "abstract": "arXiv:2508.11624v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in text-to-image diffusion models, enabling the personalisation of visual concepts such as characters, styles, and objects. However, existing approaches struggle to effectively compose multiple LoRA adapters, particularly in open-ended settings where the number and nature of required skills are not known in advance. In this work, we present LoRAtorio, a novel train-free framework for multi-LoRA composition that leverages intrinsic model behaviour. Our method is motivated by two key observations: (1) LoRA adapters trained on narrow domains produce denoised outputs that diverge from the base model, and (2) when operating out-of-distribution, LoRA outputs show behaviour closer to the base model than when conditioned in distribution. The balance between these two observations allows for exceptional performance in the single LoRA scenario, which nevertheless deteriorates when multiple LoRAs are loaded. Our method operates in the latent space by dividing it into spatial patches and computing cosine similarity between each patch's predicted noise and that of the base model. These similarities are used to construct a spatially-aware weight matrix, which guides a weighted aggregation of LoRA outputs. To address domain drift, we further propose a modification to classifier-free guidance that incorporates the base model's unconditional score into the composition. We extend this formulation to a dynamic module selection setting, enabling inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio achieves state-of-the-art performance, showing up to a 1.3% improvement in ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises effectively to multiple latent diffusion models.",
        "arxiv_id": "2508.11624",
        "ARXIVID": "2508.11624",
        "COMMENT": "Matches criterion 5 as it introduces a novel method for combining multiple LoRA adapters in text-to-image diffusion models, which involves vision and language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.11183": {
        "authors": [
            "Zhenghao Chen",
            "Zicong Chen",
            "Lei Liu",
            "Yiming Wu",
            "Dong Xu"
        ],
        "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting",
        "abstract": "arXiv:2508.11183v1 Announce Type: new  Abstract: Video tokenization procedure is critical for a wide range of video processing tasks. Most existing approaches directly transform video into fixed-grid and patch-wise tokens, which exhibit limited versatility. Spatially, uniformly allocating a fixed number of tokens often leads to over-encoding in low-information regions. Temporally, reducing redundancy remains challenging without explicitly distinguishing between static and dynamic content. In this work, we propose the Gaussian Video Transformer (GVT), a versatile video tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We first extract latent rigid features from a video clip and represent them with a set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video optimization.To enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which explicitly model static content shared across different time-steps and dynamic content specific to each time-step, enabling a compact representation.We primarily evaluate GVT on the video reconstruction, while also assessing its performance on action recognition and compression using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments demonstrate that GVT achieves a state-of-the-art video reconstruction quality, outperforms the baseline MAGVIT-v2 in action recognition, and delivers comparable compression performance.",
        "arxiv_id": "2508.11183",
        "ARXIVID": "2508.11183",
        "COMMENT": "Matches criteria 6 as it introduces a novel video tokenization method (GVT) and evaluates it on video-based tasks like reconstruction and action recognition.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.11282": {
        "authors": [
            "Muzammil Khan",
            "Enzo Kerkhof",
            "Matteo Fusaglia",
            "Koert Kuhlmann",
            "Theo Ruers",
            "Fran\\c{c}oise J. Siepel"
        ],
        "title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction",
        "abstract": "arXiv:2508.11282v1 Announce Type: new  Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction significantly enhances monocular minimally invasive surgical procedures by enabling accurate navigation and improved spatial awareness. However, monocular endoscope pose estimation and tissue reconstruction face persistent challenges, including depth ambiguity, physiological tissue deformation, inconsistent endoscope motion, limited texture fidelity, and a restricted field of view. To overcome these limitations, a unified framework for monocular endoscopic tissue reconstruction that integrates scale-aware depth prediction with temporally-constrained perceptual refinement is presented. This framework incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust initialisation and Depth Anything for efficient per-frame depth prediction, in conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth estimates. These estimates are temporally refined by computing pixel correspondences using RAFT and adaptively blending flow-warped frames based on LPIPS perceptual similarity, thereby reducing artefacts arising from physiological tissue deformation and motion. To ensure accurate registration of the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module is integrated, optimising both rotation and translation. Finally, truncated signed distance function-based volumetric fusion and marching cubes are applied to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED, with ablation and comparative analyses, demonstrate the framework's robustness and superiority over state-of-the-art methods.",
        "arxiv_id": "2508.11282",
        "ARXIVID": "2508.11282",
        "COMMENT": "Matches criterion 1 as it discusses spatial reasoning in endoscopic tissue reconstruction, enhancing spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.11524": {
        "authors": [
            "Wenkai Yu",
            "Jianhang Tang",
            "Yang Zhang",
            "Shanjiang Tang",
            "Kebing Jin",
            "Hankz Hankui Zhuo"
        ],
        "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models",
        "abstract": "arXiv:2508.11524v1 Announce Type: new  Abstract: Addressing large-scale planning problems has become one of the central challenges in the planning community, deriving from the state-space explosion caused by growing objects and actions. Recently, researchers have explored the effectiveness of leveraging Large Language Models (LLMs) to generate helpful actions and states to prune the search space. However, prior works have largely overlooked integrating LLMs with domain-specific knowledge to ensure valid plans. In this paper, we propose a novel LLM-assisted planner integrated with problem decomposition, which first decomposes large planning problems into multiple simpler sub-tasks. Then we explore two novel paradigms to utilize LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where LLM4Inspire provides heuristic guidance according to general knowledge and LLM4Predict employs domain-specific knowledge to infer intermediate conditions. We empirically validate the effectiveness of our planner across multiple domains, demonstrating the ability of search space partition when solving large-scale planning problems. The experimental results show that LLMs effectively locate feasible solutions when pruning the search space, where infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds particular promise compared with LLM4Inspire, which offers general knowledge within LLMs.",
        "arxiv_id": "2508.11524",
        "ARXIVID": "2508.11524",
        "COMMENT": "Matches criterion 1 as it explores spatial reasoning in large-scale planning problems with LLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.11616": {
        "authors": [
            "Oscar Ma\\~nas",
            "Pierluca D'Oro",
            "Koustuv Sinha",
            "Adriana Romero-Soriano",
            "Michal Drozdzal",
            "Aishwarya Agrawal"
        ],
        "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
        "abstract": "arXiv:2508.11616v1 Announce Type: new  Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.",
        "arxiv_id": "2508.11616",
        "ARXIVID": "2508.11616",
        "COMMENT": "Matches criterion 2 as it explores controlled decoding in multimodal large language models (MLLMs) with a focus on visual grounding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.11256": {
        "authors": [
            "Junjie Wang",
            "Keyu Chen",
            "Yulin Li",
            "Bin Chen",
            "Hengshuang Zhao",
            "Xiaojuan Qi",
            "Zhuotao Tian"
        ],
        "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception",
        "abstract": "arXiv:2508.11256v1 Announce Type: new  Abstract: Dense visual perception tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense perception often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. \\revise{The context features are enhanced by jointly distilling semantic correlations from Vision Foundation Models (VFMs) and object integrity cues from diffusion models, thereby enhancing spatial consistency. In parallel, the content features are aligned with image crop representations and constrained by region correlations from VFMs to improve local discriminability. Extensive experiments demonstrate that DeCLIP establishes a solid foundation for open-vocabulary dense perception, consistently achieving state-of-the-art performance across a broad spectrum of tasks, including 2D detection and segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.} Code is available at https://github.com/xiaomoguhz/DeCLIP",
        "arxiv_id": "2508.11256",
        "ARXIVID": "2508.11256",
        "COMMENT": "Matches criteria 4 as it focuses on enhancing vision foundation models (e.g., CLIP) for open-vocabulary dense perception tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2508.10972": {
        "authors": [
            "Rosiana Natalie",
            "Wenqian Xu",
            "Ruei-Che Chang",
            "Rada Mihalcea",
            "Anhong Guo"
        ],
        "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision",
        "abstract": "arXiv:2508.10972v1 Announce Type: new  Abstract: Advances in vision language models (VLMs) have enabled the simulation of general human behavior through their reasoning and problem solving capabilities. However, prior research has not investigated such simulation capabilities in the accessibility domain. In this paper, we evaluate the extent to which VLMs can simulate the vision perception of low vision individuals when interpreting images. We first compile a benchmark dataset through a survey study with 40 low vision participants, collecting their brief and detailed vision information and both open-ended and multiple-choice image perception and recognition responses to up to 25 images. Using these responses, we construct prompts for VLMs (GPT-4o) to create simulated agents of each participant, varying the included information on vision information and example image responses. We evaluate the agreement between VLM-generated responses and participants' original answers. Our results indicate that VLMs tend to infer beyond the specified vision ability when given minimal prompts, resulting in low agreement (0.59). The agreement between the agent' and participants' responses remains low when only either the vision information (0.59) or example image responses (0.59) are provided, whereas a combination of both significantly increase the agreement (0.70, p < 0.0001). Notably, a single example combining both open-ended and multiple-choice responses, offers significant performance improvements over either alone (p  0.05).",
        "arxiv_id": "2508.10972",
        "ARXIVID": "2508.10972",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models (VLMs) in a novel accessibility domain.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.11341": {
        "authors": [
            "Katarzyna Filus",
            "Jorge M. Cruz-Duarte"
        ],
        "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models",
        "abstract": "arXiv:2508.11341v1 Announce Type: new  Abstract: In targeted adversarial attacks on vision models, the selection of the target label is a critical yet often overlooked determinant of attack success. This target label corresponds to the class that the attacker aims to force the model to predict. Now, existing strategies typically rely on randomness, model predictions, or static semantic resources, limiting interpretability, reproducibility, or flexibility. This paper then proposes a semantics-guided framework for adversarial target selection using the cross-modal knowledge transfer from pretrained language and vision-language models. We evaluate several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity sources to select the most and least semantically related labels with respect to the ground truth, forming best- and worst-case adversarial scenarios. Our experiments on three vision models and five attack methods reveal that these models consistently render practical adversarial targets and surpass static lexical databases, such as WordNet, particularly for distant class relationships. We also observe that static testing of target labels offers a preliminary assessment of the effectiveness of similarity sources, \\textit{a priori} testing. Our results corroborate the suitability of pretrained models for constructing interpretable, standardized, and scalable adversarial benchmarks across architectures and datasets.",
        "arxiv_id": "2508.11341",
        "ARXIVID": "2508.11341",
        "COMMENT": "Matches criteria 2 as it explores the use of vision-language models (e.g., CLIP) for adversarial testing, which is relevant to vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.11284": {
        "authors": [
            "Yilin Mi",
            "Qixin Yan",
            "Zheng-Peng Duan",
            "Chunle Guo",
            "Hubery Yin",
            "Hao Liu",
            "Chen Li",
            "Chongyi Li"
        ],
        "title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation",
        "abstract": "arXiv:2508.11284v1 Announce Type: new  Abstract: With the advancement of generative models, facial image editing has made significant progress. However, achieving fine-grained age editing while preserving personal identity remains a challenging task.In this paper, we propose TimeMachine, a novel diffusion-based framework that achieves accurate age editing while keeping identity features unchanged. To enable fine-grained age editing, we inject high-precision age information into the multi-cross attention module, which explicitly separates age-related and identity-related features. This design facilitates more accurate disentanglement of age attributes, thereby allowing precise and controllable manipulation of facial aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that predicts age directly in the latent space, instead of performing denoising image reconstruction during training. By employing a lightweight module to incorporate age constraints, this design enhances age editing accuracy by modest increasing training cost. Additionally, to address the lack of large-scale, high-quality facial age datasets, we construct a HFFA dataset (High-quality Fine-grained Facial-Age dataset) which contains one million high-resolution images labeled with identity and facial attributes. Experimental results demonstrate that TimeMachine achieves state-of-the-art performance in fine-grained age editing while preserving identity consistency.",
        "arxiv_id": "2508.11284",
        "ARXIVID": "2508.11284",
        "COMMENT": "Does not match any specific criteria. Focuses on fine-grained facial age editing, which is outside the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.11603": {
        "authors": [
            "Zhe Zhu",
            "Honghua Chen",
            "Peng Li",
            "Mingqiang Wei"
        ],
        "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion",
        "abstract": "arXiv:2508.11603v1 Announce Type: new  Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual descriptions, and most existing approaches tackle this by adapting pre-trained 2D image editors to multi-view inputs. However, without explicit control over multi-view information exchange, they often fail to maintain cross-view consistency, leading to insufficient edits and blurry details. We introduce CoreEditor, a novel framework for consistent text-to-3D editing. The key innovation is a correspondence-constrained attention mechanism that enforces precise interactions between pixels expected to remain consistent throughout the diffusion denoising process. Beyond relying solely on geometric alignment, we further incorporate semantic similarity estimated during denoising, enabling more reliable correspondence modeling and robust multi-view editing. In addition, we design a selective editing pipeline that allows users to choose preferred results from multiple candidates, offering greater flexibility and user control. Extensive experiments show that CoreEditor produces high-quality, 3D-consistent edits with sharper details, significantly outperforming prior methods.",
        "arxiv_id": "2508.11603",
        "ARXIVID": "2508.11603",
        "COMMENT": "Does not match any specific criteria. Focuses on text-driven 3D editing, which is outside the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.11550": {
        "authors": [
            "Zuo Zuo",
            "Jiahao Dong",
            "Yanyun Qu",
            "Zongze Wu"
        ],
        "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model",
        "abstract": "arXiv:2508.11550v1 Announce Type: new  Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing where a long-standing challenge is data scarcity. A growing body of works have emerged to address insufficient anomaly data via anomaly generation. However, these anomaly generation methods suffer from lack of fidelity or need to be trained with extra data. To this end, we propose a training-free anomaly generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s strong generation ability for effective anomaly image generation. Given a normal image, mask and a simple text prompt, AAG can generate realistic and natural anomalies in the specific regions and simultaneously keep contents in other regions unchanged. In particular, we propose Cross-Attention Enhancement (CAE) to re-engineer the cross-attention mechanism within Stable Diffusion based on the given mask. CAE increases the similarity between visual tokens in specific regions and text embeddings, which guides these generated visual tokens in accordance with the text description. Besides, generated anomalies need to be more natural and plausible with object in given image. We propose Self-Attention Enhancement (SAE) which improves similarity between each normal visual token and anomaly visual tokens. SAE ensures that generated anomalies are coherent with original pattern. Extensive experiments on MVTec AD and VisA datasets demonstrate effectiveness of AAG in anomaly generation and its utility. Furthermore, anomaly images generated by AAG can bolster performance of various downstream anomaly inspection tasks.",
        "arxiv_id": "2508.11550",
        "ARXIVID": "2508.11550",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.11165": {
        "authors": [
            "Bing Liu",
            "Le Wang",
            "Mingming Liu",
            "Hao Liu",
            "Rui Yao",
            "Yong Zhou",
            "Peng Liu",
            "Tongqiang Xia"
        ],
        "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models",
        "abstract": "arXiv:2508.11165v1 Announce Type: new  Abstract: Existing dehazing methods deal with real-world haze images with difficulty, especially scenes with thick haze. One of the main reasons is the lack of real-world paired data and robust priors. To avoid the costly collection of paired hazy and clear images, we propose an efficient semi-supervised image dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first stage, we employ the EM algorithm to decouple the joint distribution of paired hazy and clear images into two conditional distributions, which are then modeled using a unified Brownian Bridge diffusion model to directly capture the structural and content-related correlations between hazy and clear images. In the second stage, we leverage the pre-trained model and large-scale unpaired hazy and clear images to further improve the performance of image dehazing. Additionally, we introduce a detail-enhanced Residual Difference Convolution block (RDC) to capture gradient-level information, significantly enhancing the model's representation capability. Extensive experiments demonstrate that our EM-B3DM achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets.",
        "arxiv_id": "2508.11165",
        "ARXIVID": "2508.11165",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.10946": {
        "authors": [
            "Wonho Lee",
            "Hyunsik Na",
            "Jisu Lee",
            "Daeseon Choi"
        ],
        "title": "IPG: Incremental Patch Generation for Generalized Adversarial Patch Training",
        "abstract": "arXiv:2508.10946v1 Announce Type: new  Abstract: The advent of adversarial patches poses a significant challenge to the robustness of AI models, particularly in the domain of computer vision tasks such as object detection. In contradistinction to traditional adversarial examples, these patches target specific regions of an image, resulting in the malfunction of AI models. This paper proposes Incremental Patch Generation (IPG), a method that generates adversarial patches up to 11.1 times more efficiently than existing approaches while maintaining comparable attack performance. The efficacy of IPG is demonstrated by experiments and ablation studies including YOLO's feature distribution visualization and adversarial training results, which show that it produces well-generalized patches that effectively cover a broader range of model vulnerabilities. Furthermore, IPG-generated datasets can serve as a robust knowledge foundation for constructing a robust model, enabling structured representation, advanced reasoning, and proactive defenses in AI security ecosystems. The findings of this study suggest that IPG has considerable potential for future utilization not only in adversarial patch defense but also in real-world applications such as autonomous vehicles, security systems, and medical imaging, where AI models must remain resilient to adversarial attacks in dynamic and high-stakes environments.",
        "arxiv_id": "2508.10946",
        "ARXIVID": "2508.10946",
        "COMMENT": "Does not match any specific criteria. Focuses on adversarial patch generation, which is outside the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10950": {
        "authors": [
            "Xinyi Wang",
            "Michael Barnett",
            "Frederique Boonstra",
            "Yael Barnett",
            "Mariano Cabezas",
            "Arkiev D'Souza",
            "Matthew C. Kiernan",
            "Kain Kyle",
            "Meng Law",
            "Lynette Masters",
            "Zihao Tang",
            "Stephen Tisch",
            "Sicong Tu",
            "Anneke Van Der Walt",
            "Dongang Wang",
            "Fernando Calamante",
            "Weidong Cai",
            "Chenyu Wang"
        ],
        "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement",
        "abstract": "arXiv:2508.10950v1 Announce Type: new  Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling technique that represents complex white matter fiber configurations, and a key step for subsequent brain tractography and connectome analysis. Its reliability and accuracy, however, heavily rely on the quality of the MRI acquisition and the subsequent estimation of the FODs at each voxel. Generating reliable FODs from widely available clinical protocols with single-shell and low-angular-resolution acquisitions remains challenging but could potentially be addressed with recent advances in deep learning-based enhancement techniques. Despite advancements, existing methods have predominantly been assessed on healthy subjects, which have proved to be a major hurdle for their clinical adoption. In this work, we validate a newly optimized enhancement framework, FastFOD-Net, across healthy controls and six neurological disorders. This accelerated end-to-end deep learning framework enhancing FODs with superior performance and delivering training/inference efficiency for clinical use ($60\\times$ faster comparing to its predecessor). With the most comprehensive clinical evaluation to date, our work demonstrates the potential of FastFOD-Net in accelerating clinical neuroscience research, empowering diffusion MRI analysis for disease differentiation, improving interpretability in connectome applications, and reducing measurement errors to lower sample size requirements. Critically, this work will facilitate the more widespread adoption of, and build clinical trust in, deep learning based methods for diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of real-world, clinical diffusion MRI data, comparable to that achievable with high-quality research acquisitions.",
        "arxiv_id": "2508.10950",
        "ARXIVID": "2508.10950",
        "COMMENT": "Does not match any specific criterion but is relevant to machine learning and medical imaging applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11419": {
        "authors": [
            "Florian Bayer",
            "Maximilian Russo",
            "Christian Rathgeb"
        ],
        "title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems",
        "abstract": "arXiv:2508.11419v1 Announce Type: new  Abstract: Biometric recognition is widely used, making the privacy and security of extracted templates a critical concern. Biometric Template Protection schemes, especially those utilizing Homomorphic Encryption, introduce significant computational challenges due to increased workload. Recent advances in deep neural networks have enabled state-of-the-art feature extraction for face, fingerprint, and iris modalities. The ubiquity and affordability of biometric sensors further facilitate multi-modal fusion, which can enhance security by combining features from different modalities. This work investigates the biometric performance of reduced multi-biometric template sizes. Experiments are conducted on an in-house virtual multi-biometric database, derived from DNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT, and CASIA databases. The evaluated approaches are (i) explainable and straightforward to implement under encryption, (ii) training-free, and (iii) capable of generalization. Dimensionality reduction of feature vectors leads to fewer operations in the Homomorphic Encryption (HE) domain, enabling more efficient encrypted processing while maintaining biometric accuracy and security at a level equivalent to or exceeding single-biometric recognition. Our results demonstrate that, by fusing feature vectors from multiple modalities, template size can be reduced by 67 % with no loss in Equal Error Rate (EER) compared to the best-performing single modality.",
        "arxiv_id": "2508.11419",
        "ARXIVID": "2508.11419",
        "COMMENT": "Does not match any specific criterion but is relevant to machine learning and privacy-preserving biometric systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11070": {
        "authors": [
            "Zahra Khotanlou",
            "Kate Larson",
            "Amir-Hossein Karimi"
        ],
        "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching",
        "abstract": "arXiv:2508.11070v1 Announce Type: new  Abstract: Decision makers are increasingly relying on machine learning in sensitive situations. In such settings, algorithmic recourse aims to provide individuals with actionable and minimally costly steps to reverse unfavorable AI-driven decisions. While existing research predominantly focuses on single-individual (i.e., seeker) and single-model (i.e., provider) scenarios, real-world applications often involve multiple interacting stakeholders. Optimizing outcomes for seekers under an individual welfare approach overlooks the inherently multi-agent nature of real-world systems, where individuals interact and compete for limited resources. To address this, we introduce a novel framework for multi-agent algorithmic recourse that accounts for multiple recourse seekers and recourse providers. We model this many-to-many interaction as a capacitated weighted bipartite matching problem, where matches are guided by both recourse cost and provider capacity. Edge weights, reflecting recourse costs, are optimized for social welfare while quantifying the welfare gap between individual welfare and this collectively feasible outcome. We propose a three-layer optimization framework: (1) basic capacitated matching, (2) optimal capacity redistribution to minimize the welfare gap, and (3) cost-aware optimization balancing welfare maximization with capacity adjustment costs. Experimental validation on synthetic and real-world datasets demonstrates that our framework enables the many-to-many algorithmic recourse to achieve near-optimal welfare with minimum modification in system settings. This work extends algorithmic recourse from individual recommendations to system-level design, providing a tractable path toward higher social welfare while maintaining individual actionability.",
        "arxiv_id": "2508.11070",
        "ARXIVID": "2508.11070",
        "COMMENT": "Does not closely match any specific criterion but is relevant to algorithmic recourse and optimization, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11265": {
        "authors": [
            "Pei He",
            "Lingling Li",
            "Licheng Jiao",
            "Ronghua Shang",
            "Fang Liu",
            "Shuang Wang",
            "Xu Liu",
            "Wenping Ma"
        ],
        "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds",
        "abstract": "arXiv:2508.11265v1 Announce Type: new  Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying models to unseen environments. Current methods mitigate the domain shift by augmenting the data distribution of point clouds. However, the model learns global geometric patterns in point clouds while ignoring the category-level distribution and alignment. In this paper, a category-level geometry learning framework is proposed to explore the domain-invariant geometric features for domain generalized 3D semantic segmentation. Specifically, Category-level Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric properties of point cloud features, which constructs the geometric properties of each class and couples geometric embedding to semantic learning. Secondly, Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D distribution and align the category-level geometric embeddings, allowing the model to focus on the geometric invariant information to improve generalization. Experimental results verify the effectiveness of the proposed method, which has very competitive segmentation accuracy compared with the state-of-the-art domain generalized point cloud methods.",
        "arxiv_id": "2508.11265",
        "ARXIVID": "2508.11265",
        "COMMENT": "Does not closely match any specific criterion but is relevant to 3D segmentation and domain generalization in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.10937": {
        "authors": [
            "Jiarui Yang",
            "Hang Guo",
            "Wen Huang",
            "Tao Dai",
            "Shutao Xia"
        ],
        "title": "Personalized Face Super-Resolution with Identity Decoupling and Fitting",
        "abstract": "arXiv:2508.10937v1 Announce Type: new  Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable progress, generally maintaining high image fidelity and identity (ID) consistency under standard settings. However, in extreme degradation scenarios (e.g., scale $> 8\\times$), critical attributes and ID information are often severely lost in the input image, making it difficult for conventional models to reconstruct realistic and ID-consistent faces. Existing methods tend to generate hallucinated faces under such conditions, producing restored images lacking authentic ID constraints. To address this challenge, we propose a novel FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID restoration under large scaling factors while mitigating hallucination effects. Our approach involves three key designs: 1) \\textbf{Masking} the facial region in the low-resolution (LR) image to eliminate unreliable ID cues; 2) \\textbf{Warping} a reference image to align with the LR input, providing style guidance; 3) Leveraging \\textbf{ID embeddings} extracted from ground truth (GT) images for fine-grained ID modeling and personalized adaptation. We first pretrain a diffusion-based model to explicitly decouple style and ID by forcing it to reconstruct masked LR face regions using both style and identity embeddings. Subsequently, we freeze most network parameters and perform lightweight fine-tuning of the ID embedding using a small set of target ID images. This embedding encodes fine-grained facial attributes and precise ID information, significantly improving both ID consistency and perceptual quality. Extensive quantitative evaluations and visual comparisons demonstrate that the proposed IDFSR substantially outperforms existing approaches under extreme degradation, particularly achieving superior performance on ID consistency.",
        "arxiv_id": "2508.10937",
        "ARXIVID": "2508.10937",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11173": {
        "authors": [
            "Ruobing Jiang",
            "Yang Liu",
            "Haobing Liu",
            "Yanwei Yu",
            "Chunyang Wang"
        ],
        "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery",
        "abstract": "arXiv:2508.11173v1 Announce Type: new  Abstract: Continuous category discovery (CCD) aims to automatically discover novel categories in continuously arriving unlabeled data. This is a challenging problem considering that there is no number of categories and labels in the newly arrived data, while also needing to mitigate catastrophic forgetting. Most CCD methods cannot handle the contradiction between novel class discovery and classification well. They are also prone to accumulate errors in the process of gradually discovering novel classes. Moreover, most of them use knowledge distillation and data replay to prevent forgetting, occupying more storage space. To address these limitations, we propose Independence-based Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes independent enrichment of diversity module, joint discovery of novelty module, and continuous increment by orthogonality module. In independent enrichment, the backbone is trained separately using contrastive loss to avoid it focusing only on features for classification. Joint discovery transforms multi-stage novel class discovery into single-stage, reducing error accumulation impact. Continuous increment by orthogonality module generates mutually orthogonal prototypes for classification and prevents forgetting with lower space overhead via representative representation replay. Experimental results show that on challenging fine-grained datasets, our method outperforms the state-of-the-art methods.",
        "arxiv_id": "2508.11173",
        "ARXIVID": "2508.11173",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11032": {
        "authors": [
            "Yanwu Yang",
            "Guinan Su",
            "Jiesi Hu",
            "Francesco Sammarco",
            "Jonas Geiping",
            "Thomas Wolfers"
        ],
        "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation",
        "abstract": "arXiv:2508.11032v1 Announce Type: new  Abstract: Universal medical image segmentation models have emerged as a promising paradigm due to their strong generalizability across diverse tasks, showing great potential for a wide range of clinical applications. This potential has been partly driven by the success of general-purpose vision models such as the Segment Anything Model (SAM), which has inspired the development of various fine-tuned variants for medical segmentation tasks. However, fine-tuned variants like MedSAM are trained on comparatively limited medical imaging data that often suffers from heterogeneity, scarce annotations, and distributional shifts. These challenges limit their ability to generalize across a wide range of medical segmentation tasks. In this regard, we propose MedSAMix, a training-free model merging method that integrates the strengths of both generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical image segmentation. In contrast to traditional model merging approaches that rely on manual configuration and often result in suboptimal outcomes, we propose a zero-order optimization method to automatically discover optimal layer-wise merging solutions. Furthermore, for clinical applications, we develop two regimes to meet the demand of domain-specificity and generalizability in different scenarios by single-task optimization and multi-objective optimization respectively. Extensive evaluations on 25 medical segmentation tasks demonstrate that MedSAMix effectively mitigates model bias and consistently improves performance in both domain-specific accuracy and generalization, achieving improvements of 6.67% on specialized tasks and 4.37% on multi-task evaluations.",
        "arxiv_id": "2508.11032",
        "ARXIVID": "2508.11032",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.11301": {
        "authors": [
            "Jiarong Li",
            "Imad Ali Shah",
            "Enda Ward",
            "Martin Glavin",
            "Edward Jones",
            "Brian Deegan"
        ],
        "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study",
        "abstract": "arXiv:2508.11301v1 Announce Type: new  Abstract: Pedestrian segmentation in automotive perception systems faces critical safety challenges due to metamerism in RGB imaging, where pedestrians and backgrounds appear visually indistinguishable.. This study investigates the potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We compared standard RGB against two dimensionality-reduction approaches by converting 128-channel HSI data into three-channel representations: Principal Component Analysis (PCA) and optimal band selection using Contrast Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM). Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25% F1-score improvements. These improved performance results from enhanced spectral discrimination of optimally selected HSI bands effectively reducing false positives. This study demonstrates robust pedestrian segmentation through optimal HSI band selection, showing significant potential for safety-critical automotive applications.",
        "arxiv_id": "2508.11301",
        "ARXIVID": "2508.11301",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning in safety-critical applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.11464": {
        "authors": [
            "Xiaoya Zhu",
            "Yibing Nan",
            "Shiguo Lian"
        ],
        "title": "Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge",
        "abstract": "arXiv:2508.11464v1 Announce Type: new  Abstract: With the rapid development of technology in the field of AI, deepfake technology has emerged as a double-edged sword. It has not only created a large amount of AI-generated content but also posed unprecedented challenges to digital security. The task of the competition is to determine whether a face image is a Deepfake image and output its probability score of being a Deepfake image. In the image track competition, our approach is based on the Swin Transformer V2-B classification network. And online data augmentation and offline sample generation methods are employed to enrich the diversity of training samples and increase the generalization ability of the model. Finally, we got the award of excellence in Deepfake image detection.",
        "arxiv_id": "2508.11464",
        "ARXIVID": "2508.11464",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}