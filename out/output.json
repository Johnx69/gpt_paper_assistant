{
    "2511.05299": {
        "authors": [
            "Zhenyu Yang",
            "Kairui Zhang",
            "Yuhang Hu",
            "Bing Wang",
            "Shengsheng Qian",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Weiming Dong",
            "Changsheng Xu"
        ],
        "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
        "abstract": "arXiv:2511.05299v1 Announce Type: new  Abstract: Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
        "arxiv_id": "2511.05299",
        "ARXIVID": "2511.05299",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 6 (Video Understanding) as it introduces a Video-LLM for real-time video understanding with novel training and inference strategies.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.05491": {
        "authors": [
            "Rui Yang",
            "Ziyu Zhu",
            "Yanwei Li",
            "Jingjia Huang",
            "Shen Yan",
            "Siyuan Zhou",
            "Zhe Liu",
            "Xiangtai Li",
            "Shuangye Li",
            "Wenqian Wang",
            "Yi Lin",
            "Hengshuang Zhao"
        ],
        "title": "Visual Spatial Tuning",
        "abstract": "arXiv:2511.05491v1 Announce Type: new  Abstract: Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including $34.8\\%$ on MMSI-Bench and $61.2\\%$ on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.",
        "arxiv_id": "2511.05491",
        "ARXIVID": "2511.05491",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a framework to enhance spatial reasoning in Vision-Language Models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.04727": {
        "authors": [
            "Ali Faraz",
            "Akash",
            "Shaharukh Khan",
            "Raja Kolla",
            "Akshat Patidar",
            "Suranjan Goswami",
            "Abhinav Ravi",
            "Chandra Khatri",
            "Shubham Agarwal"
        ],
        "title": "IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs",
        "abstract": "arXiv:2511.04727v1 Announce Type: new  Abstract: Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.",
        "arxiv_id": "2511.04727",
        "ARXIVID": "2511.04727",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it introduces a benchmark for evaluating vision-language models in culturally diverse and multilingual settings.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.04948": {
        "authors": [
            "Haoxin Lv",
            "Ijazul Haq",
            "Jin Du",
            "Jiaxin Ma",
            "Binnian Zhu",
            "Xiaobing Dang",
            "Chaoan Liang",
            "Ruxu Du",
            "Yingjie Zhang",
            "Muhammad Saqib"
        ],
        "title": "A benchmark multimodal oro-dental dataset for large vision-language models",
        "abstract": "arXiv:2511.04948v1 Announce Type: new  Abstract: The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.",
        "arxiv_id": "2511.04948",
        "ARXIVID": "2511.04948",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a benchmark multimodal dataset for vision-language models in the context of oral healthcare.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.04880": {
        "authors": [
            "Yu Bai",
            "Yukai Miao",
            "Dawei Wang",
            "Li Chen",
            "Fei Long",
            "Rundi Zhai",
            "Dan Li",
            "Yanyu Ren",
            "Tianfeng Liu",
            "Hongtao Xie",
            "Ce Yang",
            "Xuhui Cai"
        ],
        "title": "DMA: Online RAG Alignment with Human Feedback",
        "abstract": "arXiv:2511.04880v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval, limiting adaptation to evolving intent and content drift. We introduce Dynamic Memory Alignment (DMA), an online learning framework that systematically incorporates multi-granularity human feedback to align ranking in interactive settings. DMA organizes document-, list-, and response-level signals into a coherent learning pipeline: supervised training for pointwise and listwise rankers, policy optimization driven by response-level preferences, and knowledge distillation into a lightweight scorer for low-latency serving. Throughout this paper, memory refers to the model's working memory, which is the entire context visible to the LLM for In-Context Learning.   We adopt a dual-track evaluation protocol mirroring deployment: (i) large-scale online A/B ablations to isolate the utility of each feedback source, and (ii) few-shot offline tests on knowledge-intensive benchmarks. Online, a multi-month industrial deployment further shows substantial improvements in human engagement. Offline, DMA preserves competitive foundational retrieval while yielding notable gains on conversational QA (TriviaQA, HotpotQA). Taken together, these results position DMA as a principled approach to feedback-driven, real-time adaptation in RAG without sacrificing baseline capability.",
        "arxiv_id": "2511.04880",
        "ARXIVID": "2511.04880",
        "COMMENT": "Matches criterion 2 as it explores retrieval-augmented generation (RAG) systems with human feedback, which aligns with multimodal LLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.05356": {
        "authors": [
            "Manuel Gomes",
            "Bogdan Raducanu",
            "Miguel Oliveira"
        ],
        "title": "Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects",
        "abstract": "arXiv:2511.05356v1 Announce Type: new  Abstract: Articulated object perception presents significant challenges in computer vision, particularly because most existing methods ignore temporal dynamics despite the inherently dynamic nature of such objects. The use of 4D temporal data has not been thoroughly explored in articulated object perception and remains unexamined for panoptic segmentation. The lack of a benchmark dataset further hurt this field. To this end, we introduce Artic4D as a new dataset derived from PartNet Mobility and augmented with synthetic sensor data, featuring 4D panoptic annotations and articulation parameters. Building on this dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework. This approach explicitly estimates per-frame offsets mapping observed object parts to a learned canonical space, thereby enhancing part-level segmentation. The framework employs this canonical representation to achieve consistent alignment of object parts across sequential frames. Comprehensive experiments on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the art approaches in panoptic segmentation accuracy in more complex scenarios. These findings highlight the effectiveness of temporal modeling and canonical alignment in dynamic object understanding, and pave the way for future advances in 4D articulated object perception.",
        "arxiv_id": "2511.05356",
        "ARXIVID": "2511.05356",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Artic4D) and a novel method (CanonSeg4D) for 4D panoptic segmentation of articulated objects.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.05271": {
        "authors": [
            "Jack Hong",
            "Chenxiao Zhao",
            "ChengLin Zhu",
            "Weiheng Lu",
            "Guohai Xu",
            "Xing Yu"
        ],
        "title": "DeepEyesV2: Toward Agentic Multimodal Model",
        "abstract": "arXiv:2511.05271v1 Announce Type: new  Abstract: Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.",
        "arxiv_id": "2511.05271",
        "ARXIVID": "2511.05271",
        "COMMENT": "Matches criterion 2 as it explores agentic multimodal models and their integration of perception, search, and reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.05017": {
        "authors": [
            "Aakriti Agrawal",
            "Gouthaman KV",
            "Rohith Aralikatti",
            "Gauri Jagatap",
            "Jiaxin Yuan",
            "Vijay Kamarshi",
            "Andrea Fanelli",
            "Furong Huang"
        ],
        "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings",
        "abstract": "arXiv:2511.05017v1 Announce Type: new  Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.",
        "arxiv_id": "2511.05017",
        "ARXIVID": "2511.05017",
        "COMMENT": "Matches criterion 2 as it addresses hallucinations in Large Vision-Language Models and proposes a novel method for refining textual embeddings with visual features.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.04977": {
        "authors": [
            "Heng Er Metilda Chee",
            "Jiayin Wang",
            "Zhiqiang Guo",
            "Weizhi Ma",
            "Min Zhang"
        ],
        "title": "GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder",
        "abstract": "arXiv:2511.04977v1 Announce Type: new  Abstract: Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.",
        "arxiv_id": "2511.04977",
        "ARXIVID": "2511.04977",
        "COMMENT": "Matches criterion 2 as it explores multimodal models for sticker semantic similarity and introduces a benchmark (Triple-S).",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2511.05219": {
        "authors": [
            "Jiang Lin",
            "Xinyu Chen",
            "Song Wu",
            "Zhiqiu Zhang",
            "Jizhi Zhang",
            "Ye Wang",
            "Qiang Tang",
            "Qian Wang",
            "Jian Yang",
            "Zili Yi"
        ],
        "title": "FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction",
        "abstract": "arXiv:2511.05219v1 Announce Type: new  Abstract: Controlling the spatial and semantic structure of diffusion-generated images remains a challenge. Existing methods like ControlNet rely on handcrafted condition maps and retraining, limiting flexibility and generalization. Inversion-based approaches offer stronger alignment but incur high inference cost due to dual-path denoising. We present FreeControl, a training-free framework for semantic structural control in diffusion models. Unlike prior methods that extract attention across multiple timesteps, FreeControl performs one-step attention extraction from a single, optimally chosen key timestep and reuses it throughout denoising. This enables efficient structural guidance without inversion or retraining. To further improve quality and stability, we introduce Latent-Condition Decoupling (LCD): a principled separation of the key timestep and the noised latent used in attention extraction. LCD provides finer control over attention quality and eliminates structural artifacts. FreeControl also supports compositional control via reference images assembled from multiple sources - enabling intuitive scene layout design and stronger prompt alignment. FreeControl introduces a new paradigm for test-time control, enabling structurally and semantically aligned, visually coherent generation directly from raw images, with the flexibility for intuitive compositional design and compatibility with modern diffusion models at approximately 5 percent additional cost.",
        "arxiv_id": "2511.05219",
        "ARXIVID": "2511.05219",
        "COMMENT": "This paper does not directly match any specific criteria but discusses structural control in diffusion models, which may be tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.05449": {
        "authors": [
            "Tuan Anh Tran",
            "Duy M. H. Nguyen",
            "Hoai-Chau Tran",
            "Michael Barz",
            "Khoa D. Doan",
            "Roger Wattenhofer",
            "Ngo Anh Vien",
            "Mathias Niepert",
            "Daniel Sonntag",
            "Paul Swoboda"
        ],
        "title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?",
        "abstract": "arXiv:2511.05449v1 Announce Type: new  Abstract: Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at https://gitmerge3d.github.io",
        "arxiv_id": "2511.05449",
        "ARXIVID": "2511.05449",
        "COMMENT": "This paper does not directly match any specific criteria but discusses token efficiency in 3D point cloud transformers, which may be tangentially related to spatial intelligence and embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.05038": {
        "authors": [
            "Zhengxuan Li",
            "Qinhui Yang",
            "Yiyu Zhuang",
            "Chuan Guo",
            "Xinxin Zuo",
            "Xiaoxiao Long",
            "Yao Yao",
            "Xun Cao",
            "Qiu Shen",
            "Hao Zhu"
        ],
        "title": "Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance",
        "abstract": "arXiv:2511.05038v1 Announce Type: new  Abstract: We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.",
        "arxiv_id": "2511.05038",
        "ARXIVID": "2511.05038",
        "COMMENT": "Does not match any specific criteria but involves motion synthesis from pressure and text, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.04970": {
        "authors": [
            "Jian Wang",
            "Yixing Yong",
            "Haixia Bi",
            "Lijun He",
            "Fan Li"
        ],
        "title": "Learning Fourier shapes to probe the geometric world of deep neural networks",
        "abstract": "arXiv:2511.04970v1 Announce Type: new  Abstract: While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.",
        "arxiv_id": "2511.04970",
        "ARXIVID": "2511.04970",
        "COMMENT": "Does not match any specific criteria but explores geometric understanding in DNNs, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.05092": {
        "authors": [
            "Ruolin Li",
            "Min Liu",
            "Yuan Bian",
            "Zhaoyang Li",
            "Yuzhen Li",
            "Xueping Wang",
            "Yaonan Wang"
        ],
        "title": "A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification",
        "abstract": "arXiv:2511.05092v1 Announce Type: new  Abstract: With growing concerns over data privacy, researchers have started using virtual data as an alternative to sensitive real-world images for training person re-identification (Re-ID) models. However, existing virtual datasets produced by game engines still face challenges such as complex construction and poor domain generalization, making them difficult to apply in real scenarios. To address these challenges, we propose a Dual-stage Prompt-driven Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich prompts incorporating multi-dimensional attributes such as pedestrian appearance, illumination, and viewpoint that drive the diffusion model to synthesize diverse data end-to-end, building a large-scale virtual dataset named GenePerson with 130,519 images of 6,641 identities. In the second stage, we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn domain-invariant generalization features. With the aid of contrastive learning, we employ two textual inversion networks to map images into pseudo-words representing style and content, respectively, thereby constructing style-disentangled content prompts to guide the model in learning domain-invariant content features at the image level. Experiments demonstrate that models trained on GenePerson with PDM achieve state-of-the-art generalization performance, surpassing those on popular real and virtual Re-ID datasets.",
        "arxiv_id": "2511.05092",
        "ARXIVID": "2511.05092",
        "COMMENT": "Does not match any specific criteria but proposes a privacy-preserving paradigm for person re-identification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.04963": {
        "authors": [
            "Xiongri Shen",
            "Jiaqi Wang",
            "Yi Zhong",
            "Zhenxi Song",
            "Leilei Zhao",
            "Yichen Wei",
            "Lingyan Liang",
            "Shuqiang Wang",
            "Baiying Lei",
            "Demao Deng",
            "Zhiguo Zhang"
        ],
        "title": "Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement",
        "abstract": "arXiv:2511.04963v1 Announce Type: new  Abstract: Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time/gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores of 29.83 dB/90.84\\% for fMRI synthesis (+1.54 dB/+4.12\\% over baselines) and 30.00 dB/77.55\\% for dMRI synthesis (+1.02 dB/+2.2\\%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92\\%/66.02\\%/64.15\\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \\href{https://github.com/SXR3015/PDS}{PDS GitHub Repository}",
        "arxiv_id": "2511.04963",
        "ARXIVID": "2511.04963",
        "COMMENT": "Does not match any specific criteria but focuses on MRI modality synthesis using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05394": {
        "authors": [
            "Alexander Htet Kyaw",
            "Haotian Ma",
            "Sasa Zivkovic",
            "Jenny Sabin"
        ],
        "title": "AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly",
        "abstract": "arXiv:2511.05394v1 Announce Type: new  Abstract: We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.",
        "arxiv_id": "2511.05394",
        "ARXIVID": "2511.05394",
        "COMMENT": "This paper does not directly match any specific criteria but discusses AR-assisted assembly using object recognition, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.04949": {
        "authors": [
            "Tharindu Fernando",
            "Clinton Fookes",
            "Sridha Sridharan"
        ],
        "title": "DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning",
        "abstract": "arXiv:2511.04949v1 Announce Type: new  Abstract: Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.",
        "arxiv_id": "2511.04949",
        "ARXIVID": "2511.04949",
        "COMMENT": "This paper does not directly match any specific criteria but discusses watermarking for deepfake detection, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05403": {
        "authors": [
            "Zicong Fan",
            "Edoardo Remelli",
            "David Dimond",
            "Fadime Sener",
            "Liuhao Ge",
            "Bugra Tekin",
            "Cem Keskin",
            "Shreyas Hampali"
        ],
        "title": "PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior",
        "abstract": "arXiv:2511.05403v1 Announce Type: new  Abstract: The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM's scale and diversity make it a valuable real-world resource for hand modeling and related research.",
        "arxiv_id": "2511.05403",
        "ARXIVID": "2511.05403",
        "COMMENT": "Does not match any specific criteria but introduces a dataset and baseline for hand modeling, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05467": {
        "authors": [
            "Sanghyeon Chang",
            "Srikar Arani",
            "Nishant Sai Nuthalapati",
            "Youngjoon Suh",
            "Nicholas Choi",
            "Siavash Khodakarami",
            "Md Rakibul Hasan Roni",
            "Nenad Miljkovic",
            "Aparna Chandramowlishwaran",
            "Yoonjin Won"
        ],
        "title": "EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes",
        "abstract": "arXiv:2511.05467v1 Announce Type: new  Abstract: Flow boiling is an efficient heat transfer mechanism capable of dissipating high heat loads with minimal temperature variation, making it an ideal thermal management method. However, sudden shifts between flow regimes can disrupt thermal performance and system reliability, highlighting the need for accurate and low-latency real-time monitoring. Conventional optical imaging methods are limited by high computational demands and insufficient temporal resolution, making them inadequate for capturing transient flow behavior. To address this, we propose a real-time framework based on signals from neuromorphic sensors for flow regime classification. Neuromorphic sensors detect changes in brightness at individual pixels, which typically correspond to motion at edges, enabling fast and efficient detection without full-frame reconstruction, providing event-based information. We develop five classification models using both traditional image data and event-based data, demonstrating that models leveraging event data outperform frame-based approaches due to their sensitivity to dynamic flow features. Among these models, the event-based long short-term memory model provides the best balance between accuracy and speed, achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our asynchronous processing pipeline supports continuous, low-latency predictions and delivers stable output through a majority voting mechanisms, enabling reliable real-time feedback for experimental control and intelligent thermal management.",
        "arxiv_id": "2511.05467",
        "ARXIVID": "2511.05467",
        "COMMENT": "Does not match any specific criteria but involves event-driven classification, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.04951": {
        "authors": [
            "Hexu Zhao",
            "Xiwen Min",
            "Xiaoteng Liu",
            "Moonjun Gong",
            "Yiming Li",
            "Ang Li",
            "Saining Xie",
            "Jinyang Li",
            "Aurojit Panda"
        ],
        "title": "CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting",
        "abstract": "arXiv:2511.04951v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU's memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS's memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.",
        "arxiv_id": "2511.04951",
        "ARXIVID": "2511.04951",
        "COMMENT": "Does not match any specific criteria but focuses on memory optimization for 3D Gaussian Splatting, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05095": {
        "authors": [
            "Fuyang Liu",
            "Jiaqi Xu",
            "Xiaowei Hu"
        ],
        "title": "Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start",
        "abstract": "arXiv:2511.05095v1 Announce Type: new  Abstract: Adverse weather severely impairs real-world visual perception, while existing vision models trained on synthetic data with fixed parameters struggle to generalize to complex degradations. To address this, we first construct HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse weather phenomena, and then design a dual-level reinforcement learning framework initialized with HFLS-Weather for cold-start training. Within this framework, at the local level, weather-specific restoration models are refined through perturbation-driven image quality optimization, enabling reward-based learning without paired supervision; at the global level, a meta-controller dynamically orchestrates model selection and execution order according to scene degradation. This framework enables continuous adaptation to real-world conditions and achieves state-of-the-art performance across a wide range of adverse weather scenarios. Code is available at https://github.com/xxclfy/AgentRL-Real-Weather",
        "arxiv_id": "2511.05095",
        "ARXIVID": "2511.05095",
        "COMMENT": "Does not match any specific criteria but involves adverse weather image restoration, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05170": {
        "authors": [
            "Zijiang Yang",
            "Hanqing Chao",
            "Bokai Zhao",
            "Yelin Yang",
            "Yunshuo Zhang",
            "Dongmei Fu",
            "Junping Zhang",
            "Le Lu",
            "Ke Yan",
            "Dakai Jin",
            "Minfeng Xu",
            "Yun Bian",
            "Hui Jiang"
        ],
        "title": "MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification",
        "abstract": "arXiv:2511.05170v1 Announce Type: new  Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a fundamental task that underpins a wide range of high-level pathology applications. However, existing methods heavily rely on labor-intensive nucleus-level annotations and struggle to fully exploit large-scale unlabeled data for learning discriminative nucleus representations. In this work, we propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised learning method tailored for NDC. At its core is NuLo (Nucleus-based Local self-distillation), a coordinate-guided mechanism that enables flexible local self-distillation based on predicted nucleus positions. By removing the need for strict spatial alignment between augmented views, NuLo allows critical cross-scale alignment, thus unlocking the capacity of models for fine-grained nucleus-level representation. To support MUSE, we design a simple yet effective encoder-decoder architecture and a large field-of-view semi-supervised fine-tuning strategy that together maximize the value of unlabeled pathology images. Extensive experiments on three widely used benchmarks demonstrate that MUSE effectively addresses the core challenges of histopathological NDC. The resulting models not only surpass state-of-the-art supervised baselines but also outperform generic pathology foundation models.",
        "arxiv_id": "2511.05170",
        "ARXIVID": "2511.05170",
        "COMMENT": "Does not match any specific criteria but focuses on self-supervised learning for nucleus detection, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05044": {
        "authors": [
            "Xinyu Chen",
            "Yiran Wang",
            "Gaoyang Pang",
            "Jiafu Hao",
            "Chentao Yue",
            "Luping Zhou",
            "Yonghui Li"
        ],
        "title": "Medical Referring Image Segmentation via Next-Token Mask Prediction",
        "abstract": "arXiv:2511.05044v1 Announce Type: new  Abstract: Medical Referring Image Segmentation (MRIS) involves segmenting target regions in medical images based on natural language descriptions. While achieving promising results, recent approaches usually involve complex design of multimodal fusion or multi-stage decoders. In this work, we propose NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive next-token prediction task over a unified multimodal sequence of tokenized image, text, and mask representations. This formulation streamlines model design by eliminating the need for modality-specific fusion and external segmentation models, supports a unified architecture for end-to-end training. It also enables the use of pretrained tokenizers from emerging large-scale multimodal models, enhancing generalization and adaptability. More importantly, to address challenges under this formulation-such as exposure bias, long-tail token distributions, and fine-grained lesion edges-we propose three novel strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance boundary sensitivity and mitigate long-tail distribution effects, and (3) a memory-based Hard Error Token (HET) optimization strategy that emphasizes difficult tokens during training. Extensive experiments on the QaTa-COV19 and MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art performance, offering a streamlined and effective alternative to traditional MRIS pipelines.",
        "arxiv_id": "2511.05044",
        "ARXIVID": "2511.05044",
        "COMMENT": "Does not match any specific criteria but involves multimodal learning in medical image segmentation, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05073": {
        "authors": [
            "Jun Li",
            "Yanwei Xu",
            "Keran Li",
            "Xiaoli Zhang"
        ],
        "title": "Deep learning models are vulnerable, but adversarial examples are even more vulnerable",
        "abstract": "arXiv:2511.05073v1 Announce Type: new  Abstract: Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.",
        "arxiv_id": "2511.05073",
        "ARXIVID": "2511.05073",
        "COMMENT": "Does not match any specific criteria but focuses on adversarial example detection and robustness in deep learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.04811": {
        "authors": [
            "Shuo Zhao",
            "Yu Zhou",
            "Jianxu Chen"
        ],
        "title": "An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention",
        "abstract": "arXiv:2511.04811v1 Announce Type: new  Abstract: Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net's self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at https://github.com/MMV-Lab/AL_BioMed_img_seg.",
        "arxiv_id": "2511.04811",
        "ARXIVID": "2511.04811",
        "COMMENT": "Does not match any specific criteria but proposes an active learning pipeline for biomedical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.05059": {
        "authors": [
            "Mingyu Sheng",
            "Jianan Fan",
            "Dongnan Liu",
            "Guoyan Zheng",
            "Ron Kikinis",
            "Weidong Cai"
        ],
        "title": "SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery",
        "abstract": "arXiv:2511.05059v1 Announce Type: new  Abstract: During laparoscopic surgery, smoke generated by tissue cauterization can significantly degrade the visual quality of endoscopic frames, increasing the risk of surgical errors and hindering both clinical decision-making and computer-assisted visual analysis. Consequently, removing surgical smoke is critical to ensuring patient safety and maintaining operative efficiency. In this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical smoke removal. SurgiATM statistically bridges a physics-based atmospheric model and data-driven deep learning models, combining the superior generalizability of the former with the high accuracy of the latter. Furthermore, SurgiATM is designed as a lightweight, plug-and-play module that can be seamlessly integrated into diverse surgical desmoking architectures to enhance their accuracy and stability, better meeting clinical requirements. It introduces only two hyperparameters and no additional trainable weights, preserving the original network architecture with minimal computational and modification overhead. We conduct extensive experiments on three public surgical datasets with ten desmoking methods, involving multiple network architectures and covering diverse procedures, including cholecystectomy, partial nephrectomy, and diaphragm dissection. The results demonstrate that incorporating SurgiATM commonly reduces the restoration errors of existing models and relatively enhances their generalizability, without adding any trainable layers or weights. This highlights the convenience, low cost, effectiveness, and generalizability of the proposed method. The code for SurgiATM is released at https://github.com/MingyuShengSMY/SurgiATM.",
        "arxiv_id": "2511.05059",
        "ARXIVID": "2511.05059",
        "COMMENT": "Does not match any specific criteria but is an application of deep learning in surgical smoke removal.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.04803": {
        "authors": [
            "Shuo Zhao",
            "Jianxu Chen"
        ],
        "title": "Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose",
        "abstract": "arXiv:2511.04803v1 Announce Type: new  Abstract: Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at https://github.com/MMV-Lab/biomedseg-efficiency.",
        "arxiv_id": "2511.04803",
        "ARXIVID": "2511.04803",
        "COMMENT": "Does not match any specific criteria but is related to data efficiency and transfer robustness in biomedical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}