{
    "2507.23772": {
        "authors": [
            "Di Li",
            "Jie Feng",
            "Jiahao Chen",
            "Weisheng Dong",
            "Guanbin Li",
            "Yuhui Zheng",
            "Mingtao Feng",
            "Guangming Shi"
        ],
        "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting",
        "abstract": "arXiv:2507.23772v1 Announce Type: new  Abstract: 3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.",
        "arxiv_id": "2507.23772",
        "ARXIVID": "2507.23772",
        "COMMENT": "Matches criteria 1 and 3 closely as it introduces a novel method for sequential 3D affordance reasoning and a new benchmark for embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2507.23318": {
        "authors": [
            "Jiajun Cao",
            "Qizhe Zhang",
            "Peidong Jia",
            "Xuhui Zhao",
            "Bo Lan",
            "Xiaoan Zhang",
            "Xiaobao Wei",
            "Sixiang Chen",
            "Zhuo Li",
            "Yang Wang",
            "Liyun Li",
            "Xianming Liu",
            "Ming Lu",
            "Shanghang Zhang"
        ],
        "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning",
        "abstract": "arXiv:2507.23318v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential in complex scene understanding and action reasoning, leading to their increasing adoption in end-to-end autonomous driving systems. However, the long visual tokens of VLA models greatly increase computational costs. Current visual token pruning methods in Vision-Language Models (VLM) rely on either visual token similarity or visual-text attention, but both have shown poor performance in autonomous driving scenarios. Given that human drivers concentrate on relevant foreground areas while driving, we assert that retaining visual tokens containing this foreground information is essential for effective decision-making. Inspired by this, we propose FastDriveVLA, a novel reconstruction-based vision token pruning framework designed specifically for autonomous driving. FastDriveVLA includes a plug-and-play visual token pruner called ReconPruner, which prioritizes foreground information through MAE-style pixel reconstruction. A novel adversarial foreground-background reconstruction strategy is designed to train ReconPruner for the visual encoder of VLA models. Once trained, ReconPruner can be seamlessly applied to different VLA models with the same visual encoder without retraining. To train ReconPruner, we also introduce a large-scale dataset called nuScenes-FG, consisting of 241K image-mask pairs with annotated foreground regions. Our approach achieves state-of-the-art results on the nuScenes closed-loop planning benchmark across different pruning ratios.",
        "arxiv_id": "2507.23318",
        "ARXIVID": "2507.23318",
        "COMMENT": "Matches criterion 2 as it explores Vision-Language-Action models and introduces a novel token pruning framework for autonomous driving.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.23278": {
        "authors": [
            "Hao Tang",
            "Chenwei Xie",
            "Xiaoyi Bao",
            "Tingyu Weng",
            "Pandeng Li",
            "Yun Zheng",
            "Liwei Wang"
        ],
        "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing",
        "abstract": "arXiv:2507.23278v1 Announce Type: new  Abstract: In this paper, we propose UniLIP, which extends CLIP to reconstruction, generation and editing, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often require additional diffusion decoders or quantization to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance.In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dual-condition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLM's strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks.",
        "arxiv_id": "2507.23278",
        "ARXIVID": "2507.23278",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 5 (Integration of Image/Video and Large Language Models) as it extends CLIP for multimodal understanding, generation, and editing.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2507.23785": {
        "authors": [
            "Bowen Zhang",
            "Sicheng Xu",
            "Chuxin Wang",
            "Jiaolong Yang",
            "Feng Zhao",
            "Dong Chen",
            "Baining Guo"
        ],
        "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis",
        "abstract": "arXiv:2507.23785v1 Announce Type: new  Abstract: In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.",
        "arxiv_id": "2507.23785",
        "ARXIVID": "2507.23785",
        "COMMENT": "Matches criterion 6 as it focuses on video-to-4D synthesis, a novel video understanding task.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2507.23773": {
        "authors": [
            "Mingkai Deng",
            "Jinyu Hou",
            "Yilin Shen",
            "Hongxia Jin",
            "Graham Neubig",
            "Zhiting Hu",
            "Eric Xing"
        ],
        "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model",
        "abstract": "arXiv:2507.23773v1 Announce Type: new  Abstract: AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \\modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \\modelname improves the success of flight search from 0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent advantage of up to 124\\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \\modelname with pretrained LLMs, available as a research demo for public testing.",
        "arxiv_id": "2507.23773",
        "ARXIVID": "2507.23773",
        "COMMENT": "This paper aligns with Criterion 1 and Criterion 3 as it introduces a novel architecture for embodied agents using LLM-based world models, which is relevant to spatial intelligence and embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23202": {
        "authors": [
            "Chengwei Xia",
            "Fan Ma",
            "Ruijie Quan",
            "Kun Zhan",
            "Yi Yang"
        ],
        "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks",
        "abstract": "arXiv:2507.23202v1 Announce Type: new  Abstract: This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.",
        "arxiv_id": "2507.23202",
        "ARXIVID": "2507.23202",
        "COMMENT": "Matches criteria 2 as it proposes a novel adversarial attack method for multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23483": {
        "authors": [
            "Mutian Xu",
            "Chongjie Ye",
            "Haolin Liu",
            "Yushuang Wu",
            "Jiahao Chang",
            "Xiaoguang Han"
        ],
        "title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion",
        "abstract": "arXiv:2507.23483v1 Announce Type: new  Abstract: 3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes Stable-Diffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns. Project page: https://mutianxu.github.io/stable-sim2real/.",
        "arxiv_id": "2507.23483",
        "ARXIVID": "2507.23483",
        "COMMENT": "Matches criteria 3 as it explores a novel method for simulating real-captured 3D data, which is relevant for embodied AI tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23569": {
        "authors": [
            "Maxime Pietrantoni",
            "Gabriela Csurka",
            "Torsten Sattler"
        ],
        "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization",
        "abstract": "arXiv:2507.23569v1 Announce Type: new  Abstract: Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.",
        "arxiv_id": "2507.23569",
        "ARXIVID": "2507.23569",
        "COMMENT": "Matches criterion 3 as it introduces a new method for privacy-preserving visual localization using 3D Gaussian Splatting.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23374": {
        "authors": [
            "Shuangkang Fang",
            "I-Chao Shen",
            "Takeo Igarashi",
            "Yufeng Wang",
            "ZeSheng Wang",
            "Yi Yang",
            "Wenrui Ding",
            "Shuchang Zhou"
        ],
        "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
        "abstract": "arXiv:2507.23374v1 Announce Type: new  Abstract: We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.",
        "arxiv_id": "2507.23374",
        "ARXIVID": "2507.23374",
        "COMMENT": "Matches criterion 1 as it presents a novel methodological improvement in spatial reasoning for 3D scene representation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.23599": {
        "authors": [
            "Yuchen Zhou",
            "Yan Luo",
            "Xiangang Wang",
            "Xingjian Gu",
            "Mingzhou Lu"
        ],
        "title": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for Geometric Structure Preservation",
        "abstract": "arXiv:2507.23599v1 Announce Type: new  Abstract: Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring the performance of autonomous driving (AD) systems. However, many current methods focus on high accuracy at the expense of real-time processing needs. To address this challenge of balancing accuracy and inference speed, we propose a directional pure 2D approach. Our method involves slicing 3D voxel features to preserve complete vertical geometric information. This strategy compensates for the loss of height cues in Bird's-Eye View (BEV) representations, thereby maintaining the integrity of the 3D geometric structure. By employing a directional attention mechanism, we efficiently extract geometric features from different orientations, striking a balance between accuracy and computational efficiency. Experimental results highlight the significant advantages of our approach for autonomous driving. On the Occ3D-nuScenes, the proposed method achieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively balancing accuracy and efficiency. In simulations on edge devices, the inference speed reaches 14.8 FPS, further demonstrating the method's applicability for real-time deployment in resource-constrained environments.",
        "arxiv_id": "2507.23599",
        "ARXIVID": "2507.23599",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for efficient 3D voxel occupancy prediction, relevant for embodied AI in autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.23067": {
        "authors": [
            "Zhenyu Pan",
            "Yutong Zhang",
            "Jianshu Zhang",
            "Haoran Lu",
            "Haozheng Luo",
            "Yuwei Han",
            "Philip S. Yu",
            "Manling Li",
            "Han Liu"
        ],
        "title": "FairReason: Balancing Reasoning and Social Bias in MLLMs",
        "abstract": "arXiv:2507.23067v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art results across a wide range of tasks and modalities. To push their reasoning ability further, recent studies explore advanced prompting schemes and post-training fine-tuning. Although these techniques improve logical accuracy, they frequently leave the models' outputs burdened with pronounced social biases. Clarifying how reasoning gains interact with bias mitigation-and whether the two objectives inherently trade off-therefore remains an open and pressing research problem. Our study begins by benchmarking three bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation (KD), and rule-based reinforcement learning (RL)-under identical conditions, establishing their baseline strengths and weaknesses. Building on these results, we vary the proportion of debias-focused and reasoning-centric samples within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement learning cuts stereotype scores by 10% while retaining 88% of the model's original reasoning accuracy, offering concrete guidance for balancing fairness and capability in MLLMs.",
        "arxiv_id": "2507.23067",
        "ARXIVID": "2507.23067",
        "COMMENT": "Matches criteria 2 as it explores reasoning and bias mitigation in multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.23701": {
        "authors": [
            "Long Phan",
            "Mantas Mazeika",
            "Andy Zou",
            "Dan Hendrycks"
        ],
        "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
        "abstract": "arXiv:2507.23701v1 Announce Type: new  Abstract: Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.",
        "arxiv_id": "2507.23701",
        "ARXIVID": "2507.23701",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark for evaluating AI agents in text-based video games, focusing on long-context reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.23567": {
        "authors": [
            "Yung-Hsu Yang",
            "Luigi Piccinelli",
            "Mattia Segu",
            "Siyuan Li",
            "Rui Huang",
            "Yuqian Fu",
            "Marc Pollefeys",
            "Hermann Blum",
            "Zuria Bauer"
        ],
        "title": "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection",
        "abstract": "arXiv:2507.23567v1 Announce Type: new  Abstract: Monocular 3D object detection is valuable for various applications such as robotics and AR/VR. Existing methods are confined to closed-set settings, where the training and testing sets consist of the same scenes and/or object categories. However, real-world applications often introduce new environments and novel object categories, posing a challenge to these methods. In this paper, we address monocular 3D object detection in an open-set setting and introduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD). We propose to lift the open-set 2D detection into 3D space through our designed 3D bounding box head, enabling end-to-end joint training for both 2D and 3D tasks to yield better overall performance. We condition the object queries with geometry prior and overcome the generalization for 3D estimation across diverse scenes. To further improve performance, we design the canonical image space for more efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set settings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and achieve new state-of-the-art results. Code and models are available at royyang0714.github.io/3D-MOOD.",
        "arxiv_id": "2507.23567",
        "ARXIVID": "2507.23567",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on monocular 3D object detection in open-set settings, which involves foundational advancements in vision models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.23657": {
        "authors": [
            "Yang Gao",
            "Po-Chien Luan",
            "Kaouther Messaoud",
            "Lan Feng",
            "Alexandre Alahi"
        ],
        "title": "OmniTraj: Pre-Training on Heterogeneous Data for Adaptive and Zero-Shot Human Trajectory Prediction",
        "abstract": "arXiv:2507.23657v1 Announce Type: new  Abstract: While large-scale pre-training has advanced human trajectory prediction, a critical challenge remains: zero-shot transfer to unseen dataset with varying temporal dynamics. State-of-the-art pre-trained models often require fine-tuning to adapt to new datasets with different frame rates or observation horizons, limiting their scalability and practical utility. In this work, we systematically investigate this limitation and propose a robust solution. We first demonstrate that existing data-aware discrete models struggle when transferred to new scenarios with shifted temporal setups. We then isolate the temporal generalization from dataset shift, revealing that a simple, explicit conditioning mechanism for temporal metadata is a highly effective solution. Based on this insight, we present OmniTraj, a Transformer-based model pre-trained on a large-scale, heterogeneous dataset. Our experiments show that explicitly conditioning on the frame rate enables OmniTraj to achieve state-of-the-art zero-shot transfer performance, reducing prediction error by over 70\\% in challenging cross-setup scenarios. After fine-tuning, OmniTraj achieves state-of-the-art results on four datasets, including NBA, JTA, WorldPose, and ETH-UCY. The code is publicly available: https://github.com/vita-epfl/omnitraj",
        "arxiv_id": "2507.23657",
        "ARXIVID": "2507.23657",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new pre-training approach for human trajectory prediction, addressing temporal generalization challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.23251": {
        "authors": [
            "Fereshteh Aghaee Meibodi",
            "Shadi Alijani",
            "Homayoun Najjaran"
        ],
        "title": "A Deep Dive into Generic Object Tracking: A Survey",
        "abstract": "arXiv:2507.23251v1 Announce Type: new  Abstract: Generic object tracking remains an important yet challenging task in computer vision due to complex spatio-temporal dynamics, especially in the presence of occlusions, similar distractors, and appearance variations. Over the past two decades, a wide range of tracking paradigms, including Siamese-based trackers, discriminative trackers, and, more recently, prominent transformer-based approaches, have been introduced to address these challenges. While a few existing survey papers in this field have either concentrated on a single category or widely covered multiple ones to capture progress, our paper presents a comprehensive review of all three categories, with particular emphasis on the rapidly evolving transformer-based methods. We analyze the core design principles, innovations, and limitations of each approach through both qualitative and quantitative comparisons. Our study introduces a novel categorization and offers a unified visual and tabular comparison of representative methods. Additionally, we organize existing trackers from multiple perspectives and summarize the major evaluation benchmarks, highlighting the fast-paced advancements in transformer-based tracking driven by their robust spatio-temporal modeling capabilities.",
        "arxiv_id": "2507.23251",
        "ARXIVID": "2507.23251",
        "COMMENT": "Matches criteria 7 as it is a comprehensive survey on object tracking, focusing on transformer-based methods.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2507.23362": {
        "authors": [
            "Ji Ma",
            "Wei Suo",
            "Peng Wang",
            "Yanning Zhang"
        ],
        "title": "Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers",
        "abstract": "arXiv:2507.23362v1 Announce Type: new  Abstract: Although large vision-language models (LVLMs) have demonstrated impressive capabilities in multi-modal understanding and reasoning, their practical applications are still limited by massive model parameters and high computational costs. Recent efforts from natural language processing (NLP) have shown the effectiveness of layer pruning, offering a plausible training-free compression solution. However, due to the modality divergence between vision and language, it is unclear whether these NLP techniques are still effective in LVLMs. In this paper, we empirically prove that directly applying these layer pruning methods to LVLMs is ineffective. Through extensive experiments, we find that non-essential vision-language (VL) tokens and inter-layer feature gaps pose critical challenges to pruning layers in LVLMs. Based on these insights, we propose a novel framework Short-LVLM (SVL) that can utilize important VL tokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only achieves a superior trade-off between performance and efficiency but also exhibits several potential advantages, i.e., training-free, model-agnostic, and highly compatible. The code for this work is publicly available at https://github.com/ASGO-MM/Short-LVLM.",
        "arxiv_id": "2507.23362",
        "ARXIVID": "2507.23362",
        "COMMENT": "This paper matches Criterion 2 as it focuses on compressing and accelerating large vision-language models, which is relevant to advancements in VLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.23277": {
        "authors": [
            "Gyeongjin Kang",
            "Seungtae Nam",
            "Xiangyu Sun",
            "Sameh Khamis",
            "Abdelrahman Mohamed",
            "Eunbyung Park"
        ],
        "title": "iLRM: An Iterative Large 3D Reconstruction Model",
        "abstract": "arXiv:2507.23277v1 Announce Type: new  Abstract: Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.",
        "arxiv_id": "2507.23277",
        "ARXIVID": "2507.23277",
        "COMMENT": "This paper aligns with Criterion 3 as it introduces a novel method for scalable and efficient 3D reconstruction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.23601": {
        "authors": [
            "Xin Li",
            "Keren Fu",
            "Qijun Zhao"
        ],
        "title": "Mamba-based Efficient Spatio-Frequency Motion Perception for Video Camouflaged Object Detection",
        "abstract": "arXiv:2507.23601v1 Announce Type: new  Abstract: Existing video camouflaged object detection (VCOD) methods primarily rely on spatial appearance features to perceive motion cues for breaking camouflage. However, the high similarity between foreground and background in VCOD results in limited discriminability of spatial appearance features (e.g., color and texture), restricting detection accuracy and completeness. Recent studies demonstrate that frequency features can not only enhance feature representation to compensate for appearance limitations but also perceive motion through dynamic variations in frequency energy. Furthermore, the emerging state space model called Mamba, enables efficient perception of motion cues in frame sequences due to its linear-time long-sequence modeling capability. Motivated by this, we propose a novel visual camouflage Mamba (Vcamba) based on spatio-frequency motion perception that integrates frequency and spatial features for efficient and accurate VCOD. Specifically, we propose a receptive field visual state space (RFVSS) module to extract multi-scale spatial features after sequence modeling. For frequency learning, we introduce an adaptive frequency component enhancement (AFE) module with a novel frequency-domain sequential scanning strategy to maintain semantic consistency. Then we propose a space-based long-range motion perception (SLMP) module and a frequency-based long-range motion perception (FLMP) module to model spatio-temporal and frequency-temporal sequences in spatial and frequency phase domains. Finally, the space and frequency motion fusion module (SFMF) integrates dual-domain features for unified motion representation. Experimental results show that our Vcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2 datasets with lower computation cost, confirming the superiority of Vcamba. Our code is available at: https://github.com/BoydeLi/Vcamba.",
        "arxiv_id": "2507.23601",
        "ARXIVID": "2507.23601",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video camouflaged object detection, which involves video-based tasks and novel methodologies.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.23709": {
        "authors": [
            "Alfie Roddan",
            "Chi Xu",
            "Serine Ajlouni",
            "Irini Kakaletri",
            "Patra Charalampaki",
            "Stamatia Giannarou"
        ],
        "title": "Explainable Image Classification with Reduced Overconfidence for Tissue Characterisation",
        "abstract": "arXiv:2507.23709v1 Announce Type: new  Abstract: The deployment of Machine Learning models intraoperatively for tissue characterisation can assist decision making and guide safe tumour resections. For image classification models, pixel attribution methods are popular to infer explainability. However, overconfidence in deep learning model's predictions translates to overconfidence in pixel attribution. In this paper, we propose the first approach which incorporates risk estimation into a pixel attribution method for improved image classification explainability. The proposed method iteratively applies a classification model with a pixel attribution method to create a volume of PA maps. This volume is used for the first time, to generate a pixel-wise distribution of PA values. We introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. Hence, the proposed method not only provides an improved PA map but also produces an estimation of risk on the output PA values. Performance evaluation on probe-based Confocal Laser Endomicroscopy (pCLE) data and ImageNet verifies that our improved explainability method outperforms the state-of-the-art.",
        "arxiv_id": "2507.23709",
        "ARXIVID": "2507.23709",
        "COMMENT": "Does not match any specific criteria but focuses on explainability in image classification, which is tangentially related to computer vision.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2507.23751": {
        "authors": [
            "Ping Yu",
            "Jack Lanchantin",
            "Tianlu Wang",
            "Weizhe Yuan",
            "Olga Golovneva",
            "Ilia Kulikov",
            "Sainbayar Sukhbaatar",
            "Jason Weston",
            "Jing Xu"
        ],
        "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks",
        "abstract": "arXiv:2507.23751v1 Announce Type: new  Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard.",
        "arxiv_id": "2507.23751",
        "ARXIVID": "2507.23751",
        "COMMENT": "Does not closely match any specific criterion but is relevant to generative modeling and reasoning tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23373": {
        "authors": [
            "Haoran Chen",
            "Zexiao Wang",
            "Haidong Cao",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "title": "Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation",
        "abstract": "arXiv:2507.23373v1 Announce Type: new  Abstract: Large Vision-Language Models like CLIP have become a powerful foundation for Unsupervised Domain Adaptation due to their strong zero-shot generalization. State-of-the-art methods typically leverage CLIP to generate pseudo-labels for the target domain, then fine-tune the model to learn domain-invariant features. However, these methods attempt to align source and target domains using all pseudo-labeled data simultaneously. This one-shot alignment struggles with noisy, hard-to-classify samples, leading to error propagation and suboptimal feature learning. The problem is even more amplified in the multi-source scenario, where diverse domain gaps and varying noise levels across multiple source domains further destabilize the alignment process. To address this issue, in this work, we propose a progressive alignment strategy for adapting CLIP to unlabeled downstream task. Our method begins by training the model on a high-confidence subset of target samples, allowing it to first learn a well-aligned representation from the most reliable data. As training progresses, it gradually incorporates more challenging samples, guiding the model to refine its understanding without being overwhelmed by initial label noise. This progressive approach effectively mitigates confirmation bias and promotes a more robust convergence, allowing for the learning of genuinely domain-invariant features. We name our approach MP^2A and test it on three popular UDA benchmarks, namely ImageCLEF, Office-Home, and the most challenging DomainNet. Experiments showcase that MP^2A achieves state-of-the-art performance when compared with most recent CLIP-based MS-UDA approaches, demonstrating the effectiveness of our approach.",
        "arxiv_id": "2507.23373",
        "ARXIVID": "2507.23373",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to domain adaptation and vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23237": {
        "authors": [
            "Fan Lyu",
            "Linglan Zhao",
            "Chengyan Liu",
            "Yinying Mei",
            "Zhang Zhang",
            "Jian Zhang",
            "Fuyuan Hu",
            "Liang Wang"
        ],
        "title": "Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning",
        "abstract": "arXiv:2507.23237v1 Announce Type: new  Abstract: Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new concepts from limited data while retaining knowledge of previous classes. Recently, many studies have started to leverage unlabeled samples to assist models in learning from few-shot samples, giving rise to the field of Semi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However, these studies often assume that the source of unlabeled data is only confined to novel classes of the current session, which presents a narrow perspective and cannot align well with practical scenarios. To better reflect real-world scenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by incorporating both base and all the ever-seen novel classes in the unlabeled set. This change in the composition of unlabeled samples poses a new challenge for existing methods, as they struggle to distinguish between unlabeled samples from base and novel classes. To address this issue, we propose an Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC dynamically uses abundant base samples to correct biased feature distributions for few-shot novel classes. Experiments on three benchmark datasets show that our method outperforms existing works, setting new state-of-the-art results.",
        "arxiv_id": "2507.23237",
        "ARXIVID": "2507.23237",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to machine learning and incremental learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23143": {
        "authors": [
            "Xiaochen Zhao",
            "Hongyi Xu",
            "Guoxian Song",
            "You Xie",
            "Chenxu Zhang",
            "Xiu Li",
            "Linjie Luo",
            "Jinli Suo",
            "Yebin Liu"
        ],
        "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention",
        "abstract": "arXiv:2507.23143v1 Announce Type: new  Abstract: We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. We further enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance, our design eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models are available for research.",
        "arxiv_id": "2507.23143",
        "ARXIVID": "2507.23143",
        "COMMENT": "Does not match any specific criteria. Focuses on portrait animation using diffusion models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23554": {
        "authors": [
            "Ruoyu Wang",
            "Junda Wu",
            "Yu Xia",
            "Tong Yu",
            "Ryan A. Rossi",
            "Julian McAuley",
            "Lina Yao"
        ],
        "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer",
        "abstract": "arXiv:2507.23554v1 Announce Type: new  Abstract: Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks. However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance. In this paper, we address this challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance. Importantly, DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.",
        "arxiv_id": "2507.23554",
        "ARXIVID": "2507.23554",
        "COMMENT": "Does not match any specific criteria. Focuses on in-context learning for LLM agents, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.23162": {
        "authors": [
            "Xu Cao",
            "Takafumi Taketomi"
        ],
        "title": "Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues",
        "abstract": "arXiv:2507.23162v1 Announce Type: new  Abstract: We propose a neural inverse rendering approach that jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting. Unlike prior multi-view photometric stereo methods that require light calibration or intermediate cues such as per-view normal maps, our method jointly optimizes all scene parameters from raw images in a single stage. We represent both geometry and reflectance as neural implicit fields and apply shadow-aware volume rendering. A spatial network first predicts the signed distance and a reflectance latent code for each scene point. A reflectance network then estimates reflectance values conditioned on the latent code and angularly encoded surface normal, view, and light directions. The proposed method outperforms state-of-the-art normal-guided approaches in shape and lighting estimation accuracy, generalizes to view-unaligned multi-light images, and handles objects with challenging geometry and reflectance.",
        "arxiv_id": "2507.23162",
        "ARXIVID": "2507.23162",
        "COMMENT": "This paper does not directly match any specific criteria but is related to computer vision and machine learning, particularly in 3D reconstruction and photometric stereo.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23343": {
        "authors": [
            "Yingjie Zhou",
            "Jiezhang Cao",
            "Zicheng Zhang",
            "Farong Wen",
            "Yanwei Jiang",
            "Jun Jia",
            "Xiaohong Liu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads",
        "abstract": "arXiv:2507.23343v1 Announce Type: new  Abstract: Speech-driven methods for portraits are figuratively known as \"Talkers\" because of their capability to synthesize speaking mouth shapes and facial movements. Especially with the rapid development of the Text-to-Image (T2I) models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging digital human media. However, challenges persist regarding the quality of these talkers and AGTHs they generate, and comprehensive studies addressing these issues remain limited. To address this gap, this paper presents the largest AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset contains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the AGTHs and give the corresponding distortion categories. In our analysis for subjective experimental results, we evaluate the performance of talkers in terms of generalizability and quality, and also expose the distortions of existing AGTHs. Finally, an objective quality assessment method based on the first frame, Y-T slice and tone-lip consistency is proposed. Experimental results show that this method can achieve state-of-the-art (SOTA) performance in AGTH quality assessment. The work is released at https://github.com/zyj-2000/Talker.",
        "arxiv_id": "2507.23343",
        "ARXIVID": "2507.23343",
        "COMMENT": "Does not match any specific criteria but is tangentially related to AI-generated talking heads, which is outside the specified focus areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23253": {
        "authors": [
            "Mingyang Yu",
            "Xiahui Guo",
            "Peng chen",
            "Zhenkai Li",
            "Yang Shu"
        ],
        "title": "Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality",
        "abstract": "arXiv:2507.23253v1 Announce Type: new  Abstract: Time Series forecasting is critical in diverse domains such as weather forecasting, financial investment, and traffic management. While traditional numerical metrics like mean squared error (MSE) can quantify point-wise accuracy, they fail to evaluate the geometric structure of time series data, which is essential to understand temporal dynamics. To address this issue, we propose the time series Geometric Structure Index (TGSI), a novel evaluation metric that transforms time series into images to leverage their inherent two-dimensional geometric representations. However, since the image transformation process is non-differentiable, TGSI cannot be directly integrated as a training loss. We further introduce the Shape-Aware Temporal Loss (SATL), a multi-component loss function operating in the time series modality to bridge this gap and enhance structure modeling during training. SATL combines three components: a first-order difference loss that measures structural consistency through the MSE between first-order differences, a frequency domain loss that captures essential periodic patterns using the Fast Fourier Transform while minimizing noise, and a perceptual feature loss that measures geometric structure difference in time-series by aligning temporal features with geometric structure features through a pre-trained temporal feature extractor and time-series image autoencoder. Experiments across multiple datasets demonstrate that models trained with SATL achieve superior performance in both MSE and the proposed TGSI metrics compared to baseline methods, without additional computational cost during inference.",
        "arxiv_id": "2507.23253",
        "ARXIVID": "2507.23253",
        "COMMENT": "Does not match any specific criteria but is tangentially related to time-series modeling, which is outside the specified focus areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23336": {
        "authors": [
            "Ram Mohan Rao Kadiyala",
            "Siddhant Gupta",
            "Jebish Purbey",
            "Giulio Martini",
            "Suman Debnath",
            "Hamza Farooq"
        ],
        "title": "DSBC : Data Science task Benchmarking with Context engineering",
        "abstract": "arXiv:2507.23336v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents.",
        "arxiv_id": "2507.23336",
        "ARXIVID": "2507.23336",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to benchmarking and data science workflows.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23276": {
        "authors": [
            "Qiujie Xie",
            "Yixuan Weng",
            "Minjun Zhu",
            "Fuchen Shen",
            "Shulin Huang",
            "Zhen Lin",
            "Jiahui Zhou",
            "Zilan Mao",
            "Zijie Yang",
            "Linyi Yang",
            "Jian Wu",
            "Yue Zhang"
        ],
        "title": "How Far Are AI Scientists from Changing the World?",
        "abstract": "arXiv:2507.23276v1 Announce Type: new  Abstract: The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now taking the lead in scientific research. Several influential works have already appeared in the field of AI Scientist systems, with AI-generated research papers having been accepted at the ICLR 2025 workshop, suggesting that a human-level AI Scientist capable of uncovering phenomena previously unknown to humans, may soon become a reality. In this survey, we focus on the central question: How far are AI scientists from changing the world and reshaping the scientific research paradigm? To answer this question, we provide a prospect-driven review that comprehensively analyzes the current achievements of AI Scientist systems, identifying key bottlenecks and the critical components required for the emergence of a scientific agent capable of producing ground-breaking discoveries that solve grand challenges. We hope this survey will contribute to a clearer understanding of limitations of current AI Scientist systems, showing where we are, what is missing, and what the ultimate goals for scientific AI should be.",
        "arxiv_id": "2507.23276",
        "ARXIVID": "2507.23276",
        "COMMENT": "Does not closely match any specific criterion but is a survey paper on AI scientists, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23508": {
        "authors": [
            "Timing Li",
            "Bing Cao",
            "Jiahe Feng",
            "Haifang Cao",
            "Qinghau Hu",
            "Pengfei Zhu"
        ],
        "title": "Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion",
        "abstract": "arXiv:2507.23508v1 Announce Type: new  Abstract: Image fusion synthesizes complementary information from multiple sources, mitigating the inherent limitations of unimodal imaging systems. Accurate image registration is essential for effective multi-source data fusion. However, existing registration methods, often based on image translation in Euclidean space, fail to handle cross-modal misalignment effectively, resulting in suboptimal alignment and fusion quality. To overcome this limitation, we explore image alignment in non-Euclidean space and propose a Hyperbolic Cycle Alignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign is the first image registration method based on hyperbolic space. It introduces a dual-path cross-modal cyclic registration framework, in which a forward registration network aligns cross-modal inputs, while a backward registration network reconstructs the original image, forming a closed-loop registration structure with geometric consistency. Additionally, we design a Hyperbolic Hierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into hyperbolic space and imposes registration constraints, effectively reducing interference caused by modality discrepancies. We further analyze image registration in both Euclidean and hyperbolic spaces, demonstrating that hyperbolic space enables more sensitive and effective multi-modal image registration. Extensive experiments on misaligned multi-modal images demonstrate that our method significantly outperforms existing approaches in both image alignment and fusion. Our code will be publicly available.",
        "arxiv_id": "2507.23508",
        "ARXIVID": "2507.23508",
        "COMMENT": "Does not match any specific criteria. Focuses on image registration in hyperbolic space for infrared-visible image fusion, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23263": {
        "authors": [
            "Haoxian Ruan",
            "Zhihua Xu",
            "Zhijing Yang",
            "Guang Ma",
            "Jieming Xie",
            "Changxiang Fan",
            "Tianshui Chen"
        ],
        "title": "Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels",
        "abstract": "arXiv:2507.23263v1 Announce Type: new  Abstract: Multi-label image recognition with partial labels (MLR-PL) is designed to train models using a mix of known and unknown labels. Traditional methods rely on semantic or feature correlations to create pseudo-labels for unidentified labels using pre-set thresholds. This approach often overlooks the varying score distributions across categories, resulting in inaccurate and incomplete pseudo-labels, thereby affecting performance. In our study, we introduce the Semantic-Aware Threshold Learning (SATL) algorithm. This innovative approach calculates the score distribution for both positive and negative samples within each category and determines category-specific thresholds based on these distributions. These distributions and thresholds are dynamically updated throughout the learning process. Additionally, we implement a differential ranking loss to establish a significant gap between the score distributions of positive and negative samples, enhancing the discrimination of the thresholds. Comprehensive experiments and analysis on large-scale multi-label datasets, such as Microsoft COCO and VG-200, demonstrate that our method significantly improves performance in scenarios with limited labels.",
        "arxiv_id": "2507.23263",
        "ARXIVID": "2507.23263",
        "COMMENT": "Does not match any specific criteria. Focuses on multi-label image recognition with partial labels, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.23272": {
        "authors": [
            "Solha Kang",
            "Eugene Kim",
            "Joris Vankerschaver",
            "Utku Ozbulak"
        ],
        "title": "Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2",
        "abstract": "arXiv:2507.23272v1 Announce Type: new  Abstract: Breast MRI provides high-resolution volumetric imaging critical for tumor assessment and treatment planning, yet manual interpretation of 3D scans remains labor-intensive and subjective. While AI-powered tools hold promise for accelerating medical image analysis, adoption of commercial medical AI products remains limited in low- and middle-income countries due to high license costs, proprietary software, and infrastructure demands. In this work, we investigate whether the Segment Anything Model 2 (SAM2) can be adapted for low-cost, minimal-input 3D tumor segmentation in breast MRI. Using a single bounding box annotation on one slice, we propagate segmentation predictions across the 3D volume using three different slice-wise tracking strategies: top-to-bottom, bottom-to-top, and center-outward. We evaluate these strategies across a large cohort of patients and find that center-outward propagation yields the most consistent and accurate segmentations. Despite being a zero-shot model not trained for volumetric medical data, SAM2 achieves strong segmentation performance under minimal supervision. We further analyze how segmentation performance relates to tumor size, location, and shape, identifying key failure modes. Our results suggest that general-purpose foundation models such as SAM2 can support 3D medical image analysis with minimal supervision, offering an accessible and affordable alternative for resource-constrained settings.",
        "arxiv_id": "2507.23272",
        "ARXIVID": "2507.23272",
        "COMMENT": "This paper does not directly match any specific criteria but is related to computer vision applications in medical imaging, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}