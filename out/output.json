{
    "2509.19244": {
        "authors": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation",
        "abstract": "arXiv:2509.19244v1 Announce Type: new  Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM) capable of image understanding and generation tasks. Unlike existing multimodal diffsion language models such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O exhibits many new capabilities such as object grounding, image-editing, and high-resolution (1024px) image synthesis. It is also the first unified MDM that uses its understanding capabilities to improve image generation and editing results through planning and iterative self-reflection. To allow effective and efficient training and sampling, Lavida-O ntroduces many novel techniques such as Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling. \\ours~achieves state-of-the-art performance on a wide range of benchmarks such as RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference.",
        "arxiv_id": "2509.19244",
        "ARXIVID": "2509.19244",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a unified multimodal Masked Diffusion Model (MDM) for image understanding and generation tasks, integrating vision and language.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.17116": {
        "authors": [
            "Hang Xu",
            "Zang Yu",
            "Yehui Tang",
            "Pengbo Hu",
            "Yuhao Tang",
            "Hao Dong"
        ],
        "title": "MCTS-EP: Empowering Embodied Planning with Online Preference Optimization",
        "abstract": "arXiv:2509.17116v1 Announce Type: new  Abstract: This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning",
        "arxiv_id": "2509.17116",
        "ARXIVID": "2509.17116",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for training embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.19296": {
        "authors": [
            "Sherwin Bahmani",
            "Tianchang Shen",
            "Jiawei Ren",
            "Jiahui Huang",
            "Yifeng Jiang",
            "Haithem Turki",
            "Andrea Tagliasacchi",
            "David B. Lindell",
            "Zan Gojcic",
            "Sanja Fidler",
            "Huan Ling",
            "Jun Gao",
            "Xuanchi Ren"
        ],
        "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation",
        "abstract": "arXiv:2509.19296v1 Announce Type: new  Abstract: The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
        "arxiv_id": "2509.19296",
        "ARXIVID": "2509.19296",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D scene reconstruction using video diffusion models, relevant to embodied/robotic AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.18582": {
        "authors": [
            "Daiqing Qi",
            "Handong Zhao",
            "Jing Shi",
            "Simon Jenni",
            "Yifei Fan",
            "Franck Dernoncourt",
            "Scott Cohen",
            "Sheng Li"
        ],
        "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers",
        "abstract": "arXiv:2509.18582v1 Announce Type: new  Abstract: While editing directly from life, photographers have found it too difficult to see simultaneously both the blue and the sky. Photographer and curator, Szarkowski insightfully revealed one of the notable gaps between general and aesthetic visual understanding: while the former focuses on identifying the factual element in an image (sky), the latter transcends such object identification, viewing it instead as an aesthetic component--a pure color block (blue). Such fundamental distinctions between general (detection, localization, etc.) and aesthetic (color, lighting, composition, etc.) visual understanding present a significant challenge for Multimodal Large Language Models (MLLMs). Although some recent works have made initial explorations, they are often limited to general and basic aesthetic commonsense. As a result, they frequently fall short in real-world scenarios (Fig. 1), which require extensive expertise--including photographic techniques, photo pre/post-processing knowledge, and more, to provide a detailed analysis and description. To fundamentally enhance the aesthetics understanding of MLLMs, we first introduce a novel dataset, PhotoCritique, derived from extensive discussions among professional photographers and enthusiasts, and characterized by the large scale, expertise, and diversity. Then, to better learn visual aesthetics from PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a languageguided multi-view vision fusion mechanism to understand image aesthetics from multiple perspectives. Finally, we present a novel benchmark, PhotoBench, a comprehensive and professional benchmark for aesthetic visual understanding. On existing benchmarks and PhotoBench, our model demonstrates clear advantages over existing models.",
        "arxiv_id": "2509.18582",
        "ARXIVID": "2509.18582",
        "COMMENT": "Matches criterion 2 as it explores a multimodal large language model (MLLM) for aesthetic visual understanding, introducing a new dataset and benchmark.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.17158": {
        "authors": [
            "Pierre Andrews",
            "Amine Benhalloum",
            "Gerard Moreno-Torres Bertran",
            "Matteo Bettini",
            "Amar Budhiraja",
            "Ricardo Silveira Cabral",
            "Virginie Do",
            "Romain Froger",
            "Emilien Garreau",
            "Jean-Baptiste Gaya",
            "Hugo Lauren\\c{c}on",
            "Maxime Lecanu",
            "Kunal Malkan",
            "Dheeraj Mekala",
            "Pierre M\\'enard",
            "Gr\\'egoire Mialon",
            "Ulyana Piterbarg",
            "Mikhail Plekhanov",
            "Mathieu Rita",
            "Andrey Rusakov",
            "Thomas Scialom",
            "Vladislav Vorotilov",
            "Mengjue Wang",
            "Ian Yu"
        ],
        "title": "ARE: Scaling Up Agent Environments and Evaluations",
        "abstract": "arXiv:2509.17158v1 Announce Type: new  Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.",
        "arxiv_id": "2509.17158",
        "ARXIVID": "2509.17158",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Gaia2) and platform (ARE) for embodied AI, focusing on dynamic environments and agent evaluations.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.16924": {
        "authors": [
            "Jia Li",
            "Yinfeng Yu",
            "Liejun Wang",
            "Fuchun Sun",
            "Wendong Zheng"
        ],
        "title": "Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation",
        "abstract": "arXiv:2509.16924v1 Announce Type: new  Abstract: In audio-visual navigation (AVN) tasks, an embodied agent must autonomously localize a sound source in unknown and complex 3D environments based on audio-visual signals. Existing methods often rely on static modality fusion strategies and neglect the spatial cues embedded in stereo audio, leading to performance degradation in cluttered or occluded scenes. To address these issues, we propose an end-to-end reinforcement learning-based AVN framework with two key innovations: (1) a \\textbf{S}tereo-Aware \\textbf{A}ttention \\textbf{M}odule (\\textbf{SAM}), which learns and exploits the spatial disparity between left and right audio channels to enhance directional sound perception; and (2) an \\textbf{A}udio-\\textbf{G}uided \\textbf{D}ynamic \\textbf{F}usion Module (\\textbf{AGDF}), which dynamically adjusts the fusion ratio between visual and auditory features based on audio cues, thereby improving robustness to environmental changes. Extensive experiments are conducted on two realistic 3D scene datasets, Replica and Matterport3D, demonstrating that our method significantly outperforms existing approaches in terms of navigation success rate and path efficiency. Notably, our model achieves over 40\\% improvement under audio-only conditions compared to the best-performing baselines. These results highlight the importance of explicitly modeling spatial cues from stereo channels and performing deep multi-modal fusion for robust and efficient audio-visual navigation.",
        "arxiv_id": "2509.16924",
        "ARXIVID": "2509.16924",
        "COMMENT": "Matches criteria 1 and 3 as it introduces novel methods for spatial reasoning and embodied agents in audio-visual navigation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.18754": {
        "authors": [
            "Yuyang Liu",
            "Xinyuan Shi",
            "Bang Yang",
            "Peilin Zhou",
            "Jiahua Dong",
            "Long Chen",
            "Ian Reid",
            "Xiaondan Liang"
        ],
        "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage",
        "abstract": "arXiv:2509.18754v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs) has significantly propelled the research of video understanding. To harvest the benefits of well-trained expert models (i.e., tools), video LLMs prioritize the exploration of tool usage capabilities. Existing methods either prompt closed-source LLMs or employ the instruction tuning paradigm for tool-use fine-tuning. These methods, however, assume an established repository of fixed tools and struggle to generalize to real-world environments where tool data is perpetually evolving and streaming in. To this end, we propose to enhance open-source video LLMs with COntinuaL Tool usage (termed COLT), which automatically acquires tool-use ability in a successive tool stream without suffering 'catastrophic forgetting' of the past learned tools. Specifically, our COLT incorporates a learnable tool codebook as a tool-specific memory system. Then relevant tools are dynamically selected based on the similarity between user instruction and tool features within the codebook. To unleash the tool usage potential of video LLMs, we collect a video-centric tool-use instruction tuning dataset VideoToolBench. Extensive experiments on both previous video LLM benchmarks and the tool-use-specific VideoToolBench dataset demonstrate the state-of-the-art performance of our proposed COLT.",
        "arxiv_id": "2509.18754",
        "ARXIVID": "2509.18754",
        "COMMENT": "Matches criterion 2 as it enhances video large language models (Video LLMs) with continual tool usage, focusing on video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.18184": {
        "authors": [
            "Yifeng Cheng",
            "Alois Knoll",
            "Hu Cao"
        ],
        "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation",
        "abstract": "arXiv:2509.18184v1 Announce Type: new  Abstract: Event cameras provide high temporal resolution, high dynamic range, and low latency, offering significant advantages over conventional frame-based cameras. In this work, we introduce an uncertainty-aware refinement network called URNet for event-based stereo depth estimation. Our approach features a local-global refinement module that effectively captures fine-grained local details and long-range global context. Additionally, we introduce a Kullback-Leibler (KL) divergence-based uncertainty modeling method to enhance prediction reliability. Extensive experiments on the DSEC dataset demonstrate that URNet consistently outperforms state-of-the-art (SOTA) methods in both qualitative and quantitative evaluations.",
        "arxiv_id": "2509.18184",
        "ARXIVID": "2509.18184",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for event-based stereo depth estimation, which is relevant to embodied AI and robotics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.18350": {
        "authors": [
            "Oussema Dhaouadi",
            "Riccardo Marin",
            "Johannes Meier",
            "Jacques Kaiser",
            "Daniel Cremers"
        ],
        "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata",
        "abstract": "arXiv:2509.18350v1 Announce Type: new  Abstract: Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations. In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical. Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union). To fill this gap, we propose OrthoLoC, the first large-scale dataset comprising 16,425 UAV images from Germany and the United States with multiple modalities. The dataset addresses domain shifts between UAV imagery and geospatial data. Its paired structure enables fair benchmarking of existing solutions by decoupling image retrieval from feature matching, allowing isolated evaluation of localization and calibration performance. Through comprehensive evaluation, we examine the impact of domain shifts, data resolutions, and covisibility on localization accuracy. Finally, we introduce a refinement technique called AdHoP, which can be integrated with any feature matcher, improving matching by up to 95% and reducing translation error by up to 63%. The dataset and code are available at: https://deepscenario.github.io/OrthoLoC.",
        "arxiv_id": "2509.18350",
        "ARXIVID": "2509.18350",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark dataset for UAV localization and calibration, addressing domain shifts and localization challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.19191": {
        "authors": [
            "Yueyan Li",
            "Chenggong Zhao",
            "Zeyuan Zang",
            "Caixia Yuan",
            "Xiaojie Wang"
        ],
        "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models",
        "abstract": "arXiv:2509.19191v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the \"what\" and \"where\" pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model's perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.",
        "arxiv_id": "2509.19191",
        "ARXIVID": "2509.19191",
        "COMMENT": "Matches criterion 2 as it explores Vision-Language Models (VLMs) with a focus on visual processing and spatial reasoning improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.19245": {
        "authors": [
            "Benedetta Liberatori",
            "Alessandro Conti",
            "Lorenzo Vaquero",
            "Yiming Wang",
            "Elisa Ricci",
            "Paolo Rota"
        ],
        "title": "ConViS-Bench: Estimating Video Similarity Through Semantic Concepts",
        "abstract": "arXiv:2509.19245v1 Announce Type: new  Abstract: What does it mean for two videos to be similar? Videos may appear similar when judged by the actions they depict, yet entirely different if evaluated based on the locations where they were filmed. While humans naturally compare videos by taking different aspects into account, this ability has not been thoroughly studied and presents a challenge for models that often depend on broad global similarity scores. Large Multimodal Models (LMMs) with video understanding capabilities open new opportunities for leveraging natural language in comparative video tasks. We introduce Concept-based Video Similarity estimation (ConViS), a novel task that compares pairs of videos by computing interpretable similarity scores across a predefined set of key semantic concepts. ConViS allows for human-like reasoning about video similarity and enables new applications such as concept-conditioned video retrieval. To support this task, we also introduce ConViS-Bench, a new benchmark comprising carefully annotated video pairs spanning multiple domains. Each pair comes with concept-level similarity scores and textual descriptions of both differences and similarities. Additionally, we benchmark several state-of-the-art models on ConViS, providing insights into their alignment with human judgments. Our results reveal significant performance differences on ConViS, indicating that some concepts present greater challenges for estimating video similarity. We believe that ConViS-Bench will serve as a valuable resource for advancing research in language-driven video understanding.",
        "arxiv_id": "2509.19245",
        "ARXIVID": "2509.19245",
        "COMMENT": "Matches criterion 6 as it introduces a novel task and benchmark for video similarity estimation, advancing video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17544": {
        "authors": [
            "Juan Ca\\~nada",
            "Ra\\'ul Alonso",
            "Julio Molleda",
            "Fidel D\\'iez"
        ],
        "title": "A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data",
        "abstract": "arXiv:2509.17544v2 Announce Type: new  Abstract: The increasing availability of open Earth Observation (EO) and agricultural datasets holds great potential for supporting sustainable land management. However, their high technical entry barrier limits accessibility for non-expert users. This study presents an open-source conversational assistant that integrates multimodal retrieval and large language models (LLMs) to enable natural language interaction with heterogeneous agricultural and geospatial data. The proposed architecture combines orthophotos, Sentinel-2 vegetation indices, and user-provided documents through retrieval-augmented generation (RAG), allowing the system to flexibly determine whether to rely on multimodal evidence, textual knowledge, or both in formulating an answer. To assess response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional quantitative evaluation framework. Preliminary results show that the system is capable of generating clear, relevant, and context-aware responses to agricultural queries, while remaining reproducible and scalable across geographic regions. The primary contributions of this work include an architecture for fusing multimodal EO and textual knowledge sources, a demonstration of lowering the barrier to access specialized agricultural information through natural language interaction, and an open and reproducible design.",
        "arxiv_id": "2509.17544",
        "ARXIVID": "2509.17544",
        "COMMENT": "Matches criterion 5 as it integrates geospatial image data with large language models for multimodal interaction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.19297": {
        "authors": [
            "Weijie Wang",
            "Yeqing Chen",
            "Zeyu Zhang",
            "Hengyu Liu",
            "Haoxiao Wang",
            "Zhiyuan Feng",
            "Wenkang Qin",
            "Zheng Zhu",
            "Donny Y. Chen",
            "Bohan Zhuang"
        ],
        "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction",
        "abstract": "arXiv:2509.19297v1 Announce Type: new  Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
        "arxiv_id": "2509.19297",
        "ARXIVID": "2509.19297",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D reconstruction with voxel-aligned Gaussians, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.18912": {
        "authors": [
            "Yunzhe Shen",
            "Kai Peng",
            "Leiye Liu",
            "Wei Ji",
            "Jingjing Li",
            "Miao Zhang",
            "Yongri Piao",
            "Huchuan Lu"
        ],
        "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation",
        "abstract": "arXiv:2509.18912v1 Announce Type: new  Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine learning by effectively integrating audio and visual cues to precisely segment objects or regions within visual scenes. Recent AVS methods have demonstrated significant improvements. However, they overlook the inherent frequency-domain contradictions between audio and visual modalities--the pervasively interfering noise in audio high-frequency signals vs. the structurally rich details in visual high-frequency signals. Ignoring these differences can result in suboptimal performance. In this paper, we rethink the AVS task from a deeper perspective by reformulating AVS task as a frequency-domain decomposition and recomposition problem. To this end, we introduce a novel Frequency-Aware Audio-Visual Segmentation (FAVS) framework consisting of two key modules: Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal Consistency (SCMC) module. FDED module employs a residual-based iterative frequency decomposition to discriminate modality-specific semantics and structural features, and SCMC module leverages a mixture-of-experts architecture to reinforce semantic consistency and modality-specific feature preservation through dynamic expert routing. Extensive experiments demonstrate that our FAVS framework achieves state-of-the-art performance on three benchmark datasets, and abundant qualitative visualizations further verify the effectiveness of the proposed FDED and SCMC modules. The code will be released as open source upon acceptance of the paper.",
        "arxiv_id": "2509.18912",
        "ARXIVID": "2509.18912",
        "COMMENT": "Matches criterion 6 as it focuses on audio-visual segmentation, a video-based task, with novel methodologies for frequency-domain decomposition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.19002": {
        "authors": [
            "Hao Wang",
            "Eiki Murata",
            "Lingfang Zhang",
            "Ayako Sato",
            "So Fukuda",
            "Ziqi Yin",
            "Wentao Hu",
            "Keisuke Nakao",
            "Yusuke Nakamura",
            "Sebastian Zwirner",
            "Yi-Chia Chen",
            "Hiroyuki Otomo",
            "Hiroki Ouchi",
            "Daisuke Kawahara"
        ],
        "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
        "abstract": "arXiv:2509.19002v1 Announce Type: new  Abstract: Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
        "arxiv_id": "2509.19002",
        "ARXIVID": "2509.19002",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VIR-Bench) for evaluating geospatial and temporal understanding in multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.18839": {
        "authors": [
            "Gianmarco Spinaci (Department of Classical Philology and Italian Studies",
            "University of Bologna",
            "Italy",
            "Villa i Tatti",
            "The Harvard University Center for Italian Renaissance Studies",
            "Florence",
            "Italy)",
            "Lukas Klic (Villa i Tatti",
            "The Harvard University Center for Italian Renaissance Studies",
            "Florence",
            "Italy)",
            "Giovanni Colavizza (Department of Classical Philology and Italian Studies",
            "University of Bologna",
            "Italy",
            "Department of Communication",
            "University of Copenhagen",
            "Denmark)"
        ],
        "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography",
        "abstract": "arXiv:2509.18839v1 Announce Type: new  Abstract: This study evaluates the capabilities of Multimodal Large Language Models (LLMs) and Vision Language Models (VLMs) in the task of single-label classification of Christian Iconography. The goal was to assess whether general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5, can interpret the Iconography, typically addressed by supervised classifiers, and evaluate their performance. Two research questions guided the analysis: (RQ1) How do multimodal LLMs perform on image classification of Christian saints? And (RQ2), how does performance vary when enriching input with contextual information or few-shot exemplars? We conducted a benchmarking study using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and Wikidata, filtered to include the top 10 most frequent classes. Models were tested under three conditions: (1) classification using class labels, (2) classification with Iconclass descriptions, and (3) few-shot learning with five exemplars. Results were compared against ResNet50 baselines fine-tuned on the same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset, where Siglip reached the highest accuracy score, suggesting model sensitivity to image size and metadata alignment. Enriching prompts with class descriptions generally improved zero-shot performance, while few-shot learning produced lower results, with only occasional and minimal increments in accuracy. We conclude that general-purpose multimodal LLMs are capable of classification in visually complex cultural heritage domains. These results support the application of LLMs as metadata curation tools in digital humanities workflows, suggesting future research on prompt optimization and the expansion of the study to other classification strategies and models.",
        "arxiv_id": "2509.18839",
        "ARXIVID": "2509.18839",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it evaluates multimodal LLMs and VLMs in a specific domain.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.18897": {
        "authors": [
            "Jiayu Wang",
            "Ruizhi Wang",
            "Jie Song",
            "Haofei Zhang",
            "Mingli Song",
            "Zunlei Feng",
            "Li Sun"
        ],
        "title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing",
        "abstract": "arXiv:2509.18897v1 Announce Type: new  Abstract: In this paper, we introduce a novel benchmark designed to propel the advancement of general-purpose, large-scale 3D vision models for remote sensing imagery. While several datasets have been proposed within the realm of remote sensing, many existing collections either lack comprehensive depth information or fail to establish precise alignment between depth data and remote sensing images. To address this deficiency, we present a visual Benchmark for 3D understanding of Remotely Sensed images, dubbed RS3DBench. This dataset encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth maps, accompanied by corresponding textual descriptions, spanning a broad array of geographical contexts. It serves as a tool for training and assessing 3D visual perception models within remote sensing image spatial understanding tasks. Furthermore, we introduce a remotely sensed depth estimation model derived from stable diffusion, harnessing its multimodal fusion capabilities, thereby delivering state-of-the-art performance on our dataset. Our endeavor seeks to make a profound contribution to the evolution of 3D visual perception models and the advancement of geographic artificial intelligence within the remote sensing domain. The dataset, models and code will be accessed on the https://rs3dbench.github.io.",
        "arxiv_id": "2509.18897",
        "ARXIVID": "2509.18897",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a benchmark for 3D spatial perception in remote sensing.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.19082": {
        "authors": [
            "Alexey Nekrasov",
            "Ali Athar",
            "Daan de Geus",
            "Alexander Hermans",
            "Bastian Leibe"
        ],
        "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference",
        "abstract": "arXiv:2509.19082v1 Announce Type: new  Abstract: Sa2VA is a recent model for language-guided dense grounding in images and video that achieves state-of-the-art results on multiple segmentation benchmarks and that has become widely popular. However, we found that Sa2VA does not perform according to its full potential for referring video object segmentation tasks. We identify inconsistencies between training and inference procedures as the key factor holding it back. To mitigate this issue, we propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and improves the results. In fact, Sa2VA-i sets a new state of the art for multiple video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the original Sa2VA-26B model on the MeViS benchmark. We hope that this work will show the importance of seemingly trivial implementation details and that it will provide valuable insights for the referring video segmentation field. We provide the code and updated models at https://github.com/kumuji/sa2va-i",
        "arxiv_id": "2509.19082",
        "ARXIVID": "2509.19082",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on improving video object segmentation benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.17589": {
        "authors": [
            "Jun Ling",
            "Yao Qi",
            "Tao Huang",
            "Shibo Zhou",
            "Yanqin Huang",
            "Jiang Yang",
            "Ziqi Song",
            "Ying Zhou",
            "Yang Yang",
            "Heng Tao Shen",
            "Peng Wang"
        ],
        "title": "Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models",
        "abstract": "arXiv:2509.17589v1 Announce Type: new  Abstract: In this work, we address the task of table image to LaTeX code generation, with the goal of automating the reconstruction of high-quality, publication-ready tables from visual inputs. A central challenge of this task lies in accurately handling complex tables -- those with large sizes, deeply nested structures, and semantically rich or irregular cell content -- where existing methods often fail. We begin with a comprehensive analysis, identifying key challenges and highlighting the limitations of current evaluation protocols. To overcome these issues, we propose a reinforced multimodal large language model (MLLM) framework, where a pre-trained MLLM is fine-tuned on a large-scale table-to-LaTeX dataset. To further improve generation quality, we introduce a dual-reward reinforcement learning strategy based on Group Relative Policy Optimization (GRPO). Unlike standard approaches that optimize purely over text outputs, our method incorporates both a structure-level reward on LaTeX code and a visual fidelity reward computed from rendered outputs, enabling direct optimization of the visual output quality. We adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and show that our method achieves state-of-the-art performance, particularly on structurally complex tables, demonstrating the effectiveness and robustness of our approach.",
        "arxiv_id": "2509.17589",
        "ARXIVID": "2509.17589",
        "COMMENT": "Matches criterion 5 as it combines image understanding (table images) with large language models for LaTeX code generation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.17567": {
        "authors": [
            "Yang Xiao",
            "Mohan Jiang",
            "Jie Sun",
            "Keyu Li",
            "Jifan Lin",
            "Yumin Zhuang",
            "Ji Zeng",
            "Shijie Xia",
            "Qishuo Hua",
            "Xuefeng Li",
            "Xiaojie Cai",
            "Tongyu Wang",
            "Yue Zhang",
            "Liming Liu",
            "Xia Wu",
            "Jinlong Hou",
            "Yuan Cheng",
            "Wenjie Li",
            "Xiang Wang",
            "Dequan Wang",
            "Pengfei Liu"
        ],
        "title": "LIMI: Less is More for Agency",
        "abstract": "arXiv:2509.17567v1 Announce Type: new  Abstract: We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.",
        "arxiv_id": "2509.17567",
        "ARXIVID": "2509.17567",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for cultivating agency in AI systems with minimal data.",
        "RELEVANCE": 5,
        "NOVELTY": 8
    },
    "2509.19090": {
        "authors": [
            "Guoxin Wang",
            "Jun Zhao",
            "Xinyi Liu",
            "Yanbo Liu",
            "Xuyang Cao",
            "Chao Li",
            "Zhuoyun Liu",
            "Qintian Sun",
            "Fangru Zhou",
            "Haoqiang Xing",
            "Zhenhong Yang"
        ],
        "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning",
        "abstract": "arXiv:2509.19090v1 Announce Type: new  Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment planning, and surgical decisions, yet most existing imaging models are narrowly focused and require multiple specialized networks, limiting their generalization. Although large-scale language and multimodal models exhibit strong reasoning and multi-task capabilities, real-world clinical applications demand precise visual grounding, multimodal integration, and chain-of-thought reasoning. We introduce Citrus-V, a multimodal medical foundation model that combines image analysis with textual reasoning. The model integrates detection, segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level lesion localization, structured report generation, and physician-like diagnostic inference in a single framework. We propose a novel multimodal training approach and release a curated open-source data suite covering reasoning, detection, segmentation, and document understanding tasks. Evaluations demonstrate that Citrus-V outperforms existing open-source medical models and expert-level imaging systems across multiple benchmarks, delivering a unified pipeline from visual grounding to clinical reasoning and supporting precise lesion quantification, automated reporting, and reliable second opinions.",
        "arxiv_id": "2509.19090",
        "ARXIVID": "2509.19090",
        "COMMENT": "Matches criteria 5 as it integrates image understanding with multimodal reasoning in a medical context.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.18425": {
        "authors": [
            "Philip Wootaek Shin",
            "Jack Sampson",
            "Vijaykrishnan Narayanan",
            "Andres Marquez",
            "Mahantesh Halappanavar"
        ],
        "title": "Losing the Plot: How VLM responses degrade on imperfect charts",
        "abstract": "arXiv:2509.18425v1 Announce Type: new  Abstract: Vision language models (VLMs) show strong results on chart understanding, yet existing benchmarks assume clean figures and fact based queries. Real world charts often contain distortions and demand reasoning beyond simple matching. We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp performance drops under corruption or occlusion, with hallucinations such as value fabrication, trend misinterpretation, and entity confusion becoming more frequent. Models remain overconfident in degraded settings, generating plausible but unsupported explanations.   To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers, and Reasoning Testing on Noisy and Occluded Input Selections), a dataset combining chart corruptions, occlusions, and exam style multiple choice questions inspired by Korea's CSAT English section. A key innovation is prompt reverse inconsistency, where models contradict themselves when asked to confirm versus deny the same statement. Our contributions are threefold: (1) benchmarking state of the art VLMs, exposing systematic vulnerabilities in chart reasoning; (2) releasing CHART NOISe, the first dataset unifying corruption, occlusion, and reverse inconsistency; and (3) proposing baseline mitigation strategies such as quality filtering and occlusion detection. Together, these efforts establish a rigorous testbed for advancing robustness and reliability in chart understanding.",
        "arxiv_id": "2509.18425",
        "ARXIVID": "2509.18425",
        "COMMENT": "Matches criteria 2 as it evaluates vulnerabilities in vision-language models (VLMs) and introduces a new dataset for robustness testing.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.18639": {
        "authors": [
            "Yuanhuiyi Lyu",
            "Chi Kit Wong",
            "Chenfei Liao",
            "Lutao Jiang",
            "Xu Zheng",
            "Zexin Lu",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation",
        "abstract": "arXiv:2509.18639v1 Announce Type: new  Abstract: Recent works have made notable advancements in enhancing unified models for text-to-image generation through the Chain-of-Thought (CoT). However, these reasoning methods separate the processes of understanding and generation, which limits their ability to guide the reasoning of unified models in addressing the deficiencies of their generative capabilities. To this end, we propose a novel reasoning framework for unified models, Understanding-in-Generation (UiG), which harnesses the robust understanding capabilities of unified models to reinforce their performance in image generation. The core insight of our UiG is to integrate generative guidance by the strong understanding capabilities during the reasoning process, thereby mitigating the limitations of generative abilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse understanding into the generation process. Initially, we verify the generated image and incorporate the understanding of unified models into the editing instructions. Subsequently, we enhance the generated image step by step, gradually infusing the understanding into the generation process. Our UiG framework demonstrates a significant performance improvement in text-to-image generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on the long prompt setting of the TIIF benchmark. The project code: https://github.com/QC-LY/UiG",
        "arxiv_id": "2509.18639",
        "ARXIVID": "2509.18639",
        "COMMENT": "Matches criterion 5 as it integrates image generation tasks with understanding capabilities in unified models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.18894": {
        "authors": [
            "Jenna Kline",
            "Anirudh Potlapally",
            "Bharath Pillai",
            "Tanishka Wani",
            "Rugved Katole",
            "Vedant Patil",
            "Penelope Covey",
            "Hari Subramoni",
            "Tanya Berger-Wolf",
            "Christopher Stewart"
        ],
        "title": "SmartWilds: Multimodal Wildlife Monitoring Dataset",
        "abstract": "arXiv:2509.18894v1 Announce Type: new  Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring dataset. SmartWilds is a synchronized collection of drone imagery, camera trap photographs and videos, and bioacoustic recordings collected during summer 2025 at The Wilds safari park in Ohio. This dataset supports multimodal AI research for comprehensive environmental monitoring, addressing critical needs in endangered species research, conservation ecology, and habitat management. Our pilot deployment captured four days of synchronized monitoring across three modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin, Przewalski's horses, as well as species native to Ohio, including bald eagles, white-tailed deer, and coyotes. We provide a comparative analysis of sensor modality performance, demonstrating complementary strengths for landuse patterns, species detection, behavioral analysis, and habitat monitoring. This work establishes reproducible protocols for multimodal wildlife monitoring while contributing open datasets to advance conservation computer vision research. Future releases will include synchronized GPS tracking data from tagged individuals, citizen science data, and expanded temporal coverage across multiple seasons.",
        "arxiv_id": "2509.18894",
        "ARXIVID": "2509.18894",
        "COMMENT": "Matches criterion 6 as it introduces a multimodal wildlife monitoring dataset, which includes video data and supports video understanding tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.19207": {
        "authors": [
            "Israfel Salazar",
            "Desmond Elliott",
            "Yova Kementchedjhieva"
        ],
        "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs",
        "abstract": "arXiv:2509.19207v1 Announce Type: new  Abstract: Contrastive vision-language models (VLMs) have made significant progress in binding visual and textual information, but understanding long, dense captions remains an open challenge. We hypothesize that compositionality, the capacity to reason about object-attribute bindings and inter-object relationships, is key to understanding longer captions. In this paper, we investigate the interaction between compositionality and long-caption understanding, asking whether training for one property enhances the other. We train and evaluate a range of models that target each of these capabilities. Our results reveal a bidirectional relationship: compositional training improves performance on long-caption retrieval, and training on long captions promotes compositionality. However, these gains are sensitive to data quality and model design. We find that training on poorly structured captions, or with limited parameter updates, fails to support generalization. Likewise, strategies that aim at retaining general alignment, such as freezing positional embeddings, do not improve compositional understanding. Overall, we find that compositional understanding and long-caption understanding are intertwined capabilities that can be jointly learned through training on dense, grounded descriptions. Despite these challenges, we show that models trained on high-quality, long-caption data can achieve strong performance in both tasks, offering practical guidance for improving VLM generalization.",
        "arxiv_id": "2509.19207",
        "ARXIVID": "2509.19207",
        "COMMENT": "Matches criterion 2 as it investigates vision-language models (VLMs) with a focus on compositionality and long-caption understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.18711": {
        "authors": [
            "Ke Li",
            "Di Wang",
            "Ting Wang",
            "Fuyu Dong",
            "Yiming Zhang",
            "Luyao Zhang",
            "Xiangyu Wang",
            "Shaofeng Li",
            "Quan Wang"
        ],
        "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
        "abstract": "arXiv:2509.18711v1 Announce Type: new  Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.",
        "arxiv_id": "2509.18711",
        "ARXIVID": "2509.18711",
        "COMMENT": "Matches criterion 5 as it explores a framework combining vision-language models and diffusion models for visual grounding tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.18189": {
        "authors": [
            "Daxiang Dong",
            "Mingming Zheng",
            "Dong Xu",
            "Bairong Zhuang",
            "Wenyu Zhang",
            "Chunhua Luo",
            "Haoran Wang",
            "Zijian Zhao",
            "Jie Li",
            "Yuxuan Li",
            "Hanjun Zhong",
            "Mengyue Liu",
            "Jieting Chen",
            "Shupeng Li",
            "Lun Tian",
            "Yaping Feng",
            "Xin Li",
            "Donggang Jiang",
            "Yong Chen",
            "Yehua Xu",
            "Duohao Qin",
            "Chen Feng",
            "Dan Wang",
            "Henghua Zhang",
            "Jingjing Ha",
            "Jinhui He",
            "Yanfeng Zhai",
            "Chengxin Zheng",
            "Jiayi Mao",
            "Jiacheng Chen",
            "Ruchang Yao",
            "Ziye Yuan",
            "Jianmin Wu",
            "Guangjun Xie",
            "Dou Shen"
        ],
        "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models",
        "abstract": "arXiv:2509.18189v1 Announce Type: new  Abstract: We present Qianfan-VL, a series of multimodal large language models ranging from 3B to 70B parameters, achieving state-of-the-art performance through innovative domain enhancement techniques. Our approach employs multi-stage progressive training and high-precision data synthesis pipelines, which prove to be critical technologies for enhancing domain-specific capabilities while maintaining strong general performance. Qianfan-VL achieves comparable results to leading open-source models on general benchmarks, with state-of-the-art performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and MMStar. The domain enhancement strategy delivers significant advantages in OCR and document understanding, validated on both public benchmarks (OCRBench 873, DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B variants incorporate long chain-of-thought capabilities, demonstrating superior performance on mathematical reasoning (MathVista 78.6%) and logical inference tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating the capability of large-scale AI infrastructure to train SOTA-level multimodal models with over 90% scaling efficiency on 5000 chips for a single task. This work establishes an effective methodology for developing domain-enhanced multimodal models suitable for diverse enterprise deployment scenarios.",
        "arxiv_id": "2509.18189",
        "ARXIVID": "2509.18189",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal large language model with domain-specific enhancements for vision-language tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.18183": {
        "authors": [
            "Jinyue Bian",
            "Zhaoxing Zhang",
            "Zhengyu Liang",
            "Shiwei Zheng",
            "Shengtao Zhang",
            "Rong Shen",
            "Chen Yang",
            "Anzhou Hou"
        ],
        "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation",
        "abstract": "arXiv:2509.18183v1 Announce Type: new  Abstract: The Visual-Language-Action (VLA) models can follow text instructions according to visual observations of the surrounding environment. This ability to map multimodal inputs to actions is derived from the training of the VLA model on extensive standard demonstrations. These visual observations captured by third-personal global and in-wrist local cameras are inevitably varied in number and perspective across different environments, resulting in significant differences in the visual features. This perspective heterogeneity constrains the generality of VLA models. In light of this, we first propose the lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models using only 2D data. VLA-LPAF is finetuned using images from a single view and fuses other multiview observations in the latent space, which effectively and efficiently bridge the gap caused by perspective inconsistency. We instantiate our VLA-LPAF framework with the VLA model RoboFlamingo to construct RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. We also demonstrate the developed viewadaptive characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.",
        "arxiv_id": "2509.18183",
        "ARXIVID": "2509.18183",
        "COMMENT": "Matches criterion 3 as it proposes a lightweight module for improving perspective adaptivity in vision-language-action models for robotic manipulation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.18638": {
        "authors": [
            "Yiwei Lyu",
            "Samir Harake",
            "Asadur Chowdury",
            "Soumyanil Banerjee",
            "Rachel Gologorsky",
            "Shixuan Liu",
            "Anna-Katharina Meissner",
            "Akshay Rao",
            "Chenhui Zhao",
            "Akhil Kondepudi",
            "Cheng Jiang",
            "Xinhai Hou",
            "Rushikesh S. Joshi",
            "Volker Neuschmelting",
            "Ashok Srinivasan",
            "Dawn Kleindorfer",
            "Brian Athey",
            "Vikas Gulani",
            "Aditya Pandey",
            "Honglak Lee",
            "Todd Hollon"
        ],
        "title": "Learning neuroimaging models from health system-scale data",
        "abstract": "arXiv:2509.18638v1 Announce Type: new  Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological diseases. The global demand for magnetic resonance imaging (MRI) studies has risen steadily, placing significant strain on health systems, prolonging turnaround times, and intensifying physician burnout \\cite{Chen2017-bt, Rula2024-qp-1}. These challenges disproportionately impact patients in low-resource and rural settings. Here, we utilized a large academic health system as a data engine to develop Prima, the first vision language model (VLM) serving as an AI foundation for neuroimaging that supports real-world, clinical MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a hierarchical vision architecture that provides general and transferable MRI features. Prima was tested in a 1-year health system-wide study that included 30K MRI studies. Across 52 radiologic diagnoses from the major neurologic disorders, including neoplastic, inflammatory, infectious, and developmental lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0, outperforming other state-of-the-art general and medical AI models. Prima offers explainable differential diagnoses, worklist priority for radiologists, and clinical referral recommendations across diverse patient demographics and MRI systems. Prima demonstrates algorithmic fairness across sensitive groups and can help mitigate health system biases, such as prolonged turnaround times for low-resource populations. These findings highlight the transformative potential of health system-scale VLMs and Prima's role in advancing AI-driven healthcare.",
        "arxiv_id": "2509.18638",
        "ARXIVID": "2509.18638",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language model for neuroimaging tasks, focusing on real-world applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.16656": {
        "authors": [
            "Changyu Zeng",
            "Yifan Wang",
            "Zimu Wang",
            "Wei Wang",
            "Zhengni Yang",
            "Muyi Bao",
            "Jiming Xiao",
            "Ahn Nguyen",
            "Yutao Yue"
        ],
        "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities",
        "abstract": "arXiv:2509.16656v1 Announce Type: new  Abstract: Recent advancements in 2D multimodal large language models (MLLMs) have significantly improved performance in vision-language tasks. However, extending these capabilities to 3D environments remains a distinct challenge due to the complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability to perform precise spatial measurements and complex numerical reasoning. To address this gap, we introduce NUMINA, the first Natural Understanding benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities to enhance multimodal indoor perceptual understanding. NUMINA features multi-scale annotations and various question-answer pairs, generated using NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and rule-based self-verification. We evaluate the performance of various state-of-the-art LLMs on NUMINA following the Chat-Scene framework, demonstrating that current LLMs struggle with multimodal numerical reasoning, particularly in performing precise computations such as distance and volume estimation, highlighting the need for further advancements in 3D models. The dataset and source codes can be obtained from https://github.com/fengshun124/NUMINA.",
        "arxiv_id": "2509.16656",
        "ARXIVID": "2509.16656",
        "COMMENT": "Matches criterion 1 as it introduces a benchmark for spatial reasoning and numerical abilities in 3D environments.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.17259": {
        "authors": [
            "Ilham Wicaksono",
            "Zekun Wu",
            "Rahul Patel",
            "Theo King",
            "Adriano Koshiyama",
            "Philip Treleaven"
        ],
        "title": "Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B",
        "abstract": "arXiv:2509.17259v1 Announce Type: new  Abstract: As the industry increasingly adopts agentic AI systems, understanding their unique vulnerabilities becomes critical. Prior research suggests that security flaws at the model level do not fully capture the risks present in agentic deployments, where models interact with tools and external environments. This paper investigates this gap by conducting a comparative red teaming analysis of GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability framework AgentSeer to deconstruct agentic systems into granular actions and components, we apply iterative red teaming attacks with harmful objectives from HarmBench at two distinct levels: the standalone model and the model operating within an agentic loop. Our evaluation reveals fundamental differences between model level and agentic level vulnerability profiles. Critically, we discover the existence of agentic-only vulnerabilities, attack vectors that emerge exclusively within agentic execution contexts while remaining inert against standalone models. Agentic level iterative attacks successfully compromise objectives that completely failed at the model level, with tool-calling contexts showing 24\\% higher vulnerability than non-tool contexts. Conversely, certain model-specific exploits work exclusively at the model level and fail when transferred to agentic contexts, demonstrating that standalone model vulnerabilities do not always generalize to deployed systems.",
        "arxiv_id": "2509.17259",
        "ARXIVID": "2509.17259",
        "COMMENT": "Matches criterion 3 as it explores agentic AI systems and introduces a framework for analyzing vulnerabilities in agentic contexts.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.18763": {
        "authors": [
            "Xijun Wang",
            "Junyun Huang",
            "Rayyan Abdalla",
            "Chengyuan Zhang",
            "Ruiqi Xian",
            "Dinesh Manocha"
        ],
        "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models",
        "abstract": "arXiv:2509.18763v1 Announce Type: new  Abstract: We address the critical gap between the computational demands of vision-language models and the possible ultra-low-bit weight precision (bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated by the substantial computational cost and memory requirements of VLMs, which restrict their applicability in hardware-constrained environments. We propose Bi-VLM, which separates model weights non-uniformly based on the Gaussian quantiles. Our formulation groups the model weights into outlier (salient) and multiple inlier (unsalient) subsets, ensuring that each subset contains a proportion of weights corresponding to its quantile in the distribution. We propose a saliency-aware hybrid quantization algorithm and use it to quantize weights by imposing different constraints on the scaler and binary matrices based on the saliency metric and compression objective. We have evaluated our approach on different VLMs. For the language model part of the VLM, our Bi-VLM outperforms the SOTA by 3%-47% on the visual question answering task in terms of four different benchmarks and three different models. For the overall VLM, our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the quantized models and observe that there is redundancy of image tokens 90% - 99% in the quantized models. This helps us to further prune the visual tokens to improve efficiency.",
        "arxiv_id": "2509.18763",
        "ARXIVID": "2509.18763",
        "COMMENT": "Matches criterion 2 as it focuses on Vision-Language Models (VLMs) and explores ultra-low precision quantization for efficiency.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.18284": {
        "authors": [
            "Yi Gu",
            "Kuniaki Saito",
            "Jiaxin Ma"
        ],
        "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction",
        "abstract": "arXiv:2509.18284v1 Announce Type: new  Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning models are expected to effectively fuse heterogeneous information while remaining robust to missing modalities. In this work, we propose a novel multimodal learning framework that integrates enhanced modalities dropout and contrastive learning to address real-world limitations such as modality imbalance and missingness. Our approach introduces learnable modality tokens for improving missingness-aware fusion of modalities and augments conventional unimodal contrastive objectives with fused multimodal representations. We validate our framework on large-scale clinical datasets for disease detection and prediction tasks, encompassing both visual and tabular modalities. Experimental results demonstrate that our method achieves state-of-the-art performance, particularly in challenging and practical scenarios where only a single modality is available. Furthermore, we show its adaptability through successful integration with a recent CT foundation model. Our findings highlight the effectiveness, efficiency, and generalizability of our approach for multimodal learning, offering a scalable, low-cost solution with significant potential for real-world clinical applications. The code is available at https://github.com/omron-sinicx/medical-modality-dropout.",
        "arxiv_id": "2509.18284",
        "ARXIVID": "2509.18284",
        "COMMENT": "Matches criterion 2 as it explores multimodal learning with visual and tabular modalities, focusing on robustness and integration strategies.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.18917": {
        "authors": [
            "Amirhesam Aghanouri",
            "Cristina Olaverri-Monreal"
        ],
        "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models",
        "abstract": "arXiv:2509.18917v1 Announce Type: new  Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by improving efficiency and safety. Their success relies on 3D vision systems that effectively sense the environment and detect traffic agents. Among sensors AVs use to create a comprehensive view of surroundings, LiDAR provides high-resolution depth data enabling accurate object detection, safe navigation, and collision avoidance. However, collecting real-world LiDAR data is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations. This work applies a denoising diffusion probabilistic model (DDPM), enhanced with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic data for augmentation, thereby improving performance across a range of computer vision tasks, particularly in AV perception. These modifications impact the denoising process and the model's temporal awareness, allowing it to produce more realistic point clouds based on the projection. The proposed method was extensively evaluated under various configurations using the IAMCV and KITTI-360 datasets, with four performance metrics compared against state-of-the-art (SOTA) methods. The results demonstrate the model's superior performance over most existing baselines and its effectiveness in mitigating the effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.",
        "arxiv_id": "2509.18917",
        "ARXIVID": "2509.18917",
        "COMMENT": "Matches criterion 4 as it focuses on generating high-quality LiDAR point cloud data using diffusion models, which is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.18840": {
        "authors": [
            "Ismael Elsharkawi",
            "Hossam Sharara",
            "Ahmed Rafea"
        ],
        "title": "ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction",
        "abstract": "arXiv:2509.18840v1 Announce Type: new  Abstract: Image Representation Learning is an important problem in Computer Vision. Traditionally, images were processed as grids, using Convolutional Neural Networks or as a sequence of visual tokens, using Vision Transformers. Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of images as a graph of nodes; which provides a more intuitive image representation. The challenge is to construct a graph of nodes in each layer that best represents the relations between nodes and does not need a hyper-parameter search. ViG models in the literature depend on non-parameterized and non-learnable statistical methods that operate on the latent features of nodes to create a graph. This might not select the best neighborhood for each node. Starting from k-NN graph construction to HyperGraph Construction and Similarity-Thresholded graph construction, these methods lack the ability to provide a learnable hyper-parameter-free graph construction method. To overcome those challenges, we present the Learnable Reparameterized Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies key-query attention between every pair of nodes; then uses soft-threshold reparameterization for edge selection, which allows the use of a differentiable mathematical model for training. Using learnable parameters to select the neighborhood removes the bias that is induced by any clustering or thresholding methods previously introduced in the literature. In addition, LRGC allows tuning the threshold in each layer to the training data since the thresholds are learnable through training and are not provided as hyper-parameters to the model. We demonstrate that the proposed ViG-LRGC approach outperforms state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark dataset.",
        "arxiv_id": "2509.18840",
        "ARXIVID": "2509.18840",
        "COMMENT": "Matches criterion 4 as it focuses on Vision Graph Neural Networks, a novel approach to image representation learning, which aligns with foundation models in computer vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.19096": {
        "authors": [
            "Ilhan Skender",
            "Kailin Tong",
            "Selim Solmaz",
            "Daniel Watzenig"
        ],
        "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models",
        "abstract": "arXiv:2509.19096v1 Announce Type: new  Abstract: Traffic safety remains a critical global concern, with timely and accurate accident detection essential for hazard reduction and rapid emergency response. Infrastructure-based vision sensors offer scalable and efficient solutions for continuous real-time monitoring, facilitating automated detection of acci- dents directly from captured images. This research investigates the zero-shot capabilities of multimodal large language models (MLLMs) for detecting and describing traffic accidents using images from infrastructure cameras, thus minimizing reliance on extensive labeled datasets. Main contributions include: (1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA, explicitly addressing the scarcity of diverse, realistic, infrastructure-based accident data through controlled simulations; (2) Comparative performance analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent identification and descriptive capabilities without prior fine-tuning; and (3) Integration of advanced visual analytics, specifically YOLO for object detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for instance segmentation, into enhanced prompts to improve model accuracy and explainability. Key numerical results show Pixtral as the top performer with an F1-score of 0.71 and 83% recall, while Gemini models gained precision with enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and recall losses. Gemma 3 offered the most balanced performance with minimal metric fluctuation. These findings demonstrate the substantial potential of integrating MLLMs with advanced visual analytics techniques, enhancing their applicability in real-world automated traffic monitoring systems.",
        "arxiv_id": "2509.19096",
        "ARXIVID": "2509.19096",
        "COMMENT": "Matches criterion 2 as it investigates multimodal large language models for traffic accident detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.18738": {
        "authors": [
            "Ruichao Hou",
            "Xingyuan Li",
            "Tongwei Ren",
            "Dongming Zhou",
            "Gangshan Wu",
            "Jinde Cao"
        ],
        "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection",
        "abstract": "arXiv:2509.18738v1 Announce Type: new  Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent objects by integrating complementary information from RGB and thermal modalities. However, learning the precise boundaries and complete objects remains challenging due to the intrinsic insufficient feature fusion and the extrinsic limitations of data scarcity. In this paper, we propose a novel hybrid prompt-driven segment anything model (HyPSAM), which leverages the zero-shot generalization capabilities of the segment anything model (SAM) for RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that generates high-quality initial saliency maps as visual prompts. DFNet employs dynamic convolution and multi-branch decoding to facilitate adaptive cross-modality interaction, overcoming the limitations of fixed-parameter kernels and enhancing multi-modal feature representation. Moreover, we propose a plug-and-play refinement network (P2RNet), which serves as a general optimization strategy to guide SAM in refining saliency maps by using hybrid prompts. The text prompt ensures reliable modality input, while the mask and box prompts enable precise salient object localization. Extensive experiments on three public datasets demonstrate that our method achieves state-of-the-art performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating with different RGB-T SOD methods to achieve significant performance gains, thereby highlighting the potential of prompt engineering in this field. The code and results of our method are available at: https://github.com/milotic233/HyPSAM.",
        "arxiv_id": "2509.18738",
        "ARXIVID": "2509.18738",
        "COMMENT": "Matches criterion 5 as it combines RGB-Thermal data with a segment-anything model, integrating image understanding with advanced models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.18613": {
        "authors": [
            "Yuzhi Wu",
            "Li Xiao",
            "Jun Liu",
            "Guangfeng Jiang",
            "XiangGen Xia"
        ],
        "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving",
        "abstract": "arXiv:2509.18613v1 Announce Type: new  Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth, elevation, and Doppler velocity of objects, is recognized for its cost-effectiveness and robustness in autonomous driving. Nevertheless, its point clouds exhibit significant sparsity and noise, restricting its standalone application in 3D object detection. Recent 4D radar-camera fusion methods have provided effective perception. Most existing approaches, however, adopt explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the sparse and incomplete geometry of radar point clouds and restrict fusion to coarse scene-level integration. To address these problems, we propose MLF-4DRCNet, a novel two-stage framework for 3D object detection via multi-level fusion of 4D radar and camera images. Our model incorporates the point-, scene-, and proposal-level multi-modal information, enabling comprehensive feature representation. It comprises three crucial components: the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module. Operating at the point-level, ERPE densities radar point clouds with 2D image instances and encodes them into voxels via the proposed Triple-Attention Voxel Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D image features using deformable attention to capture scene context and adopts pooling to the fused features. PLFE refines region proposals by fusing image features, and further integrates with the pooled features from HSFP. Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets demonstrate that MLF-4DRCNet achieves the state-of-the-art performance. Notably, it attains performance comparable to LiDAR-based models on the VoD dataset.",
        "arxiv_id": "2509.18613",
        "ARXIVID": "2509.18613",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D object detection in autonomous driving, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.19203": {
        "authors": [
            "Ioanna Ntinou",
            "Alexandros Xenos",
            "Yassine Ouali",
            "Adrian Bulat",
            "Georgios Tzimiropoulos"
        ],
        "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions",
        "abstract": "arXiv:2509.19203v1 Announce Type: new  Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have become the standard approach for learning discriminative vision-language representations. However, these models often exhibit shallow language understanding, manifesting bag-of-words behaviour. These limitations are reinforced by their dual-encoder design, which induces a modality gap. Additionally, the reliance on vast web-collected data corpora for training makes the process computationally expensive and introduces significant privacy concerns. To address these limitations, in this work, we challenge the necessity of vision encoders for retrieval tasks by introducing a vision-free, single-encoder retrieval pipeline. Departing from the traditional text-to-image retrieval paradigm, we migrate to a text-to-text paradigm with the assistance of VLLM-generated structured image descriptions. We demonstrate that this paradigm shift has significant advantages, including a substantial reduction of the modality gap, improved compositionality, and better performance on short and long caption queries, all attainable with only a few hours of calibration on two GPUs. Additionally, substituting raw images with textual descriptions introduces a more privacy-friendly alternative for retrieval. To further assess generalisation and address some of the shortcomings of prior compositionality benchmarks, we release two benchmarks derived from Flickr30k and COCO, containing diverse compositional queries made of short captions, which we coin subFlickr and subCOCO. Our vision-free retriever matches and often surpasses traditional multimodal models. Importantly, our approach achieves state-of-the-art zero-shot performance on multiple retrieval and compositionality benchmarks, with models as small as 0.3B parameters. Code is available at: https://github.com/IoannaNti/LexiCLIP",
        "arxiv_id": "2509.19203",
        "ARXIVID": "2509.19203",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models and their applications in retrieval tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.18717": {
        "authors": [
            "Tong Zhang",
            "Kuofeng Gao",
            "Jiawang Bai",
            "Leo Yu Zhang",
            "Xin Yin",
            "Zonghui Wang",
            "Shouling Ji",
            "Wenzhi Chen"
        ],
        "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment",
        "abstract": "arXiv:2509.18717v1 Announce Type: new  Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP) models are threatened by targeted data poisoning and backdoor attacks due to massive training image-caption pairs crawled from the Internet. Previous defense methods correct poisoned image-caption pairs by matching a new caption for each image. However, the matching process relies solely on the global representations of images and captions, overlooking fine-grained features of visual and textual features. It may introduce incorrect image-caption pairs and harm the CLIP pre-training. To address their limitations, we propose an Optimal Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We propose a new optimal transport-based distance measure between fine-grained visual and textual feature sets and re-assign new captions based on the proposed optimal transport distance. Additionally, to further reduce the negative impact of mismatched pairs, we encourage the inter- and intra-modality fine-grained alignment by employing optimal transport-based objective functions. Our experiments demonstrate that OTCCLIP can successfully decrease the attack success rates of poisoning attacks. Also, compared to previous methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing performance trained on poisoned datasets.",
        "arxiv_id": "2509.18717",
        "ARXIVID": "2509.18717",
        "COMMENT": "Matches criterion 4 as it focuses on improving CLIP, a vision foundation model, with novel training strategies.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17907": {
        "authors": [
            "Xiaojing Dong",
            "Weilin Huang",
            "Liang Li",
            "Yiying Li",
            "Shu Liu",
            "Tongtong Ou",
            "Shuang Ouyang",
            "Yu Tian",
            "Fengxuan Zhao"
        ],
        "title": "MEF: A Systematic Evaluation Framework for Text-to-Image Models",
        "abstract": "arXiv:2509.17907v1 Announce Type: new  Abstract: Rapid advances in text-to-image (T2I) generation have raised higher requirements for evaluation methodologies. Existing benchmarks center on objective capabilities and dimensions, but lack an application-scenario perspective, limiting external validity. Moreover, current evaluations typically rely on either ELO for overall ranking or MOS for dimension-specific scoring, yet both methods have inherent shortcomings and limited interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF), a systematic and practical approach for evaluating T2I models. First, we propose a structured taxonomy encompassing user scenarios, elements, element compositions, and text expression forms to construct the Magic-Bench-377, which supports label-level assessment and ensures a balanced coverage of both user scenarios and capabilities. On this basis, we combine ELO and dimension-specific MOS to generate model rankings and fine-grained assessments respectively. This joint evaluation method further enables us to quantitatively analyze the contribution of each dimension to user satisfaction using multivariate logistic regression. By applying MEF to current T2I models, we obtain a leaderboard and key characteristics of the leading models. We release our evaluation framework and make Magic-Bench-377 fully open-source to advance research in the evaluation of visual generative models.",
        "arxiv_id": "2509.17907",
        "ARXIVID": "2509.17907",
        "COMMENT": "Matches criterion 4 as it focuses on evaluation frameworks for text-to-image models, which are relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.19028": {
        "authors": [
            "Ioannis Sarafis",
            "Alexandros Papadopoulos",
            "Anastasios Delopoulos"
        ],
        "title": "Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model",
        "abstract": "arXiv:2509.19028v1 Announce Type: new  Abstract: In this paper, we propose a weakly supervised semantic segmentation approach for food images which takes advantage of the zero-shot capabilities and promptability of the Segment Anything Model (SAM) along with the attention mechanisms of Vision Transformers (ViTs). Specifically, we use class activation maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable for food image segmentation. The ViT model, a Swin Transformer, is trained exclusively using image-level annotations, eliminating the need for pixel-level annotations during training. Additionally, to enhance the quality of the SAM-generated masks, we examine the use of image preprocessing techniques in combination with single-mask and multi-mask SAM generation strategies. The methodology is evaluated on the FoodSeg103 dataset, generating an average of 2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for the multi-mask scenario. We envision the proposed approach as a tool to accelerate food image annotation tasks or as an integrated component in food and nutrition tracking applications.",
        "arxiv_id": "2509.19028",
        "ARXIVID": "2509.19028",
        "COMMENT": "Matches criterion 4 as it involves Vision Transformers and SAM for segmentation tasks, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17460": {
        "authors": [
            "Jianlong Chang",
            "Haixin Wang",
            "Zhiyuan Dang",
            "Li Huang",
            "Zhiyu Wang",
            "Ruoqi Cao",
            "Shihao Piao",
            "Dongzhe Li",
            "Dianyu Gao",
            "Dongsheng Wang",
            "Yin Li",
            "Jinan Sun",
            "Lu Fang",
            "Zhouchen Lin"
        ],
        "title": "AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks",
        "abstract": "arXiv:2509.17460v1 Announce Type: new  Abstract: The pursuit of artificial general intelligence continuously demands generalization in one model across myriad tasks, even those not seen before. However, current AI models are isolated from each other for being limited to specific tasks, now first defined as Intelligence Islands. To unify Intelligence Islands into one, we propose Pangaea, the first AI supercontinent akin to the geological Pangaea. Pangaea encodes any data into a unified format and accumulates universal knowledge through pre-training on 296 datasets across diverse modalities. Eventually, it demonstrates remarkable generalization across 45 general tasks and 15 scientific tasks encompassing a wide range of scientific subjects. By investigating Pangaea deeper, the scaling effect of modality is revealed, quantifying the universal knowledge accumulation across modalities as the cumulative distribution function of a geometric distribution. On the whole, Pangaea shows strong potential to handle myriad tasks, indicating a new direction toward artificial general intelligence.",
        "arxiv_id": "2509.17460",
        "ARXIVID": "2509.17460",
        "COMMENT": "Does not match any specific criterion but is related to general AI and multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2509.18913": {
        "authors": [
            "Nguyen Van Tu",
            "Pham Nguyen Hai Long",
            "Vo Hoai Viet"
        ],
        "title": "xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision",
        "abstract": "arXiv:2509.18913v1 Announce Type: new  Abstract: Deep learning has become the de facto standard and dominant paradigm in image analysis tasks, achieving state-of-the-art performance. However, this approach often results in \"black-box\" models, whose decision-making processes are difficult to interpret, raising concerns about reliability in critical applications. To address this challenge and provide human a method to understand how AI model process and make decision, the field of xAI has emerged. This paper surveys four representative approaches in xAI for visual perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM), (iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their underlying mechanisms, strengths and limitations, as well as evaluation metrics, thereby providing a comprehensive overview to guide future research and applications.",
        "arxiv_id": "2509.18913",
        "ARXIVID": "2509.18913",
        "COMMENT": "Matches criterion 7 as it is a survey paper on explainable AI in computer vision, which aligns with vision-focused survey papers.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.16330": {
        "authors": [
            "Minxing Zhang",
            "Yi Yang",
            "Roy Xie",
            "Bhuwan Dhingra",
            "Shuyan Zhou",
            "Jian Pei"
        ],
        "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey",
        "abstract": "arXiv:2509.16330v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have emerged as a new paradigm that extends LLMs' capabilities beyond text generation to dynamic interaction with external environments. By integrating reasoning with perception, memory, and tool use, agents are increasingly deployed in diverse domains like web navigation and household robotics. A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Despite growing interest, the concept of generalizability in LLM-based agents remains underdefined, and systematic approaches to measure and improve it are lacking. In this survey, we provide the first comprehensive review of generalizability in LLM-based agents. We begin by emphasizing agent generalizability's importance by appealing to stakeholders and clarifying the boundaries of agent generalizability by situating it within a hierarchical domain-task ontology. We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Next, we categorize methods for improving generalizability into three groups: methods for the backbone LLM, for agent components, and for their interactions. Moreover, we introduce the distinction between generalizable frameworks and generalizable agents and outline how generalizable frameworks can be translated into agent-level generalizability. Finally, we identify critical challenges and future directions, including developing standardized frameworks, variance- and cost-based metrics, and approaches that integrate methodological innovations with architecture-level designs. By synthesizing progress and highlighting opportunities, this survey aims to establish a foundation for principled research on building LLM-based agents that generalize reliably across diverse applications.",
        "arxiv_id": "2509.16330",
        "ARXIVID": "2509.16330",
        "COMMENT": "Matches criterion 7 as it is a survey paper on generalizability of LLM-based agents, which includes vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2509.17393": {
        "authors": [
            "Kang-il Lee",
            "Jahyun Koo",
            "Seunghyun Yoon",
            "Minbeom Kim",
            "Hyukhun Koh",
            "Dongryeol Lee",
            "Kyomin Jung"
        ],
        "title": "Program Synthesis via Test-Time Transduction",
        "abstract": "arXiv:2509.17393v2 Announce Type: new  Abstract: We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC, and programmatic world modeling on MiniGrid. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA.",
        "arxiv_id": "2509.17393",
        "ARXIVID": "2509.17393",
        "COMMENT": "Does not match any specific criteria but discusses program synthesis, which is tangentially related to computational reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.16332": {
        "authors": [
            "Stephen Fitz",
            "Peter Romero",
            "Steven Basart",
            "Sipeng Chen",
            "Jose Hernandez-Orallo"
        ],
        "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models",
        "abstract": "arXiv:2509.16332v1 Announce Type: new  Abstract: Large Language Models increasingly mediate high-stakes interactions, intensifying research on their capabilities and safety. While recent work has shown that LLMs exhibit consistent and measurable synthetic personality traits, little is known about how modulating these traits affects model behavior. We address this gap by investigating how psychometric personality control grounded in the Big Five framework influences AI behavior in the context of capability and safety benchmarks. Our experiments reveal striking effects: for example, reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well as reduction in general capabilities as measured by MMLU. These findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and general competence. We discuss the implications for safety evaluation, alignment strategies, steering model behavior after deployment, and risks associated with possible exploitation of these findings. Our findings motivate a new line of research on personality-sensitive safety evaluations and dynamic behavioral control in LLMs.",
        "arxiv_id": "2509.16332",
        "ARXIVID": "2509.16332",
        "COMMENT": "Does not match any specific criteria but discusses personality shaping in LLMs, which is tangentially related to safety and alignment in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.16561": {
        "authors": [
            "Yue Xin",
            "Chen Shen",
            "Shaotian Yan",
            "Xiaosong Yuan",
            "Yaoming Wang",
            "Xiaofeng Zhang",
            "Chenxi Huang",
            "Jieping Ye"
        ],
        "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning",
        "abstract": "arXiv:2509.16561v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of large language models (LLMs) to a large margin. However, the mechanism underlying such improvements remains unexplored. In this paper, we present \\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed \\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd} M\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a mathematically rigorous evaluation metric for quantifying component-level contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley value for mathematical expression attribution and develop an efficient stratified sampling algorithm that significantly reduces the computational complexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality \\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance analysis. Comprehensive validation across popular LLM models and diverse mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder framework exhibits a robust monotonic correlation with model performance, not only providing theoretical explanations for the empirical success of existing few-shot CoT but also establishing mathematically rigorous principles for prompt construction optimization. Furthermore, we verify the reliability of the explanation, based on which we unify the insights of previous work.",
        "arxiv_id": "2509.16561",
        "ARXIVID": "2509.16561",
        "COMMENT": "Does not closely match any specific criteria but is related to chain-of-thought reasoning and mathematical reasoning in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.19300": {
        "authors": [
            "Chen Chen",
            "Pengsheng Guo",
            "Liangchen Song",
            "Jiasen Lu",
            "Rui Qian",
            "Xinze Wang",
            "Tsu-Jui Fu",
            "Wei Liu",
            "Yinfei Yang",
            "Alex Schwing"
        ],
        "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching",
        "abstract": "arXiv:2509.19300v1 Announce Type: new  Abstract: Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters.",
        "arxiv_id": "2509.19300",
        "ARXIVID": "2509.19300",
        "COMMENT": "Does not closely match any specific criteria but is related to generative modeling and conditional data distribution, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.18693": {
        "authors": [
            "Siyi Chen",
            "Kai Wang",
            "Weicong Pang",
            "Ruiming Yang",
            "Ziru Chen",
            "Renjun Gao",
            "Alexis Kai Hon Lau",
            "Dasa Gu",
            "Chenchen Zhang",
            "Cheng Li"
        ],
        "title": "OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery",
        "abstract": "arXiv:2509.18693v1 Announce Type: new  Abstract: Open-set land-cover analysis in remote sensing requires the ability to achieve fine-grained spatial localization and semantically open categorization. This involves not only detecting and segmenting novel objects without categorical supervision but also assigning them interpretable semantic labels through multimodal reasoning. In this study, we introduce OSDA, an integrated three-stage framework for annotation-free open-set land-cover discovery, segmentation, and description. The pipeline consists of: (1) precise discovery and mask extraction with a promptable fine-tuned segmentation model (SAM), (2) semantic attribution and contextual description via a two-phase fine-tuned multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring of the MLLMs evaluation. By combining pixel-level accuracy with high-level semantic understanding, OSDA addresses key challenges in open-world remote sensing interpretation. Designed to be architecture-agnostic and label-free, the framework supports robust evaluation across diverse satellite imagery without requiring manual annotation. Our work provides a scalable and interpretable solution for dynamic land-cover monitoring, showing strong potential for automated cartographic updating and large-scale earth observation analysis.",
        "arxiv_id": "2509.18693",
        "ARXIVID": "2509.18693",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to multimodal reasoning and segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.17677": {
        "authors": [
            "Xiyuan Zhou",
            "Xinlei Wang",
            "Yirui He",
            "Yang Wu",
            "Ruixi Zou",
            "Yuheng Cheng",
            "Yulu Xie",
            "Wenxuan Liu",
            "Huan Zhao",
            "Yan Xu",
            "Jinjin Gu",
            "Junhua Zhao"
        ],
        "title": "EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving",
        "abstract": "arXiv:2509.17677v1 Announce Type: new  Abstract: Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation -- they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the model's robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at https://github.com/EngiBench/EngiBench.",
        "arxiv_id": "2509.17677",
        "ARXIVID": "2509.17677",
        "COMMENT": "Does not match any specific criterion but introduces a benchmark for engineering problem-solving, which is tangentially relevant to reasoning in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.19156": {
        "authors": [
            "Maurf Hassan",
            "Steven Davy",
            "Muhammad Zawish",
            "Owais Bin Zuber",
            "Nouman Ashraf"
        ],
        "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit",
        "abstract": "arXiv:2509.19156v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling energy-efficient intelligence at the edge. However, performing full SNN inference at the edge can be challenging due to the latency and energy constraints arising from fixed and high timestep overheads. Edge-cloud co-inference systems present a promising solution, but their deployment is often hindered by high latency and feature transmission costs. To address these issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a learned spike-driven compression module to reduce data transmission and employs a dynamic early-exit mechanism to adaptively terminate inference based on output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16 backbones in a real edge-to-cloud testbed. Our proposed system reduces data transfer by up to 2048x and edge energy consumption by over 90%, while reducing end-to-end latency by up to 3x compared to edge-only inference, all with a negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables practical, high-performance SNN deployment in resource-constrained environments.",
        "arxiv_id": "2509.19156",
        "ARXIVID": "2509.19156",
        "COMMENT": "Does not match any specific criterion but is related to edge-cloud co-inference and energy-efficient SNNs, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.16578": {
        "authors": [
            "Wenyao Li",
            "Ran Zhang",
            "Pengyang Wang",
            "Yuanchun Zhou",
            "Pengfei Wang"
        ],
        "title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning",
        "abstract": "arXiv:2509.16578v1 Announce Type: new  Abstract: Human mobility forecasting is important for applications such as transportation planning, urban management, and personalized recommendations. However, existing methods often fail to generalize to unseen users or locations and struggle to capture dynamic intent due to limited labeled data and the complexity of mobility patterns. We propose ZHMF, a framework for zero-shot human mobility forecasting that combines a semantic enhanced retrieval and reflection mechanism with a hierarchical language model based reasoning system. The task is reformulated as a natural language question answering paradigm. Leveraging LLMs semantic understanding of user histories and context, our approach handles previously unseen prediction scenarios. We further introduce a hierarchical reflection mechanism for iterative reasoning and refinement by decomposing forecasting into an activity level planner and a location level selector, enabling collaborative modeling of long term user intentions and short term contextual preferences. Experiments on standard human mobility datasets show that our approach outperforms existing models. Ablation studies reveal the contribution of each module, and case studies illustrate how the method captures user intentions and adapts to diverse contextual scenarios.",
        "arxiv_id": "2509.16578",
        "ARXIVID": "2509.16578",
        "COMMENT": "Does not match any specific criterion but explores hierarchical reasoning in LLMs, which is tangentially relevant to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.18973": {
        "authors": [
            "Jiabao Chen",
            "Shan Xiong",
            "Jialin Peng"
        ],
        "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images",
        "abstract": "arXiv:2509.18973v1 Announce Type: new  Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from large-scale electron microscopy (EM) is a promising way to enable annotation-efficient learning. Inspired by SAM, we propose a promptable multitask framework, namely Prompt-DAS, which is flexible enough to utilize any number of point prompts during the adaptation training stage and testing stage. Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well as interactive segmentation during testing. Unlike the foundation model SAM, which necessitates a prompt for each individual object instance, Prompt-DAS is only trained on a small dataset and can utilize full points on all instances, sparse points on partial instances, or even no points at all, facilitated by the incorporation of an auxiliary center-point detection task. Moreover, a novel prompt-guided contrastive learning is proposed to enhance discriminative feature learning. Comprehensive experiments conducted on challenging benchmarks demonstrate the effectiveness of the proposed approach over existing UDA, WDA, and SAM-based approaches.",
        "arxiv_id": "2509.18973",
        "ARXIVID": "2509.18973",
        "COMMENT": "Does not match any specific criteria. Focuses on domain adaptive segmentation for electron microscopy images, which is outside the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.17066": {
        "authors": [
            "Kunrong Li",
            "Kwan Hui Lim"
        ],
        "title": "RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking",
        "abstract": "arXiv:2509.17066v1 Announce Type: new  Abstract: Next point-of-interest (POI) recommendation predicts a user's next destination from historical movements. Traditional models require intensive training, while LLMs offer flexible and generalizable zero-shot solutions but often generate generic or geographically irrelevant results due to missing trajectory and spatial context. To address these issues, we propose RALLM-POI, a framework that couples LLMs with retrieval-augmented generation and self-rectification. We first propose a Historical Trajectory Retriever (HTR) that retrieves relevant past trajectories to serve as contextual references, which are then reranked by a Geographical Distance Reranker (GDR) for prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier (ALR) is designed to refine outputs through self-reflection. Without additional training, RALLM-POI achieves substantial accuracy gains across three real-world Foursquare datasets, outperforming both conventional and LLM-based baselines. Code is released at https://github.com/LKRcrocodile/RALLM-POI.",
        "arxiv_id": "2509.17066",
        "ARXIVID": "2509.17066",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to retrieval-augmented generation and spatial reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18699": {
        "authors": [
            "Zedong Zhang",
            "Ying Tai",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping",
        "abstract": "arXiv:2509.18699v1 Announce Type: new  Abstract: Fusing cross-category objects to a single coherent object has gained increasing attention in text-to-image (T2I) generation due to its broad applications in virtual reality, digital media, film, and gaming. However, existing methods often produce biased, visually chaotic, or semantically inconsistent results due to overlapping artifacts and poor integration. Moreover, progress in this field has been limited by the absence of a comprehensive benchmark dataset. To address these problems, we propose \\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective approach comprising two key components: (1) Group-wise Embedding Swapping, which fuses semantic attributes from different concepts through feature manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism guided by a balance evaluation score to ensure coherent synthesis. Additionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a large-scale, hierarchically structured dataset built upon ImageNet-1K and WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling 451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap outperforms state-of-the-art compositional T2I methods, including GPT-Image-1 using simple and complex prompts.",
        "arxiv_id": "2509.18699",
        "ARXIVID": "2509.18699",
        "COMMENT": "Does not closely match any specific criteria but is related to text-to-image generation and compositional methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18683": {
        "authors": [
            "Lanhu Wu",
            "Zilin Gao",
            "Hao Fei",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection",
        "abstract": "arXiv:2509.18683v1 Announce Type: new  Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous objects in a scene with the incorporation of depth cues. Existing methods mainly rely on CNNs, limited by the local receptive fields, or Vision Transformers that suffer from the cost of quadratic complexity, posing a challenge in balancing performance and computational efficiency. Recently, state space models (SSM), Mamba, have shown great potential for modeling long-range dependency with linear complexity. However, directly applying SSM to RGB-D SOD may lead to deficient local semantics as well as the inadequate cross-modality fusion. To address these issues, we propose a Local Emphatic and Adaptive Fusion state space model (LEAF-Mamba) that contains two novel components: 1) a local emphatic state space module (LE-SSM) to capture multi-scale local dependencies for both modalities. 2) an SSM-based adaptive fusion module (AFM) for complementary cross-modality interaction and reliable cross-modality integration. Extensive experiments demonstrate that the LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in both efficacy and efficiency. Moreover, our method can achieve excellent performance on the RGB-T SOD task, proving a powerful generalization ability.",
        "arxiv_id": "2509.18683",
        "ARXIVID": "2509.18683",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of computer vision and RGB-D salient object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16742": {
        "authors": [
            "Mohammad Beigi",
            "Ying Shen",
            "Parshin Shojaee",
            "Qifan Wang",
            "Zichao Wang",
            "Chandan Reddy",
            "Ming Jin",
            "Lifu Huang"
        ],
        "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories",
        "abstract": "arXiv:2509.16742v1 Announce Type: new  Abstract: Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \\textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \\textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \\textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",
        "arxiv_id": "2509.16742",
        "ARXIVID": "2509.16742",
        "COMMENT": "Does not match any specific criterion but is related to general interest in improving reasoning in AI models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18427": {
        "authors": [
            "Xinyang Wu",
            "Muheng Li",
            "Xia Li",
            "Orso Pusterla",
            "Sairos Safai",
            "Philippe C. Cattin",
            "Antony J. Lomax",
            "Ye Zhang"
        ],
        "title": "CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction",
        "abstract": "arXiv:2509.18427v1 Announce Type: new  Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing respiratory-induced motion in radiation therapy planning and delivery. Conventional 4D reconstruction methods, which typically rely on phase binning or separate template scans, struggle to capture temporal variability, complicate workflows, and impose heavy computational loads. We introduce a neural representation framework that considers respiratory motion as a smooth, continuous deformation steered by a 1D surrogate signal, completely replacing the conventional discrete sorting approach. The new method fuses motion modeling with image reconstruction through two synergistic networks: the Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical representation, while a Temporal Motion Network (TMN), guided by Transformer-derived respiratory signals, produces temporally consistent deformation fields. Evaluation using a free-breathing dataset of 19 volunteers demonstrates that our template- and phase-free method accurately captures both regular and irregular respiratory patterns, while preserving vessel and bronchial continuity with high anatomical fidelity. The proposed method significantly improves efficiency, reducing the total processing time from approximately five hours required by conventional discrete sorting methods to just 15 minutes of training. Furthermore, it enables inference of each 3D volume in under one second. The framework accurately reconstructs 3D images at any respiratory state, achieves superior performance compared to conventional methods, and demonstrates strong potential for application in 4D radiation therapy planning and real-time adaptive treatment.",
        "arxiv_id": "2509.18427",
        "ARXIVID": "2509.18427",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of machine learning and image reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18190": {
        "authors": [
            "Junseong Shin",
            "Seungwoo Chung",
            "Yunjeong Yang",
            "Tae Hyun Kim"
        ],
        "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing",
        "abstract": "arXiv:2509.18190v1 Announce Type: new  Abstract: Dehazing involves removing haze or fog from images to restore clarity and improve visibility by estimating atmospheric scattering effects. While deep learning methods show promise, the lack of paired real-world training data and the resulting domain gap hinder generalization to real-world scenarios. In this context, physics-grounded learning becomes crucial; however, traditional methods based on the Atmospheric Scattering Model (ASM) often fall short in handling real-world complexities and diverse haze patterns. To solve this problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF), HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones, enhancing real-world dehazing performance with only a single inference step. Additionally, we introduce a non-homogeneous haze generation method using Markov Chain Brownian Motion (MCBM) to address the scarcity of paired real-world data. By simulating realistic haze patterns through MCBM, we enhance the adaptability of HazeFlow to diverse real-world scenarios. Through extensive experiments, we demonstrate that HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets.",
        "arxiv_id": "2509.18190",
        "ARXIVID": "2509.18190",
        "COMMENT": "Does not match any specific criterion but is related to image processing and dehazing, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16456": {
        "authors": [
            "Jiahao Yu",
            "Zelei Cheng",
            "Xian Wu",
            "Xinyu Xing"
        ],
        "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
        "abstract": "arXiv:2509.16456v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used in various domains, showing impressive potential on different tasks. Recently, reasoning LLMs have been proposed to improve the \\textit{reasoning} or \\textit{thinking} capabilities of LLMs to solve complex problems. Despite the promising results of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs still remains a significant challenge. While existing optimization methods have advanced the LLM reasoning capabilities, they often treat reasoning trajectories as a whole, without considering the underlying critical steps within the trajectory. In this paper, we introduce \\textbf{G}uided \\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that dives into the reasoning process to enable more effective improvements. GPO first identifies the `critical step' within a reasoning trajectory - a point that the model must carefully proceed to succeed at the problem. We locate the critical step by estimating the advantage function. GPO then resets the policy to the critical step, samples the new rollout and prioritizes the learning process on those rollouts. This focus allows the model to learn more effectively from pivotal moments within the reasoning process to improve the reasoning performance. We demonstrate that GPO is a general strategy that can be integrated with various optimization methods to improve reasoning performance. Besides theoretical analysis, our experiments across challenging reasoning benchmarks show that GPO can consistently and significantly enhance the performance of existing optimization methods, showcasing its effectiveness and generalizability in improving LLM reasoning by concentrating on pivotal moments within the generation process.",
        "arxiv_id": "2509.16456",
        "ARXIVID": "2509.16456",
        "COMMENT": "Does not match any specific criterion but is related to reasoning in LLMs, which is tangentially relevant to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18715": {
        "authors": [
            "Yingquan Wang",
            "Pingping Zhang",
            "Chong Sun",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification",
        "abstract": "arXiv:2509.18715v1 Announce Type: new  Abstract: Object Re-IDentification (ReID) aims to recognize individuals across non-overlapping camera views. While recent advances have achieved remarkable progress, most existing models are constrained to either single-domain or cross-domain scenarios, limiting their real-world applicability. Single-domain models tend to overfit to domain-specific features, whereas cross-domain models often rely on diverse normalization strategies that may inadvertently suppress identity-specific discriminative cues. To address these limitations, we propose an Attribute Prompt Composition (APC) framework, which exploits textual semantics to jointly enhance discrimination and generalization. Specifically, we design an Attribute Prompt Generator (APG) consisting of a Semantic Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an over-complete attribute dictionary to provide rich semantic descriptions, while PCM adaptively composes relevant attributes from SAD to generate discriminative attribute-aware features. In addition, motivated by the strong generalization ability of Vision-Language Models (VLM), we propose a Fast-Slow Training Strategy (FSTS) to balance ReID-specific discrimination and generalizable representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS) to rapidly acquire ReID-specific discriminative knowledge and a Slow Update Stream (SUS) to retain the generalizable knowledge inherited from the pre-trained VLM. Through a mutual interaction, the framework effectively focuses on ReID-relevant features while mitigating overfitting. Extensive experiments on both conventional and Domain Generalized (DG) ReID datasets demonstrate that our framework surpasses state-of-the-art methods, exhibiting superior performances in terms of both discrimination and generalization. The source code is available at https://github.com/AWangYQ/APC.",
        "arxiv_id": "2509.18715",
        "ARXIVID": "2509.18715",
        "COMMENT": "Does not match any specific criteria but involves object re-identification, which is tangentially related to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18919": {
        "authors": [
            "Chuni Liu",
            "Hongjie Li",
            "Jiaqi Du",
            "Yangyang Hou",
            "Qian Sun",
            "Lei Jin",
            "Ke Xu"
        ],
        "title": "Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset",
        "abstract": "arXiv:2509.18919v1 Announce Type: new  Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface defect detection for mitigating the challenges posed by data scarcity. However, its implementation presents a critical dilemma. Pretraining on natural image datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive self-supervised pretraining on in-domain industrial data is often ineffective due to the inability of existing learning objectives to distinguish subtle defect patterns from complex background noise and textures. To resolve this, we introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm that explicitly guides representation learning through anomaly priors. AGSSP employs a two-stage framework: (1) it first pretrains the model's backbone by distilling knowledge from anomaly maps, encouraging the network to capture defect-salient features; (2) it then pretrains the detector using pseudo-defect boxes derived from these maps, aligning it with localization tasks. To enable this, we develop a knowledge-enhanced method to generate high-quality anomaly maps and collect a large-scale industrial dataset of 120,000 images. Additionally, we present two small-scale, pixel-level labeled metallic surface defect datasets for validation. Extensive experiments demonstrate that AGSSP consistently enhances performance across various settings, achieving up to a 10\\% improvement in mAP@0.5 and 11.4\\% in mAP@0.5:0.95 compared to ImageNet-based models. All code, pretrained models, and datasets are publicly available at https://clovermini.github.io/AGSSP-Dev/.",
        "arxiv_id": "2509.18919",
        "ARXIVID": "2509.18919",
        "COMMENT": "Does not match any specific criteria but involves pretraining strategies for defect detection, which is tangentially related to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18354": {
        "authors": [
            "Mehrdad Moradi",
            "Shengzhe Chen",
            "Hao Yan",
            "Kamran Paynabar"
        ],
        "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data",
        "abstract": "arXiv:2509.18354v1 Announce Type: new  Abstract: Anomaly detection in images is typically addressed by learning from collections of training data or relying on reference samples. In many real-world scenarios, however, such training data may be unavailable, and only the test image itself is provided. We address this zero-shot setting by proposing a single-image anomaly localization method that leverages the inductive bias of convolutional neural networks, inspired by Deep Image Prior (DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key assumption is that natural images often exhibit unified textures and patterns, and that anomalies manifest as localized deviations from these repetitive or stochastic patterns. To learn the deep image prior, we design a patch-based training framework where the input image is fed directly into the network for self-reconstruction, rather than mapping random noise to the image as done in DIP. To avoid the model simply learning an identity mapping, we apply masking, patch shuffling, and small Gaussian noise. In addition, we use a perceptual loss based on inner-product similarity to capture structure beyond pixel fidelity. Our approach needs no external training data, labels, or references, and remains robust in the presence of noise or missing pixels. SSDnet achieves 0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the fabric dataset, outperforming state-of-the-art methods. The implementation code will be released at https://github.com/mehrdadmoradi124/SSDnet",
        "arxiv_id": "2509.18354",
        "ARXIVID": "2509.18354",
        "COMMENT": "Does not match any specific criteria but involves anomaly detection, which is tangentially related to computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16590": {
        "authors": [
            "Manuel Borroto",
            "Katie Gallagher",
            "Antonio Ielo",
            "Irfan Kareem",
            "Francesco Ricca",
            "Alessandra Russo"
        ],
        "title": "Question Answering with LLMs and Learning from Answer Sets",
        "abstract": "arXiv:2509.16590v1 Announce Type: new  Abstract: Large Language Models (LLMs) excel at understanding natural language but struggle with explicit commonsense reasoning. A recent trend of research suggests that the combination of LLM with robust symbolic reasoning systems can overcome this problem on story-based question answering tasks. In this setting, existing approaches typically depend on human expertise to manually craft the symbolic component. We argue, however, that this component can also be automatically learned from examples. In this work, we introduce LLM2LAS, a hybrid system that effectively combines the natural language understanding capabilities of LLMs, the rule induction power of the Learning from Answer Sets (LAS) system ILASP, and the formal reasoning strengths of Answer Set Programming (ASP). LLMs are used to extract semantic structures from text, which ILASP then transforms into interpretable logic rules. These rules allow an ASP solver to perform precise and consistent reasoning, enabling correct answers to previously unseen questions. Empirical results outline the strengths and weaknesses of our automatic approach for learning and reasoning in a story-based question answering benchmark.",
        "arxiv_id": "2509.16590",
        "ARXIVID": "2509.16590",
        "COMMENT": "Does not match any specific criteria but involves reasoning and LLMs, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.19258": {
        "authors": [
            "Dheerendranath Battalapalli",
            "Apoorva Safai",
            "Maria Jaramillo",
            "Hyemin Um",
            "Gustavo Adalfo Pineda Ortiz",
            "Ulas Bagci",
            "Manmeet Singh Ahluwalia",
            "Marwa Ismail",
            "Pallavi Tiwari"
        ],
        "title": "Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies",
        "abstract": "arXiv:2509.19258v1 Announce Type: new  Abstract: A significant challenge in solid tumors is reliably distinguishing confounding pathologies from malignant neoplasms on routine imaging. While radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI, many aggregate features across the region of interest (ROI) and miss complex spatial relationships among varying intensity compositions. We present a new Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of sub-regions using per-voxel radiomic measurements, then (2) computes graph-theoretic metrics to quantify spatial associations among clusters. The resulting weighted graphs encode higher-order spatial relationships within the ROI, aiming to reliably capture ILH and disambiguate confounding pathologies from malignancy. To assess efficacy and clinical feasibility, GrRAiL was evaluated in n=947 subjects spanning three use cases: differentiating tumor recurrence from radiation effects in glioblastoma (GBM; n=106) and brain metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In GBM, cross-validation (CV) and test accuracies for recurrence vs pseudo-progression were 89% and 78% with >10% test-accuracy gains over comparators. In brain metastasis, CV and test accuracies for recurrence vs radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk stratification, CV and test accuracies were 84% and 75%, showing >10% improvement.",
        "arxiv_id": "2509.19258",
        "ARXIVID": "2509.19258",
        "COMMENT": "Does not match any specific criterion but is related to machine learning applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17550": {
        "authors": [
            "Neslihan Kose",
            "Anthony Rhodes",
            "Umur Aybars Ciftci",
            "Ilke Demir"
        ],
        "title": "Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem",
        "abstract": "arXiv:2509.17550v1 Announce Type: new  Abstract: As generative models are advancing in quality and quantity for creating synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors are proposed to counter this effect, however, misuse of detectors claiming fake content as real or vice versa further fuels this misinformation problem. We present the first comprehensive uncertainty analysis of deepfake detectors, systematically investigating how generative artifacts influence prediction confidence. As reflected in detectors' responses, deepfake generators also contribute to this uncertainty as their generative residues vary, so we cross the uncertainty analysis of deepfake detectors and generators. Based on our observations, the uncertainty manifold holds enough consistent information to leverage uncertainty for deepfake source detection. Our approach leverages Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and epistemic uncertainties across diverse detector architectures. We evaluate uncertainty on two datasets with nine generators, with four blind and two biological detectors, compare different uncertainty methods, explore region- and pixel-based uncertainty, and conduct ablation studies. We conduct and analyze binary real/fake, multi-class real/fake, source detection, and leave-one-out experiments between the generator/detector combinations to share their generalization capability, model calibration, uncertainty, and robustness against adversarial attacks. We further introduce uncertainty maps that localize prediction confidence at the pixel level, revealing distinct patterns correlated with generator-specific artifacts. Our analysis provides critical insights for deploying reliable deepfake detection systems and establishes uncertainty quantification as a fundamental requirement for trustworthy synthetic media detection.",
        "arxiv_id": "2509.17550",
        "ARXIVID": "2509.17550",
        "COMMENT": "Does not match any specific criteria but is tangentially related to generative modeling and detection, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18502": {
        "authors": [
            "Wenjie Liu",
            "Hongmin Liu",
            "Lixin Zhang",
            "Bin Fan"
        ],
        "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment",
        "abstract": "arXiv:2509.18502v1 Announce Type: new  Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of remote sensing images has been extensively conducted. However, research on how to achieve domain adaptation in practical scenarios where source domain data is inaccessible namely, source-free domain adaptation (SFDA) remains limited. Self-training has been widely used in SFDA, which requires obtaining as many high-quality pseudo-labels as possible to train models on target domain data. Most existing methods optimize the entire pseudo-label set to obtain more supervisory information. However, as pseudo-label sets often contain substantial noise, simultaneously optimizing all labels is challenging. This limitation undermines the effectiveness of optimization approaches and thus restricts the performance of self-training. To address this, we propose a novel pseudo-label optimization framework called Diffusion-Guided Label Enrichment (DGLE), which starts from a few easily obtained high-quality pseudo-labels and propagates them to a complete set of pseudo-labels while ensuring the quality of newly generated labels. Firstly, a pseudo-label fusion method based on confidence filtering and super-resolution enhancement is proposed, which utilizes cross-validation of details and contextual information to obtain a small number of high-quality pseudo-labels as initial seeds. Then, we leverage the diffusion model to propagate incomplete seed pseudo-labels with irregular distributions due to its strong denoising capability for randomly distributed noise and powerful modeling capacity for complex distributions, thereby generating complete and high-quality pseudo-labels. This method effectively avoids the difficulty of directly optimizing the complete set of pseudo-labels, significantly improves the quality of pseudo-labels, and thus enhances the model's performance in the target domain.",
        "arxiv_id": "2509.18502",
        "ARXIVID": "2509.18502",
        "COMMENT": "Does not match any specific criteria but is related to domain adaptation in remote sensing, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18550": {
        "authors": [
            "Mohammad Junayed Hasan",
            "Nabeel Mohammed",
            "Shafin Rahman",
            "Philipp Koehn"
        ],
        "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles",
        "abstract": "arXiv:2509.18550v1 Announce Type: new  Abstract: The distinction between genuine and posed emotions represents a fundamental pattern recognition challenge with significant implications for data mining applications in social sciences, healthcare, and human-computer interaction. While recent multi-task learning frameworks have shown promise in combining deep learning architectures with handcrafted D-Marker features for smile facial emotion recognition, these approaches exhibit computational inefficiencies due to auxiliary task supervision and complex loss balancing requirements. This paper introduces HadaSmileNet, a novel feature fusion framework that directly integrates transformer-based representations with physiologically grounded D-Markers through parameter-free multiplicative interactions. Through systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard multiplicative fusion achieves optimal performance by enabling direct feature interactions while maintaining computational efficiency. The proposed approach establishes new state-of-the-art results for deep learning methods across four benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS (98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational analysis reveals 26 percent parameter reduction and simplified training compared to multi-task alternatives, while feature visualization demonstrates enhanced discriminative power through direct domain knowledge integration. The framework's efficiency and effectiveness make it particularly suitable for practical deployment in multimedia data mining applications that require real-time affective computing capabilities.",
        "arxiv_id": "2509.18550",
        "ARXIVID": "2509.18550",
        "COMMENT": "Does not match any specific criteria. Focuses on emotion recognition using feature fusion, which is unrelated to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18308": {
        "authors": [
            "Yixin Zhang",
            "Ryan Chamberlain",
            "Lawrance Ngo",
            "Kevin Kramer",
            "Maciej A. Mazurowski"
        ],
        "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model",
        "abstract": "arXiv:2509.18308v1 Announce Type: new  Abstract: In this study, we curated a densely annotated in-house dataset comprising 490 CTPA scans. Using this dataset, we systematically evaluated nine widely used segmentation architectures from both the CNN and Vision Transformer (ViT) families, initialized with either pretrained or random weights, under a unified testing framework as a performance audit. Our study leads to several important observations: (1) 3D U-Net with a ResNet encoder remains a highly effective architecture for PE segmentation; (2) 3D models are particularly well-suited to this task given the morphological characteristics of emboli; (3) CNN-based models generally yield superior performance compared to their ViT-based counterparts in PE segmentation; (4) classification-based pretraining, even on large PE datasets, can adversely impact segmentation performance compared to training from scratch, suggesting that PE classification and segmentation may rely on different sets of discriminative features; (5) different model architectures show a highly consistent pattern of segmentation performance when trained on the same data; and (6) while central and large emboli can be segmented with satisfactory accuracy, distal emboli remain challenging due to both task complexity and the scarcity of high-quality datasets. Besides these findings, our best-performing model achieves a mean Dice score of 0.7131 for segmentation. It detects 181 emboli with 49 false positives and 28 false negatives from 60 in-house testing scans. Its generalizability is further validated on public datasets.",
        "arxiv_id": "2509.18308",
        "ARXIVID": "2509.18308",
        "COMMENT": "Does not match any specific criterion but is related to segmentation tasks in medical imaging, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.18958": {
        "authors": [
            "Cristina Iacono",
            "Mariarosaria Meola",
            "Federica Conte",
            "Laura Mecozzi",
            "Umberto Bracale",
            "Pietro Falco",
            "Fanny Ficuciello"
        ],
        "title": "Generative data augmentation for biliary tract detection on intraoperative images",
        "abstract": "arXiv:2509.18958v1 Announce Type: new  Abstract: Cholecystectomy is one of the most frequently performed procedures in gastrointestinal surgery, and the laparoscopic approach is the gold standard for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the advantages of a significantly faster recovery and better cosmetic results, the laparoscopic approach bears a higher risk of bile duct injury, which has a significant impact on quality of life and survival. To avoid bile duct injury, it is essential to improve the intraoperative visualization of the bile duct. This work aims to address this problem by leveraging a deep-learning approach for the localization of the biliary tract from white-light images acquired during the surgical procedures. To this end, the construction and annotation of an image database to train the Yolo detection algorithm has been employed. Besides classical data augmentation techniques, the paper proposes Generative Adversarial Network (GAN) for the generation of a synthetic portion of the training dataset. Experimental results have been discussed along with ethical considerations.",
        "arxiv_id": "2509.18958",
        "ARXIVID": "2509.18958",
        "COMMENT": "Does not match any specific criteria but involves generative modeling for medical imaging, which is tangentially related to the friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.18326": {
        "authors": [
            "Chun Kit Wong",
            "Anders N. Christensen",
            "Cosmin I. Bercea",
            "Julia A. Schnabel",
            "Martin G. Tolsgaard",
            "Aasa Feragen"
        ],
        "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound",
        "abstract": "arXiv:2509.18326v1 Announce Type: new  Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment of deep learning models in fetal ultrasound amidst heterogeneous image characteristics and clinical settings. OOD detection relies on estimating a classification model's uncertainty, which should increase for OOD samples. While existing research has largely focused on uncertainty quantification methods, this work investigates the impact of the classification task itself. Through experiments with eight uncertainty quantification methods across four classification tasks, we demonstrate that OOD detection performance significantly varies with the task, and that the best task depends on the defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an image characteristic shift or ii) an anatomical feature shift. Furthermore, we reveal that superior OOD detection does not guarantee optimal abstained prediction, underscoring the necessity to align task selection and uncertainty strategies with the specific downstream application in medical image analysis.",
        "arxiv_id": "2509.18326",
        "ARXIVID": "2509.18326",
        "COMMENT": "Does not match any specific criteria but is tangentially related to computer vision and machine learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}