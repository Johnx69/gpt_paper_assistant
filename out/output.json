{
    "2511.07412": {
        "authors": [
            "Han Zhang",
            "Yiqing Shen",
            "Roger D. Soberanis-Mukul",
            "Ankita Ghosh",
            "Hao Ding",
            "Lalithkumar Seenivasan",
            "Jose L. Porras",
            "Zhekai Mao",
            "Chenjia Li",
            "Wenjie Xiao",
            "Lonny Yarmus",
            "Angela Christine Argento",
            "Masaru Ishii",
            "Mathias Unberath"
        ],
        "title": "TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research",
        "abstract": "arXiv:2511.07412v1 Announce Type: new  Abstract: Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real.",
        "arxiv_id": "2511.07412",
        "ARXIVID": "2511.07412",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and simulator for embodied AI in the form of photorealistic digital twins of operating rooms.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.06499": {
        "authors": [
            "Haotian Xia",
            "Haonan Ge",
            "Junbo Zou",
            "Hyun Woo Choi",
            "Xuebin Zhang",
            "Danny Suradja",
            "Botao Rui",
            "Ethan Tran",
            "Wendy Jin",
            "Zhen Ye",
            "Xiyang Lin",
            "Christopher Lai",
            "Shengjie Zhang",
            "Junwen Miao",
            "Shichao Chen",
            "Rhys Tracy",
            "Vicente Ordonez",
            "Weining Shen",
            "Hanjie Chen"
        ],
        "title": "SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports",
        "abstract": "arXiv:2511.06499v1 Announce Type: new  Abstract: Deeply understanding sports requires an intricate blend of fine-grained visual perception and rule-based reasoning - a challenge that pushes the limits of current multimodal models. To succeed, models must master three critical capabilities: perceiving nuanced visual details, applying abstract sport rule knowledge, and grounding that knowledge in specific visual evidence. Current sports benchmarks either cover single sports or lack the detailed reasoning chains and precise visual grounding needed to robustly evaluate these core capabilities in a multi-sport context. To address this gap, we introduce SportR, the first multi-sports large-scale benchmark designed to train and evaluate MLLMs on the fundamental reasoning required for sports intelligence. Our benchmark provides a dataset of 5,017 images and 2,101 videos. To enable granular evaluation, we structure our benchmark around a progressive hierarchy of question-answer (QA) pairs designed to probe reasoning at increasing depths - from simple infraction identification to complex penalty prediction. For the most advanced tasks requiring multi-step reasoning, such as determining penalties or explaining tactics, we provide 7,118 high-quality, human-authored Chain of Thought (CoT) annotations. In addition, our benchmark incorporates both image and video modalities and provides manual bounding box annotations to test visual grounding in the image part directly. Extensive experiments demonstrate the profound difficulty of our benchmark. State-of-the-art baseline models perform poorly on our most challenging tasks. While training on our data via Supervised Fine-Tuning and Reinforcement Learning improves these scores, they remain relatively low, highlighting a significant gap in current model capabilities. SportR presents a new challenge for the community, providing a critical resource to drive future research in multimodal sports reasoning.",
        "arxiv_id": "2511.06499",
        "ARXIVID": "2511.06499",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a benchmark for multimodal large language model reasoning in sports, integrating vision and language tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.06846": {
        "authors": [
            "Federico Vasile",
            "Ri-Zhao Qiu",
            "Lorenzo Natale",
            "Xiaolong Wang"
        ],
        "title": "Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders",
        "abstract": "arXiv:2511.06846v1 Announce Type: new  Abstract: System identification involving the geometry, appearance, and physical properties from video observations is a challenging task with applications in robotics and graphics. Recent approaches have relied on fully differentiable Material Point Method (MPM) and rendering for simultaneous optimization of these properties. However, they are limited to simplified object-environment interactions with planar colliders and fail in more challenging scenarios where objects collide with non-planar surfaces. We propose AS-DiffMPM, a differentiable MPM framework that enables physical property estimation with arbitrarily shaped colliders. Our approach extends existing methods by incorporating a differentiable collision handling mechanism, allowing the target object to interact with complex rigid bodies while maintaining end-to-end optimization. We show AS-DiffMPM can be easily interfaced with various novel view synthesis methods as a framework for system identification from visual observations.",
        "arxiv_id": "2511.06846",
        "ARXIVID": "2511.06846",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for system identification in robotics using differentiable physics simulation with complex colliders.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.06722": {
        "authors": [
            "Jianyu Qi",
            "Ding Zou",
            "Wenrui Yan",
            "Rui Ma",
            "Jiaxu Li",
            "Zhijie Zheng",
            "Zhiguo Yang",
            "Rongchang Zhao"
        ],
        "title": "Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View",
        "abstract": "arXiv:2511.06722v1 Announce Type: new  Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have spurred significant progress in Chain-of-Thought (CoT) reasoning. Building on the success of Deepseek-R1, researchers extended multimodal reasoning to post-training paradigms based on reinforcement learning (RL), focusing predominantly on mathematical datasets. However, existing post-training paradigms tend to neglect two critical aspects: (1) The lack of quantifiable difficulty metrics capable of strategically screening samples for post-training optimization. (2) Suboptimal post-training paradigms that fail to jointly optimize perception and reasoning capabilities. To address this gap, we propose two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) quantifies sample hardness through systematic image degradation, while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction complexity via attention distribution analysis. Leveraging these metrics, we design a hierarchical training framework that incorporates both GRPO-only and SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark datasets. Experiments demonstrate consistent superiority of GRPO applied to difficulty-stratified samples compared to conventional SFT+GRPO pipelines, indicating that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy. Our code will be released at https://github.com/qijianyu277/DifficultySampling.",
        "arxiv_id": "2511.06722",
        "ARXIVID": "2511.06722",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) with novel training strategies and difficulty-aware sampling.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2511.06665": {
        "authors": [
            "Lingran Song",
            "Yucheng Zhou",
            "Jianbing Shen"
        ],
        "title": "Sim4Seg: Boosting Multimodal Multi-disease Medical Diagnosis Segmentation with Region-Aware Vision-Language Similarity Masks",
        "abstract": "arXiv:2511.06665v1 Announce Type: new  Abstract: Despite significant progress in pixel-level medical image analysis, existing medical image segmentation models rarely explore medical segmentation and diagnosis tasks jointly. However, it is crucial for patients that models can provide explainable diagnoses along with medical segmentation results. In this paper, we introduce a medical vision-language task named Medical Diagnosis Segmentation (MDS), which aims to understand clinical queries for medical images and generate the corresponding segmentation masks as well as diagnostic results. To facilitate this task, we first present the Multimodal Multi-disease Medical Diagnosis Segmentation (M3DS) dataset, containing diverse multimodal multi-disease medical images paired with their corresponding segmentation masks and diagnosis chain-of-thought, created via an automated diagnosis chain-of-thought generation pipeline. Moreover, we propose Sim4Seg, a novel framework that improves the performance of diagnosis segmentation by taking advantage of the Region-Aware Vision-Language Similarity to Mask (RVLS2M) module. To improve overall performance, we investigate a test-time scaling strategy for MDS tasks. Experimental results demonstrate that our method outperforms the baselines in both segmentation and diagnosis.",
        "arxiv_id": "2511.06665",
        "ARXIVID": "2511.06665",
        "COMMENT": "Matches criteria 5 as it integrates vision-language similarity for medical diagnosis segmentation, combining image understanding and language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.07049": {
        "authors": [
            "Hui Lu",
            "Yi Yu",
            "Song Xia",
            "Yiming Yang",
            "Deepu Rajan",
            "Boon Poh Ng",
            "Alex Kot",
            "Xudong Jiang"
        ],
        "title": "From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge",
        "abstract": "arXiv:2511.07049v1 Announce Type: new  Abstract: Large-scale Video Foundation Models (VFMs) has significantly advanced various video-related tasks, either through task-specific models or Multi-modal Large Language Models (MLLMs). However, the open accessibility of VFMs also introduces critical security risks, as adversaries can exploit full knowledge of the VFMs to launch potent attacks. This paper investigates a novel and practical adversarial threat scenario: attacking downstream models or MLLMs fine-tuned from open-source VFMs, without requiring access to the victim task, training data, model query, and architecture. In contrast to conventional transfer-based attacks that rely on task-aligned surrogate models, we demonstrate that adversarial vulnerabilities can be exploited directly from the VFMs. To this end, we propose the Transferable Video Attack (TVA), a temporal-aware adversarial attack method that leverages the temporal representation dynamics of VFMs to craft effective perturbations. TVA integrates a bidirectional contrastive learning mechanism to maximize the discrepancy between the clean and adversarial features, and introduces a temporal consistency loss that exploits motion cues to enhance the sequential impact of perturbations. TVA avoids the need to train expensive surrogate models or access to domain-specific data, thereby offering a more practical and efficient attack strategy. Extensive experiments across 24 video-related tasks demonstrate the efficacy of TVA against downstream models and MLLMs, revealing a previously underexplored security vulnerability in the deployment of video models.",
        "arxiv_id": "2511.07049",
        "ARXIVID": "2511.07049",
        "COMMENT": "Matches criteria 2 and 6 as it explores vulnerabilities in Video Foundation Models (VFMs) and their application in video-related tasks, including MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.06908": {
        "authors": [
            "Yuzhen Li",
            "Min Liu",
            "Zhaoyang Li",
            "Yuan Bian",
            "Xueping Wang",
            "Erbo Zhai",
            "Yaonan Wang"
        ],
        "title": "Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding",
        "abstract": "arXiv:2511.06908v1 Announce Type: new  Abstract: Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D objects in RGB images using text descriptions with geometric cues. However, existing methods face two key limitations. Firstly, they often over-rely on high-certainty keywords that explicitly identify the target object while neglecting critical spatial descriptions. Secondly, generalized textual features contain both 2D and 3D descriptive information, thereby capturing an additional dimension of details compared to singular 2D or 3D visual features. This characteristic leads to cross-dimensional interference when refining visual features under text guidance. To overcome these challenges, we propose Mono3DVG-EnSD, a novel framework that integrates two key components: the CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while retaining low-certainty implicit spatial descriptions, thereby forcing the model to develop a deeper understanding of spatial relationships in captions for object localization. Meanwhile, the D2M decouples dimension-specific (2D/3D) textual features from generalized textual features to guide corresponding visual features at same dimension, which mitigates cross-dimensional interference by ensuring dimensionally-consistent cross-modal interactions. Through comprehensive comparisons and ablation studies on the Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario by a significant +13.54%.",
        "arxiv_id": "2511.06908",
        "ARXIVID": "2511.06908",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on spatial-aware text encoding for 3D visual grounding.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2511.06055": {
        "authors": [
            "Animesh Karnewar",
            "Denis Korzhenkov",
            "Ioannis Lelekas",
            "Adil Karjauv",
            "Noor Fathima",
            "Hanwen Xiong",
            "Vancheeswaran Vaidyanathan",
            "Will Zeng",
            "Rafael Esteves",
            "Tushar Singhal",
            "Fatih Porikli",
            "Mohsen Ghafoorian",
            "Amirhossein Habibian"
        ],
        "title": "Neodragon: Mobile Video Generation using Diffusion Transformer",
        "abstract": "arXiv:2511.06055v1 Announce Type: new  Abstract: We introduce Neodragon, a text-to-video system capable of generating 2s (49 frames @24 fps) videos at the 640x1024 resolution directly on a Qualcomm Hexagon NPU in a record 6.7s (7 FPS). Differing from existing transformer-based offline text-to-video generation models, Neodragon is the first to have been specifically optimised for mobile hardware to achieve efficient and high-fidelity video synthesis. We achieve this through four key technical contributions: (1) Replacing the original large 4.762B T5xxl Text-Encoder with a much smaller 0.2B DT5 (DistilT5) with minimal quality loss, enabled through a novel Text-Encoder Distillation procedure. (2) Proposing an Asymmetric Decoder Distillation approach allowing us to replace the native codec-latent-VAE decoder with a more efficient one, without disturbing the generative latent-space of the generation pipeline. (3) Pruning of MMDiT blocks within the denoiser backbone based on their relative importance, with recovery of original performance through a two-stage distillation process. (4) Reducing the NFE (Neural Functional Evaluation) requirement of the denoiser by performing step distillation using DMD adapted for pyramidal flow-matching, thereby substantially accelerating video generation. When paired with an optimised SSD1B first-frame image generator and QuickSRNet for 2x super-resolution, our end-to-end Neodragon system becomes a highly parameter (4.945B full model), memory (3.5GB peak RAM usage), and runtime (6.7s E2E latency) efficient mobile-friendly model, while achieving a VBench total score of 81.61. By enabling low-cost, private, and on-device text-to-video synthesis, Neodragon democratizes AI-based video content creation, empowering creators to generate high-quality videos without reliance on cloud services. Code and model will be made publicly available at our website: https://qualcomm-ai-research.github.io/neodragon",
        "arxiv_id": "2511.06055",
        "ARXIVID": "2511.06055",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a mobile-optimized text-to-video generation system with novel methodologies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.06457": {
        "authors": [
            "Shaoxiang Wang",
            "Shihong Zhang",
            "Christen Millerdurai",
            "R\\\"udiger Westermann",
            "Didier Stricker",
            "Alain Pagani"
        ],
        "title": "Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360{\\deg} Scenes",
        "abstract": "arXiv:2511.06457v1 Announce Type: new  Abstract: Despite recent advances in single-object front-facing inpainting using NeRF and 3D Gaussian Splatting (3DGS), inpainting in complex 360{\\deg} scenes remains largely underexplored. This is primarily due to three key challenges: (i) identifying target objects in the 3D field of 360{\\deg} environments, (ii) dealing with severe occlusions in multi-object scenes, which makes it hard to define regions to inpaint, and (iii) maintaining consistent and high-quality appearance across views effectively. To tackle these challenges, we propose Inpaint360GS, a flexible 360{\\deg} editing framework based on 3DGS that supports multi-object removal and high-fidelity inpainting in 3D space. By distilling 2D segmentation into 3D and leveraging virtual camera views for contextual guidance, our method enables accurate object-level editing and consistent scene completion. We further introduce a new dataset tailored for 360{\\deg} inpainting, addressing the lack of ground truth object-free scenes. Experiments demonstrate that Inpaint360GS outperforms existing baselines and achieves state-of-the-art performance. Project page: https://dfki-av.github.io/inpaint360gs/",
        "arxiv_id": "2511.06457",
        "ARXIVID": "2511.06457",
        "COMMENT": "Matches criterion 4 as it focuses on a novel application of 3D Gaussian Splatting for 360-degree scene inpainting, which is a vision foundation model application.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.06046": {
        "authors": [
            "Zhihui Ke",
            "Yuyang Liu",
            "Xiaobo Zhou",
            "Tie Qiu"
        ],
        "title": "StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video",
        "abstract": "arXiv:2511.06046v1 Announce Type: new  Abstract: Streaming free-viewpoint video~(FVV) in real-time still faces significant challenges, particularly in training, rendering, and transmission efficiency. Harnessing superior performance of 3D Gaussian Splatting~(3DGS), recent 3DGS-based FVV methods have achieved notable breakthroughs in both training and rendering. However, the storage requirements of these methods can reach up to $10$MB per frame, making stream FVV in real-time impossible. To address this problem, we propose a novel FVV representation, dubbed StreamSTGS, designed for real-time streaming. StreamSTGS represents a dynamic scene using canonical 3D Gaussians, temporal features, and a deformation field. For high compression efficiency, we encode canonical Gaussian attributes as 2D images and temporal features as a video. This design not only enables real-time streaming, but also inherently supports adaptive bitrate control based on network condition without any extra training. Moreover, we propose a sliding window scheme to aggregate adjacent temporal features to learn local motions, and then introduce a transformer-guided auxiliary training module to learn global motions. On diverse FVV benchmarks, StreamSTGS demonstrates competitive performance on all metrics compared to state-of-the-art methods. Notably, StreamSTGS increases the PSNR by an average of $1$dB while reducing the average frame size to just $170$KB. The code is publicly available on https://github.com/kkkzh/StreamSTGS.",
        "arxiv_id": "2511.06046",
        "ARXIVID": "2511.06046",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding with a novel representation for real-time free-viewpoint video streaming.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.06310": {
        "authors": [
            "Seunghyeok Shin",
            "Dabin Kim",
            "Hongki Lim"
        ],
        "title": "Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates",
        "abstract": "arXiv:2511.06310v1 Announce Type: new  Abstract: Reconstructing high-quality point clouds from images remains challenging in computer vision. Existing generative-model-based approaches, particularly diffusion-model approaches that directly learn the posterior, may suffer from inflexibility -- they require conditioning signals during training, support only a fixed number of input views, and need complete retraining for different measurements. Recent diffusion-based methods have attempted to address this by combining prior models with likelihood updates, but they rely on heuristic fixed step sizes for the likelihood update that lead to slow convergence and suboptimal reconstruction quality. We advance this line of approach by integrating our novel Forward Curvature-Matching (FCM) update method with diffusion sampling. Our method dynamically determines optimal step sizes using only forward automatic differentiation and finite-difference curvature estimates, enabling precise optimization of the likelihood update. This formulation enables high-fidelity reconstruction from both single-view and multi-view inputs, and supports various input modalities through simple operator substitution -- all without retraining. Experiments on ShapeNet and CO3D datasets demonstrate that our method achieves superior reconstruction quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD, validating its efficiency and adaptability for practical applications. Code is available at https://github.com/Seunghyeok0715/FCM",
        "arxiv_id": "2511.06310",
        "ARXIVID": "2511.06310",
        "COMMENT": "Matches criteria 4 as it focuses on high-quality 3D reconstruction using diffusion priors, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2511.06283": {
        "authors": [
            "Xuanle Zhao",
            "Shuxin Zeng",
            "Yinyuan Cai",
            "Xiang Cheng",
            "Duzhen Zhang",
            "Xiuyi Chen",
            "Bo Xu"
        ],
        "title": "TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks",
        "abstract": "arXiv:2511.06283v1 Announce Type: new  Abstract: While Vision Language Models (VLMs) have demonstrated remarkable capabilities in general visual understanding, their application in the chemical domain has been limited, with previous works predominantly focusing on text and thus overlooking critical visual information, such as molecular structures. Current approaches that directly adopt standard VLMs for chemical tasks suffer from two primary issues: (i) computational inefficiency of processing entire chemical images with non-informative backgrounds. (ii) a narrow scope on molecular-level tasks that restricts progress in chemical reasoning. In this work, we propose \\textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages visual token reduction and reaction-level tasks to improve model efficiency and reasoning capacity. Also, we propose \\textbf{ChemRxn-V}, a reaction-level benchmark for assessing vision-based reaction recognition and prediction tasks. Directly predicting reaction products from molecular images poses a non-trivial challenge, as it requires models to integrate both recognition and reasoning capacities. Our results demonstrate that with only 4B parameters, TinyChemVL achieves superior performance on both molecular and reaction tasks while demonstrating faster inference and training speeds compared to existing models. Notably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the visual tokens. This work builds efficient yet powerful VLMs for chemical domains by co-designing model architecture and task complexity.",
        "arxiv_id": "2511.06283",
        "ARXIVID": "2511.06283",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores a vision-language model (TinyChemVL) with novel architectural and task-specific improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.05935": {
        "authors": [
            "Lin Li",
            "Chuhan Zhang",
            "Dong Zhang",
            "Chong Sun",
            "Chen Li",
            "Long Chen"
        ],
        "title": "Interaction-Centric Knowledge Infusion and Transfer for Open-Vocabulary Scene Graph Generation",
        "abstract": "arXiv:2511.05935v1 Announce Type: new  Abstract: Open-vocabulary scene graph generation (OVSGG) extends traditional SGG by recognizing novel objects and relationships beyond predefined categories, leveraging the knowledge from pre-trained large-scale models. Existing OVSGG methods always adopt a two-stage pipeline: 1) \\textit{Infusing knowledge} into large-scale models via pre-training on large datasets; 2) \\textit{Transferring knowledge} from pre-trained models with fully annotated scene graphs during supervised fine-tuning. However, due to a lack of explicit interaction modeling, these methods struggle to distinguish between interacting and non-interacting instances of the same object category. This limitation induces critical issues in both stages of OVSGG: it generates noisy pseudo-supervision from mismatched objects during knowledge infusion, and causes ambiguous query matching during knowledge transfer. To this end, in this paper, we propose an inter\\textbf{AC}tion-\\textbf{C}entric end-to-end OVSGG framework (\\textbf{ACC}) in an interaction-driven paradigm to minimize these mismatches. For \\textit{interaction-centric knowledge infusion}, ACC employs a bidirectional interaction prompt for robust pseudo-supervision generation to enhance the model's interaction knowledge. For \\textit{interaction-centric knowledge transfer}, ACC first adopts interaction-guided query selection that prioritizes pairing interacting objects to reduce interference from non-interacting ones. Then, it integrates interaction-consistent knowledge distillation to bolster robustness by pushing relational foreground away from the background while retaining general knowledge. Extensive experimental results on three benchmarks show that ACC achieves state-of-the-art performance, demonstrating the potential of interaction-centric paradigms for real-world applications.",
        "arxiv_id": "2511.05935",
        "ARXIVID": "2511.05935",
        "COMMENT": "Matches criteria 2 as it focuses on open-vocabulary scene graph generation leveraging pre-trained large-scale models, which aligns with vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.05705": {
        "authors": [
            "David Acuna",
            "Chao-Han Huck Yang",
            "Yuntian Deng",
            "Jaehun Jung",
            "Ximing Lu",
            "Prithviraj Ammanabrolu",
            "Hyunwoo Kim",
            "Yuan-Hong Liao",
            "Yejin Choi"
        ],
        "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale",
        "abstract": "arXiv:2511.05705v1 Announce Type: new  Abstract: Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.",
        "arxiv_id": "2511.05705",
        "ARXIVID": "2511.05705",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on vision-centric reasoning datasets and multimodal reasoning with vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.05682": {
        "authors": [
            "Yujin Potter",
            "Zhun Wang",
            "Nicholas Crispino",
            "Kyle Montgomery",
            "Alexander Xiong",
            "Ethan Y. Chang",
            "Francesco Pinto",
            "Yuqi Chen",
            "Rahul Gupta",
            "Morteza Ziyadi",
            "Christos Christodoulopoulos",
            "Bo Li",
            "Chenguang Wang",
            "Dawn Song"
        ],
        "title": "VMDT: Decoding the Trustworthiness of Video Foundation Models",
        "abstract": "arXiv:2511.05682v1 Announce Type: new  Abstract: As foundation models become more sophisticated, ensuring their trustworthiness becomes increasingly critical; yet, unlike text and image, the video modality still lacks comprehensive trustworthiness benchmarks. We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness. Through our extensive evaluation of 7 T2V models and 19 V2T models using VMDT, we uncover several significant insights. For instance, all open-source T2V models evaluated fail to recognize harmful queries and often generate harmful videos, while exhibiting higher levels of unfairness compared to image modality models. In V2T models, unfairness and privacy risks rise with scale, whereas hallucination and adversarial robustness improve -- though overall performance remains low. Uniquely, safety shows no correlation with model size, implying that factors other than scale govern current safety levels. Our findings highlight the urgent need for developing more robust and trustworthy video foundation models, and VMDT provides a systematic framework for measuring and tracking progress toward this goal. The code is available at https://sunblaze-ucb.github.io/VMDT-page/.",
        "arxiv_id": "2511.05682",
        "ARXIVID": "2511.05682",
        "COMMENT": "Matches criteria 3 and 6 as it introduces a new benchmark (VMDT) for evaluating trustworthiness in video foundation models, which is relevant to video understanding and embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.07399": {
        "authors": [
            "Tianrui Feng",
            "Zhi Li",
            "Shuo Yang",
            "Haocheng Xi",
            "Muyang Li",
            "Xiuyu Li",
            "Lvmin Zhang",
            "Keting Yang",
            "Kelly Peng",
            "Song Han",
            "Maneesh Agrawala",
            "Kurt Keutzer",
            "Akio Kodaira",
            "Chenfeng Xu"
        ],
        "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
        "abstract": "arXiv:2511.07399v1 Announce Type: new  Abstract: Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
        "arxiv_id": "2511.07399",
        "ARXIVID": "2511.07399",
        "COMMENT": "Matches criteria 5 and 6 as it focuses on video generation and streaming systems with novel techniques for integrating video understanding and generation tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2511.06678": {
        "authors": [
            "Xingbo Du",
            "Qiantong Dou",
            "Lei Fan",
            "Rui Zhang"
        ],
        "title": "Flexible Concept Bottleneck Model",
        "abstract": "arXiv:2511.06678v1 Announce Type: new  Abstract: Concept bottleneck models (CBMs) improve neural network interpretability by introducing an intermediate layer that maps human-understandable concepts to predictions. Recent work has explored the use of vision-language models (VLMs) to automate concept selection and annotation. However, existing VLM-based CBMs typically require full model retraining when new concepts are involved, which limits their adaptability and flexibility in real-world scenarios, especially considering the rapid evolution of vision-language foundation models. To address these issues, we propose Flexible Concept Bottleneck Model (FCBM), which supports dynamic concept adaptation, including complete replacement of the original concept set. Specifically, we design a hypernetwork that generates prediction weights based on concept embeddings, allowing seamless integration of new concepts without retraining the entire model. In addition, we introduce a modified sparsemax module with a learnable temperature parameter that dynamically selects the most relevant concepts, enabling the model to focus on the most informative features. Extensive experiments on five public benchmarks demonstrate that our method achieves accuracy comparable to state-of-the-art baselines with a similar number of effective concepts. Moreover, the model generalizes well to unseen concepts with just a single epoch of fine-tuning, demonstrating its strong adaptability and flexibility.",
        "arxiv_id": "2511.06678",
        "ARXIVID": "2511.06678",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores a flexible concept bottleneck model leveraging vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.05929": {
        "authors": [
            "Jiaxuan Li",
            "Qing Xu",
            "Xiangjian He",
            "Ziyu Liu",
            "Chang Xing",
            "Zhen Chen",
            "Daokun Zhang",
            "Rong Qu",
            "Chang Wen Chen"
        ],
        "title": "CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework",
        "abstract": "arXiv:2511.05929v1 Announce Type: new  Abstract: Masked Autoencoders (MAE) achieve self-supervised learning of image representations by randomly removing a portion of visual tokens and reconstructing the original image as a pretext task, thereby significantly enhancing pretraining efficiency and yielding excellent adaptability across downstream tasks. However, MAE and other MAE-style paradigms that adopt random masking generally require more pre-training epochs to maintain adaptability. Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed spatial resolution across layers. To overcome these limitations, we propose the Complementary Masked Autoencoders (CoMA), which employ a complementary masking strategy to ensure uniform sampling across all pixels, thereby improving effective learning of all features and enhancing the model's adaptability. Furthermore, we introduce DyViT, a hierarchical vision transformer that employs a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the parameters and FLOPs while improving fine-grained feature learning. Pre-trained on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using only 12% of the pre-training epochs, demonstrating more effective learning. It also attains a 10% reduction in pre-training time per epoch, further underscoring its superior pre-training efficiency.",
        "arxiv_id": "2511.05929",
        "ARXIVID": "2511.05929",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes improvements to masked autoencoders for vision tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.06005": {
        "authors": [
            "Adit Desai",
            "Sudipta Roy",
            "Mohna Chakraborty"
        ],
        "title": "How Reasoning Influences Intersectional Biases in Vision Language Models",
        "abstract": "arXiv:2511.06005v1 Announce Type: new  Abstract: Vision Language Models (VLMs) are increasingly deployed across downstream tasks, yet their training data often encode social biases that surface in outputs. Unlike humans, who interpret images through contextual and social cues, VLMs process them through statistical associations, often leading to reasoning that diverges from human reasoning. By analyzing how a VLM reasons, we can understand how inherent biases are perpetuated and can adversely affect downstream performance. To examine this gap, we systematically analyze social biases in five open-source VLMs for an occupation prediction task, on the FairFace dataset. Across 32 occupations and three different prompting styles, we elicit both predictions and reasoning. Our findings reveal that the biased reasoning patterns systematically underlie intersectional disparities, highlighting the need to align VLM reasoning with human values prior to its downstream deployment.",
        "arxiv_id": "2511.06005",
        "ARXIVID": "2511.06005",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it analyzes reasoning and biases in vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2511.07122": {
        "authors": [
            "Changyue Shi",
            "Chuxiao Yang",
            "Xinyuan Hu",
            "Minghao Chen",
            "Wenwen Pan",
            "Yan Yang",
            "Jiajun Ding",
            "Zhou Yu",
            "Jun Yu"
        ],
        "title": "Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction",
        "abstract": "arXiv:2511.07122v1 Announce Type: new  Abstract: Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.",
        "arxiv_id": "2511.07122",
        "ARXIVID": "2511.07122",
        "COMMENT": "Matches criteria 3 as it introduces a method for sparse-frame dynamic scene reconstruction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.06632": {
        "authors": [
            "Chenpeng Su",
            "Wenhua Wu",
            "Chensheng Peng",
            "Tianchen Deng",
            "Zhe Liu",
            "Hesheng Wang"
        ],
        "title": "DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting",
        "abstract": "arXiv:2511.06632v1 Announce Type: new  Abstract: Urban scene reconstruction is critical for autonomous driving, enabling structured 3D representations for data synthesis and closed-loop testing. Supervised approaches rely on costly human annotations and lack scalability, while current self-supervised methods often confuse static and dynamic elements and fail to distinguish individual dynamic objects, limiting fine-grained editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction method for label-free street scenes with 4D Gaussian Splatting. We first accurately identify dynamic instances by exploiting appearance-position inconsistency between warped rendering and actual observation. Guided by instance-level dynamic perception, we employ instance-aware 4D Gaussians as the unified volumetric representation, realizing dynamic-adaptive and instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism through which identity and dynamics reinforce each other, enhancing both integrity and consistency. Experiments on urban driving scenarios show that DIAL-GS surpasses existing self-supervised baselines in reconstruction quality and instance-level editing, offering a concise yet powerful solution for urban scene modeling.",
        "arxiv_id": "2511.06632",
        "ARXIVID": "2511.06632",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for dynamic instance-aware reconstruction in urban scenes, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.06019": {
        "authors": [
            "Priyansh Srivastava",
            "Romit Chatterjee",
            "Abir Sen",
            "Aradhana Behura",
            "Ratnakar Dash"
        ],
        "title": "MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model",
        "abstract": "arXiv:2511.06019v1 Announce Type: new  Abstract: Video Frame Interpolation (VFI) remains a cornerstone in video enhancement, enabling temporal upscaling for tasks like slow-motion rendering, frame rate conversion, and video restoration. While classical methods rely on optical flow and learning-based models assume access to dense ground-truth, both struggle with occlusions, domain shifts, and ambiguous motion. This article introduces MiVID, a lightweight, self-supervised, diffusion-based framework for video interpolation. Our model eliminates the need for explicit motion estimation by combining a 3D U-Net backbone with transformer-style temporal attention, trained under a hybrid masking regime that simulates occlusions and motion uncertainty. The use of cosine-based progressive masking and adaptive loss scheduling allows our network to learn robust spatiotemporal representations without any high-frame-rate supervision. Our framework is evaluated on UCF101-7 and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and 9-frame video segments, making it a low-resource yet highly effective pipeline. Despite these constraints, our model achieves optimal results at just 50 epochs, competitive with several supervised baselines.This work demonstrates the power of self-supervised diffusion priors for temporally coherent frame synthesis and provides a scalable path toward accessible and generalizable VFI systems.",
        "arxiv_id": "2511.06019",
        "ARXIVID": "2511.06019",
        "COMMENT": "Matches criteria 6 as it introduces a novel self-supervised diffusion-based framework for video frame interpolation, which is a video understanding task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.05866": {
        "authors": [
            "Suresh Nehra",
            "Aupendu Kar",
            "Jayanta Mukhopadhyay",
            "Prabir Kumar Biswas"
        ],
        "title": "Light-Field Dataset for Disparity Based Depth Estimation",
        "abstract": "arXiv:2511.05866v1 Announce Type: new  Abstract: A Light Field (LF) camera consists of an additional two-dimensional array of micro-lenses placed between the main lens and sensor, compared to a conventional camera. The sensor pixels under each micro-lens receive light from a sub-aperture of the main lens. This enables the image sensor to capture both spatial information and the angular resolution of a scene point. This additional angular information is used to estimate the depth of a 3-D scene. The continuum of virtual viewpoints in light field data enables efficient depth estimation using Epipolar Line Images (EPIs) with robust occlusion handling. However, the trade-off between angular information and spatial information is very critical and depends on the focal position of the camera. To design, develop, implement, and test novel disparity-based light field depth estimation algorithms, the availability of suitable light field image datasets is essential. In this paper, a publicly available light field image dataset is introduced and thoroughly described. We have also demonstrated the effect of focal position on the disparity of a 3-D point as well as the shortcomings of the currently available light field dataset. The proposed dataset contains 285 light field images captured using a Lytro Illum LF camera and 13 synthetic LF images. The proposed dataset also comprises a synthetic dataset with similar disparity characteristics to those of a real light field camera. A real and synthetic stereo light field dataset is also created by using a mechanical gantry system and Blender. The dataset is available at https://github.com/aupendu/light-field-dataset.",
        "arxiv_id": "2511.05866",
        "ARXIVID": "2511.05866",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new light-field dataset for depth estimation, which could be relevant for embodied AI tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2511.06721": {
        "authors": [
            "Yuda Qiu",
            "Zitong Xiao",
            "Yiwei Zuo",
            "Zisheng Ye",
            "Weikai Chen",
            "Xiaoguang Han"
        ],
        "title": "AvatarTex: High-Fidelity Facial Texture Reconstruction from Single-Image Stylized Avatars",
        "abstract": "arXiv:2511.06721v1 Announce Type: new  Abstract: We present AvatarTex, a high-fidelity facial texture reconstruction framework capable of generating both stylized and photorealistic textures from a single image. Existing methods struggle with stylized avatars due to the lack of diverse multi-style datasets and challenges in maintaining geometric consistency in non-standard textures. To address these limitations, AvatarTex introduces a novel three-stage diffusion-to-GAN pipeline. Our key insight is that while diffusion models excel at generating diversified textures, they lack explicit UV constraints, whereas GANs provide a well-structured latent space that ensures style and topology consistency. By integrating these strengths, AvatarTex achieves high-quality topology-aligned texture synthesis with both artistic and geometric coherence. Specifically, our three-stage pipeline first completes missing texture regions via diffusion-based inpainting, refines style and structure consistency using GAN-based latent optimization, and enhances fine details through diffusion-based repainting. To address the need for a stylized texture dataset, we introduce TexHub, a high-resolution collection of 20,000 multi-style UV textures with precise UV-aligned layouts. By leveraging TexHub and our structured diffusion-to-GAN pipeline, AvatarTex establishes a new state-of-the-art in multi-style facial texture reconstruction. TexHub will be released upon publication to facilitate future research in this field.",
        "arxiv_id": "2511.06721",
        "ARXIVID": "2511.06721",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and generative modeling, focusing on facial texture reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.06115": {
        "authors": [
            "Mostofa Rafid Uddin",
            "Jana Armouti",
            "Umong Sain",
            "Md Asib Rahman",
            "Xingjian Li",
            "Min Xu"
        ],
        "title": "DiLO: Disentangled Latent Optimization for Learning Shape and Deformation in Grouped Deforming 3D Objects",
        "abstract": "arXiv:2511.06115v1 Announce Type: new  Abstract: In this work, we propose a disentangled latent optimization-based method for parameterizing grouped deforming 3D objects into shape and deformation factors in an unsupervised manner. Our approach involves the joint optimization of a generator network along with the shape and deformation factors, supported by specific regularization techniques. For efficient amortized inference of disentangled shape and deformation codes, we train two order-invariant PoinNet-based encoder networks in the second stage of our method. We demonstrate several significant downstream applications of our method, including unsupervised deformation transfer, deformation classification, and explainability analysis. Extensive experiments conducted on 3D human, animal, and facial expression datasets demonstrate that our simple approach is highly effective in these downstream tasks, comparable or superior to existing methods with much higher complexity.",
        "arxiv_id": "2511.06115",
        "ARXIVID": "2511.06115",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.06138": {
        "authors": [
            "Hossein Askari",
            "Yadan Luo",
            "Hongfu Sun",
            "Fred Roosta"
        ],
        "title": "Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving",
        "abstract": "arXiv:2511.06138v1 Announce Type: new  Abstract: Recent advances in inverse problem solving have increasingly adopted flow priors over diffusion models due to their ability to construct straight probability paths from noise to data, thereby enhancing efficiency in both training and inference. However, current flow-based inverse solvers face two primary limitations: (i) they operate directly in pixel space, which demands heavy computational resources for training and restricts scalability to high-resolution images, and (ii) they employ guidance strategies with prior-agnostic posterior covariances, which can weaken alignment with the generative trajectory and degrade posterior coverage. In this paper, we propose LFlow (Latent Refinement via Flows), a training-free framework for solving linear inverse problems via pretrained latent flow priors. LFlow leverages the efficiency of flow matching to perform ODE sampling in latent space along an optimal path. This latent formulation further allows us to introduce a theoretically grounded posterior covariance, derived from the optimal vector field, enabling effective flow guidance. Experimental results demonstrate that our proposed method outperforms state-of-the-art latent diffusion solvers in reconstruction quality across most tasks. The code will be publicly available at https://github.com/hosseinaskari-cs/LFlow .",
        "arxiv_id": "2511.06138",
        "ARXIVID": "2511.06138",
        "COMMENT": "Does not match any specific criterion but is tangentially related to generative modeling and latent space optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.06648": {
        "authors": [
            "Siqi Hui",
            "Sanping Zhou",
            "Ye deng",
            "Wenli Huang",
            "Jinjun Wang"
        ],
        "title": "FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning",
        "abstract": "arXiv:2511.06648v1 Announce Type: new  Abstract: Cross-domain few-shot learning (CD-FSL) aims to recognize novel classes with only a few labeled examples under significant domain shifts. While recent approaches leverage a limited amount of labeled target-domain data to improve performance, the severe imbalance between abundant source data and scarce target data remains a critical challenge for effective representation learning. We present the first frequency-space perspective to analyze this issue and identify two key challenges: (1) models are easily biased toward source-specific knowledge encoded in the low-frequency components of source data, and (2) the sparsity of target data hinders the learning of high-frequency, domain-generalizable features. To address these challenges, we propose \\textbf{FreqGRL}, a novel CD-FSL framework that mitigates the impact of data imbalance in the frequency space. Specifically, we introduce a Low-Frequency Replacement (LFR) module that substitutes the low-frequency components of source tasks with those from the target domain to create new source tasks that better align with target characteristics, thus reducing source-specific biases and promoting generalizable representation learning. We further design a High-Frequency Enhancement (HFE) module that filters out low-frequency components and performs learning directly on high-frequency features in the frequency space to improve cross-domain generalization. Additionally, a Global Frequency Filter (GFF) is incorporated to suppress noisy or irrelevant frequencies and emphasize informative ones, mitigating overfitting risks under limited target supervision. Extensive experiments on five standard CD-FSL benchmarks demonstrate that our frequency-guided framework achieves state-of-the-art performance.",
        "arxiv_id": "2511.06648",
        "ARXIVID": "2511.06648",
        "COMMENT": "Does not match any specific criterion but is related to cross-domain learning and representation learning in vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.06944": {
        "authors": [
            "Dongsheng Hong",
            "Chao Chen",
            "Yanhui Chen",
            "Shanshan Lin",
            "Zhihao Chen",
            "Xiangwen Liao"
        ],
        "title": "From Attribution to Action: Jointly ALIGNing Predictions and Explanations",
        "abstract": "arXiv:2511.06944v1 Announce Type: new  Abstract: Explanation-guided learning (EGL) has shown promise in aligning model predictions with interpretable reasoning, particularly in computer vision tasks. However, most approaches rely on external annotations or heuristic-based segmentation to supervise model explanations, which can be noisy, imprecise and difficult to scale. In this work, we provide both empirical and theoretical evidence that low-quality supervision signals can degrade model performance rather than improve it. In response, we propose ALIGN, a novel framework that jointly trains a classifier and a masker in an iterative manner. The masker learns to produce soft, task-relevant masks that highlight informative regions, while the classifier is optimized for both prediction accuracy and alignment between its saliency maps and the learned masks. By leveraging high-quality masks as guidance, ALIGN improves both interpretability and generalizability, showing its superiority across various settings. Experiments on the two domain generalization benchmarks, VLCS and Terra Incognita, show that ALIGN consistently outperforms six strong baselines in both in-distribution and out-of-distribution settings. Besides, ALIGN also yields superior explanation quality concerning sufficiency and comprehensiveness, highlighting its effectiveness in producing accurate and interpretable models.",
        "arxiv_id": "2511.06944",
        "ARXIVID": "2511.06944",
        "COMMENT": "Does not match any specific criterion but is related to interpretability and generalization in vision models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.06947": {
        "authors": [
            "Yulin Chen",
            "Zeyuan Wang",
            "Tianyuan Yu",
            "Yingmei Wei",
            "Liang Bai"
        ],
        "title": "FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection",
        "abstract": "arXiv:2511.06947v1 Announce Type: new  Abstract: The well-aligned attribute of CLIP-based models enables its effective application like CLIPscore as a widely adopted image quality assessment metric. However, such a CLIP-based metric is vulnerable for its delicate multimodal alignment. In this work, we propose \\textbf{FoCLIP}, a feature-space misalignment framework for fooling CLIP-based image quality metric. Based on the stochastic gradient descent technique, FoCLIP integrates three key components to construct fooling examples: feature alignment as the core module to reduce image-text modality gaps, the score distribution balance module and pixel-guard regularization, which collectively optimize multimodal output equilibrium between CLIPscore performance and image quality. Such a design can be engineered to maximize the CLIPscore predictions across diverse input prompts, despite exhibiting either visual unrecognizability or semantic incongruence with the corresponding adversarial prompts from human perceptual perspectives. Experiments on ten artistic masterpiece prompts and ImageNet subsets demonstrate that optimized images can achieve significant improvement in CLIPscore while preserving high visual fidelity. In addition, we found that grayscale conversion induces significant feature degradation in fooling images, exhibiting noticeable CLIPscore reduction while preserving statistical consistency with original images. Inspired by this phenomenon, we propose a color channel sensitivity-driven tampering detection mechanism that achieves 91% accuracy on standard benchmarks. In conclusion, this work establishes a practical pathway for feature misalignment in CLIP-based multimodal systems and the corresponding defense method.",
        "arxiv_id": "2511.06947",
        "ARXIVID": "2511.06947",
        "COMMENT": "Does not match any specific criterion but is tangentially related to vision-language integration through CLIP-based systems.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.04880": {
        "authors": [
            "Yu Bai",
            "Yukai Miao",
            "Dawei Wang",
            "Li Chen",
            "Fei Long",
            "Rundi Zhai",
            "Dan Li",
            "Yanyu Ren",
            "Tianfeng Liu",
            "Hongtao Xie",
            "Ce Yang",
            "Xuhui Cai"
        ],
        "title": "DMA: Online RAG Alignment with Human Feedback",
        "abstract": "arXiv:2511.04880v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval, limiting adaptation to evolving intent and content drift. We introduce Dynamic Memory Alignment (DMA), an online learning framework that systematically incorporates multi-granularity human feedback to align ranking in interactive settings. DMA organizes document-, list-, and response-level signals into a coherent learning pipeline: supervised training for pointwise and listwise rankers, policy optimization driven by response-level preferences, and knowledge distillation into a lightweight scorer for low-latency serving. Throughout this paper, memory refers to the model's working memory, which is the entire context visible to the LLM for In-Context Learning.   We adopt a dual-track evaluation protocol mirroring deployment: (i) large-scale online A/B ablations to isolate the utility of each feedback source, and (ii) few-shot offline tests on knowledge-intensive benchmarks. Online, a multi-month industrial deployment further shows substantial improvements in human engagement. Offline, DMA preserves competitive foundational retrieval while yielding notable gains on conversational QA (TriviaQA, HotpotQA). Taken together, these results position DMA as a principled approach to feedback-driven, real-time adaptation in RAG without sacrificing baseline capability.",
        "arxiv_id": "2511.04880",
        "ARXIVID": "2511.04880",
        "COMMENT": "Does not match any specific criteria but is relevant to retrieval-augmented generation and online learning, which are tangentially related to the friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05841": {
        "authors": [
            "Changqing Gong",
            "Huafeng Qin",
            "Mounim A. El-Yacoubi"
        ],
        "title": "Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation",
        "abstract": "arXiv:2511.05841v1 Announce Type: new  Abstract: Alzheimer's disease is a prevalent neurodegenerative disorder for which early detection is critical. Handwriting-often disrupted in prodromal AD-provides a non-invasive and cost-effective window into subtle motor and cognitive decline. Existing handwriting-based AD studies, mostly relying on online trajectories and hand-crafted features, have not systematically examined how task type influences diagnostic performance and cross-task generalization. Meanwhile, large-scale vision language models have demonstrated remarkable zero or few-shot anomaly detection in natural images and strong adaptability across medical modalities such as chest X-ray and brain MRI. However, handwriting-based disease detection remains largely unexplored within this paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA implants multi-level fusion adapters within the visual encoder to progressively align representations toward handwriting-specific medical cues, enabling prompt-free and efficient zero-shot inference. Using this framework, we systematically investigate cross-task generalization-training on a specific handwriting task and evaluating on unseen ones-to reveal which task types and writing patterns most effectively discriminate AD. Extensive analyses further highlight characteristic stroke patterns and task-level factors that contribute to early AD identification, offering both diagnostic insights and a benchmark for handwriting-based cognitive assessment.",
        "arxiv_id": "2511.05841",
        "ARXIVID": "2511.05841",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to vision-language adaptation and medical applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05934": {
        "authors": [
            "Ayantika Das",
            "Arunima Sarkar",
            "Keerthi Ram",
            "Mohanasankar Sivaprakasam"
        ],
        "title": "AD-DAE: Unsupervised Modeling of Longitudinal Alzheimer's Disease Progression with Diffusion Auto-Encoder",
        "abstract": "arXiv:2511.05934v1 Announce Type: new  Abstract: Generative modeling frameworks have emerged as an effective approach to capture high-dimensional image distributions from large datasets without requiring domain-specific knowledge, a capability essential for longitudinal disease progression modeling. Recent generative modeling approaches have attempted to capture progression by mapping images into a latent representational space and then controlling and guiding the representations to generate follow-up images from a baseline image. However, existing approaches impose constraints on distribution learning, leading to latent spaces with limited controllability to generate follow-up images without explicit supervision from subject-specific longitudinal images. In order to enable controlled movements in the latent representational space and generate progression images from a baseline image in an unsupervised manner, we introduce a conditionable Diffusion Auto-encoder framework. The explicit encoding mechanism of image-diffusion auto-encoders forms a compact latent space capturing high-level semantics, providing means to disentangle information relevant for progression. Our approach leverages this latent space to condition and apply controlled shifts to baseline representations for generating follow-up. Controllability is induced by restricting these shifts to a subspace, thereby isolating progression-related factors from subject identity-preserving components. The shifts are implicitly guided by correlating with progression attributes, without requiring subject-specific longitudinal supervision. We validate the generations through image quality metrics, volumetric progression analysis, and downstream classification in Alzheimer's disease datasets from two different sources and disease categories. This demonstrates the effectiveness of our approach for Alzheimer's progression modeling and longitudinal image generation.",
        "arxiv_id": "2511.05934",
        "ARXIVID": "2511.05934",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to generative modeling and medical applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.06817": {
        "authors": [
            "Rui Wang",
            "Ying Zhou",
            "Hao Wang",
            "Wenwei Zhang",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning",
        "abstract": "arXiv:2511.06817v1 Announce Type: new  Abstract: Stereo matching in minimally invasive surgery (MIS) is essential for next-generation navigation and augmented reality. Yet, dense disparity supervision is nearly impossible due to anatomical constraints, typically limiting annotations to only a few image-level labels acquired before the endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a promising solution by leveraging a teacher trained on sparse labels to generate pseudo labels and associated confidence maps from abundant unlabeled surgical videos. However, existing TSL methods are confined to image-level supervision, providing only spatial confidence and lacking temporal consistency estimation. This absence of spatio-temporal reliability results in unstable disparity predictions and severe flickering artifacts across video frames. To overcome these challenges, we propose TiS-TSL, a novel time-switchable teacher-student learning framework for video stereo matching under minimal supervision. At its core is a unified model that operates in three distinct modes: Image-Prediction (IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP), enabling flexible temporal modeling within a single architecture. Enabled by this unified model, TiS-TSL adopts a two-stage learning strategy. The Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal disparity predictions by comparing forward and backward predictions to calculate bidirectional spatio-temporal consistency. This consistency identifies unreliable regions across frames, filters noisy video-level pseudo labels, and enforces temporal coherence. Experimental results on two public datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts by improving TEPE and EPE by at least 2.11% and 4.54%, respectively..",
        "arxiv_id": "2511.06817",
        "ARXIVID": "2511.06817",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to video understanding in a specialized domain (surgical video stereo matching).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.06194": {
        "authors": [
            "Muhammad Usama",
            "Mohammad Sadil Khan",
            "Didier Stricker",
            "Muhammad Zeshan Afzal"
        ],
        "title": "NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling",
        "abstract": "arXiv:2511.06194v1 Announce Type: new  Abstract: Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (\\textit{i.e}, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.",
        "arxiv_id": "2511.06194",
        "ARXIVID": "2511.06194",
        "COMMENT": "Does not match any specific criteria. Focuses on text-to-CAD generation, which is outside the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.05509": {
        "authors": [
            "Joel Valdivia Ortega",
            "Lorenz Lamm",
            "Franziska Eckardt",
            "Benedikt Schworm",
            "Marion Jasnin",
            "Tingying Peng"
        ],
        "title": "Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2",
        "abstract": "arXiv:2511.05509v1 Announce Type: new  Abstract: Vision Transformers (ViTs), such as DINOv2, achieve strong performance across domains but often repurpose low-informative patch tokens in ways that reduce the interpretability of attention and feature maps. This challenge is especially evident in medical imaging, where domain shifts can degrade both performance and transparency. In this paper, we introduce Randomized-MLP (RMLP) regularization, a contrastive learning-based method that encourages more semantically aligned representations. We use RMLPs when fine-tuning DINOv2 to both medical and natural image modalities, showing that it improves or maintains downstream performance while producing more interpretable attention maps. We also provide a mathematical analysis of RMLPs, offering insights into its role in enhancing ViT-based models and advancing our understanding of contrastive learning.",
        "arxiv_id": "2511.05509",
        "ARXIVID": "2511.05509",
        "COMMENT": "Does not match any specific criterion but is tangentially related to vision foundation models and interpretability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.06016": {
        "authors": [
            "Longhua Li",
            "Lei Qi",
            "Xin Geng"
        ],
        "title": "One-Shot Knowledge Transfer for Scalable Person Re-Identification",
        "abstract": "arXiv:2511.06016v1 Announce Type: new  Abstract: Edge computing in person re-identification (ReID) is crucial for reducing the load on central cloud servers and ensuring user privacy. Conventional compression methods for obtaining compact models require computations for each individual student model. When multiple models of varying sizes are needed to accommodate different resource conditions, this leads to repetitive and cumbersome computations. To address this challenge, we propose a novel knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which consolidates the knowledge of the teacher model into an intermediate carrier called a weight chain. When a downstream scenario demands a model that meets specific resource constraints, this weight chain can be expanded to the target model size without additional computation. OSKT significantly outperforms state-of-the-art compression methods, with the added advantage of one-time knowledge transfer that eliminates the need for frequent computations for each target model.",
        "arxiv_id": "2511.06016",
        "ARXIVID": "2511.06016",
        "COMMENT": "Does not match any specific criteria but is tangentially related to machine learning and edge computing, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}