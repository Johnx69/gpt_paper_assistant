{
    "2507.05427": {
        "authors": [
            "Shiting Xiao",
            "Rishabh Kabra",
            "Yuhang Li",
            "Donghyun Lee",
            "Joao Carreira",
            "Priyadarshini Panda"
        ],
        "title": "OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts",
        "abstract": "arXiv:2507.05427v1 Announce Type: new  Abstract: The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the model's spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks, including ADE20k, PASCAL, ScanNet, and SUN-RGBD.",
        "arxiv_id": "2507.05427",
        "ARXIVID": "2507.05427",
        "COMMENT": "Matches criterion 5 as it integrates image segmentation tasks with language prompts using vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.05513": {
        "authors": [
            "Mengyao Xu",
            "Gabriel Moreira",
            "Ronay Ak",
            "Radek Osmulski",
            "Yauhen Babakhin",
            "Zhiding Yu",
            "Benedikt Schifferer",
            "Even Oldridge"
        ],
        "title": "Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model",
        "abstract": "arXiv:2507.05513v1 Announce Type: new  Abstract: Motivated by the growing demand for retrieval systems that operate across modalities, we introduce llama-nemoretriever-colembed, a unified text-image retrieval model that delivers state-of-the-art performance across multiple benchmarks. We release two model variants, 1B and 3B. The 3B model achieves state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, placing first on both leaderboards as of June 27, 2025.   Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM), modifies its architecture by replacing causal attention with bidirectional attention, and integrates a ColBERT-style late interaction mechanism to enable fine-grained multimodal retrieval in a shared embedding space. While this mechanism delivers superior retrieval accuracy, it introduces trade-offs in storage and efficiency. We provide a comprehensive analysis of these trade-offs. Additionally, we adopt a two-stage training strategy to enhance the model's retrieval capabilities.",
        "arxiv_id": "2507.05513",
        "ARXIVID": "2507.05513",
        "COMMENT": "Matches criterion 2 as it explores a multimodal large language model for text-image retrieval with state-of-the-art performance.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.06119": {
        "authors": [
            "Zhiyu Tan",
            "Hao Yang",
            "Luozheng Qin",
            "Jia Gong",
            "Mengping Yang",
            "Hao Li"
        ],
        "title": "Omni-Video: Democratizing Unified Video Understanding and Generation",
        "abstract": "arXiv:2507.06119v1 Announce Type: new  Abstract: Notable breakthroughs in unified understanding and generation modeling have led to remarkable advancements in image understanding, reasoning, production and editing, yet current foundational models predominantly focus on processing images, creating a gap in the development of unified models for video understanding and generation. This report presents Omni-Video, an efficient and effective unified framework for video understanding, generation, as well as instruction-based editing. Our key insight is to teach existing multimodal large language models (MLLMs) to produce continuous visual clues that are used as the input of diffusion decoders, which produce high-quality videos conditioned on these visual clues. To fully unlock the potential of our system for unified video modeling, we integrate several technical improvements: 1) a lightweight architectural design that respectively attaches a vision head on the top of MLLMs and a adapter before the input of diffusion decoders, the former produce visual tokens for the latter, which adapts these visual tokens to the conditional space of diffusion decoders; and 2) an efficient multi-stage training scheme that facilitates a fast connection between MLLMs and diffusion decoders with limited data and computational resources. We empirically demonstrate that our model exhibits satisfactory generalization abilities across video generation, editing and understanding tasks.",
        "arxiv_id": "2507.06119",
        "ARXIVID": "2507.06119",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding and generation with a unified framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.05698": {
        "authors": [
            "Mohsi Jawaid",
            "Marcus M\\\"artens",
            "Tat-Jun Chin"
        ],
        "title": "Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting",
        "abstract": "arXiv:2507.05698v1 Announce Type: new  Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations, such as rendezvous, docking and on-orbit servicing. Vision-based pose estimation methods, which typically employ RGB imaging sensors, is a compelling solution for spacecraft pose estimation, but are challenged by harsh lighting conditions, which produce imaging artifacts such as glare, over-exposure, blooming and lens flare. Due to their much higher dynamic range, neuromorphic or event sensors are more resilient to extreme lighting conditions. However, event sensors generally have lower spatial resolution and suffer from reduced signal-to-noise ratio during periods of low relative motion. This work addresses these individual sensor limitations by introducing a sensor fusion approach combining RGB and event sensors. A beam-splitter prism was employed to achieve precise optical and temporal alignment. Then, a RANSAC-based technique was developed to fuse the information from the RGB and event channels to achieve pose estimation that leveraged the strengths of the two modalities. The pipeline was complemented by dropout uncertainty estimation to detect extreme conditions that affect either channel. To benchmark the performance of the proposed event-RGB fusion method, we collected a comprehensive real dataset of RGB and event data for satellite pose estimation in a laboratory setting under a variety of challenging illumination conditions. Encouraging results on the dataset demonstrate the efficacy of our event-RGB fusion approach and further supports the usage of event sensors for spacecraft pose estimation. To support community research on this topic, our dataset will be released publicly.",
        "arxiv_id": "2507.05698",
        "ARXIVID": "2507.05698",
        "COMMENT": "Matches criterion 6 as it focuses on video-based spacecraft pose estimation under harsh lighting conditions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.06134": {
        "authors": [
            "Sanidhya Vijayvargiya",
            "Aditya Bharat Soni",
            "Xuhui Zhou",
            "Zora Zhiruo Wang",
            "Nouha Dziri",
            "Graham Neubig",
            "Maarten Sap"
        ],
        "title": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety",
        "abstract": "arXiv:2507.06134v1 Announce Type: new  Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from scheduling to customer service, have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based analysis with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical safety vulnerabilities and the need for stronger safeguards before real-world deployment.",
        "arxiv_id": "2507.06134",
        "ARXIVID": "2507.06134",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating embodied AI agent safety in real-world scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.05763": {
        "authors": [
            "Ruijie Lu",
            "Yu Liu",
            "Jiaxiang Tang",
            "Junfeng Ni",
            "Yuxiang Wang",
            "Diwen Wan",
            "Gang Zeng",
            "Yixin Chen",
            "Siyuan Huang"
        ],
        "title": "DreamArt: Generating Interactable Articulated Objects from a Single Image",
        "abstract": "arXiv:2507.05763v1 Announce Type: new  Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/.",
        "arxiv_id": "2507.05763",
        "ARXIVID": "2507.05763",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for generating articulated objects, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.06210": {
        "authors": [
            "Yuchen Huang",
            "Zhiyuan Fan",
            "Zhitao He",
            "Sandeep Polisetty",
            "Wenyan Li",
            "Yi R. Fung"
        ],
        "title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions",
        "abstract": "arXiv:2507.06210v1 Announce Type: new  Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal understanding but struggle with contextually relevant fine-grained visual features, making it difficult to distinguish visually similar yet culturally distinct concepts. This limitation stems from the scarcity of high-quality culture-specific datasets, the lack of integrated contextual knowledge, and the absence of hard negatives highlighting subtle distinctions. To address these challenges, we first design a data curation pipeline that leverages open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a synthetic cultural dataset. This dataset consists of paired concept-caption-image triplets, where concepts visually resemble each other but represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to create CultureCLIP, which aligns cultural concepts with contextually enhanced captions and synthetic images through customized contrastive learning, enabling finer cultural differentiation while preserving generalization capabilities. Experiments on culturally relevant benchmarks show that CultureCLIP outperforms the base CLIP, achieving up to a notable 5.49% improvement in fine-grained concept recognition on certain tasks, while preserving CLIP's original generalization ability, validating the effectiveness of our data synthesis and VLM backbone training paradigm in capturing subtle cultural distinctions.",
        "arxiv_id": "2507.06210",
        "ARXIVID": "2507.06210",
        "COMMENT": "Matches criterion 2 as it explores a vision-language model (CLIP) and improves it with cultural awareness.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.05463": {
        "authors": [
            "Md Zahid Hasan",
            "Guillermo Basulto-Elias",
            "Jun Ha Chang",
            "Sahuna Hallmark",
            "Matthew Rizzo",
            "Anuj Sharma",
            "Soumik Sarkar"
        ],
        "title": "Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video",
        "abstract": "arXiv:2507.05463v1 Announce Type: new  Abstract: We introduce scenario-based cognitive status identification in older drivers from Naturalistic driving videos and large vision models. In recent times, cognitive decline, including Alzheimer's disease (AD) and mild cognitive impairment (MCI), is often underdiagnosed due to the time-consuming and costly nature of current diagnostic methods. By analyzing real-world driving behavior captured through in-vehicle systems, this research aims to extract \"digital fingerprints\" that correlate with functional decline and clinical features of MCI and AD. Moreover, modern large vision models can draw meaningful insights from everyday driving patterns of older patients to early detect cognitive decline. We propose a framework that uses large vision models and naturalistic driving videos to analyze driver behavior, classify cognitive status and predict disease progression. We leverage the strong relationship between real-world driving behavior as an observation of the current cognitive status of the drivers where the vehicle can be utilized as a \"diagnostic tool\". Our method identifies early warning signs of functional impairment, contributing to proactive intervention strategies. This work enhances early detection and supports the development of scalable, non-invasive monitoring systems to mitigate the growing societal and economic burden of cognitive decline in the aging population.",
        "arxiv_id": "2507.05463",
        "ARXIVID": "2507.05463",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding for cognitive assessment using driving videos.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.05678": {
        "authors": [
            "Yisu Zhang",
            "Chenjie Cao",
            "Chaohui Yu",
            "Jianke Zhu"
        ],
        "title": "LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion",
        "abstract": "arXiv:2507.05678v1 Announce Type: new  Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in synthesizing realistic videos by learning from large-scale data. Although vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal movement to driven VDMs with constrained data, achieving precise control over both camera trajectories and object motion remains challenging due to the unstable fusion and non-linear scalability. To address these issues, we propose LiON-LoRA, a novel framework that rethinks LoRA fusion through three core principles: Linear scalability, Orthogonality, and Norm consistency. First, we analyze the orthogonality of LoRA features in shallow VDM layers, enabling decoupled low-level controllability. Second, norm consistency is enforced across layers to stabilize fusion during complex camera motion combinations. Third, a controllable token is integrated into the diffusion transformer (DiT) to linearly adjust motion amplitudes for both cameras and objects with a modified self-attention mechanism to ensure decoupled control. Additionally, we extend LiON-LoRA to temporal generation by leveraging static-camera videos, unifying spatial and temporal controllability. Experiments demonstrate that LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy and motion strength adjustment, achieving superior generalization with minimal training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/",
        "arxiv_id": "2507.05678",
        "ARXIVID": "2507.05678",
        "COMMENT": "Matches criterion 6 as it proposes a novel method for video diffusion models with controllable spatial and temporal generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.05426": {
        "authors": [
            "Lanqing Guo",
            "Yufei Wang",
            "Hezhen Hu",
            "Yan Zheng",
            "Yeying Jin",
            "Siyu Huang",
            "Zhangyang Wang"
        ],
        "title": "Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors",
        "abstract": "arXiv:2507.05426v1 Announce Type: new  Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\\times$ speedup, providing a more efficient and effective approach to 3D scene local editing.",
        "arxiv_id": "2507.05426",
        "ARXIVID": "2507.05426",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D scene editing, which is relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.05499": {
        "authors": [
            "Giulio Federico",
            "Fabio Carrara",
            "Claudio Gennaro",
            "Giuseppe Amato",
            "Marco Di Benedetto"
        ],
        "title": "LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving",
        "abstract": "arXiv:2507.05499v1 Announce Type: new  Abstract: Generating consistent multi-view images from a single image remains challenging. Lack of spatial consistency often degrades 3D mesh quality in surface reconstruction. To address this, we propose LoomNet, a novel multi-view diffusion architecture that produces coherent images by applying the same diffusion model multiple times in parallel to collaboratively build and leverage a shared latent space for view consistency. Each viewpoint-specific inference generates an encoding representing its own hypothesis of the novel view from a given camera pose, which is projected onto three orthogonal planes. For each plane, encodings from all views are fused into a single aggregated plane. These aggregated planes are then processed to propagate information and interpolate missing regions, combining the hypotheses into a unified, coherent interpretation. The final latent space is then used to render consistent multi-view images. LoomNet generates 16 high-quality and coherent views in just 15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on both image quality and reconstruction metrics, also showing creativity by producing diverse, plausible novel views from the same input.",
        "arxiv_id": "2507.05499",
        "ARXIVID": "2507.05499",
        "COMMENT": "Matches criterion 5 as it introduces a novel method for generating consistent multi-view images, combining image generation and spatial reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.05970": {
        "authors": [
            "Haiwen Li",
            "Delong Liu",
            "Zhaohui Hou",
            "Zhicheng Zhao",
            "Fei Su"
        ],
        "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval",
        "abstract": "arXiv:2507.05970v1 Announce Type: new  Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR) aims to retrieve target images using multimodal (image+text) queries. Although many existing CIR methods have attained promising performance, their reliance on costly, manually labeled triplets hinders scalability and zero-shot capability. To address this issue, we propose a scalable pipeline for automatic triplet generation, along with a fully synthetic dataset named Composed Image Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a large language model (LLM) to generate diverse prompts, controlling a text-to-image generative model to produce image pairs with identical elements in each pair, which are then filtered and reorganized to form the CIRHS dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a novel CIR framework, which can accomplish global alignment and local reasoning within a broader context, enabling the model to learn more robust and informative representations. By utilizing the synthetic CIRHS dataset, CoAlign achieves outstanding zero-shot performance on three commonly used benchmarks, demonstrating for the first time the feasibility of training CIR models on a fully synthetic dataset. Furthermore, under supervised training, our method outperforms all the state-of-the-art supervised CIR approaches, validating the effectiveness of our proposed retrieval framework. The code and the CIRHS dataset will be released soon.",
        "arxiv_id": "2507.05970",
        "ARXIVID": "2507.05970",
        "COMMENT": "Matches criterion 5. Proposes a synthetic dataset and framework for composed image retrieval, integrating image and text understanding.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.05791": {
        "authors": [
            "Yan Yang",
            "Dongxu Li",
            "Yutong Dai",
            "Yuhao Yang",
            "Ziyang Luo",
            "Zirui Zhao",
            "Zhiyuan Hu",
            "Junzhe Huang",
            "Amrita Saha",
            "Zeyuan Chen",
            "Ran Xu",
            "Liyuan Pan",
            "Caiming Xiong",
            "Junnan Li"
        ],
        "title": "GTA1: GUI Test-time Scaling Agent",
        "abstract": "arXiv:2507.05791v1 Announce Type: new  Abstract: Graphical user interface (GUI) agents autonomously operate across platforms (e.g., Linux) to complete tasks by interacting with visual elements. Specifically, a user instruction is decomposed into a sequence of action proposals, each corresponding to an interaction with the GUI. After each action, the agent observes the updated GUI environment to plan the next step. However, two main challenges arise: i) resolving ambiguity in task planning (i.e., the action proposal sequence), where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, i.e., precisely interacting with visual targets.   This paper investigates the two aforementioned challenges with our GUI Test-time Scaling Agent, namely GTA1. First, to select the most appropriate action proposal, we introduce a test-time scaling method. At each step, we sample multiple candidate action proposals and leverage a judge model to evaluate and select the most suitable one. It trades off computation for better decision quality by concurrent sampling, shortening task execution steps, and improving overall performance. Second, we propose a model that achieves improved accuracy when grounding the selected action proposal to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates visual grounding through inherent objective alignments, rewarding successful clicks on interface elements.   Experimentally, our method establishes state-of-the-art performance across diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When paired with a planner applying our test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld). We open-source our code and models here.",
        "arxiv_id": "2507.05791",
        "ARXIVID": "2507.05791",
        "COMMENT": "Matches criterion 3. Introduces a new method for GUI agents with test-time scaling and reinforcement learning for visual grounding.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.05934": {
        "authors": [
            "Baojiao Xiong",
            "Boheng Chen",
            "Chengzhi Wang",
            "Daxiong Luo",
            "Dongsheng Xu",
            "Dongyang Liu",
            "Fan Yang",
            "Fangyuan Li",
            "Fei Teng",
            "Feng Wang",
            "Fukang Qin",
            "Fuquan Peng",
            "Guanxin Tan",
            "Guozhi Wang",
            "Haibo Yu",
            "Haohao Gao",
            "Heng Liu",
            "Hongbo Yang",
            "Hongjian Zou",
            "Houzheng Shen",
            "Hu Meng",
            "Huan Li",
            "Hui Tan",
            "Jiali Chen",
            "Jianzhao Chen",
            "Jinliang Zhu",
            "Kai Wang",
            "Lei Wu",
            "Liangbing Liu",
            "Liuyang Bian",
            "Liyan He",
            "Long Liu",
            "Peiwen Li",
            "Penggang Shi",
            "Qi Ding",
            "Rui Hu",
            "Shuai Cao",
            "Shuai Ren",
            "Shuang Peng",
            "Teng Xie",
            "Weiji Chen",
            "Weilin Xiang",
            "Weixin Wu",
            "Xi Yin",
            "Xiaoxin Chen",
            "Xu Chen",
            "Yafei Wen",
            "Yan Hu",
            "Yanzhou Yang",
            "Yina Xie",
            "Yinghao Chen",
            "Yixuan Liao",
            "Yu Geng",
            "Yuanjiang Ouyang",
            "Yuanzhuo Yang",
            "Yuehua He",
            "Yushuai Peng",
            "Zhaoxiong Wang",
            "Zheng Wang",
            "Zhibo Zhou",
            "Ziyang Wu"
        ],
        "title": "BlueLM-2.5-3B Technical Report",
        "abstract": "arXiv:2507.05934v1 Announce Type: new  Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large Language Model (MLLM) designed for efficient edge-device deployment, offering strong general-purpose and reasoning capabilities. To the best of our knowledge, this is the first 3B-scale MLLM to support both thinking and non-thinking modes, while also enabling explicit control over thinking token budget. BlueLM-2.5-3B is developed through diversified data curation, key data resampling, hybrid heterogeneous reinforcement learning, and a high-performance training infrastructure. Our model achieves superior multimodal capacity while preserving competitive pure-text performance with only 2.9 billion parameters. We conduct comprehensive evaluations across a broad range of multimodal and text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable performance to Qwen3-4B on text-only benchmarks, and trails the larger Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency. All of the aforementioned performance is achieved with substantially less total training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to the advancement of high-performance, on-device MLLMs and provides meaningful insights to the research community.",
        "arxiv_id": "2507.05934",
        "ARXIVID": "2507.05934",
        "COMMENT": "Matches criterion 2 as it introduces a compact Multimodal Large Language Model (MLLM) with efficient multimodal reasoning capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.06231": {
        "authors": [
            "Keyan Chen",
            "Chenyang Liu",
            "Bowen Chen",
            "Jiafan Zhang",
            "Zhengxia Zou",
            "Zhenwei Shi"
        ],
        "title": "RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models",
        "abstract": "arXiv:2507.06231v1 Announce Type: new  Abstract: Referring Remote Sensing Image Segmentation provides a flexible and fine-grained framework for remote sensing scene analysis via vision-language collaborative interpretation. Current approaches predominantly utilize a three-stage pipeline encompassing dual-modal encoding, cross-modal interaction, and pixel decoding. These methods demonstrate significant limitations in managing complex semantic relationships and achieving precise cross-modal alignment, largely due to their coupled processing mechanism that conflates target localization with boundary delineation. This architectural coupling amplifies error propagation under semantic ambiguity while restricting model generalizability and interpretability. To address these issues, we propose RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow into a collaborative dual-stage framework: coarse localization followed by fine segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with SAM's segmentation generalizability through strategic foundation model collaboration. Specifically, CLIP is employed as the dual-modal encoder to activate target features within its pre-aligned semantic space and generate localization prompts. To mitigate CLIP's misactivation challenges in multi-entity scenarios described by referring texts, a cascaded second-order prompter is devised, which enhances precision through implicit reasoning via decomposition of text embeddings into complementary semantic subspaces. These optimized semantic prompts subsequently direct the SAM to generate pixel-level refined masks, thereby completing the semantic transmission pipeline. Extensive experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2 surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex semantic interpretation. Code is available at: https://github.com/KyanChen/RSRefSeg2.",
        "arxiv_id": "2507.06231",
        "ARXIVID": "2507.06231",
        "COMMENT": "Matches criterion 5 as it integrates image understanding tasks with foundation models like CLIP and SAM for remote sensing segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.05638": {
        "authors": [
            "Litian Zhang",
            "Xiaoming Zhang",
            "Bingyu Yan",
            "Ziyi Zhou",
            "Bo Zhang",
            "Zhenyu Guan",
            "Xi Zhang",
            "Chaozhuo Li"
        ],
        "title": "LLMs are Introvert",
        "abstract": "arXiv:2507.05638v1 Announce Type: new  Abstract: The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.",
        "arxiv_id": "2507.05638",
        "ARXIVID": "2507.05638",
        "COMMENT": "Matches criterion 1 as it introduces a novel SIP-CoT mechanism for improving social intelligence in LLMs, which could be relevant to embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.06233": {
        "authors": [
            "In\\`es Hyeonsu Kim",
            "Seokju Cho",
            "Jahyeok Koo",
            "Junghyun Park",
            "Jiahui Huang",
            "Joon-Young Lee",
            "Seungryong Kim"
        ],
        "title": "Learning to Track Any Points from Human Motion",
        "abstract": "arXiv:2507.06233v1 Announce Type: new  Abstract: Human motion, with its inherent complexities, such as non-rigid deformations, articulated movements, clothing distortions, and frequent occlusions caused by limbs or other individuals, provides a rich and challenging source of supervision that is crucial for training robust and generalizable point trackers. Despite the suitability of human motion, acquiring extensive training data for point tracking remains difficult due to laborious manual annotation. Our proposed pipeline, AnthroTAP, addresses this by proposing an automated pipeline to generate pseudo-labeled training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected humans in video frames, project the resulting 3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle occlusions using ray-casting, and filter out unreliable tracks based on optical flow consistency. A point tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other models trained on real videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to 256 GPUs used in recent state-of-the-art.",
        "arxiv_id": "2507.06233",
        "ARXIVID": "2507.06233",
        "COMMENT": "Matches criterion 3 as it introduces a new pipeline for generating pseudo-labeled data for point tracking, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.05948": {
        "authors": [
            "Quanzhu Niu",
            "Yikang Zhou",
            "Shihao Chen",
            "Tao Zhang",
            "Shunping Ji"
        ],
        "title": "Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation",
        "abstract": "arXiv:2507.05948v1 Announce Type: new  Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive challenges including object occlusions, motion blur, and appearance variations during temporal association. To overcome these limitations, this work introduces geometric awareness to enhance VIS robustness by strategically leveraging monocular depth estimation. We systematically investigate three distinct integration paradigms. Expanding Depth Channel (EDC) method concatenates the depth map as input channel to segmentation networks; Sharing ViT (SV) designs a uniform ViT backbone, shared between depth estimation and segmentation branches; Depth Supervision (DS) makes use of depth prediction as an auxiliary training guide for feature learning. Though DS exhibits limited effectiveness, benchmark evaluations demonstrate that EDC and SV significantly enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets 56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work conclusively establishes depth cues as critical enablers for robust video understanding.",
        "arxiv_id": "2507.05948",
        "ARXIVID": "2507.05948",
        "COMMENT": "Matches criterion 6 as it focuses on video instance segmentation and introduces geometric cues for robust video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.05515": {
        "authors": [
            "Haochen Huang",
            "Jiahuan Pei",
            "Mohammad Aliannejadi",
            "Xin Sun",
            "Moonisa Ahsan",
            "Pablo Cesar",
            "Chuang Yu",
            "Zhaochun Ren",
            "Junxiao Wang"
        ],
        "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality",
        "abstract": "arXiv:2507.05515v1 Announce Type: new  Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart assistants to interpret and reason in multimodal environments. However, their application in augmented reality (AR) training remains largely unexplored. In this work, we introduce a comprehensive dataset tailored for AR training, featuring systematized vision-language tasks, and evaluate nine state-of-the-art VLMs on it. Our results reveal that even advanced models, including GPT-4o, struggle with fine-grained assembly tasks, achieving a maximum F1 score of just 40.54% on state detection. These findings highlight the demand for enhanced datasets, benchmarks, and further research to improve fine-grained vision-language alignment. Beyond technical contributions, our work has broader social implications, particularly in empowering blind and visually impaired users with equitable access to AI-driven learning opportunities. We provide all related resources, including the dataset, source code, and evaluation results, to support the research community.",
        "arxiv_id": "2507.05515",
        "ARXIVID": "2507.05515",
        "COMMENT": "Matches criterion 2 as it explores vision-language models (VLMs) in the context of augmented reality and evaluates their performance on fine-grained tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.05675": {
        "authors": [
            "Rongsheng Wang",
            "Junying Chen",
            "Ke Ji",
            "Zhenyang Cai",
            "Shunian Chen",
            "Yunjin Yang",
            "Benyou Wang"
        ],
        "title": "MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos",
        "abstract": "arXiv:2507.05675v1 Announce Type: new  Abstract: Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at https://github.com/FreedomIntelligence/MedGen",
        "arxiv_id": "2507.05675",
        "ARXIVID": "2507.05675",
        "COMMENT": "Matches criterion 6. Focuses on medical video generation, introducing a new dataset and model for this task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.06071": {
        "authors": [
            "Chang Liu",
            "Ye Pan",
            "Chenyang Ding",
            "Susanto Rahardja",
            "Xiaokang Yang"
        ],
        "title": "MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding",
        "abstract": "arXiv:2507.06071v1 Announce Type: new  Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip movements and vivid facial expressions. However, most existing approaches focus on static and predefined emotion labels, limiting their diversity and naturalness. To address these challenges, we propose MEDTalk, a novel framework for fine-grained and dynamic emotional talking head generation. Our approach first disentangles content and emotion embedding spaces from motion sequences using a carefully designed cross-reconstruction process, enabling independent control over lip movements and facial expressions. Beyond conventional audio-driven lip synchronization, we integrate audio and speech text, predicting frame-wise intensity variations and dynamically adjusting static emotion features to generate realistic emotional expressions. Furthermore, to enhance control and personalization, we incorporate multimodal inputs-including text descriptions and reference expression images-to guide the generation of user-specified facial expressions. With MetaHuman as the priority, our generated results can be conveniently integrated into the industrial production pipeline.",
        "arxiv_id": "2507.06071",
        "ARXIVID": "2507.06071",
        "COMMENT": "Matches criterion 2. Explores multimodal integration for 3D facial animation, combining audio and text inputs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.05790": {
        "authors": [
            "Yujie Hu",
            "Xuanyu Zhang",
            "Weiqi Li",
            "Jian Zhang"
        ],
        "title": "TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model",
        "abstract": "arXiv:2507.05790v1 Announce Type: new  Abstract: Virtual try-on has made significant progress in recent years. This paper addresses how to achieve multifunctional virtual try-on guided solely by text instructions, including full outfit change and local editing. Previous methods primarily relied on end-to-end networks to perform single try-on tasks, lacking versatility and flexibility. We propose TalkFashion, an intelligent try-on assistant that leverages the powerful comprehension capabilities of large language models to analyze user instructions and determine which task to execute, thereby activating different processing pipelines accordingly. Additionally, we introduce an instruction-based local repainting model that eliminates the need for users to manually provide masks. With the help of multi-modal models, this approach achieves fully automated local editings, enhancing the flexibility of editing tasks. The experimental results demonstrate better semantic consistency and visual quality compared to the current methods.",
        "arxiv_id": "2507.05790",
        "ARXIVID": "2507.05790",
        "COMMENT": "Matches criterion 2. Discusses a multimodal large language model for virtual try-on applications.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2507.05397": {
        "authors": [
            "Pengfei Zhou",
            "Jie Xia",
            "Xiaopeng Peng",
            "Wangbo Zhao",
            "Zilong Ye",
            "Zekai Li",
            "Suorong Yang",
            "Jiadong Pan",
            "Yuanxiang Chen",
            "Ziqiao Wang",
            "Kai Wang",
            "Qian Zheng",
            "Xiaojun Chang",
            "Gang Pan",
            "Shurong Dong",
            "Kaipeng Zhang",
            "Yang You"
        ],
        "title": "Neural-Driven Image Editing",
        "abstract": "arXiv:2507.05397v1 Announce Type: new  Abstract: Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area.",
        "arxiv_id": "2507.05397",
        "ARXIVID": "2507.05397",
        "COMMENT": "Does not match any specific criteria but discusses neural-driven image editing using multimodal signals, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.06187": {
        "authors": [
            "Scott Geng",
            "Hamish Ivison",
            "Chun-Liang Li",
            "Maarten Sap",
            "Jerry Li",
            "Ranjay Krishna",
            "Pang Wei Koh"
        ],
        "title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains",
        "abstract": "arXiv:2507.06187v1 Announce Type: new  Abstract: Improvements in language models are often driven by improving the quality of the data we train them on, which can be limiting when strong supervision is scarce. In this work, we show that paired preference data consisting of individually weak data points can enable gains beyond the strength of each individual data point. We formulate the delta learning hypothesis to explain this phenomenon, positing that the relative quality delta between points suffices to drive learning via preference tuning--even when supervised finetuning on the weak data hurts. We validate our hypothesis in controlled experiments and at scale, where we post-train 8B models on preference data generated by pairing a small 3B model's responses with outputs from an even smaller 1.5B model to create a meaningful delta. Strikingly, on a standard 11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the performance of Tulu 3, a state-of-the-art open model tuned from the same base model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta learning enables simpler and cheaper open recipes for state-of-the-art post-training. To better understand delta learning, we prove in logistic regression that the performance gap between two weak teacher models provides useful signal for improving a stronger student. Overall, our work shows that models can learn surprisingly well from paired data that might typically be considered weak.",
        "arxiv_id": "2507.06187",
        "ARXIVID": "2507.06187",
        "COMMENT": "Does not match any specific criteria but discusses a novel statistical approach for learning from weak data, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.05819": {
        "authors": [
            "Yuhuan Xie",
            "Aoxuan Pan",
            "Ming-Xian Lin",
            "Wei Huang",
            "Yi-Hua Huang",
            "Xiaojuan Qi"
        ],
        "title": "2D Instance Editing in 3D Space",
        "abstract": "arXiv:2507.05819v1 Announce Type: new  Abstract: Generative models have achieved significant progress in advancing 2D image editing, demonstrating exceptional precision and realism. However, they often struggle with consistency and object identity preservation due to their inherent pixel-manipulation nature. To address this limitation, we introduce a novel \"2D-3D-2D\" framework. Our approach begins by lifting 2D objects into 3D representation, enabling edits within a physically plausible, rigidity-constrained 3D environment. The edited 3D objects are then reprojected and seamlessly inpainted back into the original 2D image. In contrast to existing 2D editing methods, such as DragGAN and DragDiffusion, our method directly manipulates objects in a 3D environment. Extensive experiments highlight that our framework surpasses previous methods in general performance, delivering highly consistent edits while robustly preserving object identity.",
        "arxiv_id": "2507.05819",
        "ARXIVID": "2507.05819",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and 3D-2D integration techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.05964": {
        "authors": [
            "Vera Soboleva",
            "Aibek Alanov",
            "Andrey Kuznetsov",
            "Konstantin Sobolev"
        ],
        "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
        "abstract": "arXiv:2507.05964v1 Announce Type: new  Abstract: While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA.",
        "arxiv_id": "2507.05964",
        "ARXIVID": "2507.05964",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and customization of diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.05631": {
        "authors": [
            "Zhiwei Chen",
            "Yupeng Hu",
            "Zixu Li",
            "Zhiheng Fu",
            "Xuemeng Song",
            "Liqiang Nie"
        ],
        "title": "OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval",
        "abstract": "arXiv:2507.05631v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is capable of expressing users' intricate retrieval requirements flexibly. It enables the user to give a multimodal query, comprising a reference image and a modification text, and subsequently retrieve the target image. Notwithstanding the considerable advances made by prevailing methodologies, CIR remains in its nascent stages due to two limitations: 1) inhomogeneity between dominant and noisy portions in visual data is ignored, leading to query feature degradation, and 2) the priority of textual data in the image modification process is overlooked, which leads to a visual focus bias. To address these two limitations, this work presents a focus mapping-based feature extractor, which consists of two modules: dominant portion segmentation and dual focus mapping. It is designed to identify significant dominant portions in images and guide the extraction of visual and textual data features, thereby reducing the impact of noise interference. Subsequently, we propose a textually guided focus revision module, which can utilize the modification requirements implied in the text to perform adaptive focus revision on the reference image, thereby enhancing the perception of the modification focus on the composed features. The aforementioned modules collectively constitute the segmentatiOn-based Focus shiFt reviSion nETwork (\\mbox{OFFSET}), and comprehensive experiments on four benchmark datasets substantiate the superiority of our proposed method. The codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/",
        "arxiv_id": "2507.05631",
        "ARXIVID": "2507.05631",
        "COMMENT": "Does not match any specific criteria but focuses on composed image retrieval with segmentation-based methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05805": {
        "authors": [
            "Xin Li",
            "Mingming Gong",
            "Yunfei Wu",
            "Jianxin Dai",
            "Antai Guo",
            "Xinghua Jiang",
            "Haoyu Cao",
            "Yinsong Liu",
            "Deqiang Jiang",
            "Xing Sun"
        ],
        "title": "DREAM: Document Reconstruction via End-to-end Autoregressive Model",
        "abstract": "arXiv:2507.05805v1 Announce Type: new  Abstract: Document reconstruction constitutes a significant facet of document analysis and recognition, a field that has been progressively accruing interest within the scholarly community. A multitude of these researchers employ an array of document understanding models to generate predictions on distinct subtasks, subsequently integrating their results into a holistic document reconstruction format via heuristic principles. Nevertheless, these multi-stage methodologies are hindered by the phenomenon of error propagation, resulting in suboptimal performance. Furthermore, contemporary studies utilize generative models to extract the logical sequence of plain text, tables and mathematical expressions in an end-to-end process. However, this approach is deficient in preserving the information related to element layouts, which are vital for document reconstruction. To surmount these aforementioned limitations, we in this paper present an innovative autoregressive model specifically designed for document reconstruction, referred to as Document Reconstruction via End-to-end Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence of document reconstruction in a comprehensive, end-to-end process, encapsulating a broader spectrum of document element information. In addition, we establish a standardized definition of the document reconstruction task, and introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for assessing the performance of the task. Empirical results substantiate that our methodology attains unparalleled performance in the realm of document reconstruction. Furthermore, the results on a variety of subtasks, encompassing document layout analysis, text recognition, table structure recognition, formula recognition and reading order detection, indicate that our model is competitive and compatible with various tasks.",
        "arxiv_id": "2507.05805",
        "ARXIVID": "2507.05805",
        "COMMENT": "Does not match any specific criteria but focuses on document reconstruction using an autoregressive model.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05838": {
        "authors": [
            "Ourui Fu",
            "Hangzhou He",
            "Xinliang Zhang",
            "Lei Zhu",
            "Shuang Zeng",
            "ZhaoHeng Xie",
            "Yanye Lu"
        ],
        "title": "I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation",
        "abstract": "arXiv:2507.05838v1 Announce Type: new  Abstract: The annotation bottleneck in semantic segmentation has driven significant interest in few-shot segmentation, which aims to develop segmentation models capable of generalizing rapidly to novel classes using minimal exemplars. Conventional training paradigms typically generate query prior maps by extracting masked-area features from support images, followed by making predictions guided by these prior maps. However, current approaches remain constrained by two critical limitations stemming from inter- and intra-image discrepancies, both of which significantly degrade segmentation performance: 1) The semantic gap between support and query images results in mismatched features and inaccurate prior maps; 2) Visually similar yet semantically distinct regions within support or query images lead to false negative or false positive predictions. We propose a novel FSS method called \\textbf{I$^2$R}: 1) Using category-specific high level representations which aggregate global semantic cues from support and query images, enabling more precise inter-image region localization and address the first limitation. 2) Directional masking strategy that suppresses inconsistent support-query pixel pairs, which exhibit high feature similarity but conflicting mask, to mitigate the second issue. Experiments demonstrate that our method outperforms state-of-the-art approaches, achieving improvements of 1.9\\% and 2.1\\% in mIoU under the 1-shot setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.",
        "arxiv_id": "2507.05838",
        "ARXIVID": "2507.05838",
        "COMMENT": "Does not match any specific criteria but is related to few-shot segmentation and semantic refinement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05620": {
        "authors": [
            "Shaojie Bai",
            "Seunghyeon Seo",
            "Yida Wang",
            "Chenghui Li",
            "Owen Wang",
            "Te-Li Wang",
            "Tianyang Ma",
            "Jason Saragih",
            "Shih-En Wei",
            "Nojun Kwak",
            "Hyung Jun Kim"
        ],
        "title": "Generative Head-Mounted Camera Captures for Photorealistic Avatars",
        "abstract": "arXiv:2507.05620v1 Announce Type: new  Abstract: Enabling photorealistic avatar animations in virtual and augmented reality (VR/AR) has been challenging because of the difficulty of obtaining ground truth state of faces. It is physically impossible to obtain synchronized images from head-mounted cameras (HMC) sensing input, which has partial observations in infrared (IR), and an array of outside-in dome cameras, which have full observations that match avatars' appearance. Prior works relying on analysis-by-synthesis methods could generate accurate ground truth, but suffer from imperfect disentanglement between expression and style in their personalized training. The reliance of extensive paired captures (HMC and dome) for the same subject makes it operationally expensive to collect large-scale datasets, which cannot be reused for different HMC viewpoints and lighting. In this work, we propose a novel generative approach, Generative HMC (GenHMC), that leverages large unpaired HMC captures, which are much easier to collect, to directly generate high-quality synthetic HMC images given any conditioning avatar state from dome captures. We show that our method is able to properly disentangle the input conditioning signal that specifies facial expression and viewpoint, from facial appearance, leading to more accurate ground truth. Furthermore, our method can generalize to unseen identities, removing the reliance on the paired captures. We demonstrate these breakthroughs by both evaluating synthetic HMC images and universal face encoders trained from these new HMC-avatar correspondences, which achieve better data efficiency and state-of-the-art accuracy.",
        "arxiv_id": "2507.05620",
        "ARXIVID": "2507.05620",
        "COMMENT": "Does not match any specific criteria but is related to photorealistic avatar generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05621": {
        "authors": [
            "Suoxiang Zhang",
            "Xiaxi Li",
            "Hongrui Chang",
            "Zhuoyan Hou",
            "Guoxin Wu",
            "Ronghua Ji"
        ],
        "title": "AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework",
        "abstract": "arXiv:2507.05621v1 Announce Type: new  Abstract: Domain-specific image generation aims to produce high-quality visual content for specialized fields while ensuring semantic accuracy and detail fidelity. However, existing methods exhibit two critical limitations: First, current approaches address prompt engineering and model adaptation separately, overlooking the inherent dependence between semantic understanding and visual representation in specialized domains. Second, these techniques inadequately incorporate domain-specific semantic constraints during content synthesis, resulting in generation outcomes that exhibit hallucinations and semantic deviations. To tackle these issues, we propose AdaptaGen, a hierarchical semantic optimization framework that integrates matrix-based prompt optimization with multi-perspective understanding, capturing comprehensive semantic relationships from both global and local perspectives. To mitigate hallucinations in specialized domains, we design a cross-modal adaptation mechanism, which, when combined with intelligent content synthesis, enables preserving core thematic elements while incorporating diverse details across images. Additionally, we introduce a two-phase caption semantic transformation during the generation phase. This approach maintains semantic coherence while enhancing visual diversity, ensuring the generated images adhere to domain-specific constraints. Experimental results confirm our approach's effectiveness, with our framework achieving superior performance across 40 categories from diverse datasets using only 16 images per category, demonstrating significant improvements in image quality, diversity, and semantic consistency.",
        "arxiv_id": "2507.05621",
        "ARXIVID": "2507.05621",
        "COMMENT": "Does not match any specific criteria but is related to domain-specific image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05302": {
        "authors": [
            "Binjia Zhou",
            "Hengrui Lou",
            "Lizhe Chen",
            "Haoyuan Li",
            "Dawei Luo",
            "Shuai Chen",
            "Jie Lei",
            "Zunlei Feng",
            "Yijun Bei"
        ],
        "title": "CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection",
        "abstract": "arXiv:2507.05302v1 Announce Type: new  Abstract: With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake detection.Existing techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of hallucinations.To address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias reduction.Experimental results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities.",
        "arxiv_id": "2507.05302",
        "ARXIVID": "2507.05302",
        "COMMENT": "Does not match any specific criteria but is related to face forgery detection and visual detail enhancement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05566": {
        "authors": [
            "David Bensa\\\"id",
            "Noam Rotstein",
            "Roy Velich",
            "Daniel Bensa\\\"id",
            "Ron Kimmel"
        ],
        "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix",
        "abstract": "arXiv:2507.05566v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient fine-tuning of large pretrained models. LoRA augments the pre-trained weights of a model by adding the product of two smaller matrices that together form a low-rank matrix update. Recent research has shown that scale disparities between these two matrices often cause unstable training dynamics, leading to suboptimal performance. In this paper, we propose SingLoRA, which reformulates low-rank adaptation by learning the weights update as a decomposition of a single low-rank matrix multiplied by its transpose. This simple design inherently removes inter-matrix scale conflicts, ensuring stable optimization, and roughly halves the parameter count. We analyze SingLoRA within the infinite-width neural network framework, showing that it guarantees stable feature learning by construction. Extensive experiments on multiple tasks validate these benefits. In common sense reasoning, fine-tuning LLama 7B on MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+ (90.2%) - while using only 60% of their parameter budget. In image generation, fine-tuning Stable Diffusion with SingLoRA significantly improves image fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to scores of 0.148 and 0.143 for DoRA and LoRA, respectively.",
        "arxiv_id": "2507.05566",
        "ARXIVID": "2507.05566",
        "COMMENT": "Does not match any specific criteria but is related to parameter-efficient fine-tuning and image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05952": {
        "authors": [
            "Aoxiang Fan",
            "Corentin Dumery",
            "Nicolas Talabot",
            "Hieu Le",
            "Pascal Fua"
        ],
        "title": "High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes",
        "abstract": "arXiv:2507.05952v1 Announce Type: new  Abstract: Generalizable neural surface reconstruction has become a compelling technique to reconstruct from few images without per-scene optimization, where dense 3D feature volume has proven effective as a global representation of scenes. However, the dense representation does not scale well to increasing voxel resolutions, severely limiting the reconstruction quality. We thus present a sparse representation method, that maximizes memory efficiency and enables significantly higher resolution reconstructions on standard hardware. We implement this through a two-stage approach: First training a network to predict voxel occupancies from posed images and associated depth maps, then computing features and performing volume rendering only in voxels with sufficiently high occupancy estimates. To support this sparse representation, we developed custom algorithms for efficient sampling, feature aggregation, and querying from sparse volumes-overcoming the dense-volume assumptions inherent in existing works. Experiments on public datasets demonstrate that our approach reduces storage requirements by more than 50 times without performance degradation, enabling reconstructions at $512^3$ resolution compared to the typical $128^3$ on similar hardware, and achieving superior reconstruction accuracy over current state-of-the-art methods.",
        "arxiv_id": "2507.05952",
        "ARXIVID": "2507.05952",
        "COMMENT": "Does not match any specific criteria but focuses on neural surface reconstruction, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.06075": {
        "authors": [
            "Francesco Milano",
            "Manuel L\\'opez-Antequera",
            "Naina Dhingra",
            "Roland Siegwart",
            "Robert Thiel"
        ],
        "title": "Discontinuity-aware Normal Integration for Generic Central Camera Models",
        "abstract": "arXiv:2507.06075v1 Announce Type: new  Abstract: Recovering a 3D surface from its surface normal map, a problem known as normal integration, is a key component for photometric shape reconstruction techniques such as shape-from-shading and photometric stereo. The vast majority of existing approaches for normal integration handle only implicitly the presence of depth discontinuities and are limited to orthographic or ideal pinhole cameras. In this paper, we propose a novel formulation that allows modeling discontinuities explicitly and handling generic central cameras. Our key idea is based on a local planarity assumption, that we model through constraints between surface normals and ray directions. Compared to existing methods, our approach more accurately approximates the relation between depth and surface normals, achieves state-of-the-art results on the standard normal integration benchmark, and is the first to directly handle generic central camera models.",
        "arxiv_id": "2507.06075",
        "ARXIVID": "2507.06075",
        "COMMENT": "Does not match any specific criterion but is relevant to 3D surface reconstruction, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.06029": {
        "authors": [
            "Courtney Ford",
            "Mark T. Keane"
        ],
        "title": "Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions",
        "abstract": "arXiv:2507.06029v1 Announce Type: new  Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable outputs for users without domain expertise. We introduce Feature-Guided Neighbor Selection (FGNS), a post hoc method that enhances interpretability by selecting class-representative examples using both local and global feature importance. In a user study (N = 98) evaluating Kannada script classifications, FGNS significantly improved non-experts' ability to identify model errors while maintaining appropriate agreement with correct predictions. Participants made faster and more accurate decisions compared to those given traditional k-NN explanations. Quantitative analysis shows that FGNS selects neighbors that better reflect class characteristics rather than merely minimizing feature-space distance, leading to more consistent selection and tighter clustering around class prototypes. These results support FGNS as a step toward more human-aligned model assessment, although further work is needed to address the gap between explanation quality and perceived trust.",
        "arxiv_id": "2507.06029",
        "ARXIVID": "2507.06029",
        "COMMENT": "Does not match any specific criterion but is relevant to explainable AI and interpretability, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05283": {
        "authors": [
            "Yue Wang",
            "Miao Zhou",
            "Guijing Huang",
            "Rui Zhuo",
            "Chao Yi",
            "Zhenliang Ma"
        ],
        "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management",
        "abstract": "arXiv:2507.05283v1 Announce Type: new  Abstract: Pre-timed traffic signal control, commonly used for operating signalized intersections and coordinated arterials, requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting. To enable a user-friendly traffic signal control plan management process, this study proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could further be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers. With curated prompts, Chat2SPaT first leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulate the plan as a combination of phase sequence and phase attribute results in the json format. Based on LLM outputs, python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and finally assemble the complete traffic signal control plan. Within a chat, the pipeline can be utilized iteratively to conduct further plan editing. Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. As the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions, Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS. The source codes, prompts and test dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.",
        "arxiv_id": "2507.05283",
        "ARXIVID": "2507.05283",
        "COMMENT": "Does not match any specific criteria but discusses automating traffic signal control using LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.05613": {
        "authors": [
            "Lei Fan",
            "Fangxue Liu",
            "Cheng Chen"
        ],
        "title": "Domain adaptation of large language models for geotechnical applications",
        "abstract": "arXiv:2507.05613v1 Announce Type: new  Abstract: Recent developments in large language models (LLMs) are opening up new opportunities in geotechnical engineering and engineering geology. While general-purpose LLMs possess broad capabilities, effective application in geotechnics often requires domain-specific adaptation. Such tailored LLMs are increasingly employed to streamline geotechnical workflows. This paper presents the first survey of the adaptation and application of LLMs in geotechnical engineering. It outlines key methodologies for adaptation to geotechnical domain, including prompt engineering, retrieval-augmented generation, domain-adaptive pretraining, and fine-tuning. The survey examines the state-of-the-art applications of geotechnical-adapted LLMs, including geological interpretation, subsurface characterization, site planning, design calculations, numerical modeling, safety and risk assessment, and educational tutoring. It also analyzes benefits and limitations of geotechnical-adapted LLMs, and identifies promising directions for future research in this interdisciplinary discipline. The findings serve as a valuable resource for practitioners seeking to integrate LLMs into geotechnical practice, while also providing a foundation to stimulate further investigation within the academic community.",
        "arxiv_id": "2507.05613",
        "ARXIVID": "2507.05613",
        "COMMENT": "Does not match any specific criteria but surveys domain adaptation of LLMs for geotechnical applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.05495": {
        "authors": [
            "Prahaladh Chandrahasan",
            "Jiahe Jin",
            "Zhihan Zhang",
            "Tevin Wang",
            "Andy Tang",
            "Lucy Mo",
            "Morteza Ziyadi",
            "Leonardo F. R. Ribeiro",
            "Zimeng Qiu",
            "Markus Dreyer",
            "Akari Asai",
            "Chenyan Xiong"
        ],
        "title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents",
        "abstract": "arXiv:2507.05495v1 Announce Type: new  Abstract: Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at https://www.youtube.com/watch?v=g4d2dnbdseg.",
        "arxiv_id": "2507.05495",
        "ARXIVID": "2507.05495",
        "COMMENT": "Does not match any specific criteria but discusses a platform for evaluating deep research agents.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.05816": {
        "authors": [
            "Shuai Zhao",
            "Yulin Zhang",
            "Luwei Xiao",
            "Xinyi Wu",
            "Yanhao Jia",
            "Zhongliang Guo",
            "Xiaobao Wu",
            "Cong-Duy Nguyen",
            "Guoming Zhang",
            "Anh Tuan Luu"
        ],
        "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity",
        "abstract": "arXiv:2507.05816v1 Announce Type: new  Abstract: Despite the remarkable progress of large language models (LLMs) across various domains, their capacity to predict retinopathy of prematurity (ROP) risk remains largely unexplored. To address this gap, we introduce a novel Chinese benchmark dataset, termed CROP, comprising 993 admission records annotated with low, medium, and high-risk labels. To systematically examine the predictive capabilities and affective biases of LLMs in ROP risk stratification, we propose Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and ICL schemes leverage external medical knowledge to enhance predictive accuracy. Crucially, we integrate emotional elements at the prompt level to investigate how different affective framings influence the model's ability to predict ROP and its bias patterns. Empirical results derived from the CROP dataset yield two principal observations. First, LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, yet exhibit marked performance gains when augmented with structured external inputs. Second, affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Third, compared to negative emotions, positive emotional framing contributes to mitigating predictive bias in model outputs. These findings highlight the critical role of affect-sensitive prompt engineering in enhancing diagnostic reliability and emphasize the utility of Affective-ROPTester as a framework for evaluating and mitigating affective bias in clinical language modeling systems.",
        "arxiv_id": "2507.05816",
        "ARXIVID": "2507.05816",
        "COMMENT": "Does not match any specific criteria. Focuses on medical risk prediction using LLMs, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}