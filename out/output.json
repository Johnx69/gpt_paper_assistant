{
    "2508.06080": {
        "authors": [
            "Bin Xia",
            "Jiyang Liu",
            "Yuechen Zhang",
            "Bohao Peng",
            "Ruihang Chu",
            "Yitong Wang",
            "Xinglong Wu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "title": "DreamVE: Unified Instruction-based Image and Video Editing",
        "abstract": "arXiv:2508.06080v1 Announce Type: new  Abstract: Instruction-based editing holds vast potential due to its simple and efficient interactive editing format. However, instruction-based editing, particularly for video, has been constrained by limited training data, hindering its practical application. To this end, we introduce DreamVE, a unified model for instruction-based image and video editing. Specifically, We propose a two-stage training strategy: first image editing, then video editing. This offers two main benefits: (1) Image data scales more easily, and models are more efficient to train, providing useful priors for faster and better video editing training. (2) Unifying image and video generation is natural and aligns with current trends. Moreover, we present comprehensive training data synthesis pipelines, including collage-based and generative model-based data synthesis. The collage-based data synthesis combines foreground objects and backgrounds to generate diverse editing data, such as object manipulation, background changes, and text modifications. It can easily generate billions of accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE on extensive collage-based data to achieve strong performance in key editing types and enhance generalization and transfer capabilities. However, collage-based data lacks some attribute editing cases, leading to a relative drop in performance. In contrast, the generative model-based pipeline, despite being hard to scale up, offers flexibility in handling attribute editing cases. Therefore, we use generative model-based data to further fine-tune DreamVE. Besides, we design an efficient and powerful editing framework for DreamVE. We build on the SOTA T2V model and use a token concatenation with early drop approach to inject source image guidance, ensuring strong consistency and editability. The codes and models will be released.",
        "arxiv_id": "2508.06080",
        "ARXIVID": "2508.06080",
        "COMMENT": "Matches criterion 5 as it introduces a unified model for instruction-based image and video editing, integrating image and video tasks with generative models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.06139": {
        "authors": [
            "Shaohua Pan",
            "Xinyu Yi",
            "Yan Zhou",
            "Weihua Jian",
            "Yuan Zhang",
            "Pengfei Wan",
            "Feng Xu"
        ],
        "title": "DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera",
        "abstract": "arXiv:2508.06139v1 Announce Type: new  Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to perform real-time human motion capture. This paper proposes a diffusion-based solution to learn human motion priors and fuse the two modalities of signals together seamlessly in a unified framework. By delicately considering the characteristics of the two signals, the sequential visual information is considered as a whole and transformed into a condition embedding, while the inertial measurement is concatenated with the noisy body pose frame by frame to construct a sequential input for the diffusion model. Firstly, we observe that the visual information may be unavailable in some frames due to occlusions or subjects moving out of the camera view. Thus incorporating the sequential visual features as a whole to get a single feature embedding is robust to the occasional degenerations of visual information in those frames. On the other hand, the IMU measurements are robust to occlusions and always stable when signal transmission has no problem. So incorporating them frame-wisely could better explore the temporal information for the system. Experiments have demonstrated the effectiveness of the system design and its state-of-the-art performance in pose estimation compared with the previous works. Our codes are available for research at https://shaohua-pan.github.io/diffcap-page.",
        "arxiv_id": "2508.06139",
        "ARXIVID": "2508.06139",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for real-time human motion capture using a diffusion-based approach.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.06205": {
        "authors": [
            "Ruiyan Wang",
            "Lin Zuo",
            "Zonghao Lin",
            "Qiang Wang",
            "Zhengxue Cheng",
            "Rong Xie",
            "Jun Ling",
            "Li Song"
        ],
        "title": "PA-HOI: A Physics-Aware Human and Object Interaction Dataset",
        "abstract": "arXiv:2508.06205v1 Announce Type: new  Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions between humans and objects in physical environments, providing essential biomechanical and cognitive-behavioral foundations for fields such as robotics, virtual reality, and human-computer interaction. However, existing HOI data sets focus on details of affordance, often neglecting the influence of physical properties of objects on human long-term motion. To bridge this gap, we introduce the PA-HOI Motion Capture dataset, which highlights the impact of objects' physical attributes on human motion dynamics, including human posture, moving velocity, and other motion characteristics. The dataset comprises 562 motion sequences of human-object interactions, with each sequence performed by subjects of different genders interacting with 35 3D objects that vary in size, shape, and weight. This dataset stands out by significantly extending the scope of existing ones for understanding how the physical attributes of different objects influence human posture, speed, motion scale, and interacting strategies. We further demonstrate the applicability of the PA-HOI dataset by integrating it with existing motion generation methods, validating its capacity to transfer realistic physical awareness.",
        "arxiv_id": "2508.06205",
        "ARXIVID": "2508.06205",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset for human-object interaction, focusing on physical attributes and their impact on motion.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2508.05982": {
        "authors": [
            "Qingyang Liu",
            "Bingjie Gao",
            "Weiheng Huang",
            "Jun Zhang",
            "Zhongqian Sun",
            "Yang Wei",
            "Zelin Peng",
            "Qianli Ma",
            "Shuai Yang",
            "Zhaohe Liao",
            "Haonan Zhao",
            "Li Niu"
        ],
        "title": "AnimateScene: Camera-controllable Animation in Any Scene",
        "abstract": "arXiv:2508.05982v1 Announce Type: new  Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and broad adoption in recent years. However, seamlessly integrating reconstructed scenes with 4D human animation to produce visually engaging results remains challenging. One key difficulty lies in placing the human at the correct location and scale within the scene while avoiding unrealistic interpenetration. Another challenge is that the human and the background may exhibit different lighting and style, leading to unrealistic composites. In addition, appealing character motion videos are often accompanied by camera movements, which means that the viewpoints need to be reconstructed along a specified trajectory. We present AnimateScene, which addresses the above issues in a unified framework. First, we design an accurate placement module that automatically determines a plausible 3D position for the human and prevents any interpenetration within the scene during motion. Second, we propose a training-free style alignment method that adapts the 4D human representation to match the background's lighting and style, achieving coherent visual integration. Finally, we design a joint post-reconstruction method for both the 4D human and the 3D scene that allows camera trajectories to be inserted, enabling the final rendered video to feature visually appealing camera movements. Extensive experiments show that AnimateScene generates dynamic scene videos with high geometric detail and spatiotemporal coherence across various camera and action combinations.",
        "arxiv_id": "2508.05982",
        "ARXIVID": "2508.05982",
        "COMMENT": "Matches criterion 3 as it introduces a unified framework for integrating 3D scene reconstruction and 4D human animation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.05819": {
        "authors": [
            "Jong-Ik Park",
            "Carlee Joe-Wong",
            "Gary K. Fedder"
        ],
        "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses",
        "abstract": "arXiv:2508.05819v1 Announce Type: new  Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from multiple 2D images, even those taken with unknown camera poses. However, they still miss the fine-detailed structures that matter in industrial inspection, e.g., detecting sub-micron defects on a production line or analyzing chips with Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution is fixed and compute budgets are tight, so the only way to expose fine structure is to add zoom-in images; yet, this breaks the multi-view consistency that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF (MZEN), the first NeRF framework that natively handles multi-zoom image sets. MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom scalar that scales the focal length, and (ii) introduces a novel pose strategy: wide-field images are solved first to establish a global metric frame, and zoom-in images are then pose-primed to the nearest wide-field counterpart via a zoom-consistent crop-and-match procedure before joint refinement. Across eight forward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of micro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently outperforms pose-free baselines and even high-resolution variants, boosting PSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$. MZEN, therefore, extends NeRF to real-world factory settings, preserving global accuracy while capturing the micron-level details essential for industrial inspection.",
        "arxiv_id": "2508.05819",
        "ARXIVID": "2508.05819",
        "COMMENT": "Matches criterion 4 as it focuses on extending NeRF for 3D reconstruction with unknown camera poses, which is a foundation model in vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06113": {
        "authors": [
            "Jian Wang",
            "Chaokang Jiang",
            "Haitao Xu"
        ],
        "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving",
        "abstract": "arXiv:2508.06113v1 Announce Type: new  Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end autonomous driving, yet their performance is increasingly hampered by a reliance on transformer-based fusion. These architectures face fundamental limitations: quadratic computational complexity restricts the use of high-resolution features, and a lack of spatial priors prevents them from effectively modeling the inherent structure of Bird's Eye View (BEV) representations. This paper introduces GMF-Drive (Gated Mamba Fusion for Driving), an end-to-end framework that overcomes these challenges through two principled innovations. First, we supersede the information-limited histogram-based LiDAR representation with a geometrically-augmented pillar format encoding shape descriptors and statistical features, preserving critical 3D geometric details. Second, we propose a novel hierarchical gated mamba fusion (GM-Fusion) architecture that substitutes an expensive transformer with a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM leverages directional sequencing and adaptive fusion mechanisms to capture long-range dependencies with linear complexity, while explicitly respecting the unique spatial properties of the driving scene. Extensive experiments on the challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new state-of-the-art performance, significantly outperforming DiffusionDrive. Comprehensive ablation studies validate the efficacy of each component, demonstrating that task-specific SSMs can surpass a general-purpose transformer in both performance and efficiency for autonomous driving.",
        "arxiv_id": "2508.06113",
        "ARXIVID": "2508.06113",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for end-to-end autonomous driving with spatial-aware BEV representation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06093": {
        "authors": [
            "Chen Zhu",
            "Buzhen Huang",
            "Zijing Wu",
            "Binghui Zuo",
            "Yangang Wang"
        ],
        "title": "E-React: Towards Emotionally Controlled Synthesis of Human Reactions",
        "abstract": "arXiv:2508.06093v1 Announce Type: new  Abstract: Emotion serves as an essential component in daily human interactions. Existing human motion generation frameworks do not consider the impact of emotions, which reduces naturalness and limits their application in interactive tasks, such as human reaction synthesis. In this work, we introduce a novel task: generating diverse reaction motions in response to different emotional cues. However, learning emotion representation from limited motion data and incorporating it into a motion generation framework remains a challenging problem. To address the above obstacles, we introduce a semi-supervised emotion prior in an actor-reactor diffusion model to facilitate emotion-driven reaction synthesis. Specifically, based on the observation that motion clips within a short sequence tend to share the same emotion, we first devise a semi-supervised learning framework to train an emotion prior. With this prior, we further train an actor-reactor diffusion model to generate reactions by considering both spatial interaction and emotional response. Finally, given a motion sequence of an actor, our approach can generate realistic reactions under various emotional conditions. Experimental results demonstrate that our model outperforms existing reaction generation methods. The code and data will be made publicly available at https://ereact.github.io/",
        "arxiv_id": "2508.06093",
        "ARXIVID": "2508.06093",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for emotion-driven human reaction synthesis, which is relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06317": {
        "authors": [
            "Jian Hu",
            "Zixu Cheng",
            "Shaogang Gong",
            "Isabel Guan",
            "Jianye Hao",
            "Jun Wang",
            "Kun Shao"
        ],
        "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding",
        "abstract": "arXiv:2508.06317v1 Announce Type: new  Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments matching a natural language description (a query) in a long video. While Vision-Language Models (VLMs) are effective at holistic semantic matching, they often struggle with fine-grained temporal localisation. Recently, Group Relative Policy Optimisation (GRPO) reformulates the inference process as a reinforcement learning task, enabling fine-grained grounding and achieving strong in-domain performance. However, GRPO relies on labelled data, making it unsuitable in unlabelled domains. Moreover, because videos are large and expensive to store and process, performing full-scale adaptation introduces prohibitive latency and computational overhead, making it impractical for real-time deployment. To overcome both problems, we introduce a Data-Efficient Unlabelled Cross-domain Temporal Grounding method, from which a model is first trained on a labelled source domain, then adapted to a target domain using only a small number of unlabelled videos from the target domain. This approach eliminates the need for target annotation and keeps both computational and storage overhead low enough to run in real time. Specifically, we introduce. Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain knowledge transfer in learning video temporal grounding without target labels. URPA generates multiple candidate predictions using GRPO rollouts, averages them to form a pseudo label, and estimates confidence from the variance across these rollouts. This confidence then weights the training rewards, guiding the model to focus on reliable supervision. Experiments on three datasets across six cross-domain settings show that URPA generalises well using only a few unlabelled target videos. Codes will be released once published.",
        "arxiv_id": "2508.06317",
        "ARXIVID": "2508.06317",
        "COMMENT": "Matches criterion 6 as it addresses video temporal grounding with a novel uncertainty-quantified rollout policy adaptation method.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06072": {
        "authors": [
            "Zijian Chen",
            "Lirong Deng",
            "Zhengyu Chen",
            "Kaiwei Zhang",
            "Qi Jia",
            "Yuan Tian",
            "Yucheng Zhu",
            "Guangtao Zhai"
        ],
        "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation",
        "abstract": "arXiv:2508.06072v1 Announce Type: new  Abstract: Evaluating the abilities of large models and manifesting their gaps are challenging. Current benchmarks adopt either ground-truth-based score-form evaluation on static datasets or indistinct textual chatbot-style human preferences collection, which may not provide users with immediate, intuitive, and perceptible feedback on performance differences. In this paper, we introduce BioMotion Arena, a novel framework for evaluating large language models (LLMs) and multimodal large language models (MLLMs) via visual animation. Our methodology draws inspiration from the inherent visual perception of motion patterns characteristic of living organisms that utilizes point-light source imaging to amplify the performance discrepancies between models. Specifically, we employ a pairwise comparison evaluation and collect more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion variants. Data analyses show that the crowd-sourced human votes are in good agreement with those of expert raters, demonstrating the superiority of our BioMotion Arena in offering discriminative feedback. We also find that over 90\\% of evaluated models, including the cutting-edge open-source InternVL3 and proprietary Claude-4 series, fail to produce fundamental humanoid point-light groups, much less smooth and biologically plausible motions. This enables BioMotion Arena to serve as a challenging benchmark for performance visualization and a flexible evaluation framework without restrictions on ground-truth.",
        "arxiv_id": "2508.06072",
        "ARXIVID": "2508.06072",
        "COMMENT": "Matches criterion 2 as it evaluates multimodal large language models (MLLMs) using a novel visual animation framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06014": {
        "authors": [
            "Minsu Kim",
            "Subin Jeon",
            "In Cho",
            "Mijin Yoo",
            "Seon Joo Kim"
        ],
        "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors",
        "abstract": "arXiv:2508.06014v1 Announce Type: new  Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration. To address this, we propose a 3DGS-based pipeline that generates additional training views to enhance reconstruction. We introduce an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views significantly improves reconstruction quality. To evaluate our method, we present Wild-Explore, a benchmark designed for challenging scene exploration. Experiments demonstrate that our approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.   https://exploregs.github.io",
        "arxiv_id": "2508.06014",
        "ARXIVID": "2508.06014",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Wild-Explore) and a novel method for 3D scene exploration in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.06202": {
        "authors": [
            "Chang Che",
            "Ziqi Wang",
            "Pengwan Yang",
            "Qi Wang",
            "Hui Ma",
            "Zenglin Shi"
        ],
        "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning",
        "abstract": "arXiv:2508.06202v1 Announce Type: new  Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language Models (MLLMs) to incrementally learn new tasks over time. However, this process is challenged by catastrophic forgetting, where performance on previously learned tasks deteriorates as the model adapts to new ones. A common approach to mitigate forgetting is architecture expansion, which introduces task-specific modules to prevent interference. Yet, existing methods often expand entire layers for each task, leading to significant parameter overhead and poor scalability. To overcome these issues, we introduce LoRA in LoRA (LiLoRA), a highly efficient architecture expansion method tailored for CVIT in MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy, applies an additional low-rank decomposition to matrix B to minimize task-specific parameters, and incorporates a cosine-regularized stability loss to preserve consistency in shared representations over time. Extensive experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves superior performance in sequential task learning while significantly improving parameter efficiency compared to existing approaches.",
        "arxiv_id": "2508.06202",
        "ARXIVID": "2508.06202",
        "COMMENT": "Matches criterion 2 as it proposes a parameter-efficient method for continual visual instruction tuning in multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.05851": {
        "authors": [
            "Ka-Wai Yung",
            "Felix J. S. Bragman",
            "Jialang Xu",
            "Imanol Luengo",
            "Danail Stoyanov",
            "Evangelos B. Mazomenos"
        ],
        "title": "Temporal Cluster Assignment for Efficient Real-Time Video Segmentation",
        "abstract": "arXiv:2508.05851v1 Announce Type: new  Abstract: Vision Transformers have substantially advanced the capabilities of segmentation models across both image and video domains. Among them, the Swin Transformer stands out for its ability to capture hierarchical, multi-scale representations, making it a popular backbone for segmentation in videos. However, despite its window-attention scheme, it still incurs a high computational cost, especially in larger variants commonly used for dense prediction in videos. This remains a major bottleneck for real-time, resource-constrained applications. Whilst token reduction methods have been proposed to alleviate this, the window-based attention mechanism of Swin requires a fixed number of tokens per window, limiting the applicability of conventional pruning techniques. Meanwhile, training-free token clustering approaches have shown promise in image segmentation while maintaining window consistency. Nevertheless, they fail to exploit temporal redundancy, missing a key opportunity to further optimize video segmentation performance. We introduce Temporal Cluster Assignment (TCA), a lightweight and effective, fine-tuning-free strategy that enhances token clustering by leveraging temporal coherence across frames. Instead of indiscriminately dropping redundant tokens, TCA refines token clusters using temporal correlations, thereby retaining fine-grained details while significantly reducing computation. Extensive evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical video dataset show that TCA consistently boosts the accuracy-speed trade-off of existing clustering-based methods. Our results demonstrate that TCA generalizes competently across both natural and domain-specific videos.",
        "arxiv_id": "2508.05851",
        "ARXIVID": "2508.05851",
        "COMMENT": "Matches criterion 6 as it proposes a novel method for efficient real-time video segmentation, improving video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.06453": {
        "authors": [
            "Ruida Cheng",
            "Tejas Sudharshan Mathai",
            "Pritam Mukherjee",
            "Benjamin Hou",
            "Qingqing Zhu",
            "Zhiyong Lu",
            "Matthew McAuliffe",
            "Ronald M. Summers"
        ],
        "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation",
        "abstract": "arXiv:2508.06453v1 Announce Type: new  Abstract: Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba",
        "arxiv_id": "2508.06453",
        "ARXIVID": "2508.06453",
        "COMMENT": "Matches criterion 5 as it integrates text and image understanding for lesion segmentation using a novel architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.06009": {
        "authors": [
            "Jun Feng",
            "Zixin Wang",
            "Zhentao Zhang",
            "Yue Guo",
            "Zhihan Zhou",
            "Xiuyi Chen",
            "Zhenyang Li",
            "Dawei Yin"
        ],
        "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models",
        "abstract": "arXiv:2508.06009v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual mathematical reasoning across various existing benchmarks. However, these benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users. To address this gap, we introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios. Each question is an image, containing the question text and visual element. We systematically classify the real images into three primary categories: image quality degradation, perspective variation, and irrelevant content interference, which are further delineated into 14 subcategories. Additionally, MathReal spans five core knowledge and ability categories, which encompass three question types and are divided into three difficulty levels. To comprehensively evaluate the multimodal mathematical reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we design six experimental settings that enable a systematic analysis of their performance. Through extensive experimentation, we find that the problem-solving abilities of existing MLLMs are significantly challenged in realistic educational contexts. Based on this, we conduct a thorough analysis of their performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities, and outlining directions for future improvements. Data and code: https://github.com/junfeng0288/MathReal.",
        "arxiv_id": "2508.06009",
        "ARXIVID": "2508.06009",
        "COMMENT": "Matches criterion 2 as it explores Multimodal Large Language Models (MLLMs) and their application in real-world scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.06082": {
        "authors": [
            "Yanxiao Sun",
            "Jiafu Wu",
            "Yun Cao",
            "Chengming Xu",
            "Yabiao Wang",
            "Weijian Cao",
            "Donghao Luo",
            "Chengjie Wang",
            "Yanwei Fu"
        ],
        "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment",
        "abstract": "arXiv:2508.06082v1 Announce Type: new  Abstract: Diffusion-based or flow-based models have achieved significant progress in video synthesis but require multiple iterative sampling steps, which incurs substantial computational overhead. While many distillation methods that are solely based on trajectory-preserving or distribution-matching have been developed to accelerate video generation models, these approaches often suffer from performance breakdown or increased artifacts under few-step settings. To address these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and stable distillation framework that combines the advantages of trajectory-preserving and distribution-matching strategies. Our approach introduces continuous-time consistency distillation to ensure precise preservation of ODE trajectories. Subsequently, we propose a dual-perspective alignment that includes distribution alignment between synthetic and real data along with trajectory alignment across different inference steps. Our method maintains high-quality video generation while substantially reducing the number of inference steps. Quantitative evaluations on the OpenVid-1M benchmark demonstrate that our method significantly outperforms existing approaches in few-step video generation.",
        "arxiv_id": "2508.06082",
        "ARXIVID": "2508.06082",
        "COMMENT": "Matches criterion 6 as it proposes a novel framework for video generation with fewer steps, improving computational efficiency.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.06032": {
        "authors": [
            "Kiran Chhatre",
            "Christopher Peters",
            "Srikrishna Karanam"
        ],
        "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts",
        "abstract": "arXiv:2508.06032v1 Announce Type: new  Abstract: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types. Recent open-vocabulary segmentation approaches leverage pretrained text-to-image (T2I) diffusion model features for strong zero-shot transfer, but typically group entire humans into a single person category, failing to distinguish diverse clothing or detailed body parts. To address this, we propose Spectrum, a unified network for part-level pixel parsing (body parts and clothing) and instance-level grouping. While diffusion-based open-vocabulary models generalize well across tasks, their internal representations are not specialized for detailed human parsing. We observe that, unlike diffusion models with broad representations, image-driven 3D texture generators maintain faithful correspondence to input images, enabling stronger representations for parsing diverse clothing and body parts. Spectrum introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model -- obtained by fine-tuning a T2I model on 3D human texture maps -- for improved alignment with body parts and clothing. From an input image, we extract human-part internal features via the I2Tx diffusion model and generate semantically valid masks aligned to diverse clothing categories through prompt-guided grounding. Once trained, Spectrum produces semantic segmentation maps for every visible body part and clothing category, ignoring standalone garments or irrelevant objects, for any number of humans in the scene. We conduct extensive cross-dataset experiments -- separately assessing body parts, clothing parts, unseen clothing categories, and full-body masks -- and demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation.",
        "arxiv_id": "2508.06032",
        "ARXIVID": "2508.06032",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model (Spectrum) for detailed human parsing and segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.06203": {
        "authors": [
            "Zhaopeng Gu",
            "Bingke Zhu",
            "Guibo Zhu",
            "Yingying Chen",
            "Wei Ge",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection",
        "abstract": "arXiv:2508.06203v1 Announce Type: new  Abstract: Anomaly detection is a critical task across numerous domains and modalities, yet existing methods are often highly specialized, limiting their generalizability. These specialized models, tailored for specific anomaly types like textural defects or logical errors, typically exhibit limited performance when deployed outside their designated contexts. To overcome this limitation, we propose AnomalyMoE, a novel and universal anomaly detection framework based on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the complex anomaly detection problem into three distinct semantic hierarchies: local structural anomalies, component-level semantic anomalies, and global logical anomalies. AnomalyMoE correspondingly employs three dedicated expert networks at the patch, component, and global levels, and is specialized in reconstructing features and identifying deviations at its designated semantic level. This hierarchical design allows a single model to concurrently understand and detect a wide spectrum of anomalies. Furthermore, we introduce an Expert Information Repulsion (EIR) module to promote expert diversity and an Expert Selection Balancing (ESB) module to ensure the comprehensive utilization of all experts. Experiments on 8 challenging datasets spanning industrial imaging, 3D point clouds, medical imaging, video surveillance, and logical anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art performance, significantly outperforming specialized methods in their respective domains.",
        "arxiv_id": "2508.06203",
        "ARXIVID": "2508.06203",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model (AnomalyMoE) for visual anomaly detection across multiple domains.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.06142": {
        "authors": [
            "Hanqing Wang",
            "Yuan Tian",
            "Mingyu Liu",
            "Zhenhao Zhang",
            "Xiangyang Zhu"
        ],
        "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models",
        "abstract": "arXiv:2508.06142v1 Announce Type: new  Abstract: In the rapidly evolving landscape of Multimodal Large Language Models (MLLMs), the safety concerns of their outputs have earned significant attention. Although numerous datasets have been proposed, they may become outdated with MLLM advancements and are susceptible to data contamination issues. To address these problems, we propose \\textbf{SDEval}, the \\textit{first} safety dynamic evaluation framework to controllably adjust the distribution and complexity of safety benchmarks. Specifically, SDEval mainly adopts three dynamic strategies: text, image, and text-image dynamics to generate new samples from original benchmarks. We first explore the individual effects of text and image dynamics on model safety. Then, we find that injecting text dynamics into images can further impact safety, and conversely, injecting image dynamics into text also leads to safety risks. SDEval is general enough to be applied to various existing safety and even capability benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and capability benchmarks, MMBench and MMVet, show that SDEval significantly influences safety evaluation, mitigates data contamination, and exposes safety limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval",
        "arxiv_id": "2508.06142",
        "ARXIVID": "2508.06142",
        "COMMENT": "Matches criterion 2 as it focuses on safety evaluation for multimodal large language models (MLLMs) with a dynamic evaluation framework.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.06228": {
        "authors": [
            "Daniel Feijoo",
            "Paula Garrido-Mellado",
            "Jaesung Rim",
            "Alvaro Garcia",
            "Marcos V. Conde"
        ],
        "title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder",
        "abstract": "arXiv:2508.06228v1 Announce Type: new  Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental task in computational photography and low-level computer vision. Existing approaches focus on specialized solutions tailored to particular blur types, thus, these solutions lack generalization. This limitation in current methods implies requiring multiple models to cover several blur types, which is not practical in many real scenarios. In this paper, we introduce the first all-in-one deblurring method capable of efficiently restoring images affected by diverse blur degradations, including global motion, local motion, blur in low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE) decoding module, which dynamically routes image features based on the recognized blur degradation, enabling precise and efficient restoration in an end-to-end manner. Our unified approach not only achieves performance comparable to dedicated task-specific models, but also demonstrates remarkable robustness and generalization capabilities on unseen blur degradation scenarios.",
        "arxiv_id": "2508.06228",
        "ARXIVID": "2508.06228",
        "COMMENT": "Matches criterion 4 as it introduces a mixture-of-experts model for unified image deblurring, a foundational task in computer vision.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.05990": {
        "authors": [
            "Haichao Wang",
            "Xinyue Xi",
            "Jiangtao Wen",
            "Yuxing Han"
        ],
        "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision",
        "abstract": "arXiv:2508.05990v1 Announce Type: new  Abstract: The efficiency of video computer vision system remains a challenging task due to the high temporal redundancy inside a video. Existing works have been proposed for efficient vision computer vision. However, they do not fully reduce the temporal redundancy and neglect the front end computation overhead. In this paper, we propose an efficient video computer vision system. First, image signal processor is removed and Bayer-format data is directly fed into video computer vision models, thus saving the front end computation. Second, instead of optical flow models and video codecs, a fast block matching-based motion estimation algorithm is proposed specifically for efficient video computer vision, with a MV refinement module. To correct the error, context-aware block refinement network is introduced to refine regions with large error. To further balance the accuracy and efficiency, a frame selection strategy is employed. Experiments on multiple video computer vision tasks demonstrate that our method achieves significant acceleration with slight performance loss.",
        "arxiv_id": "2508.05990",
        "ARXIVID": "2508.05990",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks with novel methodologies for efficient video vision systems.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.06326": {
        "authors": [
            "Nathaniel Virgo",
            "Martin Biehl",
            "Manuel Baltieri",
            "Matteo Capucci"
        ],
        "title": "A \"good regulator theorem\" for embodied agents",
        "abstract": "arXiv:2508.06326v1 Announce Type: new  Abstract: In a classic paper, Conant and Ashby claimed that \"every good regulator of a system must be a model of that system.\" Artificial Life has produced many examples of systems that perform tasks with apparently no model in sight; these suggest Conant and Ashby's theorem doesn't easily generalise beyond its restricted setup. Nevertheless, here we show that a similar intuition can be fleshed out in a different way: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having \"beliefs\" about its environment, which it \"updates\" in response to sensory input. This notion of belief updating provides a notion of model that is more sophisticated than Conant and Ashby's, as well as a theorem that is more broadly applicable. However, it necessitates a change in perspective, in that the observer plays an essential role in the theory: models are not a mere property of the system but are imposed on it from outside. Our theorem holds regardless of whether the system is regulating its environment in a classic control theory setup, or whether it's regulating its own internal state; the model is of its environment either way. The model might be trivial, however, and this is how the apparent counterexamples are resolved.",
        "arxiv_id": "2508.06326",
        "ARXIVID": "2508.06326",
        "COMMENT": "Matches criterion 3 as it provides a theoretical perspective on embodied agents and their regulation tasks, offering a fresh perspective.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.05855": {
        "authors": [
            "Zixia Wang",
            "Jia Hu",
            "Ronghui Mu"
        ],
        "title": "Safety of Embodied Navigation: A Survey",
        "abstract": "arXiv:2508.05855v1 Announce Type: new  Abstract: As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.",
        "arxiv_id": "2508.05855",
        "ARXIVID": "2508.05855",
        "COMMENT": "Matches criterion 7 as it is a survey paper on safety in embodied navigation, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2508.06492": {
        "authors": [
            "Yuwei Yang",
            "Zeyu Zhang",
            "Yunzhong Hou",
            "Zhuowan Li",
            "Gaowen Liu",
            "Ali Payani",
            "Yuan-Sen Ting",
            "Liang Zheng"
        ],
        "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding",
        "abstract": "arXiv:2508.06492v1 Announce Type: new  Abstract: Being able to effectively read scientific plots, or chart understanding, is a central part toward building effective agents for science. However, existing multimodal large language models (MLLMs), especially open-source ones, are still falling behind with a typical success rate of 30%-50% on challenging benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are often restricted by their inadequate similarity to the real charts, which could compromise model training and performance on complex real-world charts. In this study, we show that modularizing chart generation and diversifying visual details improves chart understanding capabilities. In particular, we design a five-step data synthesis pipeline, where we separate data and function creation for single plot generation, condition the generation of later subplots on earlier ones for multi-subplot figures, visually diversify the generated figures, filter out low quality data, and finally generate the question-answer (QA) pairs with GPT-4o. This approach allows us to streamline the generation of fine-tuning datasets and introduce the effective chart dataset (ECD), which contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring 250+ chart type combinations with high visual complexity. We show that ECD consistently improves the performance of various MLLMs on a range of real-world and synthetic test sets. Code, data and models are available at: https://github.com/yuweiyang-anu/ECD.",
        "arxiv_id": "2508.06492",
        "ARXIVID": "2508.06492",
        "COMMENT": "Matches criterion 2 as it focuses on improving multimodal large language models for chart understanding, a vision-language integration task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.05954": {
        "authors": [
            "Han Lin",
            "Jaemin Cho",
            "Amir Zadeh",
            "Chuan Li",
            "Mohit Bansal"
        ],
        "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents",
        "abstract": "arXiv:2508.05954v1 Announce Type: new  Abstract: There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.",
        "arxiv_id": "2508.05954",
        "ARXIVID": "2508.05954",
        "COMMENT": "Matches criterion 5 as it integrates multimodal LLMs and diffusion models for image generation, focusing on vision-language tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.06452": {
        "authors": [
            "Mattia Litrico",
            "Mario Valerio Giuffrida",
            "Sebastiano Battiato",
            "Devis Tuia"
        ],
        "title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation",
        "abstract": "arXiv:2508.06452v1 Announce Type: new  Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance.",
        "arxiv_id": "2508.06452",
        "ARXIVID": "2508.06452",
        "COMMENT": "Matches criterion 2 as it explores a novel multimodal approach leveraging language robustness for domain adaptation, which aligns with vision-language integration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.06051": {
        "authors": [
            "Linhan Cao",
            "Wei Sun",
            "Weixia Zhang",
            "Xiangyang Zhu",
            "Jun Jia",
            "Kaiwei Zhang",
            "Dandan Zhu",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning",
        "abstract": "arXiv:2508.06051v1 Announce Type: new  Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual quality degradation in alignment with human visual perception. Despite recent advances, existing VQA models still suffer from two critical limitations: \\textit{poor generalization to out-of-distribution (OOD) videos} and \\textit{limited explainability}, which restrict their applicability in real-world scenarios. To address these challenges, we propose \\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large multimodal models (LMMs) with reinforcement learning to jointly model video quality understanding and scoring, emulating human perceptual decision-making. Specifically, we adopt group relative policy optimization (GRPO), a rule-guided reinforcement learning algorithm that enables reasoning over video quality under score-level supervision, and introduce three VQA-specific rewards: (1) a \\textbf{bell-shaped regression reward} that increases rapidly as the prediction error decreases and becomes progressively less sensitive near the ground truth; (2) a \\textbf{pairwise ranking reward} that guides the model to correctly determine the relative quality between video pairs; and (3) a \\textbf{temporal consistency reward} that encourages the model to prefer temporally coherent videos over their perturbed counterparts. Extensive experiments demonstrate that VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs. These findings demonstrate that reinforcement learning offers an effective pathway toward building generalizable and explainable VQA models solely with score-level supervision.",
        "arxiv_id": "2508.06051",
        "ARXIVID": "2508.06051",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video quality assessment with reinforcement learning and multimodal models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.05976": {
        "authors": [
            "Zhihao Zhu",
            "Yifan Zheng",
            "Siyu Pan",
            "Yaohui Jin",
            "Yao Mu"
        ],
        "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation",
        "abstract": "arXiv:2508.05976v1 Announce Type: new  Abstract: The fragmentation between high-level task semantics and low-level geometric features remains a persistent challenge in robotic manipulation. While vision-language models (VLMs) have shown promise in generating affordance-aware visual representations, the lack of semantic grounding in canonical spaces and reliance on manual annotations severely limit their ability to capture dynamic semantic-affordance relationships. To address these, we propose Primitive-Aware Semantic Grounding (PASG), a closed-loop framework that introduces: (1) Automatic primitive extraction through geometric feature aggregation, enabling cross-category detection of keypoints and axes; (2) VLM-driven semantic anchoring that dynamically couples geometric primitives with functional affordances and task-relevant description; (3) A spatial-semantic reasoning benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's effectiveness in practical robotic manipulation tasks across diverse scenarios, achieving performance comparable to manual annotations. PASG achieves a finer-grained semantic-affordance understanding of objects, establishing a unified paradigm for bridging geometric primitives with task semantics in robotic manipulation.",
        "arxiv_id": "2508.05976",
        "ARXIVID": "2508.05976",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a framework for semantic grounding in robotic manipulation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.06382": {
        "authors": [
            "Xiangyu Wu",
            "Feng Yu",
            "Yang Yang",
            "Jianfeng Lu"
        ],
        "title": "Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning",
        "abstract": "arXiv:2508.06382v1 Announce Type: new  Abstract: The integration of prompt tuning with multimodal learning has shown significant generalization abilities for various downstream tasks. Despite advancements, existing methods heavily depend on massive modality-specific labeled data (e.g., video, audio, and image), or are customized for a single modality. In this study, we present Text as Any-Modality by Consistent Prompt Tuning (TaAM-CPT), a scalable approach for constructing a general representation model toward unlimited modalities using solely text data. TaAM-CPT comprises modality prompt pools, text construction, and modality-aligned text encoders from pre-trained models, which allows for extending new modalities by simply adding prompt pools and modality-aligned text encoders. To harmonize the learning across different modalities, TaAM-CPT designs intra- and inter-modal learning objectives, which can capture category details within modalities while maintaining semantic consistency across different modalities. Benefiting from its scalable architecture and pre-trained models, TaAM-CPT can be seamlessly extended to accommodate unlimited modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT achieves leading results on diverse datasets spanning various modalities, including video classification, image classification, and audio classification. The code is available at https://github.com/Jinx630/TaAM-CPT.",
        "arxiv_id": "2508.06382",
        "ARXIVID": "2508.06382",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores multimodal learning with text and other modalities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.06092": {
        "authors": [
            "Yachun Mi",
            "Yu Li",
            "Yanting Li",
            "Shixin Sun",
            "Chen Hui",
            "Tong Zhang",
            "Yuanyuan Liu",
            "Chenyue Song",
            "Shaohui Liu"
        ],
        "title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation",
        "abstract": "arXiv:2508.06092v1 Announce Type: new  Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key research challenge. Current mainstream VQA methods typically improve performance by pretraining on large-scale classification datasets (e.g., ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this strategy presents two significant challenges: (1) merely transferring semantic knowledge learned from pretraining is insufficient for VQA, as video quality depends on multiple factors (e.g., semantics, distortion, motion, aesthetics); (2) pretraining on large-scale datasets demands enormous computational resources, often dozens or even hundreds of times greater than training directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown remarkable generalization capabilities across a wide range of visual tasks, and have begun to demonstrate promising potential in quality assessment. In this work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP enhances both visual and textual representations through a Shared Cross-Modal Adapter (SCMA), which contains only a minimal number of trainable parameters and is the only component that requires training. This design significantly reduces computational cost. In addition, we introduce a set of five learnable quality-level prompts to guide the VLMs in perceiving subtle quality variations, thereby further enhancing the model's sensitivity to video quality. Furthermore, we investigate the impact of different frame sampling strategies on VQA performance, and find that frame-difference-based sampling leads to better generalization performance across datasets. Extensive experiments demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.",
        "arxiv_id": "2508.06092",
        "ARXIVID": "2508.06092",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video quality assessment using vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.05991": {
        "authors": [
            "Juewen Hu",
            "Yexin Li",
            "Jiulin Li",
            "Shuo Chen",
            "Pring Wong"
        ],
        "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge",
        "abstract": "arXiv:2508.05991v1 Announce Type: new  Abstract: Emotion recognition plays a vital role in enhancing human-computer interaction. In this study, we tackle the MER-SEMI challenge of the MER2025 competition by proposing a novel multimodal emotion recognition framework. To address the issue of data scarcity, we leverage large-scale pre-trained models to extract informative features from visual, audio, and textual modalities. Specifically, for the visual modality, we design a dual-branch visual encoder that captures both global frame-level features and localized facial representations. For the textual modality, we introduce a context-enriched method that employs large language models to enrich emotional cues within the input text. To effectively integrate these multimodal features, we propose a fusion strategy comprising two key components, i.e., self-attention mechanisms for dynamic modality weighting, and residual connections to preserve original representations. Beyond architectural design, we further refine noisy labels in the training set by a multi-source labeling strategy. Our approach achieves a substantial performance improvement over the official baseline on the MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to 78.63%, thereby validating the effectiveness of the proposed framework.",
        "arxiv_id": "2508.05991",
        "ARXIVID": "2508.05991",
        "COMMENT": "Matches criterion 2 as it proposes a multimodal emotion recognition framework leveraging visual, audio, and textual modalities.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2508.05950": {
        "authors": [
            "Yanxing Liang",
            "Yinghui Wang",
            "Jinlong Yang",
            "Wei Li"
        ],
        "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image",
        "abstract": "arXiv:2508.05950v1 Announce Type: new  Abstract: The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.",
        "arxiv_id": "2508.05950",
        "ARXIVID": "2508.05950",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and self-supervised learning, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.06136": {
        "authors": [
            "YoungChan Choi",
            "HengFei Wang",
            "YiHua Cheng",
            "Boeun Kim",
            "Hyung Jin Chang",
            "YoungGeun Choi",
            "Sang-Il Choi"
        ],
        "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation",
        "abstract": "arXiv:2508.06136v1 Announce Type: new  Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.",
        "arxiv_id": "2508.06136",
        "ARXIVID": "2508.06136",
        "COMMENT": "Does not match any specific criterion but introduces a novel 3D gaze redirection framework, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.05772": {
        "authors": [
            "Can Zhao",
            "Pengfei Guo",
            "Dong Yang",
            "Yucheng Tang",
            "Yufan He",
            "Benjamin Simon",
            "Mason Belue",
            "Stephanie Harmon",
            "Baris Turkbey",
            "Daguang Xu"
        ],
        "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss",
        "abstract": "arXiv:2508.05772v1 Announce Type: new  Abstract: Medical image synthesis is an important topic for both clinical and research applications. Recently, diffusion models have become a leading approach in this area. Despite their strengths, many existing methods struggle with (1) limited generalizability that only work for specific body regions or voxel spacings, (2) slow inference, which is a common issue for diffusion models, and (3) weak alignment with input conditions, which is a critical issue for medical imaging. MAISI, a previously proposed framework, addresses generalizability issues but still suffers from slow inference and limited condition consistency. In this work, we present MAISI-v2, the first accelerated 3D medical image synthesis framework that integrates rectified flow to enable fast and high quality generation. To further enhance condition fidelity, we introduce a novel region-specific contrastive loss to enhance the sensitivity to region of interest. Our experiments show that MAISI-v2 can achieve SOTA image quality with $33 \\times$ acceleration for latent diffusion model. We also conducted a downstream segmentation experiment to show that the synthetic images can be used for data augmentation. We release our code, training details, model weights, and a GUI demo to facilitate reproducibility and promote further development within the community.",
        "arxiv_id": "2508.05772",
        "ARXIVID": "2508.05772",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in medical imaging, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.05783": {
        "authors": [
            "Mengyu Li",
            "Guoyao Shen",
            "Chad W. Farris",
            "Xin Zhang"
        ],
        "title": "Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks",
        "abstract": "arXiv:2508.05783v1 Announce Type: new  Abstract: Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications.",
        "arxiv_id": "2508.05783",
        "ARXIVID": "2508.05783",
        "COMMENT": "Does not match any specific criterion but is related to transformers and medical imaging, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.06076": {
        "authors": [
            "Michael Wehrli",
            "Alicia Durrer",
            "Paul Friedrich",
            "Sidaty El Hadramy",
            "Edwin Li",
            "Luana Brahaj",
            "Carol C. Hasler",
            "Philippe C. Cattin"
        ],
        "title": "Towards MR-Based Trochleoplasty Planning",
        "abstract": "arXiv:2508.06076v1 Announce Type: new  Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition. The surgeries are planned based on surgeons experience, have limited adoption of minimally invasive techniques, and lead to inconsistent outcomes. We propose a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. First, we compute an isotropic super-resolved MR volume using an Implicit Neural Representation (INR). Next, we segment femur, tibia, patella, and fibula with a multi-label custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region. In contrast to prior work producing pseudo-healthy low-resolution 3D MR images, our approach enables the generation of sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use. These can serve as preoperative blueprints for reshaping the femoral groove while preserving the native patella articulation. Furthermore, and in contrast to other work, we do not require a CT for our pipeline - reducing the amount of radiation. We evaluated our approach on 25 TD patients and could show that our target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The code and interactive visualization are available at https://wehrlimi.github.io/sr-3d-planning/.",
        "arxiv_id": "2508.06076",
        "ARXIVID": "2508.06076",
        "COMMENT": "Does not match any specific criterion but is related to medical imaging and neural networks, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.06104": {
        "authors": [
            "Gui Zou",
            "Chaofan Gan",
            "Chern Hong Lim",
            "Supavadee Aramvith",
            "Weiyao Lin"
        ],
        "title": "MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment",
        "abstract": "arXiv:2508.06104v1 Announce Type: new  Abstract: With the increasing availability of 2D and 3D data, significant advancements have been made in the field of cross-modal retrieval. Nevertheless, the existence of imperfect annotations presents considerable challenges, demanding robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label conditions. Existing methods generally address the issue of noise by dividing samples independently within each modality, making them susceptible to overfitting on corrupted labels. To address these issues, we propose a robust 2D-3D \\textbf{M}ulti-level cross-modal adaptive \\textbf{C}orrection and \\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal Joint label Correction (MJC) mechanism that leverages multimodal historical self-predictions to jointly model the modality prediction consistency, enabling reliable label refinement. Additionally, we propose a Multi-level Adaptive Alignment (MAA) strategy to effectively enhance cross-modal feature semantics and discrimination across different levels. Extensive experiments demonstrate the superiority of our method, MCA, which achieves state-of-the-art performance on both conventional and realistic noisy 3D benchmarks, highlighting its generality and effectiveness.",
        "arxiv_id": "2508.06104",
        "ARXIVID": "2508.06104",
        "COMMENT": "Does not match any specific criterion but is relevant to cross-modal retrieval and noisy label correction, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05989": {
        "authors": [
            "Younjoon Chung",
            "Hyoungseob Park",
            "Patrick Rim",
            "Xiaoran Zhang",
            "Jihe He",
            "Ziyao Zeng",
            "Safa Cicek",
            "Byung-Woo Hong",
            "James S. Duncan",
            "Alex Wong"
        ],
        "title": "ETA: Energy-based Test-time Adaptation for Depth Completion",
        "abstract": "arXiv:2508.05989v1 Announce Type: new  Abstract: We propose a method for test-time adaptation of pretrained depth completion models. Depth completion models, trained on some ``source'' data, often predict erroneous outputs when transferred to ``target'' data captured in novel environmental conditions due to a covariate shift. The crux of our method lies in quantifying the likelihood of depth predictions belonging to the source data distribution. The challenge is in the lack of access to out-of-distribution (target) data prior to deployment. Hence, rather than making assumptions regarding the target distribution, we utilize adversarial perturbations as a mechanism to explore the data space. This enables us to train an energy model that scores local regions of depth predictions as in- or out-of-distribution. We update the parameters of pretrained depth completion models at test time to minimize energy, effectively aligning test-time predictions to those of the source distribution. We call our method ``Energy-based Test-time Adaptation'', or ETA for short. We evaluate our method across three indoor and three outdoor datasets, where ETA improve over the previous state-of-the-art method by an average of 6.94% for outdoors and 10.23% for indoors. Project Page: https://fuzzythecat.github.io/eta.",
        "arxiv_id": "2508.05989",
        "ARXIVID": "2508.05989",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and test-time adaptation, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06063": {
        "authors": [
            "Chao Hao",
            "Zitong Yu",
            "Xin Liu",
            "Yuhao Wang",
            "Weicheng Xie",
            "Jingang Shi",
            "Huanjing Yue",
            "Jingyu Yang"
        ],
        "title": "Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection",
        "abstract": "arXiv:2508.06063v1 Announce Type: new  Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two closely related but distinct computer vision tasks. Although both are class-agnostic segmentation tasks that map from RGB space to binary space, the former aims to identify the most salient objects in the image, while the latter focuses on detecting perfectly camouflaged objects that blend into the background in the image. These two tasks exhibit strong contradictory attributes. Previous works have mostly believed that joint learning of these two tasks would confuse the network, reducing its performance on both tasks. However, here we present an opposite perspective: with the correct approach to learning, the network can simultaneously possess the capability to find both salient and camouflaged objects, allowing both tasks to benefit from joint learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks, assuming that the decoding processes of SOD and COD have different distribution characteristics. The key to our method is to learn the respective means and variances of the decoding processes for both tasks by inserting a minimal amount of task-specific learnable parameters within a fully shared network structure, thereby decoupling the contradictory attributes of the two tasks at a minimal cost. Furthermore, we propose a saliency-based sampling strategy (SBSS) to sample the training set of the SOD task to balance the training set sizes of the two tasks. In addition, SBSS improves the training set quality and shortens the training time. Based on the proposed SCJoint and SBSS, we train a powerful generalist network, named JoNet, which has the ability to simultaneously capture both ``salient\" and ``camouflaged\". Extensive experiments demonstrate the competitive performance and effectiveness of our proposed method. The code is available at https://github.com/linuxsino/JoNet.",
        "arxiv_id": "2508.06063",
        "ARXIVID": "2508.06063",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and segmentation tasks, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05776": {
        "authors": [
            "Thomas L. Griffiths",
            "Brenden M. Lake",
            "R. Thomas McCoy",
            "Ellie Pavlick",
            "Taylor W. Webb"
        ],
        "title": "Whither symbols in the era of advanced neural networks?",
        "abstract": "arXiv:2508.05776v1 Announce Type: new  Abstract: Some of the strongest evidence that human minds should be thought about in terms of symbolic systems has been the way they combine ideas, produce novelty, and learn quickly. We argue that modern neural networks -- and the artificial intelligence systems built upon them -- exhibit similar abilities. This undermines the argument that the cognitive processes and representations used by human minds are symbolic, although the fact that these neural networks are typically trained on data generated by symbolic systems illustrates that such systems play an important role in characterizing the abstract problems that human minds have to solve. This argument leads us to offer a new agenda for research on the symbolic basis of human thought.",
        "arxiv_id": "2508.05776",
        "ARXIVID": "2508.05776",
        "COMMENT": "Does not match any specific criterion but is generally relevant to the broader interest in AI and cognitive modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06160": {
        "authors": [
            "Zhenbang Du (Celine)",
            "Yonggan Fu (Celine)",
            "Lifu Wang (Celine)",
            "Jiayi Qian (Celine)",
            "Xiao Luo (Celine)",
            "Yingyan (Celine)",
            "Lin"
        ],
        "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment",
        "abstract": "arXiv:2508.06160v1 Announce Type: new  Abstract: Diffusion models have shown remarkable success across generative tasks, yet their high computational demands challenge deployment on resource-limited platforms. This paper investigates a critical question for compute-optimal diffusion model deployment: Under a post-training setting without fine-tuning, is it more effective to reduce the number of denoising steps or to use a cheaper per-step inference? Intuitively, reducing the number of denoising steps increases the variability of the distributions across steps, making the model more sensitive to compression. In contrast, keeping more denoising steps makes the differences smaller, preserving redundancy, and making post-training compression more feasible. To systematically examine this, we propose PostDiff, a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at both the input level and module level in a post-training manner. At the input level, we propose a mixed-resolution denoising scheme based on the insight that reducing generation resolution in early denoising steps can enhance low-frequency components and improve final generation fidelity. At the module level, we employ a hybrid module caching strategy to reuse computations across denoising steps. Extensive experiments and ablation studies demonstrate that (1) PostDiff can significantly improve the fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to boost efficiency while maintaining decent generation fidelity, reducing per-step inference cost is often more effective than reducing the number of denoising steps. Our code is available at https://github.com/GATECH-EIC/PostDiff.",
        "arxiv_id": "2508.06160",
        "ARXIVID": "2508.06160",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06101": {
        "authors": [
            "Yachun Mi",
            "Xingyang He",
            "Shixin Sun",
            "Yu Li",
            "Yanting Li",
            "Zhixuan Li",
            "Jian Jin",
            "Chen Hui",
            "Shaohui Liu"
        ],
        "title": "UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization",
        "abstract": "arXiv:2508.06101v1 Announce Type: new  Abstract: In the digital age, advanced image editing tools pose a serious threat to the integrity of visual content, making image forgery detection and localization a key research focus. Most existing Image Manipulation Localization (IML) methods rely on discriminative learning and require large, high-quality annotated datasets. However, current datasets lack sufficient scale and diversity, limiting model performance in real-world scenarios. To overcome this, recent studies have explored Constrained IML (CIML), which generates pixel-level annotations through algorithmic supervision. However, existing CIML approaches often depend on complex multi-stage pipelines, making the annotation process inefficient. In this work, we propose a novel generative framework based on diffusion models, named UGD-IML, which for the first time unifies both IML and CIML tasks within a single framework. By learning the underlying data distribution, generative diffusion models inherently reduce the reliance on large-scale labeled datasets, allowing our approach to perform effectively even under limited data conditions. In addition, by leveraging a class embedding mechanism and a parameter-sharing design, our model seamlessly switches between IML and CIML modes without extra components or training overhead. Furthermore, the end-to-end design enables our model to avoid cumbersome steps in the data annotation process. Extensive experimental results on multiple datasets demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and 4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the proposed method also excels in uncertainty estimation, visualization and robustness.",
        "arxiv_id": "2508.06101",
        "ARXIVID": "2508.06101",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05903": {
        "authors": [
            "Lang Nie",
            "Yuan Mei",
            "Kang Liao",
            "Yunqiu Xu",
            "Chunyu Lin",
            "Bin Xiao"
        ],
        "title": "Robust Image Stitching with Optimal Plane",
        "abstract": "arXiv:2508.05903v1 Announce Type: new  Abstract: We present \\textit{RopStitch}, an unsupervised deep image stitching framework with both robustness and naturalness. To ensure the robustness of \\textit{RopStitch}, we propose to incorporate the universal prior of content perception into the image stitching model by a dual-branch architecture. It separately captures coarse and fine features and integrates them to achieve highly generalizable performance across diverse unseen real-world scenes. Concretely, the dual-branch model consists of a pretrained branch to capture semantically invariant representations and a learnable branch to extract fine-grained discriminative features, which are then merged into a whole by a controllable factor at the correlation level. Besides, considering that content alignment and structural preservation are often contradictory to each other, we propose a concept of virtual optimal planes to relieve this conflict. To this end, we model this problem as a process of estimating homography decomposition coefficients, and design an iterative coefficient predictor and minimal semantic distortion constraint to identify the optimal plane. This scheme is finally incorporated into \\textit{RopStitch} by warping both views onto the optimal plane bidirectionally. Extensive experiments across various datasets demonstrate that \\textit{RopStitch} significantly outperforms existing methods, particularly in scene robustness and content naturalness. The code is available at {\\color{red}https://github.com/MmelodYy/RopStitch}.",
        "arxiv_id": "2508.05903",
        "ARXIVID": "2508.05903",
        "COMMENT": "Does not match any specific criterion but is related to image stitching and robustness, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05813": {
        "authors": [
            "Raphael Du Sablon",
            "David Hart"
        ],
        "title": "Optimization-Free Style Transfer for 3D Gaussian Splats",
        "abstract": "arXiv:2508.05813v1 Announce Type: new  Abstract: The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.",
        "arxiv_id": "2508.05813",
        "ARXIVID": "2508.05813",
        "COMMENT": "Does not match any specific criterion but is related to 3D Gaussian splats and style transfer, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.05857": {
        "authors": [
            "Qiaomu Miao",
            "Vivek Raju Golani",
            "Jingyi Xu",
            "Progga Paromita Dutta",
            "Minh Hoai",
            "Dimitris Samaras"
        ],
        "title": "Multi-view Gaze Target Estimation",
        "abstract": "arXiv:2508.05857v1 Announce Type: new  Abstract: This paper presents a method that utilizes multiple camera views for the gaze target estimation (GTE) task. The approach integrates information from different camera views to improve accuracy and expand applicability, addressing limitations in existing single-view methods that face challenges such as face occlusion, target ambiguity, and out-of-view targets. Our method processes a pair of camera views as input, incorporating a Head Information Aggregation (HIA) module for leveraging head information from both views for more accurate gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module for cross-view background information sharing. This approach significantly outperforms single-view baselines, especially when the second camera provides a clear view of the person's face. Additionally, our method can estimate the gaze target in the first view using the image of the person in the second view only, a capability not possessed by single-view GTE methods. Furthermore, the paper introduces a multi-view dataset for developing and evaluating multi-view GTE methods. Data and code are available at https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html",
        "arxiv_id": "2508.05857",
        "ARXIVID": "2508.05857",
        "COMMENT": "Does not match any specific criterion but is related to multi-view vision tasks, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.06122": {
        "authors": [
            "Ting-Shuo Yo",
            "Shih-Hao Su",
            "Chien-Ming Wu",
            "Wei-Ting Chen",
            "Jung-Lien Chu",
            "Chiao-Wei Chang",
            "Hung-Chi Kuo"
        ],
        "title": "Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events",
        "abstract": "arXiv:2508.06122v1 Announce Type: new  Abstract: This study applied representation learning algorithms to satellite images and evaluated the learned latent spaces with classifications of various weather events. The algorithms investigated include the classical linear transformation, i.e., principal component analysis (PCA), state-of-the-art deep learning method, i.e., convolutional autoencoder (CAE), and a residual network pre-trained with large image datasets (PT). The experiment results indicated that the latent space learned by CAE consistently showed higher threat scores for all classification tasks. The classifications with PCA yielded high hit rates but also high false-alarm rates. In addition, the PT performed exceptionally well at recognizing tropical cyclones but was inferior in other tasks. Further experiments suggested that representations learned from higher-resolution datasets are superior in all classification tasks for deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent space sizes had minor impact on the classification task's hit rate. Still, a latent space dimension smaller than 128 caused a significantly higher false alarm rate. Though the CAE can learn latent spaces effectively and efficiently, the interpretation of the learned representation lacks direct connections to physical attributions. Therefore, developing a physics-informed version of CAE can be a promising outlook for the current work.",
        "arxiv_id": "2508.06122",
        "ARXIVID": "2508.06122",
        "COMMENT": "This paper does not closely match any of the specified criteria. It focuses on representation learning for satellite images and weather event classification, which is outside the scope of the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.05769": {
        "authors": [
            "Seyed Hadi Seyed",
            "Ayberk Cansever",
            "David Hart"
        ],
        "title": "Improving Masked Style Transfer using Blended Partial Convolution",
        "abstract": "arXiv:2508.05769v1 Announce Type: new  Abstract: Artistic style transfer has long been possible with the advancements of convolution- and transformer-based neural networks. Most algorithms apply the artistic style transfer to the whole image, but individual users may only need to apply a style transfer to a specific region in the image. The standard practice is to simply mask the image after the stylization. This work shows that this approach tends to improperly capture the style features in the region of interest. We propose a partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest. Additionally, we present network-internal blending techniques that account for imperfections in the region selection. We show that this visually and quantitatively improves stylization using examples from the SA-1B dataset. Code is publicly available at https://github.com/davidmhart/StyleTransferMasked.",
        "arxiv_id": "2508.05769",
        "ARXIVID": "2508.05769",
        "COMMENT": "Does not match any specific criterion but is relevant to artistic style transfer and computer vision, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}