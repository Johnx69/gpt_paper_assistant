{
    "2510.25765": {
        "authors": [
            "Chuhao Chen",
            "Isabella Liu",
            "Xinyue Wei",
            "Hao Su",
            "Minghua Liu"
        ],
        "title": "FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion",
        "abstract": "arXiv:2510.25765v1 Announce Type: new  Abstract: Articulated 3D objects are central to many applications in robotics, AR/VR, and animation. Recent approaches to modeling such objects either rely on optimization-based reconstruction pipelines that require dense-view supervision or on feed-forward generative models that produce coarse geometric approximations and often overlook surface texture. In contrast, open-world 3D generation of static objects has achieved remarkable success, especially with the advent of native 3D diffusion models such as Trellis. However, extending these methods to articulated objects by training native 3D diffusion models poses significant challenges. In this work, we present FreeArt3D, a training-free framework for articulated 3D object generation. Instead of training a new model on limited articulated data, FreeArt3D repurposes a pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by treating articulation as an additional generative dimension. Given a few images captured in different articulation states, FreeArt3D jointly optimizes the object's geometry, texture, and articulation parameters without requiring task-specific training or access to large-scale articulated datasets. Our method generates high-fidelity geometry and textures, accurately predicts underlying kinematic structures, and generalizes well across diverse object categories. Despite following a per-instance optimization paradigm, FreeArt3D completes in minutes and significantly outperforms prior state-of-the-art approaches in both quality and versatility.",
        "arxiv_id": "2510.25765",
        "ARXIVID": "2510.25765",
        "COMMENT": "Matches criterion 5 as it focuses on articulated 3D object generation using 3D diffusion, combining vision and generative modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.25332": {
        "authors": [
            "Yuhang Hu",
            "Zhenyu Yang",
            "Shihan Wang",
            "Shengsheng Qian",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Changsheng Xu"
        ],
        "title": "StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA",
        "abstract": "arXiv:2510.25332v1 Announce Type: new  Abstract: The rapid growth of streaming video applications demands multimodal models with enhanced capabilities for temporal dynamics understanding and complex reasoning. However, current Video Question Answering (VideoQA) datasets suffer from two critical limitations: 1) Static annotation mechanisms fail to capture the evolving nature of answers in temporal video streams, and 2) The absence of explicit reasoning process annotations restricts model interpretability and logical deduction capabilities. To address these challenges, We introduce StreamingCoT, the first dataset explicitly designed for temporally evolving reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our framework first establishes a dynamic hierarchical annotation architecture that generates per-second dense descriptions and constructs temporally-dependent semantic segments through similarity fusion, paired with question-answer sets constrained by temporal evolution patterns. We further propose an explicit reasoning chain generation paradigm that extracts spatiotemporal objects via keyframe semantic alignment, derives object state transition-based reasoning paths using large language models, and ensures logical coherence through human-verified validation. This dataset establishes a foundation for advancing research in streaming video understanding, complex temporal reasoning, and multimodal inference. Our StreamingCoT and its construction toolkit can be accessed at https://github.com/Fleeting-hyh/StreamingCoT.",
        "arxiv_id": "2510.25332",
        "ARXIVID": "2510.25332",
        "COMMENT": "Matches criterion 6 as it introduces a dataset for video understanding with temporal dynamics and reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.25263": {
        "authors": [
            "Yang Miao",
            "Jan-Nico Zaech",
            "Xi Wang",
            "Fabien Despinoy",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ],
        "title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation",
        "abstract": "arXiv:2510.25263v1 Announce Type: new  Abstract: We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based framework for open-vocabulary object-part instance segmentation. Given an image, LangHOPS can jointly detect and segment hierarchical object and part instances from open-vocabulary candidate categories. Unlike prior approaches that rely on heuristic or learnable visual grouping, our approach grounds object-part hierarchies in language space. It integrates the MLLM into the object-part parsing pipeline to leverage its rich knowledge and reasoning capabilities, and link multi-granularity concepts within the hierarchies. We evaluate LangHOPS across multiple challenging scenarios, including in-domain and cross-dataset object-part instance segmentation, and zero-shot semantic segmentation. LangHOPS achieves state-of-the-art results, surpassing previous methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K (zero-shot). Ablation studies further validate the effectiveness of the language-grounded hierarchy and MLLM driven part query refinement strategy. The code will be released here.",
        "arxiv_id": "2510.25263",
        "ARXIVID": "2510.25263",
        "COMMENT": "Matches criterion 2 as it explores a Multimodal Large Language Model (MLLM) for open-vocabulary object-part instance segmentation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.25772": {
        "authors": [
            "Baolu Li",
            "Yiming Zhang",
            "Qinghe Wang",
            "Liqian Ma",
            "Xiaoyu Shi",
            "Xintao Wang",
            "Pengfei Wan",
            "Zhenfei Yin",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "Xu Jia"
        ],
        "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning",
        "abstract": "arXiv:2510.25772v1 Announce Type: new  Abstract: Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.",
        "arxiv_id": "2510.25772",
        "ARXIVID": "2510.25772",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a unified framework for VFX video generation using in-context learning, which integrates vision and language.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2510.24821": {
        "authors": [
            "Inclusion AI",
            ":",
            "Bowen Ma",
            "Cheng Zou",
            "Canxiang Yan",
            "Chunxiang Jin",
            "Chunjie Shen",
            "Dandan Zheng",
            "Fudong Wang",
            "Furong Xu",
            "GuangMing Yao",
            "Jun Zhou",
            "Jingdong Chen",
            "Jianing Li",
            "Jianxin Sun",
            "Jiajia Liu",
            "Jianjiang Zhu",
            "Jianping Jiang",
            "Jun Peng",
            "Kaixiang Ji",
            "Kaimeng Ren",
            "Libin Wang",
            "Lixiang Ru",
            "Longhua Tan",
            "Lan Wang",
            "Mochen Bai",
            "Ning Gao",
            "Qingpei Guo",
            "Qinglong Zhang",
            "Qiang Xu",
            "Rui Liu",
            "Ruijie Xiong",
            "Ruobing Zheng",
            "Sirui Gao",
            "Tianqi Li",
            "Tinghao Liu",
            "Weilong Chai",
            "Xinyu Xiao",
            "Xiaomei Wang",
            "Xiaolong Wang",
            "Xiao Lu",
            "Xiaoyu Li",
            "Xingning Dong",
            "Xuzheng Yu",
            "Yi Yuan",
            "Yuting Gao",
            "Yuting Xiao",
            "Yunxiao Sun",
            "Yipeng Chen",
            "Yifan Mao",
            "Yifei Wu",
            "Yongjie Lyu",
            "Ziping Ma",
            "Zhiqiang Fang",
            "Zhihao Qiu",
            "Ziyuan Huang",
            "Zizheng Yang",
            "Zhengyu He"
        ],
        "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
        "abstract": "arXiv:2510.24821v1 Announce Type: new  Abstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.",
        "arxiv_id": "2510.24821",
        "ARXIVID": "2510.24821",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it presents a multimodal architecture with advancements in vision, speech, and language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.25094": {
        "authors": [
            "Chanhyeong Yang",
            "Taehoon Song",
            "Jihwan Park",
            "Hyunwoo J. Kim"
        ],
        "title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection",
        "abstract": "arXiv:2510.25094v1 Announce Type: new  Abstract: Zero-shot Human-Object Interaction detection aims to localize humans and objects in an image and recognize their interaction, even when specific verb-object pairs are unseen during training. Recent works have shown promising results using prompt learning with pretrained vision-language models such as CLIP, which align natural language prompts with visual features in a shared embedding space. However, existing approaches still fail to handle the visual complexity of interaction, including (1) intra-class visual diversity, where instances of the same verb appear in diverse poses and contexts, and (2) inter-class visual entanglement, where distinct verbs yield visually similar patterns. To address these challenges, we propose VDRP, a framework for Visual Diversity and Region-aware Prompt learning. First, we introduce a visual diversity-aware prompt learning strategy that injects group-wise visual variance into the context embedding. We further apply Gaussian perturbation to encourage the prompts to capture diverse visual variations of a verb. Second, we retrieve region-specific concepts from the human, object, and union regions. These are used to augment the diversity-aware prompt embeddings, yielding region-aware prompts that enhance verb-level discrimination. Experiments on the HICO-DET benchmark demonstrate that our method achieves state-of-the-art performance under four zero-shot evaluation settings, effectively addressing both intra-class diversity and inter-class visual entanglement. Code is available at https://github.com/mlvlab/VDRP.",
        "arxiv_id": "2510.25094",
        "ARXIVID": "2510.25094",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it enhances zero-shot HOI detection using vision-language models like CLIP.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.24980": {
        "authors": [
            "Reza Saadati Fard",
            "Emmanuel Agu",
            "Palawat Busaranuvong",
            "Deepak Kumar",
            "Shefalika Gautam",
            "Bengisu Tulu",
            "Diane Strong",
            "Lorraine Loretz"
        ],
        "title": "FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning",
        "abstract": "arXiv:2510.24980v1 Announce Type: new  Abstract: Pressure ulcers (PUs) are a serious and prevalent healthcare concern. Accurate classification of PU severity (Stages I-IV) is essential for proper treatment but remains challenging due to subtle visual distinctions and subjective interpretation, leading to variability among clinicians. Prior AI-based approaches using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) achieved promising accuracy but offered limited interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal model), a fine-tuned multimodal large language model (MLLM) with an agentic self-reflection mechanism for pressure ulcer severity classification. Inspired by clinician-style diagnostic reassessment, FT-ARM iteratively refines its predictions by reasoning over visual features and encoded clinical knowledge from text, enhancing both accuracy and consistency. On the publicly available Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B, achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline evaluations, FT-ARM is designed and tested for live inference, reflecting real-time deployment conditions. Furthermore, it produces clinically grounded natural-language explanations, improving interpretability and trust. By integrating fine-tuning and reflective reasoning across multimodal inputs, FT-ARM advances the reliability, transparency, and clinical applicability of automated wound assessment systems, addressing the critical need for consistent and explainable PU staging to support improved patient care.",
        "arxiv_id": "2510.24980",
        "ARXIVID": "2510.24980",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it presents a multimodal LLM for medical image classification with reasoning, integrating vision and language.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.24734": {
        "authors": [
            "Qirui Hou",
            "Wenzhang Sun",
            "Chang Zeng",
            "Chunfeng Wang",
            "Hao Li",
            "Jianxun Cui"
        ],
        "title": "DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes",
        "abstract": "arXiv:2510.24734v1 Announce Type: new  Abstract: Real-time, high-fidelity reconstruction of dynamic driving scenes is challenged by complex dynamics and sparse views, with prior methods struggling to balance quality and efficiency. We propose DrivingScene, an online, feed-forward framework that reconstructs 4D dynamic scenes from only two consecutive surround-view images. Our key innovation is a lightweight residual flow network that predicts the non-rigid motion of dynamic objects per camera on top of a learned static scene prior, explicitly modeling dynamics via scene flow. We also introduce a coarse-to-fine training paradigm that circumvents the instabilities common to end-to-end approaches. Experiments on nuScenes dataset show our image-only method simultaneously generates high-quality depth, scene flow, and 3D Gaussian point clouds online, significantly outperforming state-of-the-art methods in both dynamic reconstruction and novel view synthesis.",
        "arxiv_id": "2510.24734",
        "ARXIVID": "2510.24734",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for dynamic scene reconstruction in driving environments, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.25760": {
        "authors": [
            "Xu Zheng",
            "Zihao Dongfang",
            "Lutao Jiang",
            "Boyuan Zheng",
            "Yulong Guo",
            "Zhenquan Zhang",
            "Giuliano Albanese",
            "Runyi Yang",
            "Mengjiao Ma",
            "Zixin Zhang",
            "Chenfei Liao",
            "Dingcheng Zhen",
            "Yuanhuiyi Lyu",
            "Yuqian Fu",
            "Bin Ren",
            "Linfeng Zhang",
            "Danda Pani Paudel",
            "Nicu Sebe",
            "Luc Van Gool",
            "Xuming Hu"
        ],
        "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks",
        "abstract": "arXiv:2510.25760v1 Announce Type: new  Abstract: Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
        "arxiv_id": "2510.25760",
        "ARXIVID": "2510.25760",
        "COMMENT": "Matches criterion 7 as it is a survey paper on multimodal spatial reasoning, covering embodied AI and vision-language tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2510.25739": {
        "authors": [
            "Zhi-Kai Chen",
            "Jun-Peng Jiang",
            "Han-Jia Ye",
            "De-Chuan Zhan"
        ],
        "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation",
        "abstract": "arXiv:2510.25739v1 Announce Type: new  Abstract: Autoregressive (AR) image generation models are capable of producing high-fidelity images but often suffer from slow inference due to their inherently sequential, token-by-token decoding process. Speculative decoding, which employs a lightweight draft model to approximate the output of a larger AR model, has shown promise in accelerating text generation without compromising quality. However, its application to image generation remains largely underexplored. The challenges stem from a significantly larger sampling space, which complicates the alignment between the draft and target model outputs, coupled with the inadequate use of the two-dimensional spatial structure inherent in images, thereby limiting the modeling of local dependencies. To overcome these challenges, we introduce Hawk, a new approach that harnesses the spatial structure of images to guide the speculative model toward more accurate and efficient predictions. Experimental results on multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR models, while preserving both image fidelity and diversity.",
        "arxiv_id": "2510.25739",
        "ARXIVID": "2510.25739",
        "COMMENT": "Matches criterion 5 as it focuses on integrating spatial context in text-to-image generation, which involves vision and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.24788": {
        "authors": [
            "Xinjian Zhao",
            "Wei Pang",
            "Zhongkai Xue",
            "Xiangru Jian",
            "Lei Zhang",
            "Yaoyao Xu",
            "Xiaozhuang Song",
            "Shu Wu",
            "Tianshu Yu"
        ],
        "title": "The Underappreciated Power of Vision Models for Graph Structural Understanding",
        "abstract": "arXiv:2510.24788v1 Announce Type: new  Abstract: Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition.",
        "arxiv_id": "2510.24788",
        "ARXIVID": "2510.24788",
        "COMMENT": "Matches criterion 1 as it explores spatial intelligence and reasoning in vision models for graph structural understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.25529": {
        "authors": [
            "Likun Wang",
            "Xiangteng Zhang",
            "Yinuo Wang",
            "Guojian Zhan",
            "Wenxuan Wang",
            "Haoyu Gao",
            "Jingliang Duan",
            "Shengbo Eben Li"
        ],
        "title": "Off-policy Reinforcement Learning with Model-based Exploration Augmentation",
        "abstract": "arXiv:2510.25529v1 Announce Type: new  Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines how effectively an agent discovers and exploits the underlying structure of its environment to achieve optimal performance. Existing exploration methods generally fall into two categories: active exploration and passive exploration. The former introduces stochasticity into the policy but struggles in high-dimensional environments, while the latter adaptively prioritizes transitions in the replay buffer to enhance exploration, yet remains constrained by limited sample diversity. To address the limitation in passive exploration, we propose Modelic Generative Exploration (MoGE), which augments exploration through the generation of under-explored critical states and synthesis of dynamics-consistent experiences through transition models. MoGE is composed of two components: (1) a diffusion-based generator that synthesizes critical states under the guidance of a utility function evaluating each state's potential influence on policy exploration, and (2) a one-step imagination world model for constructing critical transitions based on the critical states for agent learning. Our method adopts a modular formulation that aligns with the principles of off-policy learning, allowing seamless integration with existing algorithms to improve exploration without altering their core structures. Empirical results on OpenAI Gym and DeepMind Control Suite reveal that MoGE effectively bridges exploration and policy learning, leading to remarkable gains in both sample efficiency and performance across complex control tasks.",
        "arxiv_id": "2510.25529",
        "ARXIVID": "2510.25529",
        "COMMENT": "This paper closely matches criterion 3 as it introduces a novel method for exploration in reinforcement learning, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.25237": {
        "authors": [
            "Yinqi Cai",
            "Jichang Li",
            "Zhaolun Li",
            "Weikai Chen",
            "Rushi Lan",
            "Xi Xie",
            "Xiaonan Luo",
            "Guanbin Li"
        ],
        "title": "DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis",
        "abstract": "arXiv:2510.25237v1 Announce Type: new  Abstract: Recent advances in deep generative models have made it easier to manipulate face videos, raising significant concerns about their potential misuse for fraud and misinformation. Existing detectors often perform well in in-domain scenarios but fail to generalize across diverse manipulation techniques due to their reliance on forgery-specific artifacts. In this work, we introduce DeepShield, a novel deepfake detection framework that balances local sensitivity and global generalization to improve robustness across unseen forgeries. DeepShield enhances the CLIP-ViT encoder through two key components: Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG applies spatiotemporal artifact modeling and patch-wise supervision to capture fine-grained inconsistencies often overlooked by global models. GFD introduces domain feature augmentation, leveraging domain-bridging and boundary-expanding feature generation to synthesize diverse forgeries, mitigating overfitting and enhancing cross-domain adaptability. Through the integration of novel local and global analysis for deepfake detection, DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, achieving superior robustness against unseen deepfake attacks.",
        "arxiv_id": "2510.25237",
        "ARXIVID": "2510.25237",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks, specifically deepfake video detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.24816": {
        "authors": [
            "Cui Yakun",
            "Fushuo Huo",
            "Weijie Shi",
            "Juntao Dai",
            "Hang Du",
            "Zhenghao Zhu",
            "Sirui Han",
            "Yike Guo"
        ],
        "title": "Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection",
        "abstract": "arXiv:2510.24816v1 Announce Type: new  Abstract: The advent of multi-modal large language models (MLLMs) has greatly advanced research into applications for Video fake news detection (VFND) tasks. Traditional video-based FND benchmarks typically focus on the accuracy of the final decision, often failing to provide fine-grained assessments for the entire detection process, making the detection process a black box. Therefore, we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based on the empirical analysis, which provides foundation for tasks definition. The benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs' perception, understanding, and reasoning capacities during detection, featuring 9730 human-annotated video-related questions based on a carefully constructed taxonomy ability of VFND. To validate the impact of combining multiple features on the final results, we design a novel framework named MVFND-CoT, which incorporates both creator-added content and original shooting footage reasoning. Building upon the benchmark, we conduct an in-depth analysis of the deeper factors influencing accuracy, including video processing strategies and the alignment between video features and model capabilities. We believe this benchmark will lay a solid foundation for future evaluations and advancements of MLLMs in the domain of video fake news detection.",
        "arxiv_id": "2510.24816",
        "ARXIVID": "2510.24816",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark for video understanding tasks, specifically video fake news detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.25257": {
        "authors": [
            "Zijun Liao",
            "Yian Zhao",
            "Xin Shan",
            "Yu Yan",
            "Chang Liu",
            "Lei Lu",
            "Xiangyang Ji",
            "Jie Chen"
        ],
        "title": "RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models",
        "abstract": "arXiv:2510.25257v1 Announce Type: new  Abstract: Real-time object detection has achieved substantial progress through meticulously designed architectures and optimization strategies. However, the pursuit of high-speed inference via lightweight network designs often leads to degraded feature representation, which hinders further performance improvements and practical on-device deployment. In this paper, we propose a cost-effective and highly adaptable distillation framework that harnesses the rapidly evolving capabilities of Vision Foundation Models (VFMs) to enhance lightweight object detectors. Given the significant architectural and learning objective disparities between VFMs and resource-constrained detectors, achieving stable and task-aligned semantic transfer is challenging. To address this, on one hand, we introduce a Deep Semantic Injector (DSI) module that facilitates the integration of high-level representations from VFMs into the deep layers of the detector. On the other hand, we devise a Gradient-guided Adaptive Modulation (GAM) strategy, which dynamically adjusts the intensity of semantic transfer based on gradient norm ratios. Without increasing deployment and inference overhead, our approach painlessly delivers striking and consistent performance gains across diverse DETR-based models, underscoring its practical utility for real-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art results on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding speeds of 273/169/124/78 FPS.",
        "arxiv_id": "2510.25257",
        "ARXIVID": "2510.25257",
        "COMMENT": "Matches criterion 4 as it focuses on Vision Foundation Models and their applications in real-time object detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.24904": {
        "authors": [
            "Qiucheng Wu",
            "Handong Zhao",
            "Zhixin Shu",
            "Jing Shi",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "title": "VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos",
        "abstract": "arXiv:2510.24904v1 Announce Type: new  Abstract: Although recent text-to-video generative models are getting more capable of following external camera controls, imposed by either text descriptions or camera trajectories, they still struggle to generalize to unconventional camera motions, which is crucial in creating truly original and artistic videos. The challenge lies in the difficulty of finding sufficient training videos with the intended uncommon camera motions. To address this challenge, we propose VividCam, a training paradigm that enables diffusion models to learn complex camera motions from synthetic videos, releasing the reliance on collecting realistic training videos. VividCam incorporates multiple disentanglement strategies that isolates camera motion learning from synthetic appearance artifacts, ensuring more robust motion representation and mitigating domain shift. We demonstrate that our design synthesizes a wide range of precisely controlled and complex camera motions using surprisingly simple synthetic data. Notably, this synthetic data often consists of basic geometries within a low-poly 3D scene and can be efficiently rendered by engines like Unity. Our video results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .",
        "arxiv_id": "2510.24904",
        "ARXIVID": "2510.24904",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on learning unconventional camera motions for video generation, which is a novel methodology in video-based tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.25463": {
        "authors": [
            "Hongjie Zhang",
            "Gideon Billings",
            "Stefan B. Williams"
        ],
        "title": "SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments",
        "abstract": "arXiv:2510.25463v1 Announce Type: new  Abstract: Underwater infrastructure requires frequent inspection and maintenance due to harsh marine conditions. Current reliance on human divers or remotely operated vehicles is limited by perceptual and operational challenges, especially around complex structures or in turbid water. Enhancing the spatial awareness of underwater vehicles is key to reducing piloting risks and enabling greater autonomy. To address these challenges, we present SPADE: SParsity Adaptive Depth Estimator, a monocular depth estimation pipeline that combines pre-trained relative depth estimator with sparse depth priors to produce dense, metric scale depth maps. Our two-stage approach first scales the relative depth map with the sparse depth points, then refines the final metric prediction with our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves improved accuracy and generalisation over state-of-the-art baselines and runs efficiently at over 15 FPS on embedded hardware, promising to support practical underwater inspection and intervention. This work has been submitted to IEEE Journal of Oceanic Engineering Special Issue of AUV 2026.",
        "arxiv_id": "2510.25463",
        "ARXIVID": "2510.25463",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for embodied AI in underwater environments, enhancing spatial awareness.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.25129": {
        "authors": [
            "Xiyu Zhang",
            "Chong Bao",
            "Yipeng Chen",
            "Hongjia Zhai",
            "Yitong Dong",
            "Hujun Bao",
            "Zhaopeng Cui",
            "Guofeng Zhang"
        ],
        "title": "AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians",
        "abstract": "arXiv:2510.25129v1 Announce Type: new  Abstract: 3D reconstruction of indoor and urban environments is a prominent research topic with various downstream applications. However, existing geometric priors for addressing low-texture regions in indoor and urban settings often lack global consistency. Moreover, Gaussian Splatting and implicit SDF fields often suffer from discontinuities or exhibit computational inefficiencies, resulting in a loss of detail. To address these issues, we propose an Atlanta-world guided implicit-structured Gaussian Splatting that achieves smooth indoor and urban scene reconstruction while preserving high-frequency details and rendering efficiency. By leveraging the Atlanta-world model, we ensure the accurate surface reconstruction for low-texture regions, while the proposed novel implicit-structured GS representations provide smoothness without sacrificing efficiency and high-frequency details. Specifically, we propose a semantic GS representation to predict the probability of all semantic regions and deploy a structure plane regularization with learnable plane indicators for global accurate surface reconstruction. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in both indoor and urban scenes, delivering superior surface reconstruction quality.",
        "arxiv_id": "2510.25129",
        "ARXIVID": "2510.25129",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it proposes a novel method for 3D reconstruction with structured Gaussian representations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.25146": {
        "authors": [
            "Xiaoyu Zhou",
            "Jingqi Wang",
            "Yuang Jia",
            "Yongtao Wang",
            "Deqing Sun",
            "Ming-Hsuan Yang"
        ],
        "title": "EA3D: Online Open-World 3D Object Extraction from Streaming Videos",
        "abstract": "arXiv:2510.25146v1 Announce Type: new  Abstract: Current 3D scene understanding methods are limited by offline-collected multi-view data or pre-constructed 3D geometry. In this paper, we present ExtractAnything3D (EA3D), a unified online framework for open-world 3D object extraction that enables simultaneous geometric reconstruction and holistic scene understanding. Given a streaming video, EA3D dynamically interprets each frame using vision-language and 2D vision foundation encoders to extract object-level knowledge. This knowledge is integrated and embedded into a Gaussian feature map via a feed-forward online update strategy. We then iteratively estimate visual odometry from historical frames and incrementally update online Gaussian features with new observations. A recurrent joint optimization module directs the model's attention to regions of interest, simultaneously enhancing both geometric reconstruction and semantic understanding. Extensive experiments across diverse benchmarks and tasks, including photo-realistic rendering, semantic and instance segmentation, 3D bounding box and semantic occupancy estimation, and 3D mesh generation, demonstrate the effectiveness of EA3D. Our method establishes a unified and efficient framework for joint online 3D reconstruction and holistic scene understanding, enabling a broad range of downstream tasks.",
        "arxiv_id": "2510.25146",
        "ARXIVID": "2510.25146",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a unified framework for 3D object extraction and scene understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.25387": {
        "authors": [
            "Bill Psomas",
            "George Retsinas",
            "Nikos Efthymiadis",
            "Panagiotis Filntisis",
            "Yannis Avrithis",
            "Petros Maragos",
            "Ondrej Chum",
            "Giorgos Tolias"
        ],
        "title": "Instance-Level Composed Image Retrieval",
        "abstract": "arXiv:2510.25387v1 Announce Type: new  Abstract: The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.   To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.",
        "arxiv_id": "2510.25387",
        "ARXIVID": "2510.25387",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a new dataset and method for composed image retrieval combining visual and textual queries.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.24813": {
        "authors": [
            "Binbin Li",
            "Guimiao Yang",
            "Zisen Qi",
            "Haiping Wang",
            "Yu Ding"
        ],
        "title": "DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts",
        "abstract": "arXiv:2510.24813v1 Announce Type: new  Abstract: Recent lightweight retrieval-augmented image caption models often utilize retrieved data solely as text prompts, thereby creating a semantic gap by leaving the original visual features unenhanced, particularly for object details or complex scenes. To address this limitation, we propose $DualCap$, a novel approach that enriches the visual representation by generating a visual prompt from retrieved similar images. Our model employs a dual retrieval mechanism, using standard image-to-text retrieval for text prompts and a novel image-to-image retrieval to source visually analogous scenes. Specifically, salient keywords and phrases are derived from the captions of visually similar scenes to capture key objects and similar details. These textual features are then encoded and integrated with the original image features through a lightweight, trainable feature fusion network. Extensive experiments demonstrate that our method achieves competitive performance while requiring fewer trainable parameters compared to previous visual-prompting captioning approaches.",
        "arxiv_id": "2510.24813",
        "ARXIVID": "2510.24813",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a dual retrieval mechanism combining image and text features for image captioning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.25668": {
        "authors": [
            "Tianyu Yang",
            "Terry Ruas",
            "Yijun Tian",
            "Jan Philip Wahle",
            "Daniel Kurzawe",
            "Bela Gipp"
        ],
        "title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents",
        "abstract": "arXiv:2510.25668v1 Announce Type: new  Abstract: Vision-language models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages. Existing approaches typically rely on fixed reasoning templates or rigid pipelines, which force VLMs into a passive role and hinder both efficiency and generalization. We present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents capable of actively navigating long, visually rich documents. ALDEN introduces a novel fetch action that directly accesses the page by index, complementing the classic search action and better exploiting document structure. For dense process supervision and efficient training, we propose a rule-based cross-level reward that provides both turn- and token-level signals. To address the empirically observed training instability caused by numerous visual tokens from long documents, we further propose a visual-semantic anchoring mechanism that applies a dual-path KL-divergence constraint to stabilize visual and textual representations separately during training. Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks. Overall, ALDEN marks a step beyond passive document reading toward agents that autonomously navigate and reason across long, visually rich documents, offering a robust path to more accurate and efficient long-document understanding.",
        "arxiv_id": "2510.25668",
        "ARXIVID": "2510.25668",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a reinforcement learning framework for navigating and reasoning in long, visually rich documents.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.25590": {
        "authors": [
            "Pengtao Chen",
            "Xianfang Zeng",
            "Maosen Zhao",
            "Mingzhu Shen",
            "Peng Ye",
            "Bangyin Xiang",
            "Zhibo Wang",
            "Wei Cheng",
            "Gang Yu",
            "Tao Chen"
        ],
        "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
        "abstract": "arXiv:2510.25590v1 Announce Type: new  Abstract: Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.",
        "arxiv_id": "2510.25590",
        "ARXIVID": "2510.25590",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it proposes a region-aware generation framework for image editing.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.25067": {
        "authors": [
            "Yusen Peng",
            "Sachin Kumar"
        ],
        "title": "DRIP: Dynamic patch Reduction via Interpretable Pooling",
        "abstract": "arXiv:2510.25067v1 Announce Type: new  Abstract: Recently, the advances in vision-language models, including contrastive pretraining and instruction tuning, have greatly pushed the frontier of multimodal AI. However, owing to the large-scale and hence expensive pretraining, the efficiency concern has discouraged researchers from attempting to pretrain a vision language model from scratch. In this work, we propose Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the input images and dynamically merges tokens in the deeper layers of a visual encoder. Our results on both ImageNet training from scratch and CLIP contrastive pretraining demonstrate a significant GFLOP reduction while maintaining comparable classification/zero-shot performance. To further validate our proposed method, we conduct continual pretraining on a large biology dataset, extending its impact into scientific domains.",
        "arxiv_id": "2510.25067",
        "ARXIVID": "2510.25067",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a method to improve efficiency in vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.25238": {
        "authors": [
            "Qianqian Qiao",
            "DanDan Zheng",
            "Yihang Bo",
            "Bao Peng",
            "Heng Huang",
            "Longteng Jiang",
            "Huaye Wang",
            "Jingdong Chen",
            "Jun Zhou",
            "Xin Jin"
        ],
        "title": "VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations",
        "abstract": "arXiv:2510.25238v1 Announce Type: new  Abstract: Video aesthetic assessment, a vital area in multimedia computing, integrates computer vision with human cognition. Its progress is limited by the lack of standardized datasets and robust models, as the temporal dynamics of video and multimodal fusion challenges hinder direct application of image-based methods. This study introduces VADB, the largest video aesthetic database with 10,490 diverse videos annotated by 37 professionals across multiple aesthetic dimensions, including overall and attribute-specific aesthetic scores, rich language comments and objective tags. We propose VADB-Net, a dual-modal pre-training framework with a two-stage training strategy, which outperforms existing video quality assessment models in scoring tasks and supports downstream video aesthetic assessment tasks. The dataset and source code are available at https://github.com/BestiVictory/VADB.",
        "arxiv_id": "2510.25238",
        "ARXIVID": "2510.25238",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a large-scale video aesthetic database and a novel model for video aesthetic assessment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.24820": {
        "authors": [
            "Ruiyang Zhang",
            "Jiahao Luo",
            "Xiaoru Feng",
            "Qiufan Pang",
            "Yaodong Yang",
            "Juntao Dai"
        ],
        "title": "SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing",
        "abstract": "arXiv:2510.24820v1 Announce Type: new  Abstract: With the rapid advancement of text-to-image (T2I) models, ensuring their safety has become increasingly critical. Existing safety approaches can be categorized into training-time and inference-time methods. While inference-time methods are widely adopted due to their cost-effectiveness, they often suffer from limitations such as over-refusal and imbalance between safety and utility. To address these challenges, we propose a multi-round safety editing framework that functions as a model-agnostic, plug-and-play module, enabling efficient safety alignment for any text-to-image model. Central to this framework is MR-SafeEdit, a multi-round image-text interleaved dataset specifically constructed for safety editing in text-to-image generation. We introduce a post-hoc safety editing paradigm that mirrors the human cognitive process of identifying and refining unsafe content. To instantiate this paradigm, we develop SafeEditor, a unified MLLM capable of multi-round safety editing on generated images. Experimental results show that SafeEditor surpasses prior safety approaches by reducing over-refusal while achieving a more favorable safety-utility balance.",
        "arxiv_id": "2510.24820",
        "ARXIVID": "2510.24820",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on safety editing in text-to-image generation using a multimodal large language model.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2510.25210": {
        "authors": [
            "Junsheng Zhou",
            "Xingyu Shi",
            "Haichuan Song",
            "Yi Fang",
            "Yu-Shen Liu",
            "Zhizhong Han"
        ],
        "title": "U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching",
        "abstract": "arXiv:2510.25210v1 Announce Type: new  Abstract: Point clouds captured by scanning sensors are often perturbed by noise, which have a highly negative impact on downstream tasks (e.g. surface reconstruction and shape understanding). Previous works mostly focus on training neural networks with noisy-clean point cloud pairs for learning denoising priors, which requires extensively manual efforts. In this work, we introduce U-CAN, an Unsupervised framework for point cloud denoising with Consistency-Aware Noise2Noise matching. Specifically, we leverage a neural network to infer a multi-step denoising path for each point of a shape or scene with a noise to noise matching scheme. We achieve this by a novel loss which enables statistical reasoning on multiple noisy point cloud observations. We further introduce a novel constraint on the denoised geometry consistency for learning consistency-aware denoising patterns. We justify that the proposed constraint is a general term which is not limited to 3D domain and can also contribute to the area of 2D image denoising. Our evaluations under the widely used benchmarks in point cloud denoising, upsampling and image denoising show significant improvement over the state-of-the-art unsupervised methods, where U-CAN also produces comparable results with the supervised methods.",
        "arxiv_id": "2510.25210",
        "ARXIVID": "2510.25210",
        "COMMENT": "This paper does not match any specific criteria but is tangentially related to computer vision through its focus on point cloud denoising, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.25206": {
        "authors": [
            "Tianqianjin Lin",
            "Xi Zhao",
            "Xingyao Zhang",
            "Rujiao Long",
            "Yi Xu",
            "Zhuoren Jiang",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "title": "RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models",
        "abstract": "arXiv:2510.25206v1 Announce Type: new  Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large language models (LLMs), but critically depends on a key prerequisite: the LLM can already generate high-utility reasoning paths with non-negligible probability. For tasks beyond the LLM's current competence, such reasoning path can be hard to sample, and learning risks reinforcing familiar but suboptimal reasoning. We are motivated by the insight from cognitive science that Why is this the answer is often an easier question than What is the answer, as it avoids the heavy cognitive load of open-ended exploration, opting instead for explanatory reconstruction-systematically retracing the reasoning that links a question to its answer. We show that LLMs can similarly leverage answers to derive high-quality reasoning paths. We formalize this phenomenon and prove that conditioning on answer provably increases the expected utility of sampled reasoning paths, thereby transforming intractable problems into learnable ones. Building on this insight, we introduce RAVR (Reference-Answer-guided Variational Reasoning), an end-to-end framework that uses answer-conditioned reasoning as a variational surrogate for question-only reasoning. Experiments in both general and math domains demonstrate consistent improvements over strong baselines. We further analyze the reasoning behavior and find that RAVR reduces hesitation, strengthens conclusion consolidation, and promotes problem-specific strategies in reasoning.",
        "arxiv_id": "2510.25206",
        "ARXIVID": "2510.25206",
        "COMMENT": "Does not match any specific criteria but explores reasoning in large language models, which is tangentially related to your friend's interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.25163": {
        "authors": [
            "Wenhao Zheng",
            "Chenwei Sun",
            "Wenbo Zhang",
            "Jiancheng Lv",
            "Xianggen Liu"
        ],
        "title": "Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD Generation",
        "abstract": "arXiv:2510.25163v1 Announce Type: new  Abstract: Deep generative models, such as diffusion models, have shown promising progress in image generation and audio generation via simplified continuity assumptions. However, the development of generative modeling techniques for generating multi-modal data, such as parametric CAD sequences, still lags behind due to the challenges in addressing long-range constraints and parameter sensitivity. In this work, we propose a novel framework for quantitatively constrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN). For the first time, TGBFN handles the multi-modality of CAD sequences (i.e., discrete commands and continuous parameters) in a unified continuous and differentiable parameter space rather than in the discrete data space. In addition, TGBFN penetrates the parameter update kernel and introduces a guided Bayesian flow to control the CAD properties. To evaluate TGBFN, we construct a new dataset for quantitatively constrained CAD generation. Extensive comparisons across single-condition and multi-condition constrained generation tasks demonstrate that TGBFN achieves state-of-the-art performance in generating high-fidelity, condition-aware CAD sequences. The code is available at https://github.com/scu-zwh/TGBFN.",
        "arxiv_id": "2510.25163",
        "ARXIVID": "2510.25163",
        "COMMENT": "Does not closely match any specific criterion but is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.25758": {
        "authors": [
            "He Hu",
            "Yucheng Zhou",
            "Chiyuan Ma",
            "Qianning Wang",
            "Zheng Zhang",
            "Fei Ma",
            "Laizhong Cui",
            "Qi Tian"
        ],
        "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling",
        "abstract": "arXiv:2510.25758v1 Announce Type: new  Abstract: Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.",
        "arxiv_id": "2510.25758",
        "ARXIVID": "2510.25758",
        "COMMENT": "Does not match any specific criterion but is an interesting application of large language models in psychological counseling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.25445": {
        "authors": [
            "Mohamad Abou Ali",
            "Fadi Dornaika"
        ],
        "title": "Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions",
        "abstract": "arXiv:2510.25445v1 Announce Type: new  Abstract: Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models -- a practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the Symbolic/Classical (relying on algorithmic planning and persistent state) and the Neural/Generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018--2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.",
        "arxiv_id": "2510.25445",
        "ARXIVID": "2510.25445",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on agentic AI, including applications in robotics.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2510.24795": {
        "authors": [
            "Zhaoshu Yu",
            "Bo Wang",
            "Pengpeng Zeng",
            "Haonan Zhang",
            "Ji Zhang",
            "Lianli Gao",
            "Jingkuan Song",
            "Nicu Sebe",
            "Heng Tao Shen"
        ],
        "title": "A Survey on Efficient Vision-Language-Action Models",
        "abstract": "arXiv:2510.24795v1 Announce Type: new  Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
        "arxiv_id": "2510.24795",
        "ARXIVID": "2510.24795",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on Vision-Language-Action models.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2510.24832": {
        "authors": [
            "Hong Wang",
            "Zhezheng Hao",
            "Jian Luo",
            "Chenxing Wei",
            "Yao Shu",
            "Lei Liu",
            "Qiang Lin",
            "Hande Dong",
            "Jiawei Chen"
        ],
        "title": "Scheduling Your LLM Reinforcement Learning with Reasoning Trees",
        "abstract": "arXiv:2510.24832v1 Announce Type: new  Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large Language Models (LLMs) can be conceptualized as progressively editing a query's `Reasoning Tree'. This process involves exploring nodes (tokens) and dynamically modifying the model's policy at each node. When combined with data scheduling, this process yields further gains in data efficiency and accuracy. However, existing RLVR data scheduling methods typically rely on path-based metrics to rank queries, overlooking the reasoning tree structures of these queries. In this paper, we introduce a novel metric, namely Reasoning Score (r-score), which measures the query's learning difficulty based on the structure of its reasoning tree. Based on the r-score, we propose the Reasoning Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a curriculum progressing from structurally simple (high r-score) to complex (low r-score) queries. Experiments on six math-reasoning benchmarks show that Re-Schedule significantly improves average accuracy, achieving gains of up to 3.2%. These strong results validate our approach and demonstrate that a structural understanding of the reasoning tree provides a more powerful and principled foundation for RLVR data scheduling.",
        "arxiv_id": "2510.24832",
        "ARXIVID": "2510.24832",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reinforcement learning and reasoning in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.25528": {
        "authors": [
            "Yuyuan Zeng",
            "Yufei Huang",
            "Can Xu",
            "Qingfeng Sun",
            "Jianfeng Yan",
            "Guanghui Xu",
            "Tao Yang",
            "Fengzong Lian"
        ],
        "title": "Zero Reinforcement Learning Towards General Domains",
        "abstract": "arXiv:2510.25528v1 Announce Type: new  Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach for enhancing the reasoning capabilities of large language models (LLMs) by directly applying reinforcement learning with verifiable rewards on pretrained models, without the need for a supervised fine-tuning phase. However, current research on zero-RL primarily focuses on domains with easily verifiable reward signals, such as mathematics, programming, and other reasoning tasks. The challenge of eliciting reasoning abilities in more diverse scenarios, where verification is not straightforward, remains underexplored. To address this gap, we propose a novel zero-RL paradigm designed to improve a model's reasoning ability across both verifiable and non-verifiable domains. By combining verifiable rewards with a generative reward model, we conduct multi-task zero-RL training across both domains, facilitating the transfer of reasoning capabilities between them. Furthermore, to mitigate reward hacking in the generative reward model, we design a smooth length penalty that encourages the generation of more comprehensive thinking tokens in general domains. Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our approach achieves superior reasoning performance, not only on tasks requiring extensive reasoning but also on more general tasks.",
        "arxiv_id": "2510.25528",
        "ARXIVID": "2510.25528",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reinforcement learning and reasoning in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.25345": {
        "authors": [
            "Zhigang Tu",
            "Zhengbo Zhang",
            "Jia Gong",
            "Junsong Yuan",
            "Bo Du"
        ],
        "title": "Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples",
        "abstract": "arXiv:2510.25345v1 Announce Type: new  Abstract: Skeleton-based human action recognition aims to classify human skeletal sequences, which are spatiotemporal representations of actions, into predefined categories. To reduce the reliance on costly annotations of skeletal sequences while maintaining competitive recognition accuracy, the task of 3D Action Recognition with Limited Training Samples, also known as semi-supervised 3D Action Recognition, has been proposed. In addition, active learning, which aims to proactively select the most informative unlabeled samples for annotation, has been explored in semi-supervised 3D Action Recognition for training sample selection. Specifically, researchers adopt an encoder-decoder framework to embed skeleton sequences into a latent space, where clustering information, combined with a margin-based selection strategy using a multi-head mechanism, is utilized to identify the most informative sequences in the unlabeled set for annotation. However, the most representative skeleton sequences may not necessarily be the most informative for the action recognizer, as the model may have already acquired similar knowledge from previously seen skeleton samples. To solve it, we reformulate Semi-supervised 3D action recognition via active learning from a novel perspective by casting it as a Markov Decision Process (MDP). Built upon the MDP framework and its training paradigm, we train an informative sample selection model to intelligently guide the selection of skeleton sequences for annotation. To enhance the representational capacity of the factors in the state-action pairs within our method, we project them from Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning strategy to accelerate the deployment of our method in real-world scenarios. Extensive experiments on three 3D action recognition benchmarks demonstrate the effectiveness of our method.",
        "arxiv_id": "2510.25345",
        "ARXIVID": "2510.25345",
        "COMMENT": "Does not match any specific criteria but is related to skeleton-based action recognition, which is tangentially related to video understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.25279": {
        "authors": [
            "Yuyang Huang",
            "Yabo Chen",
            "Junyu Zhou",
            "Wenrui Dai",
            "Xiaopeng Zhang",
            "Junni Zou",
            "Hongkai Xiong",
            "Qi Tian"
        ],
        "title": "Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation",
        "abstract": "arXiv:2510.25279v1 Announce Type: new  Abstract: Source-free domain adaptation (SFDA) is a challenging task that tackles domain shifts using only a pre-trained source model and unlabeled target data. Existing SFDA methods are restricted by the fundamental limitation of source-target domain discrepancy. Non-generation SFDA methods suffer from unreliable pseudo-labels in challenging scenarios with large domain discrepancies, while generation-based SFDA methods are evidently degraded due to enlarged domain discrepancies in creating pseudo-source data. To address this limitation, we propose a novel generation-based framework named Diffusion-Driven Progressive Target Manipulation (DPTM) that leverages unlabeled target data as references to reliably generate and progressively refine a pseudo-target domain for SFDA. Specifically, we divide the target samples into a trust set and a non-trust set based on the reliability of pseudo-labels to sufficiently and reliably exploit their information. For samples from the non-trust set, we develop a manipulation strategy to semantically transform them into the newly assigned categories, while simultaneously maintaining them in the target distribution via a latent diffusion model. Furthermore, we design a progressive refinement mechanism that progressively reduces the domain discrepancy between the pseudo-target domain and the real target domain via iterative refinement. Experimental results demonstrate that DPTM outperforms existing methods by a large margin and achieves state-of-the-art performance on four prevailing SFDA benchmark datasets with different scales. Remarkably, DPTM can significantly enhance the performance by up to 18.6% in scenarios with large source-target gaps.",
        "arxiv_id": "2510.25279",
        "ARXIVID": "2510.25279",
        "COMMENT": "Does not match any specific criteria but is related to domain adaptation and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.25221": {
        "authors": [
            "Shiyu Qin",
            "Zhihao Cai",
            "Kaixuan Wang",
            "Lin Qi",
            "Junyu Dong"
        ],
        "title": "MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust Photometric Stereo",
        "abstract": "arXiv:2510.25221v1 Announce Type: new  Abstract: Photometric stereo is a technique aimed at determining surface normals through the utilization of shading cues derived from images taken under different lighting conditions. However, existing learning-based approaches often fail to accurately capture features at multiple stages and do not adequately promote interaction between these features. Consequently, these models tend to extract redundant features, especially in areas with intricate details such as wrinkles and edges. To tackle these issues, we propose MSF-Net, a novel framework for extracting information at multiple stages, paired with selective update strategy, aiming to extract high-quality feature information, which is critical for accurate normal construction. Additionally, we have developed a feature fusion module to improve the interplay among different features. Experimental results on the DiLiGenT benchmark show that our proposed MSF-Net significantly surpasses previous state-of-the-art methods in the accuracy of surface normal estimation.",
        "arxiv_id": "2510.25221",
        "ARXIVID": "2510.25221",
        "COMMENT": "Does not closely match any specific criterion but is relevant to photometric stereo and feature extraction in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.25134": {
        "authors": [
            "Qingdong Cai",
            "Charith Abhayaratne"
        ],
        "title": "Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks",
        "abstract": "arXiv:2510.25134v1 Announce Type: new  Abstract: Class Activation Mapping (CAM) methods are widely applied in weakly supervised learning tasks due to their ability to highlight object regions. However, conventional CAM methods highlight only the most discriminative regions of the target. These highlighted regions often fail to cover the entire object and are frequently misaligned with object boundaries, thereby limiting the performance of downstream weakly supervised learning tasks, particularly Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise accurate activation maps to get the best results. To alleviate the above problems, we propose a novel activation method, Region-CAM. Distinct from network feature weighting approaches, Region-CAM generates activation maps by extracting semantic information maps (SIMs) and performing semantic information propagation (SIP) by considering both gradients and features in each of the stages of the baseline classification model. Our approach highlights a greater proportion of object regions while ensuring activation maps to have precise boundaries that align closely with object edges. Region-CAM achieves 60.12% and 58.43% mean intersection over union (mIoU) using the baseline model on the PASCAL VOC training and validation datasets, respectively, which are improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement over the original CAM (20.15%). We also demonstrate the superiority of Region-CAM in object localization tasks, using the ILSVRC2012 validation set. Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with LayerCAM, an activation method designed for weakly supervised object localization, Region-CAM achieves 4.5% better performance in Loc1.",
        "arxiv_id": "2510.25134",
        "ARXIVID": "2510.25134",
        "COMMENT": "Does not closely match any specific criterion but is relevant to weakly supervised learning in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.25174": {
        "authors": [
            "Huadong Tang",
            "Youpeng Zhao",
            "Min Xu",
            "Jun Wang",
            "Qiang Wu"
        ],
        "title": "Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation",
        "abstract": "arXiv:2510.25174v1 Announce Type: new  Abstract: Prevalent semantic segmentation methods generally adopt a vanilla classifier to categorize each pixel into specific classes.   Although such a classifier learns global information from the training data, this information is represented by a set of fixed parameters (weights and biases).   However, each image has a different class distribution, which prevents the classifier from addressing the unique characteristics of individual images.   At the dataset level, class imbalance leads to segmentation results being biased towards majority classes, limiting the model's effectiveness in identifying and segmenting minority class regions.   In this paper, we propose an Extended Context-Aware Classifier (ECAC) that dynamically adjusts the classifier using global (dataset-level) and local (image-level) contextual information.   Specifically, we leverage a memory bank to learn dataset-level contextual information of each class, incorporating the class-specific contextual information from the current image to improve the classifier for precise pixel labeling.   Additionally, a teacher-student network paradigm is adopted, where the domain expert (teacher network) dynamically adjusts contextual information with ground truth and transfers knowledge to the student network.   Comprehensive experiments illustrate that the proposed ECAC can achieve state-of-the-art performance across several datasets, including ADE20K, COCO-Stuff10K, and Pascal-Context.",
        "arxiv_id": "2510.25174",
        "ARXIVID": "2510.25174",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.25101": {
        "authors": [
            "Zhuo Chen",
            "Fei Wang",
            "Zixuan Li",
            "Zhao Zhang",
            "Weiwei Ding",
            "Chuanguang Yang",
            "Yongjun Xu",
            "Xiaolong Jin",
            "Jiafeng Guo"
        ],
        "title": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA",
        "abstract": "arXiv:2510.25101v1 Announce Type: new  Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.",
        "arxiv_id": "2510.25101",
        "ARXIVID": "2510.25101",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in reasoning and AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.24791": {
        "authors": [
            "Jingjun Bi",
            "Fadi Dornaika"
        ],
        "title": "A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data",
        "abstract": "arXiv:2510.24791v1 Announce Type: new  Abstract: Recently, graph-based semi-supervised learning and pseudo-labeling have gained attention due to their effectiveness in reducing the need for extensive data annotations. Pseudo-labeling uses predictions from unlabeled data to improve model training, while graph-based methods are characterized by processing data represented as graphs. However, the lack of clear graph structures in images combined with the complexity of multi-view data limits the efficiency of traditional and existing techniques. Moreover, the integration of graph structures in multi-view data is still a challenge. In this paper, we propose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view Data (RSGSLM). Our method addresses these challenges by (i) combining linear feature transformation and multi-view graph fusion within a Graph Convolutional Network (GCN) framework, (ii) dynamically incorporating pseudo-labels into the GCN loss function to improve classification in multi-view data, and (iii) correcting topological imbalances by adjusting the weights of labeled samples near class boundaries. Additionally, (iv) we introduce an unsupervised smoothing loss applicable to all samples. This combination optimizes performance while maintaining computational efficiency. Experimental results on multi-view benchmark image datasets demonstrate that RSGSLM surpasses existing semi-supervised learning approaches in multi-view contexts.",
        "arxiv_id": "2510.24791",
        "ARXIVID": "2510.24791",
        "COMMENT": "Does not match any specific criterion but is generally relevant to graph-based semi-supervised learning in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.24773": {
        "authors": [
            "Ziyang Xu",
            "Olaf Wysocki",
            "Christoph Holst"
        ],
        "title": "Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds",
        "abstract": "arXiv:2510.24773v1 Announce Type: new  Abstract: Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point clouds is essential for ensuring the accuracy and credibility of downstream applications such as 3D mapping, modeling, and change analysis. Traditional backward uncertainty modeling heavily rely on high-precision reference data, which are often costly or infeasible to obtain at large scales. To address this issue, this study proposes a machine learning-based framework for point-level uncertainty evaluation that learns the relationship between local geometric features and point-level errors. The framework is implemented using two ensemble learning models, Random Forest (RF) and XGBoost, which are trained and validated on a spatially partitioned real-world dataset to avoid data leakage. Experimental results demonstrate that both models can effectively capture the nonlinear relationships between geometric characteristics and uncertainty, achieving mean ROC-AUC values above 0.87. The analysis further reveals that geometric features describing elevation variation, point density, and local structural complexity play a dominant role in predicting uncertainty. The proposed framework offers a data-driven perspective of uncertainty evaluation, providing a scalable and adaptable foundation for future quality control and error analysis of large-scale point clouds.",
        "arxiv_id": "2510.24773",
        "ARXIVID": "2510.24773",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on uncertainty evaluation in 3D mapping.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.25388": {
        "authors": [
            "Robin Schm\\\"ocker",
            "Alexander Dockhorn",
            "Bodo Rosenhahn"
        ],
        "title": "Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm",
        "abstract": "arXiv:2510.25388v1 Announce Type: new  Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency, which can be improved by grouping state-action pairs and using their aggregate statistics instead of single-node statistics. On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS abstraction algorithm for deterministic environments that builds its abstraction using the Abstractions of State-Action Pairs (ASAP) framework, which aims to detect states and state-action pairs with the same value under optimal play by analysing the search graph. ASAP, however, requires two state-action pairs to have the same immediate reward, which is a rigid condition that limits the number of abstractions that can be found and thereby the sample efficiency. In this paper, we break with the paradigm of grouping value-equivalent states or state-action pairs and instead group states and state-action pairs with possibly different values as long as the difference between their values can be inferred. We call this abstraction framework Known Value Difference Abstractions (KVDA), which infers the value differences by analysis of the immediate rewards and modifies OGA-UCT to use this framework instead. The modification is called KVDA-UCT, which detects significantly more abstractions than OGA-UCT, introduces no additional parameter, and outperforms OGA-UCT on a variety of deterministic environments and parameter settings.",
        "arxiv_id": "2510.25388",
        "ARXIVID": "2510.25388",
        "COMMENT": "Does not match any specific criterion but is related to optimization in MCTS, which is tangentially relevant to computational methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.24787": {
        "authors": [
            "Mingzhi Zhu",
            "Ding Shang",
            "Sai Qian Zhang"
        ],
        "title": "ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality",
        "abstract": "arXiv:2510.24787v1 Announce Type: new  Abstract: Photorealistic Codec Avatars (PCA), which generate high-fidelity human face renderings, are increasingly being used in Virtual Reality (VR) environments to enable immersive communication and interaction through deep learning-based generative models. However, these models impose significant computational demands, making real-time inference challenging on resource-constrained VR devices such as head-mounted displays, where latency and power efficiency are critical. To address this challenge, we propose an efficient post-training quantization (PTQ) method tailored for Codec Avatar models, enabling low-precision execution without compromising output quality. In addition, we design a custom hardware accelerator that can be integrated into the system-on-chip of VR devices to further enhance processing efficiency. Building on these components, we introduce ESCA, a full-stack optimization framework that accelerates PCA inference on edge VR platforms. Experimental results demonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over the best 4-bit baseline, delivers up to $3.36\\times$ latency reduction, and sustains a rendering rate of 100 frames per second in end-to-end tests, satisfying real-time VR requirements. These results demonstrate the feasibility of deploying high-fidelity codec avatars on resource-constrained devices, opening the door to more immersive and portable VR experiences.",
        "arxiv_id": "2510.24787",
        "ARXIVID": "2510.24787",
        "COMMENT": "Does not match any specific criterion but is related to computational efficiency in VR, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.25166": {
        "authors": [
            "Zhuojin Li",
            "Marco Paolieri",
            "Leana Golubchik"
        ],
        "title": "A Study on Inference Latency for Vision Transformers on Mobile Devices",
        "abstract": "arXiv:2510.25166v1 Announce Type: new  Abstract: Given the significant advances in machine learning techniques on mobile devices, particularly in the domain of computer vision, in this work we quantitatively study the performance characteristics of 190 real-world vision transformers (ViTs) on mobile devices. Through a comparison with 102 real-world convolutional neural networks (CNNs), we provide insights into the factors that influence the latency of ViT architectures on mobile devices. Based on these insights, we develop a dataset including measured latencies of 1000 synthetic ViTs with representative building blocks and state-of-the-art architectures from two machine learning frameworks and six mobile platforms. Using this dataset, we show that inference latency of new ViTs can be predicted with sufficient accuracy for real-world applications.",
        "arxiv_id": "2510.25166",
        "ARXIVID": "2510.25166",
        "COMMENT": "Does not closely match any specific criterion but provides insights into Vision Transformers on mobile devices.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.24777": {
        "authors": [
            "Yujie Nie",
            "Jianzhang Ni",
            "Yonglong Ye",
            "Yuan-Ting Zhang",
            "Yun Kwok Wing",
            "Xiangqing Xu",
            "Xin Ma",
            "Lizhou Fan"
        ],
        "title": "Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis",
        "abstract": "arXiv:2510.24777v1 Announce Type: new  Abstract: Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling timely intervention and slowing disease progression. Multimodal diagnostic approaches offer considerable promise by integrating complementary information across behavioral and perceptual domains. Eye-tracking and facial features, in particular, are important indicators of cognitive function, reflecting attentional distribution and neurocognitive state. However, few studies have explored their joint integration for auxiliary AD diagnosis. In this study, we propose a multimodal cross-enhanced fusion framework that synergistically leverages eye-tracking and facial features for AD detection. The framework incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module (CEFAM), which models inter-modal interactions through cross-attention and global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which captures fine-grained directional facial features via horizontal-vertical receptive fields. Together, these modules enable adaptive and discriminative multimodal representation learning. To support this work, we constructed a synchronized multimodal dataset, including 25 patients with AD and 25 healthy controls (HC), by recording aligned facial video and eye-tracking sequences during a visual memory-search paradigm, providing an ecologically valid resource for evaluating integration strategies. Extensive experiments on this dataset demonstrate that our framework outperforms traditional late fusion and feature concatenation methods, achieving a classification accuracy of 95.11% in distinguishing AD from HC, highlighting superior robustness and diagnostic performance by explicitly modeling inter-modal dependencies and modality-specific contributions.",
        "arxiv_id": "2510.24777",
        "ARXIVID": "2510.24777",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2510.25007": {
        "authors": [
            "Islam Nassar",
            "Yang Lin",
            "Yuan Jin",
            "Rongxin Zhu",
            "Chang Wei Tan",
            "Zenan Zhai",
            "Nitika Mathur",
            "Thanh Tien Vu",
            "Xu Zhong",
            "Long Duong",
            "Yuan-Fang Li"
        ],
        "title": "Taming the Real-world Complexities in CPT E/M Coding with Large Language Models",
        "abstract": "arXiv:2510.25007v1 Announce Type: new  Abstract: Evaluation and Management (E/M) coding, under the Current Procedural Terminology (CPT) taxonomy, documents medical services provided to patients by physicians. Used primarily for billing purposes, it is in physicians' best interest to provide accurate CPT E/M codes. %While important, it is an auxiliary task that adds to physicians' documentation burden. Automating this coding task will help alleviate physicians' documentation burden, improve billing efficiency, and ultimately enable better patient care. However, a number of real-world complexities have made E/M encoding automation a challenging task. In this paper, we elaborate some of the key complexities and present ProFees, our LLM-based framework that tackles them, followed by a systematic evaluation. On an expert-curated real-world dataset, ProFees achieves an increase in coding accuracy of more than 36\\% over a commercial CPT E/M coding system and almost 5\\% over our strongest single-prompt baseline, demonstrating its effectiveness in addressing the real-world complexities.",
        "arxiv_id": "2510.25007",
        "ARXIVID": "2510.25007",
        "COMMENT": "Does not match any specific criterion but is related to LLM applications, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}