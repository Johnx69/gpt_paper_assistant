{
    "2507.22431": {
        "authors": [
            "Zhixiang Wei",
            "Guangting Wang",
            "Xiaoxiao Ma",
            "Ke Mei",
            "Huaian Chen",
            "Yi Jin",
            "Fengyun Rao"
        ],
        "title": "HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models",
        "abstract": "arXiv:2507.22431v1 Announce Type: new  Abstract: Large-scale but noisy image-text pair data have paved the way for the success of Contrastive Language-Image Pretraining (CLIP). As the foundation vision encoder, CLIP in turn serves as the cornerstone for most large vision-language models (LVLMs). This interdependence naturally raises an interesting question: Can we reciprocally leverage LVLMs to enhance the quality of image-text pair data, thereby opening the possibility of a self-reinforcing cycle for continuous improvement? In this work, we take a significant step toward this vision by introducing an LVLM-driven data refinement pipeline. Our framework leverages LVLMs to process images and their raw alt-text, generating four complementary textual formulas: long positive descriptions, long negative descriptions, short positive tags, and short negative tags. Applying this pipeline to the curated DFN-Large dataset yields VLM-150M, a refined dataset enriched with multi-grained annotations. Based on this dataset, we further propose a training paradigm that extends conventional contrastive learning by incorporating negative descriptions and short tags as additional supervised signals. The resulting model, namely HQ-CLIP, demonstrates remarkable improvements across diverse benchmarks. Within a comparable training data scale, our approach achieves state-of-the-art performance in zero-shot classification, cross-modal retrieval, and fine-grained visual understanding tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models trained on the DFN-2B dataset, which contains 10$\\times$ more training data than ours. All code, data, and models are available at https://zxwei.site/hqclip.",
        "arxiv_id": "2507.22431",
        "ARXIVID": "2507.22431",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores leveraging large vision-language models (LVLMs) for dataset refinement and proposes a new training paradigm for CLIP models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.22522": {
        "authors": [
            "Ziyi Wang",
            "Peiming Li",
            "Hong Liu",
            "Zhichao Deng",
            "Can Wang",
            "Jun Liu",
            "Junsong Yuan",
            "Mengyuan Liu"
        ],
        "title": "Recognizing Actions from Robotic View for Natural Human-Robot Interaction",
        "abstract": "arXiv:2507.22522v1 Announce Type: new  Abstract: Natural Human-Robot Interaction (N-HRI) requires robots to recognize human actions at varying distances and states, regardless of whether the robot itself is in motion or stationary. This setup is more flexible and practical than conventional human action recognition tasks. However, existing benchmarks designed for traditional action recognition fail to address the unique complexities in N-HRI due to limited data, modalities, task categories, and diversity of subjects and environments. To address these challenges, we introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored specifically for perception-centric robotic views prevalent in mobile service robots. ACTIVE comprises 30 composite action categories, 80 participants, and 46,868 annotated video instances, covering both RGB and point cloud modalities. Participants performed various human actions in diverse environments at distances ranging from 3m to 50m, while the camera platform was also mobile, simulating real-world scenarios of robot perception with varying camera heights due to uneven ground. This comprehensive and challenging benchmark aims to advance action and attribute recognition research in N-HRI. Furthermore, we propose ACTIVE-PC, a method that accurately perceives human actions at long distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic Ellipse Query, and precise decoupling of kinematic interference from human actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our code is available at: https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.",
        "arxiv_id": "2507.22522",
        "ARXIVID": "2507.22522",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new dataset and method for action recognition in natural human-robot interaction scenarios.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.22346": {
        "authors": [
            "Pei Deng",
            "Wenqian Zhou",
            "Hanlin Wu"
        ],
        "title": "DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception",
        "abstract": "arXiv:2507.22346v1 Announce Type: new  Abstract: Accurate interpretation of land-cover changes in multi-temporal satellite imagery is critical for real-world scenarios. However, existing methods typically provide only one-shot change masks or static captions, limiting their ability to support interactive, query-driven analysis. In this work, we introduce remote sensing image change analysis (RSICA) as a new paradigm that combines the strengths of change detection and visual question answering to enable multi-turn, instruction-guided exploration of changes in bi-temporal remote sensing images. To support this task, we construct ChangeChat-105k, a large-scale instruction-following dataset, generated through a hybrid rule-based and GPT-assisted process, covering six interaction types: change captioning, classification, quantification, localization, open-ended question answering, and multi-turn dialogues. Building on this dataset, we propose DeltaVLM, an end-to-end architecture tailored for interactive RSICA. DeltaVLM features three innovations: (1) a fine-tuned bi-temporal vision encoder to capture temporal differences; (2) a visual difference perception module with a cross-semantic relation measuring (CSRM) mechanism to interpret changes; and (3) an instruction-guided Q-former to effectively extract query-relevant difference information from visual changes, aligning them with textual instructions. We train DeltaVLM on ChangeChat-105k using a frozen large language model, adapting only the vision and alignment modules to optimize efficiency. Extensive experiments and ablation studies demonstrate that DeltaVLM achieves state-of-the-art performance on both single-turn captioning and multi-turn interactive change analysis, outperforming existing multimodal large language models and remote sensing vision-language models. Code, dataset and pre-trained weights are available at https://github.com/hanlinwu/DeltaVLM.",
        "arxiv_id": "2507.22346",
        "ARXIVID": "2507.22346",
        "COMMENT": "Matches criterion 5 as it combines image understanding tasks with large language models for remote sensing image change analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.22668": {
        "authors": [
            "Hongbin Lin",
            "Yifan Jiang",
            "Juangui Xu",
            "Jesse Jiaxi Xu",
            "Yi Lu",
            "Zhengyu Hu",
            "Ying-Cong Chen",
            "Hao Wang"
        ],
        "title": "Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation",
        "abstract": "arXiv:2507.22668v1 Announce Type: new  Abstract: 3D point cloud segmentation aims to assign semantic labels to individual points in a scene for fine-grained spatial understanding. Existing methods typically adopt data augmentation to alleviate the burden of large-scale annotation. However, most augmentation strategies only focus on local transformations or semantic recomposition, lacking the consideration of global structural dependencies within scenes. To address this limitation, we propose a graph-guided data augmentation framework with dual-level constraints for realistic 3D scene synthesis. Our method learns object relationship statistics from real-world data to construct guiding graphs for scene generation. Local-level constraints enforce geometric plausibility and semantic consistency between objects, while global-level constraints maintain the topological structure of the scene by aligning the generated layout with the guiding graph. Extensive experiments on indoor and outdoor datasets demonstrate that our framework generates diverse and high-quality augmented scenes, leading to consistent improvements in point cloud segmentation performance across various models.",
        "arxiv_id": "2507.22668",
        "ARXIVID": "2507.22668",
        "COMMENT": "Matches criterion 1 as it focuses on spatial intelligence and 3D scene segmentation with novel graph-guided augmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.22454": {
        "authors": [
            "Jiuming Liu",
            "Zheng Huang",
            "Mengmeng Liu",
            "Tianchen Deng",
            "Francesco Nex",
            "Hao Cheng",
            "Hesheng Wang"
        ],
        "title": "TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation",
        "abstract": "arXiv:2507.22454v1 Announce Type: new  Abstract: LiDAR scene generation is critical for mitigating real-world LiDAR data collection costs and enhancing the robustness of downstream perception tasks in autonomous driving. However, existing methods commonly struggle to capture geometric realism and global topological consistency. Recent LiDAR Diffusion Models (LiDMs) predominantly embed LiDAR points into the latent space for improved generation efficiency, which limits their interpretable ability to model detailed geometric structures and preserve global topological consistency. To address these challenges, we propose TopoLiDM, a novel framework that integrates graph neural networks (GNNs) with diffusion models under topological regularization for high-fidelity LiDAR generation. Our approach first trains a topological-preserving VAE to extract latent graph representations by graph construction and multiple graph convolutional layers. Then we freeze the VAE and generate novel latent topological graphs through the latent diffusion models. We also introduce 0-dimensional persistent homology (PH) constraints, ensuring the generated LiDAR scenes adhere to real-world global topological structures. Extensive experiments on the KITTI-360 dataset demonstrate TopoLiDM's superiority over state-of-the-art methods, achieving improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower Minimum Matching Distance (MMD). Notably, our model also enables fast generation speed with an average inference time of 1.68 samples/s, showcasing its scalability for real-world applications. We will release the related codes at https://github.com/IRMVLab/TopoLiDM.",
        "arxiv_id": "2507.22454",
        "ARXIVID": "2507.22454",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel method for LiDAR point cloud generation with topological regularization, which is relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.22627": {
        "authors": [
            "Federico Girella",
            "Davide Talon",
            "Ziyue Liu",
            "Zanxi Ruan",
            "Yiming Wang",
            "Marco Cristani"
        ],
        "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing",
        "abstract": "arXiv:2507.22627v1 Announce Type: new  Abstract: Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.",
        "arxiv_id": "2507.22627",
        "ARXIVID": "2507.22627",
        "COMMENT": "Matches criterion 5 as it focuses on combining sketch and text for fashion image generation, integrating visual and textual modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.22886": {
        "authors": [
            "Kaining Ying",
            "Henghui Ding",
            "Guanquan Jie",
            "Yu-Gang Jiang"
        ],
        "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation",
        "abstract": "arXiv:2507.22886v1 Announce Type: new  Abstract: Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks.",
        "arxiv_id": "2507.22886",
        "ARXIVID": "2507.22886",
        "COMMENT": "Matches criterion 5 as it introduces a dataset and method for integrating audio, visual, and textual cues in segmentation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.22607": {
        "authors": [
            "Ruifeng Yuan",
            "Chenghao Xiao",
            "Sicong Leng",
            "Jianyu Wang",
            "Long Li",
            "Weiwen Xu",
            "Hou Pong Chan",
            "Deli Zhao",
            "Tingyang Xu",
            "Zhongyu Wei",
            "Hao Zhang",
            "Yu Rong"
        ],
        "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning",
        "abstract": "arXiv:2507.22607v1 Announce Type: new  Abstract: Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.",
        "arxiv_id": "2507.22607",
        "ARXIVID": "2507.22607",
        "COMMENT": "Matches criterion 2 as it explores multimodal reasoning with a novel reinforcement learning framework for vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.22742": {
        "authors": [
            "Yang Gao",
            "Saeed Saadatnejad",
            "Alexandre Alahi"
        ],
        "title": "Social-Pose: Enhancing Trajectory Prediction with Human Body Pose",
        "abstract": "arXiv:2507.22742v1 Announce Type: new  Abstract: Accurate human trajectory prediction is one of the most crucial tasks for autonomous driving, ensuring its safety. Yet, existing models often fail to fully leverage the visual cues that humans subconsciously communicate when navigating the space. In this work, we study the benefits of predicting human trajectories using human body poses instead of solely their Cartesian space locations in time. We propose `Social-pose', an attention-based pose encoder that effectively captures the poses of all humans in a scene and their social relations. Our method can be integrated into various trajectory prediction architectures. We have conducted extensive experiments on state-of-the-art models (based on LSTM, GAN, MLP, and Transformer), and showed improvements over all of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians and Cyclists in Road Traffic, and JRDB) datasets. We also explored the advantages of using 2D versus 3D poses, as well as the effect of noisy poses and the application of our pose-based predictor in robot navigation scenarios.",
        "arxiv_id": "2507.22742",
        "ARXIVID": "2507.22742",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it proposes a novel method for human trajectory prediction using body poses, which is relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.22264": {
        "authors": [
            "Shaoan Xie",
            "Lingjing Kong",
            "Yujia Zheng",
            "Yu Yao",
            "Zeyu Tang",
            "Eric P. Xing",
            "Guangyi Chen",
            "Kun Zhang"
        ],
        "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees",
        "abstract": "arXiv:2507.22264v1 Announce Type: new  Abstract: Contrastive Language-Image Pre-training (CLIP)~\\citep{radford2021learning} has emerged as a pivotal model in computer vision and multimodal learning, achieving state-of-the-art performance at aligning visual and textual representations through contrastive learning. However, CLIP struggles with potential information misalignment in many image-text datasets and suffers from entangled representation. On the one hand, short captions for a single image in datasets like MSCOCO may describe disjoint regions in the image, leaving the model uncertain about which visual features to retain or disregard. On the other hand, directly aligning long captions with images can lead to the retention of entangled details, preventing the model from learning disentangled, atomic concepts -- ultimately limiting its generalization on certain downstream tasks involving short prompts.   In this paper, we establish theoretical conditions that enable flexible alignment between textual and visual representations across varying levels of granularity. Specifically, our framework ensures that a model can not only \\emph{preserve} cross-modal semantic information in its entirety but also \\emph{disentangle} visual representations to capture fine-grained textual concepts. Building on this foundation, we introduce \\ours, a novel approach that identifies and aligns the most relevant visual and textual representations in a modular manner. Superior performance across various tasks demonstrates its capability to handle information misalignment and supports our identification theory. The code is available at https://github.com/Mid-Push/SmartCLIP.",
        "arxiv_id": "2507.22264",
        "ARXIVID": "2507.22264",
        "COMMENT": "Matches criterion 2 as it explores improvements in vision-language alignment for CLIP, a foundational vision-language model.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.22733": {
        "authors": [
            "Hang Su",
            "Yunlong Feng",
            "Daniel Gehrig",
            "Panfeng Jiang",
            "Ling Gao",
            "Xavier Lagorce",
            "Laurent Kneip"
        ],
        "title": "A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks",
        "abstract": "arXiv:2507.22733v1 Announce Type: new  Abstract: Structure and continuous motion estimation from point correspondences is a fundamental problem in computer vision that has been powered by well-known algorithms such as the familiar 5-point or 8-point algorithm. However, despite their acclaim, these algorithms are limited to processing point correspondences originating from a pair of views each one representing an instantaneous capture of the scene. Yet, in the case of rolling shutter cameras, or more recently, event cameras, this synchronization breaks down. In this work, we present a unified approach for structure and linear motion estimation from 2D point correspondences with arbitrary timestamps, from an arbitrary set of views. By formulating the problem in terms of first-order dynamics and leveraging a constant velocity motion model, we derive a novel, linear point incidence relation allowing for the efficient recovery of both linear velocity and 3D points with predictable degeneracies and solution multiplicities. Owing to its general formulation, it can handle correspondences from a wide range of sensing modalities such as global shutter, rolling shutter, and event cameras, and can even combine correspondences from different collocated sensors. We validate the effectiveness of our solver on both simulated and real-world data, where we show consistent improvement across all modalities when compared to recent approaches. We believe our work opens the door to efficient structure and motion estimation from asynchronous data. Code can be found at https://github.com/suhang99/AsyncTrack-Motion-Solver.",
        "arxiv_id": "2507.22733",
        "ARXIVID": "2507.22733",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for structure and motion estimation from asynchronous data, addressing underexplored challenges.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2507.22802": {
        "authors": [
            "Dongli He",
            "Hu Wang",
            "Mohammad Yaqub"
        ],
        "title": "Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings",
        "abstract": "arXiv:2507.22802v1 Announce Type: new  Abstract: Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: https://github.com/donglihe-hub/FetalCLIP-IQA.",
        "arxiv_id": "2507.22802",
        "ARXIVID": "2507.22802",
        "COMMENT": "Matches criterion 5 as it integrates vision-language models for ultrasound image quality assessment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.22465": {
        "authors": [
            "Zheng Xiangyu",
            "He Songcheng",
            "Li Wanyun",
            "Li Xiaoqiang",
            "Zhang Wei"
        ],
        "title": "Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation",
        "abstract": "arXiv:2507.22465v1 Announce Type: new  Abstract: Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level masks for the most salient objects in videos without any prior annotations. While memory mechanisms have been proven critical in various video segmentation paradigms, their application in UVOS yield only marginal performance gains despite sophisticated design. Our analysis reveals a simple but fundamental flaw in existing methods: over-reliance on memorizing high-level semantic features. UVOS inherently suffers from the deficiency of lacking fine-grained information due to the absence of pixel-level prior knowledge. Consequently, memory design relying solely on high-level features, which predominantly capture abstract semantic cues, is insufficient to generate precise predictions. To resolve this fundamental issue, we propose a novel hierarchical memory architecture to incorporate both shallow- and high-level features for memory, which leverages the complementary benefits of pixel and semantic information. Furthermore, to balance the simultaneous utilization of the pixel and semantic memory features, we propose a heterogeneous interaction mechanism to perform pixel-semantic mutual interactions, which explicitly considers their inherent feature discrepancies. Through the design of Pixel-guided Local Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM), we achieve delicate integration of the fine-grained details in shallow-level memory and the semantic representations in high-level memory. Our Hierarchical Memory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves state-of-the-art performance across all UVOS and video saliency detection benchmarks. Moreover, HMHI-Net consistently exhibits high performance across different backbones, further demonstrating its superiority and robustness. Project page: https://github.com/ZhengxyFlow/HMHI-Net .",
        "arxiv_id": "2507.22465",
        "ARXIVID": "2507.22465",
        "COMMENT": "Matches criterion 6 as it introduces a novel hierarchical memory architecture for video object segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.22530": {
        "authors": [
            "Xincheng Yao",
            "Yijun Yang",
            "Kangwei Guo",
            "Ruiqiang Xiao",
            "Haipeng Zhou",
            "Haisu Tao",
            "Jian Yang",
            "Lei Zhu"
        ],
        "title": "HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors",
        "abstract": "arXiv:2507.22530v1 Announce Type: new  Abstract: The segmentation of the hepatic vasculature in surgical videos holds substantial clinical significance in the context of hepatectomy procedures. However, owing to the dearth of an appropriate dataset and the inherently complex task characteristics, few researches have been reported in this domain. To address this issue, we first introduce a high quality frame-by-frame annotated hepatic vasculature dataset containing 35 long hepatectomy videos and 11442 high-resolution frames. On this basis, we propose a novel high-resolution video vasculature segmentation network, dubbed as HRVVS. We innovatively embed a pretrained visual autoregressive modeling (VAR) model into different layers of the hierarchical encoder as prior information to reduce the information degradation generated during the downsampling process. In addition, we designed a dynamic memory decoder on a multi-view segmentation network to minimize the transmission of redundant information while preserving more details between frames. Extensive experiments on surgical video datasets demonstrate that our proposed HRVVS significantly outperforms the state-of-the-art methods. The source code and dataset will be publicly available at \\href{https://github.com/scott-yjyang/xx}{https://github.com/scott-yjyang/HRVVS}.",
        "arxiv_id": "2507.22530",
        "ARXIVID": "2507.22530",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a novel method for video vasculature segmentation in surgical videos, which is a video-based task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.22412": {
        "authors": [
            "Sijie Wang",
            "Siqi Li",
            "Yawei Zhang",
            "Shangshu Yu",
            "Shenghai Yuan",
            "Rui She",
            "Quanjiang Guo",
            "JinXuan Zheng",
            "Ong Kang Howe",
            "Leonrich Chandra",
            "Shrivarshann Srijeyan",
            "Aditya Sivadas",
            "Toshan Aggarwal",
            "Heyuan Liu",
            "Hongming Zhang",
            "Chujie Chen",
            "Junyu Jiang",
            "Lihua Xie",
            "Wee Peng Tay"
        ],
        "title": "UAVScenes: A Multi-Modal Dataset for UAVs",
        "abstract": "arXiv:2507.22412v1 Announce Type: new  Abstract: Multi-modal perception is essential for unmanned aerial vehicle (UAV) operations, as it enables a comprehensive understanding of the UAVs' surrounding environment. However, most existing multi-modal UAV datasets are primarily biased toward localization and 3D reconstruction tasks, or only support map-level semantic segmentation due to the lack of frame-wise annotations for both camera images and LiDAR point clouds. This limitation prevents them from being used for high-level scene understanding tasks. To address this gap and advance multi-modal UAV perception, we introduce UAVScenes, a large-scale dataset designed to benchmark various tasks across both 2D and 3D modalities. Our benchmark dataset is built upon the well-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only for simultaneous localization and mapping (SLAM). We enhance this dataset by providing manually labeled semantic annotations for both frame-wise images and LiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses. These additions enable a wide range of UAV perception tasks, including segmentation, depth estimation, 6-DoF localization, place recognition, and novel view synthesis (NVS). Our dataset is available at https://github.com/sijieaaa/UAVScenes",
        "arxiv_id": "2507.22412",
        "ARXIVID": "2507.22412",
        "COMMENT": "Matches criterion 3 as it introduces a new multi-modal UAV dataset for various perception tasks, including segmentation and localization.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.22781": {
        "authors": [
            "Xuecheng Wu",
            "Danlei Huang",
            "Heli Sun",
            "Xinyi Yin",
            "Yifan Wang",
            "Hao Wang",
            "Jia Zhang",
            "Fei Wang",
            "Peihao Guo",
            "Suyu Xing",
            "Junxiao Xue",
            "Liang He"
        ],
        "title": "HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training",
        "abstract": "arXiv:2507.22781v1 Announce Type: new  Abstract: Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pre-training in the general domain, we first scale audio-visual self-supervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples, thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set.",
        "arxiv_id": "2507.22781",
        "ARXIVID": "2507.22781",
        "COMMENT": "Matches criterion 6 as it focuses on video-level deepfake detection with novel methodologies and pre-training strategies.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.22824": {
        "authors": [
            "Mian Zou",
            "Nan Zhong",
            "Baosheng Yu",
            "Yibing Zhan",
            "Kede Ma"
        ],
        "title": "Bi-Level Optimization for Self-Supervised AI-Generated Face Detection",
        "abstract": "arXiv:2507.22824v1 Announce Type: new  Abstract: AI-generated face detectors trained via supervised learning typically rely on synthesized images from specific generators, limiting their generalization to emerging generative techniques. To overcome this limitation, we introduce a self-supervised method based on bi-level optimization. In the inner loop, we pretrain a vision encoder only on photographic face images using a set of linearly weighted pretext tasks: classification of categorical exchangeable image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of artificial face manipulations. The outer loop then optimizes the relative weights of these pretext tasks to enhance the coarse-grained detection of manipulated faces, serving as a proxy task for identifying AI-generated faces. In doing so, it aligns self-supervised learning more closely with the ultimate goal of AI-generated face detection. Once pretrained, the encoder remains fixed, and AI-generated faces are detected either as anomalies under a Gaussian mixture model fitted to photographic face features or by a lightweight two-layer perceptron serving as a binary classifier. Extensive experiments demonstrate that our detectors significantly outperform existing approaches in both one-class and binary classification settings, exhibiting strong generalization to unseen generators.",
        "arxiv_id": "2507.22824",
        "ARXIVID": "2507.22824",
        "COMMENT": "Matches criterion 4 as it focuses on foundation models for AI-generated face detection.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.22792": {
        "authors": [
            "Guoping Xu",
            "Jayaram K. Udupa",
            "Yajun Yu",
            "Hua-Chieh Shao",
            "Songlin Zhao",
            "Wei Liu",
            "You Zhang"
        ],
        "title": "Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future",
        "abstract": "arXiv:2507.22792v1 Announce Type: new  Abstract: Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models.",
        "arxiv_id": "2507.22792",
        "ARXIVID": "2507.22792",
        "COMMENT": "Matches criterion 7 as it is a survey paper on video object segmentation and tracking, synthesizing the state of the art.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2507.22480": {
        "authors": [
            "Haipeng Li",
            "Tianhao Zhou",
            "Zhanglei Yang",
            "Yi Wu",
            "Yan Chen",
            "Zijing Mao",
            "Shen Cheng",
            "Bing Zeng",
            "Shuaicheng Liu"
        ],
        "title": "Estimating 2D Camera Motion with Hybrid Motion Basis",
        "abstract": "arXiv:2507.22480v1 Announce Type: new  Abstract: Estimating 2D camera motion is a fundamental computer vision task that models the projection of 3D camera movements onto the 2D image plane. Current methods rely on either homography-based approaches, limited to planar scenes, or meshflow techniques that use grid-based local homographies but struggle with complex non-linear transformations. A key insight of our work is that combining flow fields from different homographies creates motion patterns that cannot be represented by any single homography. We introduce CamFlow, a novel framework that represents camera motion using hybrid motion bases: physical bases derived from camera geometry and stochastic bases for complex scenarios. Our approach includes a hybrid probabilistic loss function based on the Laplace distribution that enhances training robustness. For evaluation, we create a new benchmark by masking dynamic objects in existing optical flow datasets to isolate pure camera motion. Experiments show CamFlow outperforms state-of-the-art methods across diverse scenarios, demonstrating superior robustness and generalization in zero-shot settings. Code and datasets are available at our project page: https://lhaippp.github.io/CamFlow/.",
        "arxiv_id": "2507.22480",
        "ARXIVID": "2507.22480",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for camera motion estimation and a novel method for hybrid motion basis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.22791": {
        "authors": [
            "Weide Liu",
            "Wei Zhou",
            "Jun Liu",
            "Ping Hu",
            "Jun Cheng",
            "Jungong Han",
            "Weisi Lin"
        ],
        "title": "Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques",
        "abstract": "arXiv:2507.22791v1 Announce Type: new  Abstract: Feature matching is a cornerstone task in computer vision, essential for applications such as image retrieval, stereo matching, 3D reconstruction, and SLAM. This survey comprehensively reviews modality-based feature matching, exploring traditional handcrafted methods and emphasizing contemporary deep learning approaches across various modalities, including RGB images, depth images, 3D point clouds, LiDAR scans, medical images, and vision-language interactions. Traditional methods, leveraging detectors like Harris corners and descriptors such as SIFT and ORB, demonstrate robustness under moderate intra-modality variations but struggle with significant modality gaps. Contemporary deep learning-based methods, exemplified by detector-free strategies like CNN-based SuperPoint and transformer-based LoFTR, substantially improve robustness and adaptability across modalities. We highlight modality-aware advancements, such as geometric and depth-specific descriptors for depth images, sparse and dense learning methods for 3D point clouds, attention-enhanced neural networks for LiDAR scans, and specialized solutions like the MIND descriptor for complex medical image matching. Cross-modal applications, particularly in medical image registration and vision-language tasks, underscore the evolution of feature matching to handle increasingly diverse data interactions.",
        "arxiv_id": "2507.22791",
        "ARXIVID": "2507.22791",
        "COMMENT": "Matches criterion 7 as it is a comprehensive survey on feature matching, including vision-language interactions.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.22615": {
        "authors": [
            "Daehee Park",
            "Monu Surana",
            "Pranav Desai",
            "Ashish Mehta",
            "Reuben MV John",
            "Kuk-Jin Yoon"
        ],
        "title": "Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model",
        "abstract": "arXiv:2507.22615v1 Announce Type: new  Abstract: While data-driven trajectory prediction has enhanced the reliability of autonomous driving systems, it still struggles with rarely observed long-tail scenarios. Prior works addressed this by modifying model architectures, such as using hypernetworks. In contrast, we propose refining the training process to unlock each model's potential without altering its structure. We introduce Generative Active Learning for Trajectory prediction (GALTraj), the first method to successfully deploy generative active learning into trajectory prediction. It actively identifies rare tail samples where the model fails and augments these samples with a controllable diffusion model during training. In our framework, generating scenarios that are diverse, realistic, and preserve tail-case characteristics is paramount. Accordingly, we design a tail-aware generation method that applies tailored diffusion guidance to generate trajectories that both capture rare behaviors and respect traffic rules. Unlike prior simulation methods focused solely on scenario diversity, GALTraj is the first to show how simulator-driven augmentation benefits long-tail learning in trajectory prediction. Experiments on multiple trajectory datasets (WOMD, Argoverse2) with popular backbones (QCNet, MTR) confirm that our method significantly boosts performance on tail samples and also enhances accuracy on head samples.",
        "arxiv_id": "2507.22615",
        "ARXIVID": "2507.22615",
        "COMMENT": "Does not match any specific criterion but is relevant to trajectory prediction and generative modeling, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.22359": {
        "authors": [
            "Qianhong Guo",
            "Wei Xie",
            "Xiaofang Cai",
            "Enze Wang",
            "Shuoyoucheng Ma",
            "Kai Chen",
            "Xiaofeng Wang",
            "Baosheng Wang"
        ],
        "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models",
        "abstract": "arXiv:2507.22359v1 Announce Type: new  Abstract: Although large language models (LLMs) demonstrate remarkable capabilities across various tasks, evaluating their capabilities remains a challenging task. Existing evaluation methods suffer from issues such as data contamination, black-box operation, and subjective preference. These issues make it difficult to evaluate the LLMs' true capabilities comprehensively. To tackle these challenges, we propose a novel benchmark-free evaluation paradigm, LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently, and evaluate mutually. This method integrates four key evaluation criteria: dynamic, transparent, objective, and professional, which existing evaluation methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs across mathematics and programming verify the advantages of our method in distinguishing LLM performance. Furthermore, our study reveals several novel findings that are difficult for traditional methods to detect, including but not limited to: (1) Gemini demonstrates the highest original and professional question-design capabilities among others; (2) Some LLMs exhibit ''memorization-based answering'' by misrecognizing questions as familiar ones with a similar structure; (3) LLM evaluation results demonstrate high consistency (robustness).",
        "arxiv_id": "2507.22359",
        "ARXIVID": "2507.22359",
        "COMMENT": "Does not match any specific criterion but proposes a novel benchmark-free evaluation paradigm for large language models, which is tangentially related to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.22872": {
        "authors": [
            "Siqi Luo",
            "Haoran Yang",
            "Yi Xin",
            "Mingyang Yi",
            "Guangyang Wu",
            "Guangtao Zhai",
            "Xiaohong Liu"
        ],
        "title": "TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning",
        "abstract": "arXiv:2507.22872v1 Announce Type: new  Abstract: Large pre-trained models achieve remarkable performance in vision tasks but are impractical for fine-tuning due to high computational and storage costs. Parameter-Efficient Fine-Tuning (PEFT) methods mitigate this issue by updating only a subset of parameters; however, most existing approaches are task-agnostic, failing to fully exploit task-specific adaptations, which leads to suboptimal efficiency and performance. To address this limitation, we propose Task-Relevant Parameter and Token Selection (TR-PTS), a task-driven framework that enhances both computational efficiency and accuracy. Specifically, we introduce Task-Relevant Parameter Selection, which utilizes the Fisher Information Matrix (FIM) to identify and fine-tune only the most informative parameters in a layer-wise manner, while keeping the remaining parameters frozen. Simultaneously, Task-Relevant Token Selection dynamically preserves the most informative tokens and merges redundant ones, reducing computational overhead. By jointly optimizing parameters and tokens, TR-PTS enables the model to concentrate on task-discriminative information. We evaluate TR-PTS on benchmark, including FGVC and VTAB-1k, where it achieves state-of-the-art performance, surpassing full fine-tuning by 3.40% and 10.35%, respectively. The code are available at https://github.com/synbol/TR-PTS.",
        "arxiv_id": "2507.22872",
        "ARXIVID": "2507.22872",
        "COMMENT": "Does not match any specific criterion but proposes a task-relevant fine-tuning framework, which is tangentially related to vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.22291": {
        "authors": [
            "Christopher F. Brown",
            "Michal R. Kazmierski",
            "Valerie J. Pasquarella",
            "William J. Rucklidge",
            "Masha Samsikova",
            "Chenhui Zhang",
            "Evan Shelhamer",
            "Estefania Lahera",
            "Olivia Wiles",
            "Simon Ilyushchenko",
            "Noel Gorelick",
            "Lihui Lydia Zhang",
            "Sophia Alj",
            "Emily Schechter",
            "Sean Askay",
            "Oliver Guinan",
            "Rebecca Moore",
            "Alexis Boukouvalas",
            "Pushmeet Kohli"
        ],
        "title": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data",
        "abstract": "arXiv:2507.22291v1 Announce Type: new  Abstract: Unprecedented volumes of Earth observation data are continually collected around the world, but high-quality labels remain scarce given the effort required to make physical measurements and observations. This has led to considerable investment in bespoke modeling efforts translating sparse labels into maps. Here we introduce AlphaEarth Foundations, an embedding field model yielding a highly general, geospatial representation that assimilates spatial, temporal, and measurement contexts across multiple sources, enabling accurate and efficient production of maps and monitoring systems from local to global scales. The embeddings generated by AlphaEarth Foundations are the only to consistently outperform all previous featurization approaches tested on a diverse set of mapping evaluations without re-training. We will release a dataset of global, annual, analysis-ready embedding field layers from 2017 through 2024.",
        "arxiv_id": "2507.22291",
        "ARXIVID": "2507.22291",
        "COMMENT": "Does not match any specific criteria. Focuses on geospatial embedding fields for Earth observation, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.22358": {
        "authors": [
            "Hussein Mozannar",
            "Gagan Bansal",
            "Cheng Tan",
            "Adam Fourney",
            "Victor Dibia",
            "Jingya Chen",
            "Jack Gerrits",
            "Tyler Payne",
            "Matheus Kunzler Maldaner",
            "Madeleine Grunde-McLaughlin",
            "Eric Zhu",
            "Griffin Bassman",
            "Jacob Alber",
            "Peter Chang",
            "Ricky Loynd",
            "Friederike Niedtner",
            "Ece Kamar",
            "Maya Murad",
            "Rafah Hosn",
            "Saleema Amershi"
        ],
        "title": "Magentic-UI: Towards Human-in-the-loop Agentic Systems",
        "abstract": "arXiv:2507.22358v1 Announce Type: new  Abstract: AI agents powered by large language models are increasingly capable of autonomously completing complex, multi-step tasks using external tools. Yet, they still fall short of human-level performance in most domains including computer use, software development, and research. Their growing autonomy and ability to interact with the outside world, also introduces safety and security risks including potentially misaligned actions and adversarial manipulation. We argue that human-in-the-loop agentic systems offer a promising path forward, combining human oversight and control with AI efficiency to unlock productivity from imperfect systems. We introduce Magentic-UI, an open-source web interface for developing and studying human-agent interaction. Built on a flexible multi-agent architecture, Magentic-UI supports web browsing, code execution, and file manipulation, and can be extended with diverse tools via Model Context Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for enabling effective, low-cost human involvement: co-planning, co-tasking, multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI across four dimensions: autonomous task completion on agentic benchmarks, simulated user testing of its interaction capabilities, qualitative studies with real users, and targeted safety assessments. Our findings highlight Magentic-UI's potential to advance safe and efficient human-agent collaboration.",
        "arxiv_id": "2507.22358",
        "ARXIVID": "2507.22358",
        "COMMENT": "Does not match any specific criterion but is generally relevant to human-agent interaction and AI systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22152": {
        "authors": [
            "A. Piffer (Division of Oncology and Children's Research Center",
            "University Children's Hospital Zurich",
            "Zurich",
            "Switzerland)",
            "J. A. Buchner (Department of Radiation Oncology",
            "TUM School of Medicine and Health",
            "Klinikum rechts der Isar",
            "Technical University of Munich",
            "Munich",
            "Germany",
            "Institute of Radiation Medicine",
            "Partner Site Munich",
            "German Consortium for Translational Cancer Research)",
            "A. G. Gennari (Department of Neuropaediatrics",
            "University Children's Hospital Zurich",
            "Switzerland",
            "Center for MR- Research",
            "University Children's Hospital Zurich",
            "Zurich",
            "Switzerland)",
            "P. Grehten (Department of Diagnostic Imaging",
            "University Children's Hospital Zurich",
            "Zurich",
            "Switzerland)",
            "S. Sirin (Department of Diagnostic Imaging",
            "University Children's Hospital Zurich",
            "Zurich",
            "Switzerland)",
            "E. Ross (Georgia Institute of Technology",
            "Geisel School of Medicine at Dartmouth)",
            "I. Ezhov (Department of Computer Science",
            "TUM School of Computation",
            "Information and Technology",
            "Technical University of Munich",
            "Munich",
            "Germany",
            "TranslaTUM - Central Institute for Translational Cancer Research",
            "Technical University of Munich",
            "Munich)",
            "M. Rosier (Helmholtz AI",
            "Helmholtz Zentrum Munich",
            "Munich",
            "Germany",
            "Department of Quantitative Biomedicine",
            "University of Zurich",
            "Zurich",
            "Switzerland)",
            "J. C. Peeken (Department of Radiation Oncology",
            "TUM School of Medicine and Health",
            "Klinikum rechts der Isar",
            "Technical University of Munich",
            "Munich",
            "Germany)",
            "M. Piraud (Helmholtz AI",
            "Helmholtz Zentrum Munich",
            "Munich",
            "Germany)",
            "B. Menze (Department of Quantitative Biomedicine",
            "University of Zurich",
            "Zurich",
            "Switzerland)",
            "A. Guerreiro St\\\"ucklin (Division of Oncology and Children's Research Center",
            "University Children's Hospital Zurich",
            "Zurich",
            "Switzerland)",
            "A. Jakab (Center for MR- Research",
            "University Children's Hospital Zurich",
            "Zurich",
            "Switzerland",
            "Faculty of Medicine",
            "University of Z\\\"urich",
            "Switzerland)",
            "F. Kofler (TranslaTUM - Central Institute for Translational Cancer Research",
            "Technical University of Munich",
            "Munich",
            "Helmholtz AI",
            "Helmholtz Zentrum Munich",
            "Munich",
            "Germany",
            "Department of Quantitative Biomedicine",
            "University of Zurich",
            "Zurich",
            "Switzerland",
            "Department of Diagnostic and Interventional Neuroradiology",
            "Klinikum rechts der Isar",
            "Technical University of Munich",
            "Munich",
            "Germany)"
        ],
        "title": "Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset",
        "abstract": "arXiv:2507.22152v1 Announce Type: new  Abstract: Background Brain tumours are the most common solid malignancies in children, encompassing diverse histological, molecular subtypes and imaging features and outcomes. Paediatric brain tumours (PBTs), including high- and low-grade gliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose diagnostic and therapeutic challenges. Deep learning (DL)-based segmentation offers promising tools for tumour delineation, yet its performance across heterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A retrospective single-centre cohort of 174 paediatric patients with HGG, LGG, medulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI sequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual annotations were provided for four tumour subregions: whole tumour (WT), T2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D nnU-Net model was trained and tested (121/53 split), with segmentation performance assessed using the Dice similarity coefficient (DSC) and compared against intra- and inter-rater variability. Results The model achieved robust performance for WT and T2H (mean DSC: 0.85), comparable to human annotator variability (mean DSC: 0.86). ET segmentation was moderately accurate (mean DSC: 0.75), while CC performance was poor. Segmentation accuracy varied by tumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2 alone produced results nearly equivalent to the full protocol. Conclusions DL is feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and CC segmentation, highlighting the need for further refinement. These findings support the potential for protocol simplification and automation to enhance volumetric assessment and streamline paediatric neuro-oncology workflows.",
        "arxiv_id": "2507.22152",
        "ARXIVID": "2507.22152",
        "COMMENT": "Does not match any specific criterion but is relevant to segmentation and medical imaging, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22477": {
        "authors": [
            "Hui Liu",
            "Chen Jia",
            "Fan Shi",
            "Xu Cheng",
            "Mengfei Shi",
            "Xia Xie",
            "Shengyong Chen"
        ],
        "title": "LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks",
        "abstract": "arXiv:2507.22477v1 Announce Type: new  Abstract: Achieving pixel-level segmentation with low computational cost using multimodal data remains a key challenge in crack segmentation tasks. Existing methods lack the capability for adaptive perception and efficient interactive fusion of cross-modal features. To address these challenges, we propose a Lightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently perceives and integrates morphological and textural cues from different modalities under multimodal crack scenarios, generating clear pixel-level crack segmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive Cue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain Dynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack cues through the proposed mask-guided Efficient Dynamic Guided Scanning Strategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain Perceptron (AFDP) and a dual-pooling fusion strategy to effectively capture spatial and frequency-domain cues across modalities. Moreover, we design a Lightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive complex morphological structures with minimal computational overhead, replacing most convolutional operations in LIDAR. Experiments on three datasets demonstrate that our method outperforms other state-of-the-art (SOTA) methods. On the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465 in mIoU with only 5.35M parameters. Code and datasets are available at https://github.com/Karl1109/LIDAR-Mamba.",
        "arxiv_id": "2507.22477",
        "ARXIVID": "2507.22477",
        "COMMENT": "Does not match any specific criterion but is relevant to multimodal segmentation and efficient architectures, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22650": {
        "authors": [
            "Shahriar Kabir",
            "Istiak Ahmmed Rifti",
            "H. M. Shadman Tabib",
            "Mushfiqur Rahman",
            "Sadatul Islam Sadi",
            "Hasnaen Adil",
            "Ahmed Mahir Sultan Rumi",
            "Ch Md Rakin Haider"
        ],
        "title": "SpectraSentinel: LightWeight Dual-Stream Real-Time Drone Detection, Tracking and Payload Identification",
        "abstract": "arXiv:2507.22650v1 Announce Type: new  Abstract: The proliferation of drones in civilian airspace has raised urgent security concerns, necessitating robust real-time surveillance systems. In response to the 2025 VIP Cup challenge tasks - drone detection, tracking, and payload identification - we propose a dual-stream drone monitoring framework. Our approach deploys independent You Only Look Once v11-nano (YOLOv11n) object detectors on parallel infrared (thermal) and visible (RGB) data streams, deliberately avoiding early fusion. This separation allows each model to be specifically optimized for the distinct characteristics of its input modality, addressing the unique challenges posed by small aerial objects in diverse environmental conditions. We customize data preprocessing and augmentation strategies per domain - such as limiting color jitter for IR imagery - and fine-tune training hyperparameters to enhance detection performance under conditions of heavy noise, low light, and motion blur. The resulting lightweight YOLOv11n models demonstrate high accuracy in distinguishing drones from birds and in classifying payload types, all while maintaining real-time performance. This report details the rationale for a dual-modality design, the specialized training pipelines, and the architectural optimizations that collectively enable efficient and accurate drone surveillance across RGB and IR channels.",
        "arxiv_id": "2507.22650",
        "ARXIVID": "2507.22650",
        "COMMENT": "Does not match any specific criterion but is relevant to multimodal learning and drone detection, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22440": {
        "authors": [
            "Yiya Diao",
            "Changhe Li",
            "Sanyou Zeng",
            "Xinye Cai",
            "Wenjian Luo",
            "Shengxiang Yang",
            "Carlos A. Coello Coello"
        ],
        "title": "Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool",
        "abstract": "arXiv:2507.22440v1 Announce Type: new  Abstract: The Nearest-Better Network (NBN) is a powerful method to visualize sampled data for continuous optimization problems while preserving multiple landscape features. However, the calculation of NBN is very time-consuming, and the extension of the method to combinatorial optimization problems is challenging but very important for analyzing the algorithm's behavior. This paper provides a straightforward theoretical derivation showing that the NBN network essentially functions as the maximum probability transition network for algorithms. This paper also presents an efficient NBN computation method with logarithmic linear time complexity to address the time-consuming issue. By applying this efficient NBN algorithm to the OneMax problem and the Traveling Salesman Problem (TSP), we have made several remarkable discoveries for the first time: The fitness landscape of OneMax exhibits neutrality, ruggedness, and modality features. The primary challenges of TSP problems are ruggedness, modality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and LKH) have limitations when addressing challenges related to modality and deception, respectively. LKH, based on local search operators, fails when there are deceptive solutions near global optima. EAX, which is based on a single population, can efficiently maintain diversity. However, when multiple attraction basins exist, EAX retains individuals within multiple basins simultaneously, reducing inter-basin interaction efficiency and leading to algorithm's stagnation.",
        "arxiv_id": "2507.22440",
        "ARXIVID": "2507.22440",
        "COMMENT": "Does not match any specific criterion but is relevant to optimization and algorithm analysis, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22136": {
        "authors": [
            "Chaofei Qi",
            "Zhitai Liu",
            "Jianbin Qiu"
        ],
        "title": "Color as the Impetus: Transforming Few-Shot Learner",
        "abstract": "arXiv:2507.22136v1 Announce Type: new  Abstract: Humans possess innate meta-learning capabilities, partly attributable to their exceptional color perception. In this paper, we pioneer an innovative viewpoint on few-shot learning by simulating human color perception mechanisms. We propose the ColorSense Learner, a bio-inspired meta-learning framework that capitalizes on inter-channel feature extraction and interactive learning. By strategically emphasizing distinct color information across different channels, our approach effectively filters irrelevant features while capturing discriminative characteristics. Color information represents the most intuitive visual feature, yet conventional meta-learning methods have predominantly neglected this aspect, focusing instead on abstract feature differentiation across categories. Our framework bridges the gap via synergistic color-channel interactions, enabling better intra-class commonality extraction and larger inter-class differences. Furthermore, we introduce a meta-distiller based on knowledge distillation, ColorSense Distiller, which incorporates prior teacher knowledge to augment the student network's meta-learning capacity. We've conducted comprehensive coarse/fine-grained and cross-domain experiments on eleven few-shot benchmarks for validation. Numerous experiments reveal that our methods have extremely strong generalization ability, robustness, and transferability, and effortless handle few-shot classification from the perspective of color perception.",
        "arxiv_id": "2507.22136",
        "ARXIVID": "2507.22136",
        "COMMENT": "Does not match any specific criterion but proposes a bio-inspired meta-learning framework for few-shot learning, which is tangentially related to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22847": {
        "authors": [
            "Han Jiang",
            "Pengda Wang",
            "Xiaoyuan Yi",
            "Xing Xie",
            "Ziang Xiao"
        ],
        "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology",
        "abstract": "arXiv:2507.22847v1 Announce Type: new  Abstract: Social sciences have accumulated a rich body of theories and methodologies for investigating the human mind and behaviors, while offering valuable insights into the design and understanding of Artificial Intelligence (AI) systems. Focusing on psychology as a prominent case, this study explores the interdisciplinary synergy between AI and the field by analyzing 1,006 LLM-related papers published in premier AI venues between 2023 and 2025, along with the 2,544 psychology publications they cite. Through our analysis, we identify key patterns of interdisciplinary integration, locate the psychology domains most frequently referenced, and highlight areas that remain underexplored. We further examine how psychology theories/frameworks are operationalized and interpreted, identify common types of misapplication, and offer guidance for more effective incorporation. Our work provides a comprehensive map of interdisciplinary engagement between AI and psychology, thereby facilitating deeper collaboration and advancing AI systems.",
        "arxiv_id": "2507.22847",
        "ARXIVID": "2507.22847",
        "COMMENT": "Does not match any specific criterion but provides an interdisciplinary analysis of AI and psychology, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22326": {
        "authors": [
            "Qun Ma",
            "Xiao Xue",
            "Ming Zhang",
            "Yifan Shen",
            "Zihan Zhao"
        ],
        "title": "An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem",
        "abstract": "arXiv:2507.22326v1 Announce Type: new  Abstract: Metaverse service is a product of the convergence between Metaverse and service systems, designed to address service-related challenges concerning digital avatars, digital twins, and digital natives within Metaverse. With the rise of large language models (LLMs), agents now play a pivotal role in Metaverse service ecosystem, serving dual functions: as digital avatars representing users in the virtual realm and as service assistants (or NPCs) providing personalized support. However, during the modeling of Metaverse service ecosystems, existing LLM-based agents face significant challenges in bridging virtual-world services with real-world services, particularly regarding issues such as character data fusion, character knowledge association, and ethical safety concerns. This paper proposes an explainable emotion alignment framework for LLM-based agents in Metaverse Service Ecosystem. It aims to integrate factual factors into the decision-making loop of LLM-based agents, systematically demonstrating how to achieve more relational fact alignment for these agents. Finally, a simulation experiment in the Offline-to-Offline food delivery scenario is conducted to evaluate the effectiveness of this framework, obtaining more realistic social emergence.",
        "arxiv_id": "2507.22326",
        "ARXIVID": "2507.22326",
        "COMMENT": "Does not match any specific criteria. Focuses on emotion alignment in LLM-based agents for the Metaverse, which is tangential to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.22885": {
        "authors": [
            "Brent Yi",
            "Chung Min Kim",
            "Justin Kerr",
            "Gina Wu",
            "Rebecca Feng",
            "Anthony Zhang",
            "Jonas Kulhanek",
            "Hongsuk Choi",
            "Yi Ma",
            "Matthew Tancik",
            "Angjoo Kanazawa"
        ],
        "title": "Viser: Imperative, Web-based 3D Visualization in Python",
        "abstract": "arXiv:2507.22885v1 Announce Type: new  Abstract: We present Viser, a 3D visualization library for computer vision and robotics. Viser aims to bring easy and extensible 3D visualization to Python: we provide a comprehensive set of 3D scene and 2D GUI primitives, which can be used independently with minimal setup or composed to build specialized interfaces. This technical report describes Viser's features, interface, and implementation. Key design choices include an imperative-style API and a web-based viewer, which improve compatibility with modern programming patterns and workflows.",
        "arxiv_id": "2507.22885",
        "ARXIVID": "2507.22885",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and robotics visualization tools.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.22101": {
        "authors": [
            "Umair Nawaz",
            "Muhammad Zaigham Zaheer",
            "Fahad Shahbaz Khan",
            "Hisham Cholakkal",
            "Salman Khan",
            "Rao Muhammad Anwer"
        ],
        "title": "AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock",
        "abstract": "arXiv:2507.22101v1 Announce Type: new  Abstract: Crops, fisheries and livestock form the backbone of global food production, essential to feed the ever-growing global population. However, these sectors face considerable challenges, including climate variability, resource limitations, and the need for sustainable management. Addressing these issues requires efficient, accurate, and scalable technological solutions, highlighting the importance of artificial intelligence (AI). This survey presents a systematic and thorough review of more than 200 research works covering conventional machine learning approaches, advanced deep learning techniques (e.g., vision transformers), and recent vision-language foundation models (e.g., CLIP) in the agriculture domain, focusing on diverse tasks such as crop disease detection, livestock health management, and aquatic species monitoring. We further cover major implementation challenges such as data variability and experimental aspects: datasets, performance evaluation metrics, and geographical focus. We finish the survey by discussing potential open research directions emphasizing the need for multimodal data integration, efficient edge-device deployment, and domain-adaptable AI models for diverse farming environments. Rapid growth of evolving developments in this field can be actively tracked on our project page: https://github.com/umair1221/AI-in-Agriculture",
        "arxiv_id": "2507.22101",
        "ARXIVID": "2507.22101",
        "COMMENT": "Does not match any specific criterion but provides a survey on AI in agriculture, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}