{
    "2507.04151": {
        "authors": [
            "Fernando Gabriela Garcia",
            "Spencer Burns",
            "Ryan Shaw",
            "Hunter Young"
        ],
        "title": "Unlocking Compositional Control: Self-Supervision for LVLM-Based Image Generation",
        "abstract": "arXiv:2507.04151v1 Announce Type: new  Abstract: This paper introduces Hierarchical Self-Supervised LVLM (Hi-SSLVLM), a novel generative model designed to significantly advance text-to-image synthesis, particularly for complex and compositionally challenging prompts. Traditional methods often grapple with the high cost of meticulously curated paired image-text datasets and struggle with precise control over fine-grained visual attributes and intricate spatial relationships. Our Hi-SSLVLM addresses these limitations through a unique two-stage self-supervised learning strategy. The first stage, Multi-Granularity Visual-Language Grounding, enables the Large Vision-Language Model (LVLM) backbone to autonomously generate and align hierarchical captions (global and local) to images, cultivating a deep internal semantic understanding without reliance on extensive human annotation. The second stage, Self-Refinement and Guided Image Generation, leverages this acquired knowledge by an Internal Compositional Planning (ICP) mechanism, where the LVLM first formulates detailed textual sub-prompts to guide the image generation process, complemented by a novel Semantic Consistency Loss for precise output alignment. Comprehensive experiments against leading baselines, including Janus-Pro-1B, Stable Diffusion XL 1.0, DeepFloyd IF v1.0, and ControlNet-XL, on multi-dimensional benchmarks such as Gemini-2.0-Flash and InternVL3-78B, demonstrate Hi-SSLVLM's superior performance across all fine-grained metrics. An in-depth ablation study confirms the critical role of each proposed component. Furthermore, human evaluations corroborate our quantitative findings, highlighting Hi-SSLVLM's enhanced fidelity to prompt, compositional accuracy, and overall aesthetic quality, marking a significant step towards more controllable and semantically consistent open-ended text-to-image generation.",
        "arxiv_id": "2507.04151",
        "ARXIVID": "2507.04151",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a novel LVLM-based image generation model with a focus on vision-language integration and compositional control.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.04451": {
        "authors": [
            "Zheyuan Liu",
            "Munan Ning",
            "Qihui Zhang",
            "Shuo Yang",
            "Zhongrui Wang",
            "Yiwei Yang",
            "Xianzhe Xu",
            "Yibing Song",
            "Weihua Chen",
            "Fan Wang",
            "Li Yuan"
        ],
        "title": "CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step",
        "abstract": "arXiv:2507.04451v1 Announce Type: new  Abstract: Current text-to-image (T2I) generation models struggle to align spatial composition with the input text, especially in complex scenes. Even layout-based approaches yield suboptimal spatial control, as their generation process is decoupled from layout planning, making it difficult to refine the layout during synthesis. We present CoT-Diff, a framework that brings step-by-step CoT-style reasoning into T2I generation by tightly integrating Multimodal Large Language Model (MLLM)-driven 3D layout planning with the diffusion process. CoT-Diff enables layout-aware reasoning inline within a single diffusion round: at each denoising step, the MLLM evaluates intermediate predictions, dynamically updates the 3D scene layout, and continuously guides the generation process. The updated layout is converted into semantic conditions and depth maps, which are fused into the diffusion model via a condition-aware attention mechanism, enabling precise spatial control and semantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff significantly improves spatial alignment and compositional fidelity, and outperforms the state-of-the-art method by 34.7% in complex scene spatial accuracy, thereby validating the effectiveness of this entangled generation paradigm.",
        "arxiv_id": "2507.04451",
        "ARXIVID": "2507.04451",
        "COMMENT": "Matches criteria 2 and 5 as it integrates Multimodal Large Language Models (MLLMs) with diffusion models for text-to-image generation, focusing on spatial alignment and compositional fidelity.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.05255": {
        "authors": [
            "Yana Wei",
            "Liang Zhao",
            "Jianjian Sun",
            "Kangheng Lin",
            "Jisheng Yin",
            "Jingcheng Hu",
            "Yinmin Zhang",
            "En Yu",
            "Haoran Lv",
            "Zejia Weng",
            "Jia Wang",
            "Chunrui Han",
            "Yuang Peng",
            "Qi Han",
            "Zheng Ge",
            "Xiangyu Zhang",
            "Daxin Jiang",
            "Vishal M. Patel"
        ],
        "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning",
        "abstract": "arXiv:2507.05255v1 Announce Type: new  Abstract: The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners.",
        "arxiv_id": "2507.05255",
        "ARXIVID": "2507.05255",
        "COMMENT": "Matches criterion 2 as it focuses on multimodal reinforcement learning and reasoning in vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.04664": {
        "authors": [
            "Tao Zhang",
            "Shiqing Wei",
            "Shihao Chen",
            "Wenling Yu",
            "Muying Luo",
            "Shunping Ji"
        ],
        "title": "VectorLLM: Human-like Extraction of Structured Building Contours vis Multimodal LLMs",
        "abstract": "arXiv:2507.04664v1 Announce Type: new  Abstract: Automatically extracting vectorized building contours from remote sensing imagery is crucial for urban planning, population estimation, and disaster assessment. Current state-of-the-art methods rely on complex multi-stage pipelines involving pixel segmentation, vectorization, and polygon refinement, which limits their scalability and real-world applicability. Inspired by the remarkable reasoning capabilities of Large Language Models (LLMs), we introduce VectorLLM, the first Multi-modal Large Language Model (MLLM) designed for regular building contour extraction from remote sensing images. Unlike existing approaches, VectorLLM performs corner-point by corner-point regression of building contours directly, mimicking human annotators' labeling process. Our architecture consists of a vision foundation backbone, an MLP connector, and an LLM, enhanced with learnable position embeddings to improve spatial understanding capability. Through comprehensive exploration of training strategies including pretraining, supervised fine-tuning, and preference optimization across WHU, WHU-Mix, and CrowdAI datasets, VectorLLM significantly outperformed the previous SOTA methods by 5.6 AP, 7.1 AP, 13.6 AP, respectively in the three datasets. Remarkably, VectorLLM exhibits strong zero-shot performance on unseen objects including aircraft, water bodies, and oil tanks, highlighting its potential for unified modeling of diverse remote sensing object contour extraction tasks. Overall, this work establishes a new paradigm for vector extraction in remote sensing, leveraging the topological reasoning capabilities of LLMs to achieve both high accuracy and exceptional generalization. All the codes and weights will be published for promoting community development.",
        "arxiv_id": "2507.04664",
        "ARXIVID": "2507.04664",
        "COMMENT": "Matches criteria 2 and 5 as it explores a Multi-modal Large Language Model (MLLM) for vision-language integration and focuses on combining image understanding tasks with LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.04590": {
        "authors": [
            "Rui Meng",
            "Ziyan Jiang",
            "Ye Liu",
            "Mingyi Su",
            "Xinyi Yang",
            "Yuepeng Fu",
            "Can Qin",
            "Zeyuan Chen",
            "Ran Xu",
            "Caiming Xiong",
            "Yingbo Zhou",
            "Wenhu Chen",
            "Semih Yavuz"
        ],
        "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents",
        "abstract": "arXiv:2507.04590v1 Announce Type: new  Abstract: Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.",
        "arxiv_id": "2507.04590",
        "ARXIVID": "2507.04590",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 5 (Integration of Image/Video and Large Language Models) due to its focus on unified embeddings for text, image, video, and visual documents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.03262": {
        "authors": [
            "Song Mao",
            "Yang Chen",
            "Pinglong Cai",
            "Ding Wang",
            "Guohang Yan",
            "Zhi Yu",
            "Botian Shi"
        ],
        "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
        "abstract": "arXiv:2507.03262v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) increasingly adopt multiple vision encoders to capture diverse visual information, ranging from coarse semantics to fine grained details. While this approach is intended to enhance visual understanding capability, we observe that the performance gains from adding encoders often diminish and can even lead to performance degradation, a phenomenon we term encoder redundancy. This paper presents a systematic investigation into this issue. Through comprehensive ablation studies on state of the art multi encoder MLLMs, we empirically demonstrate that significant redundancy exists. To quantify each encoder's unique contribution, we propose a principled metric: the Conditional Utilization Rate (CUR). Building on CUR, we introduce the Information Gap (IG) to capture the overall disparity in encoder utility within a model.Our experiments reveal that certain vision encoders contribute little, or even negatively, to overall performance, confirming substantial redundancy. Our experiments reveal that certain vision encoders contribute minimally, or even negatively, to the model's performance, confirming the prevalence of redundancy. These findings highlight critical inefficiencies in current multi encoder designs and establish that our proposed metrics can serve as valuable diagnostic tools for developing more efficient and effective multimodal architectures.",
        "arxiv_id": "2507.03262",
        "ARXIVID": "2507.03262",
        "COMMENT": "Matches criterion 2 as it investigates multimodal large language models with multiple vision encoders.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.03578": {
        "authors": [
            "Yana Hasson",
            "Pauline Luc",
            "Liliane Momeni",
            "Maks Ovsjanikov",
            "Guillaume Le Moing",
            "Alina Kuznetsova",
            "Ira Ktena",
            "Jennifer J. Sun",
            "Skanda Koppula",
            "Dilara Gokay",
            "Joseph Heyward",
            "Etienne Pot",
            "Andrew Zisserman"
        ],
        "title": "SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications",
        "abstract": "arXiv:2507.03578v1 Announce Type: new  Abstract: In recent years, there has been a proliferation of spatiotemporal foundation models in different scientific disciplines. While promising, these models are often domain-specific and are only assessed within the particular applications for which they are designed. Given that many tasks can be represented as video modeling problems, video foundation models (ViFMs) hold considerable promise as general-purpose domain-agnostic approaches. However, it is not known whether the knowledge acquired on large-scale but potentially out-of-domain data can be effectively transferred across diverse scientific disciplines, and if a single, pretrained ViFM can be competitive with domain-specific baselines. To address this, we introduce SciVid, a comprehensive benchmark comprising five *Sci*entific *Vid*eo tasks, across medical computer vision, animal behavior, and weather forecasting. We adapt six leading ViFMs to SciVid using simple trainable readout modules, establishing strong baselines and demonstrating the potential for effective transfer learning. Specifically, we show that state-of-the-art results can be obtained in several applications by leveraging the general-purpose representations from ViFM backbones. Furthermore, our results reveal the limitations of existing ViFMs, and highlight opportunities for the development of generalizable models for high-impact scientific applications. We release our code at https://github.com/google-deepmind/scivid to facilitate further research in the development of ViFMs.",
        "arxiv_id": "2507.03578",
        "ARXIVID": "2507.03578",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for evaluating video models in scientific applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.04036": {
        "authors": [
            "Jingwei Shi",
            "Zeyu Zhang",
            "Biao Wu",
            "Yanjie Liang",
            "Meng Fang",
            "Ling Chen",
            "Yang Zhao"
        ],
        "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
        "abstract": "arXiv:2507.04036v1 Announce Type: new  Abstract: We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.",
        "arxiv_id": "2507.04036",
        "ARXIVID": "2507.04036",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates multimodal content generation for presentation videos.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.02948": {
        "authors": [
            "Zhiyi Hou",
            "Enhui Ma",
            "Fang Li",
            "Zhiyi Lai",
            "Kalok Ho",
            "Zhanqian Wu",
            "Lijun Zhou",
            "Long Chen",
            "Chitian Sun",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Kaicheng Yu"
        ],
        "title": "DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction",
        "abstract": "arXiv:2507.02948v1 Announce Type: new  Abstract: Autonomous driving has seen significant progress, driven by extensive real-world data. However, in long-tail scenarios, accurately predicting the safety of the ego vehicle's future motion remains a major challenge due to uncertainties in dynamic environments and limitations in data coverage. In this work, we aim to explore whether it is possible to enhance the motion risk prediction capabilities of Vision-Language Models (VLM) by synthesizing high-risk motion data. Specifically, we introduce a Bird's-Eye View (BEV) based motion simulation method to model risks from three aspects: the ego-vehicle, other vehicles, and the environment. This allows us to synthesize plug-and-play, high-risk motion data suitable for VLM training, which we call DriveMRP-10K. Furthermore, we design a VLM-agnostic motion risk estimation framework, named DriveMRP-Agent. This framework incorporates a novel information injection strategy for global context, ego-vehicle perspective, and trajectory projection, enabling VLMs to effectively reason about the spatial relationships between motion waypoints and the environment. Extensive experiments demonstrate that by fine-tuning with DriveMRP-10K, our DriveMRP-Agent framework can significantly improve the motion risk prediction performance of multiple VLM baselines, with the accident recognition accuracy soaring from 27.13% to 88.03%. Moreover, when tested via zero-shot evaluation on an in-house real-world high-risk motion dataset, DriveMRP-Agent achieves a significant performance leap, boosting the accuracy from base_model's 29.42% to 68.50%, which showcases the strong generalization capabilities of our method in real-world scenarios.",
        "arxiv_id": "2507.02948",
        "ARXIVID": "2507.02948",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores enhancing Vision-Language Models with synthetic motion data for motion risk prediction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.04289": {
        "authors": [
            "Shenxi Liu",
            "Kan Li",
            "Mingyang Zhao",
            "Yuhang Tian",
            "Bin Li",
            "Shoujun Zhou",
            "Hongliang Li",
            "Fuxia Yang"
        ],
        "title": "M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding",
        "abstract": "arXiv:2507.04289v1 Announce Type: new  Abstract: With the rapid progress of artificial intelligence (AI) in multi-modal understanding, there is increasing potential for video comprehension technologies to support professional domains such as medical education. However, existing benchmarks suffer from two primary limitations: (1) Linguistic Singularity: they are largely confined to English, neglecting the need for multilingual resources; and (2) Shallow Reasoning: their questions are often designed for surface-level information retrieval, failing to properly assess deep multi-modal integration. To address these limitations, we present M3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop reasoning in Medical instructional video understanding. M3-Med consists of medical questions paired with corresponding video segments, annotated by a team of medical experts. A key innovation of M3-Med is its multi-hop reasoning task, which requires a model to first locate a key entity in the text, then find corresponding visual evidence in the video, and finally synthesize information across both modalities to derive the answer. This design moves beyond simple text matching and poses a substantial challenge to a model's deep cross-modal understanding capabilities. We define two tasks: Temporal Answer Grounding in Single Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We evaluated several state-of-the-art models and Large Language Models (LLMs) on M3-Med. The results reveal a significant performance gap between all models and human experts, especially on the complex multi-hop questions where model performance drops sharply. M3-Med effectively highlights the current limitations of AI models in deep cross-modal reasoning within specialized domains and provides a new direction for future research.",
        "arxiv_id": "2507.04289",
        "ARXIVID": "2507.04289",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on multi-hop reasoning in medical instructional video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.04749": {
        "authors": [
            "Chengyu Wang",
            "Isabella Bennett",
            "Henry Scott",
            "Liang Zhang",
            "Mei Chen",
            "Hao Li",
            "Rui Zhao"
        ],
        "title": "MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images",
        "abstract": "arXiv:2507.04749v1 Announce Type: new  Abstract: We present MatDecompSDF, a novel framework for recovering high-fidelity 3D shapes and decomposing their physically-based material properties from multi-view images. The core challenge of inverse rendering lies in the ill-posed disentanglement of geometry, materials, and illumination from 2D observations. Our method addresses this by jointly optimizing three neural components: a neural Signed Distance Function (SDF) to represent complex geometry, a spatially-varying neural field for predicting PBR material parameters (albedo, roughness, metallic), and an MLP-based model for capturing unknown environmental lighting. The key to our approach is a physically-based differentiable rendering layer that connects these 3D properties to the input images, allowing for end-to-end optimization. We introduce a set of carefully designed physical priors and geometric regularizations, including a material smoothness loss and an Eikonal loss, to effectively constrain the problem and achieve robust decomposition. Extensive experiments on both synthetic and real-world datasets (e.g., DTU) demonstrate that MatDecompSDF surpasses state-of-the-art methods in geometric accuracy, material fidelity, and novel view synthesis. Crucially, our method produces editable and relightable assets that can be seamlessly integrated into standard graphics pipelines, validating its practical utility for digital content creation.",
        "arxiv_id": "2507.04749",
        "ARXIVID": "2507.04749",
        "COMMENT": "Matches criteria 4 as it focuses on a novel framework for high-fidelity 3D shape and material decomposition using neural fields and physically-based rendering.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.04631": {
        "authors": [
            "Yun Wang",
            "Longguang Wang",
            "Chenghao Zhang",
            "Yongjian Zhang",
            "Zhanjie Zhang",
            "Ao Ma",
            "Chenyou Fan",
            "Tin Lun Lam",
            "Junjie Hu"
        ],
        "title": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts",
        "abstract": "arXiv:2507.04631v1 Announce Type: new  Abstract: Recently, learning-based stereo matching networks have advanced significantly. However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets. Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge. To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction. Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at \\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.",
        "arxiv_id": "2507.04631",
        "ARXIVID": "2507.04631",
        "COMMENT": "Matches criteria 4 as it discusses Vision Foundation Models (VFMs) and their application to stereo matching with a novel framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.03905": {
        "authors": [
            "Rang Meng",
            "Yan Wang",
            "Weipeng Wu",
            "Ruobing Zheng",
            "Yuming Li",
            "Chenguang Ma"
        ],
        "title": "EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation",
        "abstract": "arXiv:2507.03905v1 Announce Type: new  Abstract: Human animation recently has advanced rapidly, achieving increasingly realistic and vivid results, especially with the integration of large-scale video generation models. However, the slow inference speed and high computational cost of these large models bring significant challenges for practical applications. Additionally, various tasks in human animation, such as lip-syncing, audio-driven full-body animation, and video generation from start and end frames, often require different specialized models. The introduction of large video models has not alleviated this dilemma. This raises an important question: Can we make human animation Faster, Higher in quality, Stronger in generalization, and make various tasks Together in one model? To address this, we dive into video generation models and discover that the devil lies in the details: Inspired by MAE, we propose a novel unified Multi-Task paradigm for human animation, treating diverse generation tasks as spatial-temporal local reconstructions, requiring modifications only on the input side; Given the interplay and division among multi-modal conditions including text, image, and audio, we introduce a multi-modal decoupled cross-attention module to fuse multi-modals in a divide-and-conquer manner; We propose a new SFT+Reward alternating training paradigm, enabling the minimal model with 1.3B parameters to achieve generation quality comparable to models with 10 times the parameters count. Through these innovations, our work paves the way for efficient, high-quality, and versatile digital human generation, addressing both performance and practicality challenges in the field. Extensive experiments demonstrate that EchoMimicV3 outperforms existing models in both facial and semi-body video generation, providing precise text-based control for creating videos in a wide range of scenarios.",
        "arxiv_id": "2507.03905",
        "ARXIVID": "2507.03905",
        "COMMENT": "Matches criteria 2 and 5 as it discusses a multi-modal model for human animation integrating text, image, and audio, and focuses on efficient video generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.04702": {
        "authors": [
            "Feng Yue",
            "Zhaoxing Zhang",
            "Junming Jiao",
            "Zhengyu Liang",
            "Shiwen Cao",
            "Feifei Zhang",
            "Rong Shen"
        ],
        "title": "Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning",
        "abstract": "arXiv:2507.04702v1 Announce Type: new  Abstract: Temporal Video Grounding (TVG), which requires pinpointing relevant temporal segments from video based on language query, has always been a highly challenging task in the field of video understanding. Videos often have a larger volume of information and redundancy than texts or images. Models should present comprehensive understanding of the whole video to accurately retrieve query-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large Language Model (Video-MLLM) for the temporal video grounding task via multimodal temporal sensing reinforcement. Specifically, during the preprocessing stage of our pipeline, we employ Self-adaptive Attention Allocation (SAA) method based on frame content variation to efficiently use the MLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is also utilized to strengthen our model's capability to perceive the boundaries of events in the video. In the fine-tuning part of our pipeline, we creatively apply Partial Irrelevance Refusing-based Group Relative Policy Optimization (PIR-GRPO) in TVG area to foster model's temporal reasoning from not only accepting relevant video-query pairs but also refusing irrelevant ones. Experiments demonstrate that our method accomplishes a notable advantage over SOTA solutions by around 3.5% on both the original QVHighlights testbench and its corrected version with more reasonable ground truth annotations.",
        "arxiv_id": "2507.04702",
        "ARXIVID": "2507.04702",
        "COMMENT": "Matches criterion 6 as it introduces a Video-MLLM for temporal video grounding, which is a video understanding task.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.05259": {
        "authors": [
            "Chun-Hsiao Yeh",
            "Yilin Wang",
            "Nanxuan Zhao",
            "Richard Zhang",
            "Yuheng Li",
            "Yi Ma",
            "Krishna Kumar Singh"
        ],
        "title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing",
        "abstract": "arXiv:2507.05259v1 Announce Type: new  Abstract: Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark.",
        "arxiv_id": "2507.05259",
        "ARXIVID": "2507.05259",
        "COMMENT": "Matches criterion 2 as it explores a Multimodal Large Language Model (MLLM) for complex instruction-based image editing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.05201": {
        "authors": [
            "Andrew Sellergren (Dima)",
            "Sahar Kazemzadeh (Dima)",
            "Tiam Jaroensri (Dima)",
            "Atilla Kiraly (Dima)",
            "Madeleine Traverse (Dima)",
            "Timo Kohlberger (Dima)",
            "Shawn Xu (Dima)",
            "Fayaz Jamil (Dima)",
            "C\\'ian Hughes (Dima)",
            "Charles Lau (Dima)",
            "Justin Chen (Dima)",
            "Fereshteh Mahvar (Dima)",
            "Liron Yatziv (Dima)",
            "Tiffany Chen (Dima)",
            "Bram Sterling (Dima)",
            "Stefanie Anna Baby (Dima)",
            "Susanna Maria Baby (Dima)",
            "Jeremy Lai (Dima)",
            "Samuel Schmidgall (Dima)",
            "Lu Yang (Dima)",
            "Kejia Chen (Dima)",
            "Per Bjornsson (Dima)",
            "Shashir Reddy (Dima)",
            "Ryan Brush (Dima)",
            "Kenneth Philbrick (Dima)",
            "Howard Hu (Dima)",
            "Howard Yang (Dima)",
            "Richa Tiwari (Dima)",
            "Sunny Jansen (Dima)",
            "Preeti Singh (Dima)",
            "Yun Liu (Dima)",
            "Shekoofeh Azizi (Dima)",
            "Aishwarya Kamath (Dima)",
            "Johan Ferret (Dima)",
            "Shreya Pathak (Dima)",
            "Nino Vieillard (Dima)",
            "Ramona Merhej (Dima)",
            "Sarah Perrin (Dima)",
            "Tatiana Matejovicova (Dima)",
            "Alexandre Ram\\'e (Dima)",
            "Morgane Riviere (Dima)",
            "Louis Rouillard (Dima)",
            "Thomas Mesnard (Dima)",
            "Geoffrey Cideron (Dima)",
            "Jean-bastien Grill (Dima)",
            "Sabela Ramos (Dima)",
            "Edouard Yvinec (Dima)",
            "Michelle Casbon (Dima)",
            "Elena Buchatskaya (Dima)",
            "Jean-Baptiste Alayrac (Dima)",
            "Dmitry (Dima)",
            "Lepikhin",
            "Vlad Feinberg",
            "Sebastian Borgeaud",
            "Alek Andreev",
            "Cassidy Hardin",
            "Robert Dadashi",
            "L\\'eonard Hussenot",
            "Armand Joulin",
            "Olivier Bachem",
            "Yossi Matias",
            "Katherine Chou",
            "Avinatan Hassidim",
            "Kavi Goel",
            "Clement Farabet",
            "Joelle Barral",
            "Tris Warkentin",
            "Jonathon Shlens",
            "David Fleet",
            "Victor Cotruta",
            "Omar Sanseviero",
            "Gus Martins",
            "Phoebe Kirk",
            "Anand Rao",
            "Shravya Shetty",
            "David F. Steiner",
            "Can Kirmizibayrak",
            "Rory Pilgrim",
            "Daniel Golden",
            "Lin Yang"
        ],
        "title": "MedGemma Technical Report",
        "abstract": "arXiv:2507.05201v1 Announce Type: new  Abstract: Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.",
        "arxiv_id": "2507.05201",
        "ARXIVID": "2507.05201",
        "COMMENT": "Matches criterion 2 and 4 as it introduces a medical vision-language foundation model with advanced reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.03304": {
        "authors": [
            "Hai Huang",
            "Yan Xia",
            "Sashuai Zhou",
            "Hanting Wang",
            "Shulei Wang",
            "Zhou Zhao"
        ],
        "title": "Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations",
        "abstract": "arXiv:2507.03304v1 Announce Type: new  Abstract: Domain Generalization (DG) aims to enhance model robustness in unseen or distributionally shifted target domains through training exclusively on source domains. Although existing DG techniques, such as data manipulation, learning strategies, and representation learning, have shown significant progress, they predominantly address single-modal data. With the emergence of numerous multi-modal datasets and increasing demand for multi-modal tasks, a key challenge in Multi-modal Domain Generalization (MMDG) has emerged: enabling models trained on multi-modal sources to generalize to unseen target distributions within the same modality set. Due to the inherent differences between modalities, directly transferring methods from single-modal DG to MMDG typically yields sub-optimal results. These methods often exhibit randomness during generalization due to the invisibility of target domains and fail to consider inter-modal consistency. Applying these methods independently to each modality in the MMDG setting before combining them can lead to divergent generalization directions across different modalities, resulting in degraded generalization capabilities. To address these challenges, we propose a novel approach that leverages Unified Representations to map different paired modalities together, effectively adapting DG methods to MMDG by enabling synchronized multi-modal improvements within the unified space. Additionally, we introduce a supervised disentanglement framework that separates modal-general and modal-specific information, further enhancing the alignment of unified representations. Extensive experiments on benchmark datasets, including EPIC-Kitchens and Human-Animal-Cartoon, demonstrate the effectiveness and superiority of our method in enhancing multi-modal domain generalization.",
        "arxiv_id": "2507.03304",
        "ARXIVID": "2507.03304",
        "COMMENT": "Matches criterion 2 and 5 as it focuses on multi-modal domain generalization and unified representations for vision-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.04635": {
        "authors": [
            "Zhicheng Zhang",
            "Wuyou Xia",
            "Chenxi Zhao",
            "Zhou Yan",
            "Xiaoqiang Liu",
            "Yongjie Zhu",
            "Wenyu Qin",
            "Pengfei Wan",
            "Di Zhang",
            "Jufeng Yang"
        ],
        "title": "MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding",
        "abstract": "arXiv:2507.04635v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) recently showed strong capacity in integrating data among multiple modalities, empowered by a generalizable attention architecture. Advanced methods predominantly focus on language-centric tuning while less exploring multimodal tokens mixed through attention, posing challenges in high-level tasks that require fine-grained cognition and emotion understanding. In this work, we identify the attention deficit disorder problem in multimodal learning, caused by inconsistent cross-modal attention and layer-by-layer decayed attention activation. To address this, we propose a novel attention mechanism, termed MOdular Duplex Attention (MODA), simultaneously conducting the inner-modal refinement and inter-modal interaction. MODA employs a correct-after-align strategy to effectively decouple modality alignment from cross-layer token mixing. In the alignment phase, tokens are mapped to duplex modality spaces based on the basis vectors, enabling the interaction between visual and language modality. Further, the correctness of attention scores is ensured through adaptive masked attention, which enhances the model's flexibility by allowing customizable masking patterns for different modalities. Extensive experiments on 21 benchmark datasets verify the effectiveness of MODA in perception, cognition, and emotion tasks. Source code and demo are available in https://zzcheng.top/MODA.",
        "arxiv_id": "2507.04635",
        "ARXIVID": "2507.04635",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a novel attention mechanism for multimodal perception and cognition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.03745": {
        "authors": [
            "Akio Kodaira",
            "Tingbo Hou",
            "Ji Hou",
            "Masayoshi Tomizuka",
            "Yue Zhao"
        ],
        "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation",
        "abstract": "arXiv:2507.03745v1 Announce Type: new  Abstract: Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: this https URL.",
        "arxiv_id": "2507.03745",
        "ARXIVID": "2507.03745",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) and criterion 6 (Video Understanding) as it focuses on real-time text-to-video generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.03254": {
        "authors": [
            "Bruce Yang",
            "Xinfeng He",
            "Huan Gao",
            "Yifan Cao",
            "Xiaofan Li",
            "David Hsu"
        ],
        "title": "CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs",
        "abstract": "arXiv:2507.03254v1 Announce Type: new  Abstract: Effective prompt design is essential for improving the planning capabilities of large language model (LLM)-driven agents. However, existing structured prompting strategies are typically limited to single-agent, plan-only settings, and often evaluate performance solely based on task accuracy - overlooking critical factors such as token efficiency, modularity, and scalability in multi-agent environments. To address these limitations, we introduce CodeAgents, a prompting framework that codifies multi-agent reasoning and enables structured, token-efficient planning in multi-agent systems. In CodeAgents, all components of agent interaction - Task, Plan, Feedback, system roles, and external tool invocations - are codified into modular pseudocode enriched with control structures (e.g., loops, conditionals), boolean logic, and typed variables. This design transforms loosely connected agent plans into cohesive, interpretable, and verifiable multi-agent reasoning programs. We evaluate the proposed framework across three diverse benchmarks - GAIA, HotpotQA, and VirtualHome - using a range of representative LLMs. Results show consistent improvements in planning performance, with absolute gains of 3-36 percentage points over natural language prompting baselines. On VirtualHome, our method achieves a new state-of-the-art success rate of 56%. In addition, our approach reduces input and output token usage by 55-87% and 41-70%, respectively, underscoring the importance of token-aware evaluation metrics in the development of scalable multi-agent LLM systems. The code and resources are available at: https://anonymous.4open.science/r/CodifyingAgent-5A86",
        "arxiv_id": "2507.03254",
        "ARXIVID": "2507.03254",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel framework for multi-agent reasoning and planning, which is relevant to spatial intelligence and embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.04017": {
        "authors": [
            "Hongrui Shi",
            "Lisa Norton",
            "Lucy Ridding",
            "Simon Rolph",
            "Tom August",
            "Claire M Wood",
            "Lan Qie",
            "Petra Bosilj",
            "James M Brown"
        ],
        "title": "Habitat Classification from Ground-Level Imagery Using Deep Neural Networks",
        "abstract": "arXiv:2507.04017v1 Announce Type: new  Abstract: Habitat assessment at local scales -- critical for enhancing biodiversity and guiding conservation priorities -- often relies on expert field survey that can be costly, motivating the exploration of AI-driven tools to automate and refine this process. While most AI-driven habitat mapping depends on remote sensing, it is often constrained by sensor availability, weather, and coarse resolution. In contrast, ground-level imagery captures essential structural and compositional cues invisible from above and remains underexplored for robust, fine-grained habitat classification. This study addresses this gap by applying state-of-the-art deep neural network architectures to ground-level habitat imagery. Leveraging data from the UK Countryside Survey covering 18 broad habitat types, we evaluate two families of models -- convolutional neural networks (CNNs) and vision transformers (ViTs) -- under both supervised and supervised contrastive learning paradigms. Our results demonstrate that ViTs consistently outperform state-of-the-art CNN baselines on key classification metrics (Top-3 accuracy = 91\\%, MCC = 0.66) and offer more interpretable scene understanding tailored to ground-level images. Moreover, supervised contrastive learning significantly reduces misclassification rates among visually similar habitats (e.g., Improved vs. Neutral Grassland), driven by a more discriminative embedding space. Finally, our best model performs on par with experienced ecological experts in habitat classification from images, underscoring the promise of expert-level automated assessment. By integrating advanced AI with ecological expertise, this research establishes a scalable, cost-effective framework for ground-level habitat monitoring to accelerate biodiversity conservation and inform land-use decisions at the national scale.",
        "arxiv_id": "2507.04017",
        "ARXIVID": "2507.04017",
        "COMMENT": "Matches criterion 4 as it applies vision foundation models to habitat classification and demonstrates their real-world use case.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.03019": {
        "authors": [
            "Shuo Yang",
            "Yuwei Niu",
            "Yuyang Liu",
            "Yang Ye",
            "Bin Lin",
            "Li Yuan"
        ],
        "title": "Look-Back: Implicit Visual Re-focusing in MLLM Reasoning",
        "abstract": "arXiv:2507.03019v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in multimodal reasoning. However, they often excessively rely on textual information during the later stages of inference, neglecting the crucial integration of visual input. Current methods typically address this by explicitly injecting visual information to guide the reasoning process. In this work, through an analysis of MLLM attention patterns, we made an intriguing observation: with appropriate guidance, MLLMs can spontaneously re-focus their attention on visual inputs during the later stages of reasoning, even without explicit visual information injection. This spontaneous shift in focus suggests that MLLMs are intrinsically capable of performing visual fusion reasoning. Building on this insight, we introduce Look-Back, an implicit approach designed to guide MLLMs to ``look back\" at visual information in a self-directed manner during reasoning. Look-Back empowers the model to autonomously determine when, where, and how to re-focus on visual inputs, eliminating the need for explicit model-structure constraints or additional input. We demonstrate that Look-Back significantly enhances the model's reasoning and perception capabilities, as evidenced by extensive empirical evaluations on multiple multimodal benchmarks.",
        "arxiv_id": "2507.03019",
        "ARXIVID": "2507.03019",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on improving multimodal reasoning in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.03698": {
        "authors": [
            "Zhiling Yan",
            "Sifan Song",
            "Dingjie Song",
            "Yiwei Li",
            "Rong Zhou",
            "Weixiang Sun",
            "Zhennong Chen",
            "Sekeun Kim",
            "Hui Ren",
            "Tianming Liu",
            "Quanzheng Li",
            "Xiang Li",
            "Lifang He",
            "Lichao Sun"
        ],
        "title": "SAMed-2: Selective Memory Enhanced Medical Segment Anything Model",
        "abstract": "arXiv:2507.03698v1 Announce Type: new  Abstract: Recent \"segment anything\" efforts show promise by learning from large-scale data, but adapting such models directly to medical images remains challenging due to the complexity of medical data, noisy annotations, and continual learning requirements across diverse modalities and anatomical structures. In this work, we propose SAMed-2, a new foundation model for medical image segmentation built upon the SAM-2 architecture. Specifically, we introduce a temporal adapter into the image encoder to capture image correlations and a confidence-driven memory mechanism to store high-certainty features for later retrieval. This memory-based strategy counters the pervasive noise in large-scale medical datasets and mitigates catastrophic forgetting when encountering new tasks or modalities. To train and evaluate SAMed-2, we curate MedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21 medical segmentation tasks. Our experiments on both internal benchmarks and 10 external datasets demonstrate superior performance over state-of-the-art baselines in multi-task scenarios. The code is available at: https://github.com/ZhilingYan/Medical-SAM-Bench.",
        "arxiv_id": "2507.03698",
        "ARXIVID": "2507.03698",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on a foundation model for medical image segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.04049": {
        "authors": [
            "Ziying Song",
            "Lin Liu",
            "Hongyu Pan",
            "Bencheng Liao",
            "Mingzhe Guo",
            "Lei Yang",
            "Yongchang Zhang",
            "Shaoqing Xu",
            "Caiyan Jia",
            "Yadan Luo"
        ],
        "title": "Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation",
        "abstract": "arXiv:2507.04049v1 Announce Type: new  Abstract: Most end-to-end autonomous driving methods rely on imitation learning from single expert demonstrations, often leading to conservative and homogeneous behaviors that limit generalization in complex real-world scenarios. In this work, we propose DIVER, an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories. At the core of DIVER lies a reinforced diffusion-based generation mechanism. First, the model conditions on map elements and surrounding agents to generate multiple reference trajectories from a single ground-truth trajectory, alleviating the limitations of imitation learning that arise from relying solely on single expert demonstrations. Second, reinforcement learning is employed to guide the diffusion process, where reward-based supervision enforces safety and diversity constraints on the generated trajectories, thereby enhancing their practicality and generalization capability. Furthermore, to address the limitations of L2-based open-loop metrics in capturing trajectory diversity, we propose a novel Diversity metric to evaluate the diversity of multi-mode predictions.Extensive experiments on the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop nuScenes dataset, demonstrate that DIVER significantly improves trajectory diversity, effectively addressing the mode collapse problem inherent in imitation learning.",
        "arxiv_id": "2507.04049",
        "ARXIVID": "2507.04049",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for trajectory generation in autonomous driving, integrating reinforcement learning and diffusion-based generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.04559": {
        "authors": [
            "Dawit Mureja Argaw",
            "Xian Liu",
            "Joon Son Chung",
            "Ming-Yu Liu",
            "Fitsum Reda"
        ],
        "title": "MambaVideo for Discrete Video Tokenization with Channel-Split Quantization",
        "abstract": "arXiv:2507.04559v1 Announce Type: new  Abstract: Discrete video tokenization is essential for efficient autoregressive generative modeling due to the high dimensionality of video data. This work introduces a state-of-the-art discrete video tokenizer with two key contributions. First, we propose a novel Mamba-based encoder-decoder architecture that overcomes the limitations of previous sequencebased tokenizers. Second, we introduce a new quantization scheme, channel-split quantization, which significantly enhances the representational power of quantized latents while preserving the token count. Our model sets a new state-of-the-art, outperforming both causal 3D convolutionbased and Transformer-based approaches across multiple datasets. Experimental results further demonstrate its robustness as a tokenizer for autoregressive video generation.",
        "arxiv_id": "2507.04559",
        "ARXIVID": "2507.04559",
        "COMMENT": "Matches criterion 6 as it introduces a novel tokenizer for video generation tasks, which is relevant to video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.04511": {
        "authors": [
            "Xinhua Lu",
            "Runhe Lai",
            "Yanqi Wu",
            "Kanghao Chen",
            "Wei-Shi Zheng",
            "Ruixuan Wang"
        ],
        "title": "FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection",
        "abstract": "arXiv:2507.04511v1 Announce Type: new  Abstract: Pre-trained vision-language models (VLMs) have advanced out-of-distribution (OOD) detection recently. However, existing CLIP-based methods often focus on learning OOD-related knowledge to improve OOD detection, showing limited generalization or reliance on external large-scale auxiliary datasets. In this study, instead of delving into the intricate OOD-related knowledge, we propose an innovative CLIP-based framework based on Forced prompt leArning (FA), designed to make full use of the In-Distribution (ID) knowledge and ultimately boost the effectiveness of OOD detection. Our key insight is to learn a prompt (i.e., forced prompt) that contains more diversified and richer descriptions of the ID classes beyond the textual semantics of class labels. Specifically, it promotes better discernment for ID images, by forcing more notable semantic similarity between ID images and the learnable forced prompt. Moreover, we introduce a forced coefficient, encouraging the forced prompt to learn more comprehensive and nuanced descriptions of the ID classes. In this way, FA is capable of achieving notable improvements in OOD detection, even when trained without any external auxiliary datasets, while maintaining an identical number of trainable parameters as CoOp. Extensive empirical evaluations confirm our method consistently outperforms current state-of-the-art methods. Code is available at https://github.com/0xFAFA/FA.",
        "arxiv_id": "2507.04511",
        "ARXIVID": "2507.04511",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it focuses on vision-language models for out-of-distribution detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.05011": {
        "authors": [
            "Maxence Boels",
            "Harry Robertshaw",
            "Alejandro Granados",
            "Prokar Dasgupta",
            "Sebastien Ourselin"
        ],
        "title": "When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning",
        "abstract": "arXiv:2507.05011v1 Announce Type: new  Abstract: Surgical action planning requires predicting future instrument-verb-target triplets for real-time assistance. While teleoperated robotic surgery provides natural expert demonstrations for imitation learning (IL), reinforcement learning (RL) could potentially discover superior strategies through exploration. We present the first comprehensive comparison of IL versus RL for surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and 33.6% next frame prediction mAP with smooth planning degradation to 29.2% at 10-second horizons. We evaluated three RL variants: world model-based RL, direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches underperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while direct video RL achieved only 15.9%. Our analysis reveals that distribution matching on expert-annotated test sets systematically favors IL over potentially valid RL policies that differ from training demonstrations. This challenges assumptions about RL superiority in sequential decision making and provides crucial insights for surgical AI development.",
        "arxiv_id": "2507.05011",
        "ARXIVID": "2507.05011",
        "COMMENT": "Matches criteria 3 as it provides a novel comparison of IL and RL for surgical action planning, which is relevant to embodied/robotic AI methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.04116": {
        "authors": [
            "Fred Lydeard",
            "Bashar I. Ahmad",
            "Simon Godsill"
        ],
        "title": "Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking",
        "abstract": "arXiv:2507.04116v1 Announce Type: new  Abstract: This paper presents a computationally efficient multi-object tracking approach that can minimise track breaks (e.g., in challenging environments and against agile targets), learn the measurement model parameters on-line (e.g., in dynamically changing scenes) and infer the class of the tracked objects, if joint tracking and kinematic behaviour classification is sought. It capitalises on the flexibilities offered by the integrated Gaussian process as a motion model and the convenient statistical properties of non-homogeneous Poisson processes as a suitable observation model. This can be combined with the proposed effective track revival / stitching mechanism. We accordingly introduce the two robust and adaptive trackers, Gaussian and Poisson Process with Classification (GaPP-Class) and GaPP with Revival and Classification (GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme that efficiently integrates track management and hyperparameter learning (including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class with the addition of a Markov Chain Monte Carlo kernel applied to each particle permitting track revival and stitching (e.g., within a few time steps after deleting a trajectory). Performance evaluation and benchmarking using synthetic and real data show that GaPP-Class and GaPP-ReaCtion outperform other state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly reduces track breaks (e.g., by around 30% from real radar data and markedly more from simulated data).",
        "arxiv_id": "2507.04116",
        "ARXIVID": "2507.04116",
        "COMMENT": "Matches criterion 3 as it introduces novel methods for multi-object tracking in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.04503": {
        "authors": [
            "Xiaofan Li",
            "Zhihao Xu",
            "Chenming Wu",
            "Zhao Yang",
            "Yumeng Zhang",
            "Jiang-Jiang Liu",
            "Haibao Yu",
            "Fan Duan",
            "Xiaoqing Ye",
            "Yuan Wang",
            "Shirui Li",
            "Xun Sun",
            "Ji Wan",
            "Jun Wang"
        ],
        "title": "U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration",
        "abstract": "arXiv:2507.04503v1 Announce Type: new  Abstract: Accurate localization using visual information is a critical yet challenging task, especially in urban environments where nearby buildings and construction sites significantly degrade GNSS (Global Navigation Satellite System) signal quality. This issue underscores the importance of visual localization techniques in scenarios where GNSS signals are unreliable. This paper proposes U-ViLAR, a novel uncertainty-aware visual localization framework designed to address these challenges while enabling adaptive localization using high-definition (HD) maps or navigation maps. Specifically, our method first extracts features from the input visual data and maps them into Bird's-Eye-View (BEV) space to enhance spatial consistency with the map input. Subsequently, we introduce: a) Perceptual Uncertainty-guided Association, which mitigates errors caused by perception uncertainty, and b) Localization Uncertainty-guided Registration, which reduces errors introduced by localization uncertainty. By effectively balancing the coarse-grained large-scale localization capability of association with the fine-grained precise localization capability of registration, our approach achieves robust and accurate localization. Experimental results demonstrate that our method achieves state-of-the-art performance across multiple localization tasks. Furthermore, our model has undergone rigorous testing on large-scale autonomous driving fleets and has demonstrated stable performance in various challenging urban scenarios.",
        "arxiv_id": "2507.04503",
        "ARXIVID": "2507.04503",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it proposes a novel uncertainty-aware visual localization framework for autonomous driving.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.03870": {
        "authors": [
            "Rahil P Mehta",
            "Yashwanthi Anand",
            "Manish Motwani",
            "Sandhya Saisubramanian"
        ],
        "title": "Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing",
        "abstract": "arXiv:2507.03870v1 Announce Type: new  Abstract: When an autonomous agent behaves undesirably, including failure to complete a task, it can be difficult to determine whether the behavior is due to a systemic agent error, such as flaws in the model or policy, or an environment error, where a task is inherently infeasible under a given environment configuration, even for an ideal agent. As agents and their environments grow more complex, identifying the error source becomes increasingly difficult but critical for reliable deployment. We introduce AIProbe, a novel black-box testing technique that applies differential testing to attribute undesirable agent behaviors either to agent deficiencies, such as modeling or training flaws, or due to environmental infeasibility. AIProbe first generates diverse environmental configurations and tasks for testing the agent, by modifying configurable parameters using Latin Hypercube sampling. It then solves each generated task using a search-based planner, independent of the agent. By comparing the agent's performance to the planner's solution, AIProbe identifies whether failures are due to errors in the agent's model or policy, or due to unsolvable task conditions. Our evaluation across multiple domains shows that AIProbe significantly outperforms state-of-the-art techniques in detecting both total and unique errors, thereby contributing to a reliable deployment of autonomous agents.",
        "arxiv_id": "2507.03870",
        "ARXIVID": "2507.03870",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel testing technique for autonomous systems.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.05116": {
        "authors": [
            "Juyi Lin",
            "Amir Taherin",
            "Arash Akbari",
            "Arman Akbari",
            "Lei Lu",
            "Guangyu Chen",
            "Taskin Padir",
            "Xiaomeng Yang",
            "Weiwei Chen",
            "Yiqian Li",
            "Xue Lin",
            "David Kaeli",
            "Pu Zhao",
            "Yanzhi Wang"
        ],
        "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting",
        "abstract": "arXiv:2507.05116v1 Announce Type: new  Abstract: Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, their generalization remains limited when applied to novel objects or unfamiliar environments that lie outside the training distribution. To address this, many existing approaches integrate additional components such as depth estimation, segmentation, or even diffusion to improve generalization, at the cost of adding significant computation overhead, resulting in low efficiency. This motivates the exploration of efficient action prediction methods, which are independent of additional high-level visual representations or diffusion techniques. In this work, we propose VOTE, an efficient and general framework for the optimization and acceleration of VLA models. In details, we propose a novel tokenizer-free fine-tuning approach for parallel accurate action prediction, which reduces computational overhead and accelerates inference speed. Additionally, we adopt an ensemble voting strategy for the action sampling, which significantly improves model performance and enhances generalization. Experimental results show that our method achieves state-of-the-art performance with 35$\\times$ faster inference and 145 Hz throughput. All the details and codes will be open-sourced.",
        "arxiv_id": "2507.05116",
        "ARXIVID": "2507.05116",
        "COMMENT": "Matches criterion 3. Proposes a novel method for improving efficiency and generalization in Vision-Language-Action models for robotic manipulation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.05007": {
        "authors": [
            "Britty Baby",
            "Vinkle Srivastav",
            "Pooja P. Jain",
            "Kun Yuan",
            "Pietro Mascagni",
            "Nicolas Padoy"
        ],
        "title": "Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition",
        "abstract": "arXiv:2507.05007v1 Announce Type: new  Abstract: The Critical View of Safety (CVS) is crucial for safe laparoscopic cholecystectomy, yet assessing CVS criteria remains a complex and challenging task, even for experts. Traditional models for CVS recognition depend on vision-only models learning with costly, labor-intensive spatial annotations. This study investigates how text can be harnessed as a powerful tool for both training and inference in multi-modal surgical foundation models to automate CVS recognition. Unlike many existing multi-modal models, which are primarily adapted for multi-class classification, CVS recognition requires a multi-label framework. Zero-shot evaluation of existing multi-modal surgical models shows a significant performance gap for this task. To address this, we propose CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained, binary classification across multiple labels by aligning image embeddings with textual descriptions of each CVS criterion using positive and negative prompts. By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts, boosts CVS recognition over image-only methods. We also propose text-specific inference methods, that helps in analysing the image-text alignment. While further work is needed to match state-of-the-art spatial annotation-based methods, this approach highlights the potential of adapting generalist models to specialized surgical tasks. Code: https://github.com/CAMMA-public/CVS-AdaptNet",
        "arxiv_id": "2507.05007",
        "ARXIVID": "2507.05007",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on multi-modal surgical foundation models and vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.03730": {
        "authors": [
            "Gongwei Chen",
            "Xurui Zhou",
            "Rui Shao",
            "Yibo Lyu",
            "Kaiwen Zhou",
            "Shuai Wang",
            "Wentao Li",
            "Yinchuan Li",
            "Zhongang Qi",
            "Liqiang Nie"
        ],
        "title": "Less is More: Empowering GUI Agent with Context-Aware Simplification",
        "abstract": "arXiv:2507.03730v1 Announce Type: new  Abstract: The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agent and summarize: 1) the high-density and loose-relation of element context highlight the existence of many unrelated elements and their negative influence; 2) the high redundancy of history context reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed SimpAgent. To mitigate potential interference from numerous unrelated elements, we introduce a masking-based element pruning method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a consistency-guided history compression module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent.",
        "arxiv_id": "2507.03730",
        "ARXIVID": "2507.03730",
        "COMMENT": "Matches criterion 3 as it proposes a novel context-aware simplification framework for GUI agents, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.04464": {
        "authors": [
            "Ashish Bastola",
            "Mert D. Pes\\'e",
            "Long Cheng",
            "Jonathon Smereka",
            "Abolfazl Razi"
        ],
        "title": "Anomalous Decision Discovery using Inverse Reinforcement Learning",
        "abstract": "arXiv:2507.04464v1 Announce Type: new  Abstract: Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by identifying unusual behaviors through perception systems that could compromise safety and lead to hazardous situations. Current approaches, which often rely on predefined thresholds or supervised learning paradigms, exhibit reduced efficacy when confronted with unseen scenarios, sensor noise, and occlusions, leading to potential safety-critical failures. Moreover, supervised methods require large annotated datasets, limiting their real-world feasibility. To address these gaps, we propose an anomaly detection framework based on Inverse Reinforcement Learning (IRL) to infer latent driving intentions from sequential perception data, thus enabling robust identification. Specifically, we present Trajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework for anomaly detection, to address two critical limitations of existing methods: noise robustness and generalization to unseen scenarios. Our core innovation is implicitly learning temporal credit assignments via reward and worst-case supervision. We leverage pre-training with variable-horizon sampling to maximize time-to-consequence, resulting in early detection of behavior deviation. Experiments on 14,000+ simulated trajectories demonstrate state-of-the-art performance, achieving 0.90 AUC and 82.2\\% F1-score - outperforming similarly trained supervised and unsupervised baselines by 39\\% on Recall and 12\\% on F1-score, respectively. Similar performance is achieved while exhibiting robustness to various noise types and generalization to unseen anomaly types. Our code will be available at: https://github.com/abastola0/TRAP.git",
        "arxiv_id": "2507.04464",
        "ARXIVID": "2507.04464",
        "COMMENT": "Matches criterion 3 as it introduces a novel anomaly detection framework for autonomous vehicles using inverse reinforcement learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.03441": {
        "authors": [
            "Matthias Zeller",
            "Daniel Casado Herraez",
            "Jens Behley",
            "Michael Heidingsfeld",
            "Cyrill Stachniss"
        ],
        "title": "Radar Tracker: Moving Instance Tracking in Sparse and Noisy Radar Point Clouds",
        "abstract": "arXiv:2507.03441v1 Announce Type: new  Abstract: Robots and autonomous vehicles should be aware of what happens in their surroundings. The segmentation and tracking of moving objects are essential for reliable path planning, including collision avoidance. We investigate this estimation task for vehicles using radar sensing. We address moving instance tracking in sparse radar point clouds to enhance scene interpretation. We propose a learning-based radar tracker incorporating temporal offset predictions to enable direct center-based association and enhance segmentation performance by including additional motion cues. We implement attention-based tracking for sparse radar scans to include appearance features and enhance performance. The final association combines geometric and appearance features to overcome the limitations of center-based tracking to associate instances reliably. Our approach shows an improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to the current state of the art.",
        "arxiv_id": "2507.03441",
        "ARXIVID": "2507.03441",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for moving instance tracking in radar point clouds, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.04613": {
        "authors": [
            "Jiaqi Cui",
            "Lu Wen",
            "Yuchen Fei",
            "Bo Liu",
            "Luping Zhou",
            "Dinggang Shen",
            "Yan Wang"
        ],
        "title": "HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction",
        "abstract": "arXiv:2507.04613v1 Announce Type: new  Abstract: Survival prediction using whole-slide images (WSIs) is crucial in cancer re-search. Despite notable success, existing approaches are limited by their reliance on sparse slide-level labels, which hinders the learning of discriminative repre-sentations from gigapixel WSIs. Recently, vision language (VL) models, which incorporate additional language supervision, have emerged as a promising solu-tion. However, VL-based survival prediction remains largely unexplored due to two key challenges. First, current methods often rely on only one simple lan-guage prompt and basic cosine similarity, which fails to learn fine-grained associ-ations between multi-faceted linguistic information and visual features within WSI, resulting in inadequate vision-language alignment. Second, these methods primarily exploit patch-level information, overlooking the intrinsic hierarchy of WSIs and their interactions, causing ineffective modeling of hierarchical interac-tions. To tackle these problems, we propose a novel Hierarchical vision-Language collaboration (HiLa) framework for improved survival prediction. Specifically, HiLa employs pretrained feature extractors to generate hierarchical visual features from WSIs at both patch and region levels. At each level, a series of language prompts describing various survival-related attributes are constructed and aligned with visual features via Optimal Prompt Learning (OPL). This ap-proach enables the comprehensive learning of discriminative visual features cor-responding to different survival-related attributes from prompts, thereby improv-ing vision-language alignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation (CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical cooperation by promoting interactions and consistency be-tween patch and region levels. Experiments on three TCGA datasets demonstrate our SOTA performance.",
        "arxiv_id": "2507.04613",
        "ARXIVID": "2507.04613",
        "COMMENT": "Matches criterion 2 as it explores vision-language collaboration for cancer survival prediction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.05254": {
        "authors": [
            "Fabian Konstantinidis",
            "Ariel Dallari Guerreiro",
            "Raphael Trumpp",
            "Moritz Sackmann",
            "Ulrich Hofmann",
            "Marco Caccamo",
            "Christoph Stiller"
        ],
        "title": "From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving",
        "abstract": "arXiv:2507.05254v1 Announce Type: new  Abstract: Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/.",
        "arxiv_id": "2507.05254",
        "ARXIVID": "2507.05254",
        "COMMENT": "Matches criterion 3 as it evaluates joint motion prediction methods, which is relevant to embodied AI and benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.05244": {
        "authors": [
            "Benjamin Li",
            "Shuyang Shi",
            "Lucia Romero",
            "Huao Li",
            "Yaqi Xie",
            "Woojun Kim",
            "Stefanos Nikolaidis",
            "Michael Lewis",
            "Katia Sycara",
            "Simon Stepputtis"
        ],
        "title": "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration",
        "abstract": "arXiv:2507.05244v1 Announce Type: new  Abstract: In collaborative tasks, being able to adapt to your teammates is a necessary requirement for success. When teammates are heterogeneous, such as in human-agent teams, agents need to be able to observe, recognize, and adapt to their human partners in real time. This becomes particularly challenging in tasks with time pressure and complex strategic spaces where the dynamics can change rapidly. In this work, we introduce TALENTS, a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a range of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a variational autoencoder to learn a latent strategy space from trajectory data. This latent space represents the underlying strategies that agents employ. Subsequently, the system identifies different types of strategy by clustering the data. Finally, a cooperator agent is trained to generate partners for each type of strategy, conditioned on these clusters. In order to adapt to previously unseen partners, we leverage a fixed-share regret minimization algorithm that infers and adjusts the estimated partner strategy dynamically. We assess our approach in a customized version of the Overcooked environment, posing a challenging cooperative cooking task that demands strong coordination across a wide range of possible strategies. Using an online user study, we show that our agent outperforms current baselines when working with unfamiliar human partners.",
        "arxiv_id": "2507.05244",
        "ARXIVID": "2507.05244",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a framework for adaptive human-agent collaboration in a challenging environment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.03257": {
        "authors": [
            "L\\'eopold Maillard",
            "Tom Durand",
            "Adrien Ramanana Rahary",
            "Maks Ovsjanikov"
        ],
        "title": "LACONIC: A 3D Layout Adapter for Controllable Image Creation",
        "abstract": "arXiv:2507.03257v1 Announce Type: new  Abstract: Existing generative approaches for guided image synthesis of multi-object scenes typically rely on 2D controls in the image or text space. As a result, these methods struggle to maintain and respect consistent three-dimensional geometric structure, underlying the scene. In this paper, we propose a novel conditioning approach, training method and adapter network that can be plugged into pretrained text-to-image diffusion models. Our approach provides a way to endow such models with 3D-awareness, while leveraging their rich prior knowledge. Our method supports camera control, conditioning on explicit 3D geometries and, for the first time, accounts for the entire context of a scene, i.e., both on and off-screen items, to synthesize plausible and semantically rich images. Despite its multi-modal nature, our model is lightweight, requires a reasonable number of data for supervised learning and shows remarkable generalization power. We also introduce methods for intuitive and consistent image editing and restyling, e.g., by positioning, rotating or resizing individual objects in a scene. Our method integrates well within various image creation workflows and enables a richer set of applications compared to previous approaches.",
        "arxiv_id": "2507.03257",
        "ARXIVID": "2507.03257",
        "COMMENT": "Matches criterion 5. Introduces a 3D layout adapter for controllable image creation, integrating 3D geometry with text-to-image diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.03330": {
        "authors": [
            "Franklin Mingzhe Li",
            "Kaitlyn Ng",
            "Bin Zhu",
            "Patrick Carrington"
        ],
        "title": "Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking",
        "abstract": "arXiv:2507.03330v1 Announce Type: new  Abstract: Cooking plays a vital role in everyday independence and well-being, yet remains challenging for people with vision impairments due to limited support for tracking progress and receiving contextual feedback. Object status - the condition or transformation of ingredients and tools - offers a promising but underexplored foundation for context-aware cooking support. In this paper, we present OSCAR (Object Status Context Awareness for Recipes), a technical pipeline that explores the use of object status recognition to enable recipe progress tracking in non-visual cooking. OSCAR integrates recipe parsing, object status extraction, visual alignment with cooking steps, and time-causal modeling to support real-time step tracking. We evaluate OSCAR on 173 instructional videos and a real-world dataset of 12 non-visual cooking sessions recorded by BLV individuals in their homes. Our results show that object status consistently improves step prediction accuracy across vision-language models, and reveal key factors that impact performance in real-world conditions, such as implicit tasks, camera placement, and lighting. We contribute the pipeline of context-aware recipe progress tracking, an annotated real-world non-visual cooking dataset, and design insights to guide future context-aware assistive cooking systems.",
        "arxiv_id": "2507.03330",
        "ARXIVID": "2507.03330",
        "COMMENT": "Matches criterion 6. Proposes a pipeline for object status recognition in non-visual cooking, leveraging vision-language models for real-world applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.04999": {
        "authors": [
            "Qinkai Yu",
            "Jianyang Xie",
            "Yitian Zhao",
            "Cheng Chen",
            "Lijun Zhang",
            "Liming Chen",
            "Jun Cheng",
            "Lu Liu",
            "Yalin Zheng",
            "Yanda Meng"
        ],
        "title": "Robust Incomplete-Modality Alignment for Ophthalmic Disease Grading and Diagnosis via Labeled Optimal Transport",
        "abstract": "arXiv:2507.04999v1 Announce Type: new  Abstract: Multimodal ophthalmic imaging-based diagnosis integrates color fundus image with optical coherence tomography (OCT) to provide a comprehensive view of ocular pathologies. However, the uneven global distribution of healthcare resources often results in real-world clinical scenarios encountering incomplete multimodal data, which significantly compromises diagnostic accuracy. Existing commonly used pipelines, such as modality imputation and distillation methods, face notable limitations: 1)Imputation methods struggle with accurately reconstructing key lesion features, since OCT lesions are localized, while fundus images vary in style. 2)distillation methods rely heavily on fully paired multimodal training data. To address these challenges, we propose a novel multimodal alignment and fusion framework capable of robustly handling missing modalities in the task of ophthalmic diagnostics. By considering the distinctive feature characteristics of OCT and fundus images, we emphasize the alignment of semantic features within the same category and explicitly learn soft matching between modalities, allowing the missing modality to utilize existing modality information, achieving robust cross-modal feature alignment under the missing modality. Specifically, we leverage the Optimal Transport for multi-scale modality feature alignment: class-wise alignment through predicted class prototypes and feature-wise alignment via cross-modal shared feature transport. Furthermore, we propose an asymmetric fusion strategy that effectively exploits the distinct characteristics of OCT and fundus modalities. Extensive evaluations on three large ophthalmic multimodal datasets demonstrate our model's superior performance under various modality-incomplete scenarios, achieving Sota performance in both complete modality and inter-modality incompleteness conditions. Code is available at https://github.com/Qinkaiyu/RIMA",
        "arxiv_id": "2507.04999",
        "ARXIVID": "2507.04999",
        "COMMENT": "Matches criterion 5. Proposes a robust multimodal alignment framework for ophthalmic diagnostics, integrating image and modality-specific features.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.03531": {
        "authors": [
            "Namho Kim",
            "Junhwa Kim"
        ],
        "title": "Multimodal Alignment with Cross-Attentive GRUs for Fine-Grained Video Understanding",
        "abstract": "arXiv:2507.03531v1 Announce Type: new  Abstract: Fine-grained video classification requires understanding complex spatio-temporal and semantic cues that often exceed the capacity of a single modality. In this paper, we propose a multimodal framework that fuses video, image, and text representations using GRU-based sequence encoders and cross-modal attention mechanisms. The model is trained using a combination of classification or regression loss, depending on the task, and is further regularized through feature-level augmentation and autoencoding techniques. To evaluate the generality of our framework, we conduct experiments on two challenging benchmarks: the DVD dataset for real-world violence detection and the Aff-Wild2 dataset for valence-arousal estimation. Our results demonstrate that the proposed fusion strategy significantly outperforms unimodal baselines, with cross-attention and feature augmentation contributing notably to robustness and performance.",
        "arxiv_id": "2507.03531",
        "ARXIVID": "2507.03531",
        "COMMENT": "Matches criterion 6. Proposes a multimodal framework for fine-grained video understanding using cross-modal attention and GRUs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.03463": {
        "authors": [
            "Matthias Zeller",
            "Vardeep S. Sandhu",
            "Benedikt Mersch",
            "Jens Behley",
            "Michael Heidingsfeld",
            "Cyrill Stachniss"
        ],
        "title": "Radar Velocity Transformer: Single-scan Moving Object Segmentation in Noisy Radar Point Clouds",
        "abstract": "arXiv:2507.03463v1 Announce Type: new  Abstract: The awareness about moving objects in the surroundings of a self-driving vehicle is essential for safe and reliable autonomous navigation. The interpretation of LiDAR and camera data achieves exceptional results but typically requires to accumulate and process temporal sequences of data in order to extract motion information. In contrast, radar sensors, which are already installed in most recent vehicles, can overcome this limitation as they directly provide the Doppler velocity of the detections and, hence incorporate instantaneous motion information within a single measurement. % In this paper, we tackle the problem of moving object segmentation in noisy radar point clouds. We also consider differentiating parked from moving cars, to enhance scene understanding. Instead of exploiting temporal dependencies to identify moving objects, we develop a novel transformer-based approach to perform single-scan moving object segmentation in sparse radar scans accurately. The key to our Radar Velocity Transformer is to incorporate the valuable velocity information throughout each module of the network, thereby enabling the precise segmentation of moving and non-moving objects. Additionally, we propose a transformer-based upsampling, which enhances the performance by adaptively combining information and overcoming the limitation of interpolation of sparse point clouds. Finally, we create a new radar moving object segmentation benchmark based on the RadarScenes dataset and compare our approach to other state-of-the-art methods. Our network runs faster than the frame rate of the sensor and shows superior segmentation results using only single-scan radar data.",
        "arxiv_id": "2507.03463",
        "ARXIVID": "2507.03463",
        "COMMENT": "Matches criterion 3. Introduces a novel transformer-based method for radar point cloud segmentation and a new benchmark for radar moving object segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.03779": {
        "authors": [
            "Jiaqi Zhang",
            "Juntuo Wang",
            "Zhixin Sun",
            "John Zou",
            "Randall Balestriero"
        ],
        "title": "FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed",
        "abstract": "arXiv:2507.03779v1 Announce Type: new  Abstract: Large-scale vision foundation models such as DINOv2 boast impressive performances by leveraging massive architectures and training datasets. But numerous scenarios require practitioners to reproduce those pre-training solutions, such as on private data, new modalities, or simply for scientific questioning--which is currently extremely demanding computation-wise. We thus propose a novel pre-training strategy for DINOv2 that simultaneously accelerates convergence--and strengthens robustness to common corruptions as a by-product. Our approach involves a frequency filtering curriculum--low-frequency being seen first--and the Gaussian noise patching augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while pre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still achieves matching robustness in corruption benchmarks (ImageNet-C) and maintains competitive linear probing performance compared with baseline. This dual benefit of efficiency and robustness makes large-scale self-supervised foundation modeling more attainable, while opening the door to novel exploration around data curriculum and augmentation as means to improve self-supervised learning models robustness. The code is available at https://github.com/KevinZ0217/fast_dinov2",
        "arxiv_id": "2507.03779",
        "ARXIVID": "2507.03779",
        "COMMENT": "Matches criterion 4. Focuses on improving training strategies for vision foundation models like DINOv2.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.04465": {
        "authors": [
            "Konstantinos Foteinos",
            "Jorgen Cani",
            "Manousos Linardakis",
            "Panagiotis Radoglou-Grammatikis",
            "Vasileios Argyriou",
            "Panagiotis Sarigiannidis",
            "Iraklis Varlamis",
            "Georgios Th. Papadopoulos"
        ],
        "title": "Visual Hand Gesture Recognition with Deep Learning: A Comprehensive Review of Methods, Datasets, Challenges and Future Research Directions",
        "abstract": "arXiv:2507.04465v1 Announce Type: new  Abstract: The rapid evolution of deep learning (DL) models and the ever-increasing size of available datasets have raised the interest of the research community in the always important field of vision-based hand gesture recognition (VHGR), and delivered a wide range of applications, such as sign language understanding and human-computer interaction using cameras. Despite the large volume of research works in the field, a structured and complete survey on VHGR is still missing, leaving researchers to navigate through hundreds of papers in order to find the right combination of data, model, and approach for each task. The current survey aims to fill this gap by presenting a comprehensive overview of this aspect of computer vision. With a systematic research methodology that identifies the state-of-the-art works and a structured presentation of the various methods, datasets, and evaluation metrics, this review aims to constitute a useful guideline for researchers, helping them to choose the right strategy for delving into a certain VHGR task. Starting with the methodology used for study selection, literature retrieval, and the analytical framing, the survey identifies and organizes key VHGR approaches using a taxonomy-based format in various dimensions such as input modality and application domain. The core of the survey provides an in-depth analysis of state-of-the-art techniques across three primary VHGR tasks: static gesture recognition, isolated dynamic gestures and continuous gesture recognition. For each task, the architectural trends and learning strategies are listed. Additionally, the study reviews commonly used datasets - emphasizing on annotation schemes - and evaluates standard performance metrics. It concludes by identifying major challenges in VHGR, including both general computer vision issues and domain-specific obstacles, and outlines promising directions for future research.",
        "arxiv_id": "2507.04465",
        "ARXIVID": "2507.04465",
        "COMMENT": "Matches criterion 7 as it is a comprehensive survey on vision-based hand gesture recognition, synthesizing state-of-the-art methods and challenges.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.03190": {
        "authors": [
            "Theo Bourdais",
            "Abeynaya Gnanasekaran",
            "Houman Owhadi",
            "Tuhin Sahai"
        ],
        "title": "Discovering Algorithms with Computational Language Processing",
        "abstract": "arXiv:2507.03190v1 Announce Type: new  Abstract: Algorithms are the engine for reproducible problem-solving. We present a framework automating algorithm discovery by conceptualizing them as sequences of operations, represented as tokens. These computational tokens are chained using a grammar, enabling the formation of increasingly sophisticated procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement learning (RL) explores token chaining and drives the creation of new tokens. This methodology rediscovers, improves, and generates new algorithms that substantially outperform existing methods for strongly NP-hard combinatorial optimization problems and foundational quantum computing approaches such as Grover's and Quantum Approximate Optimization Algorithm. Operating at the computational rather than code-generation level, our framework produces algorithms that can be tailored specifically to problem instances, not merely classes.",
        "arxiv_id": "2507.03190",
        "ARXIVID": "2507.03190",
        "COMMENT": "Does not match any specific criteria but discusses algorithm discovery, which is tangentially related to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2507.05241": {
        "authors": [
            "Jingyi Chai",
            "Shuo Tang",
            "Rui Ye",
            "Yuwen Du",
            "Xinyu Zhu",
            "Mengcheng Zhou",
            "Yanfeng Wang",
            "Weinan E",
            "Siheng Chen"
        ],
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "abstract": "arXiv:2507.05241v1 Announce Type: new  Abstract: The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.",
        "arxiv_id": "2507.05241",
        "ARXIVID": "2507.05241",
        "COMMENT": "Does not match any specific criteria but discusses general-purpose scientific AI agents, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.03738": {
        "authors": [
            "Yansong Peng",
            "Kai Zhu",
            "Yu Liu",
            "Pingyu Wu",
            "Hebei Li",
            "Xiaoyan Sun",
            "Feng Wu"
        ],
        "title": "Flow-Anchored Consistency Models",
        "abstract": "arXiv:2507.03738v1 Announce Type: new  Abstract: Continuous-time Consistency Models (CMs) promise efficient few-step generation but face significant challenges with training instability. We argue this instability stems from a fundamental conflict: by training a network to learn only a shortcut across a probability flow, the model loses its grasp on the instantaneous velocity field that defines the flow. Our solution is to explicitly anchor the model in the underlying flow during training. We introduce the Flow-Anchored Consistency Model (FACM), a simple but effective training strategy that uses a Flow Matching (FM) task as an anchor for the primary CM shortcut objective. This Flow-Anchoring approach requires no architectural modifications and is broadly compatible with standard model architectures. By distilling a pre-trained LightningDiT model, our method achieves a state-of-the-art FID of 1.32 with two steps (NFE=2) and 1.76 with just one step (NFE=1) on ImageNet 256x256, significantly outperforming previous methods. This provides a general and effective recipe for building high-performance, few-step generative models. Our code and pretrained models: https://github.com/ali-vilab/FACM.",
        "arxiv_id": "2507.03738",
        "ARXIVID": "2507.03738",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and efficient training strategies.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.03979": {
        "authors": [
            "Tianyao He",
            "Runqi Wang",
            "Yang Chen",
            "Dejia Song",
            "Nemo Chen",
            "Xu Tang",
            "Yao Hu"
        ],
        "title": "Flux-Sculptor: Text-Driven Rich-Attribute Portrait Editing through Decomposed Spatial Flow Control",
        "abstract": "arXiv:2507.03979v1 Announce Type: new  Abstract: Text-driven portrait editing holds significant potential for various applications but also presents considerable challenges. An ideal text-driven portrait editing approach should achieve precise localization and appropriate content modification, yet existing methods struggle to balance reconstruction fidelity and editing flexibility. To address this issue, we propose Flux-Sculptor, a flux-based framework designed for precise text-driven portrait editing. Our framework introduces a Prompt-Aligned Spatial Locator (PASL) to accurately identify relevant editing regions and a Structure-to-Detail Edit Control (S2D-EC) strategy to spatially guide the denoising process through sequential mask-guided fusion of latent representations and attention values. Extensive experiments demonstrate that Flux-Sculptor surpasses existing methods in rich-attribute editing and facial information preservation, making it a strong candidate for practical portrait editing applications. Project page is available at https://flux-sculptor.github.io/.",
        "arxiv_id": "2507.03979",
        "ARXIVID": "2507.03979",
        "COMMENT": "Does not match any specific criteria but is related to text-driven portrait editing, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.05256": {
        "authors": [
            "Jiahao Zhu",
            "Zixuan Chen",
            "Guangcong Wang",
            "Xiaohua Xie",
            "Yi Zhou"
        ],
        "title": "SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation",
        "abstract": "arXiv:2507.05256v1 Announce Type: new  Abstract: Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).",
        "arxiv_id": "2507.05256",
        "ARXIVID": "2507.05256",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of generative modeling and 3D synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.04290": {
        "authors": [
            "Weilun Feng",
            "Chuanguang Yang",
            "Haotong Qin",
            "Yuqi Li",
            "Xiangqi Li",
            "Zhulin An",
            "Libo Huang",
            "Boyu Diao",
            "Fuzhen Zhuang",
            "Michele Magno",
            "Yongjun Xu",
            "Yingli Tian",
            "Tingwen Huang"
        ],
        "title": "MPQ-DMv2: Flexible Residual Mixed Precision Quantization for Low-Bit Diffusion Models with Temporal Distillation",
        "abstract": "arXiv:2507.04290v1 Announce Type: new  Abstract: Diffusion models have demonstrated remarkable performance on vision generation tasks. However, the high computational complexity hinders its wide application on edge devices. Quantization has emerged as a promising technique for inference acceleration and memory reduction. However, existing quantization methods do not generalize well under extremely low-bit (2-4 bit) quantization. Directly applying these methods will cause severe performance degradation. We identify that the existing quantization framework suffers from the outlier-unfriendly quantizer design, suboptimal initialization, and optimization strategy. We present MPQ-DMv2, an improved \\textbf{M}ixed \\textbf{P}recision \\textbf{Q}uantization framework for extremely low-bit \\textbf{D}iffusion \\textbf{M}odels. For the quantization perspective, the imbalanced distribution caused by salient outliers is quantization-unfriendly for uniform quantizer. We propose \\textit{Flexible Z-Order Residual Mixed Quantization} that utilizes an efficient binary residual branch for flexible quant steps to handle salient error. For the optimization framework, we theoretically analyzed the convergence and optimality of the LoRA module and propose \\textit{Object-Oriented Low-Rank Initialization} to use prior quantization error for informative initialization. We then propose \\textit{Memory-based Temporal Relation Distillation} to construct an online time-aware pixel queue for long-term denoising temporal information distillation, which ensures the overall temporal consistency between quantized and full-precision model. Comprehensive experiments on various generation tasks show that our MPQ-DMv2 surpasses current SOTA methods by a great margin on different architectures, especially under extremely low-bit widths.",
        "arxiv_id": "2507.04290",
        "ARXIVID": "2507.04290",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and computational efficiency in vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.04673": {
        "authors": [
            "Wei Duan",
            "Li Qian"
        ],
        "title": "Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message",
        "abstract": "arXiv:2507.04673v1 Announce Type: new  Abstract: The rise of conversational interfaces has greatly enhanced LLM usability by leveraging dialogue history for sophisticated reasoning. However, this reliance introduces an unexplored attack surface. This paper introduces Trojan Horse Prompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by forging the model's own past utterances within the conversational history provided to its API. A malicious payload is injected into a model-attributed message, followed by a benign user prompt to trigger harmful content generation. This vulnerability stems from Asymmetric Safety Alignment: models are extensively trained to refuse harmful user requests but lack comparable skepticism towards their own purported conversational history. This implicit trust in its \"past\" creates a high-impact vulnerability. Experimental validation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan Horse Prompting achieves a significantly higher Attack Success Rate (ASR) than established user-turn jailbreaking methods. These findings reveal a fundamental flaw in modern conversational AI security, necessitating a paradigm shift from input-level filtering to robust, protocol-level validation of conversational context integrity.",
        "arxiv_id": "2507.04673",
        "ARXIVID": "2507.04673",
        "COMMENT": "Does not closely match any specific criteria but is tangentially related to conversational multimodal models, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03928": {
        "authors": [
            "Yiliu Sun",
            "Zicheng Zhao",
            "Sheng Wan",
            "Chen Gong"
        ],
        "title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate",
        "abstract": "arXiv:2507.03928v1 Announce Type: new  Abstract: Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called \"CortexDebate\". Inspired by the human brain's tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.",
        "arxiv_id": "2507.03928",
        "ARXIVID": "2507.03928",
        "COMMENT": "Does not match any specific criteria but discusses multi-agent debate strategies, which is tangentially related to AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.02965": {
        "authors": [
            "Andi Zhang",
            "Xuan Ding",
            "Steven McDonagh",
            "Samuel Kaski"
        ],
        "title": "Concept-based Adversarial Attack: a Probabilistic Perspective",
        "abstract": "arXiv:2507.02965v1 Announce Type: new  Abstract: We propose a concept-based adversarial attack framework that extends beyond single-image perturbations by adopting a probabilistic perspective. Rather than modifying a single image, our method operates on an entire concept -- represented by a probabilistic generative model or a set of images -- to generate diverse adversarial examples. Preserving the concept is essential, as it ensures that the resulting adversarial images remain identifiable as instances of the original underlying category or identity. By sampling from this concept-based adversarial distribution, we generate images that maintain the original concept but vary in pose, viewpoint, or background, thereby misleading the classifier. Mathematically, this framework remains consistent with traditional adversarial attacks in a principled manner. Our theoretical and empirical results demonstrate that concept-based adversarial attacks yield more diverse adversarial examples and effectively preserve the underlying concept, while achieving higher attack efficiency.",
        "arxiv_id": "2507.02965",
        "ARXIVID": "2507.02965",
        "COMMENT": "Does not match any specific criteria but discusses adversarial attacks in a probabilistic framework, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04306": {
        "authors": [
            "Zhipeng Li",
            "Kegang Wang",
            "Hanguang Xiao",
            "Xingyue Liu",
            "Feizhong Zhou",
            "Jiaxin Jiang",
            "Tianqi Liu"
        ],
        "title": "Exploring Remote Physiological Signal Measurement under Dynamic Lighting Conditions at Night: Dataset, Experiment, and Analysis",
        "abstract": "arXiv:2507.04306v1 Announce Type: new  Abstract: Remote photoplethysmography (rPPG) is a non-contact technique for measuring human physiological signals. Due to its convenience and non-invasiveness, it has demonstrated broad application potential in areas such as health monitoring and emotion recognition. In recent years, the release of numerous public datasets has significantly advanced the performance of rPPG algorithms under ideal lighting conditions. However, the effectiveness of current rPPG methods in realistic nighttime scenarios with dynamic lighting variations remains largely unknown. Moreover, there is a severe lack of datasets specifically designed for such challenging environments, which has substantially hindered progress in this area of research. To address this gap, we present and release a large-scale rPPG dataset collected under dynamic lighting conditions at night, named DLCN. The dataset comprises approximately 13 hours of video data and corresponding synchronized physiological signals from 98 participants, covering four representative nighttime lighting scenarios. DLCN offers high diversity and realism, making it a valuable resource for evaluating algorithm robustness in complex conditions. Built upon the proposed Happy-rPPG Toolkit, we conduct extensive experiments and provide a comprehensive analysis of the challenges faced by state-of-the-art rPPG methods when applied to DLCN. The dataset and code are publicly available at https://github.com/dalaoplan/Happp-rPPG-Toolkit.",
        "arxiv_id": "2507.04306",
        "ARXIVID": "2507.04306",
        "COMMENT": "Does not match any specific criteria but introduces a new dataset for physiological signal measurement under challenging conditions.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04801": {
        "authors": [
            "Abiao Li",
            "Chenlei Lv",
            "Yuming Fang",
            "Yifan Zuo",
            "Jian Zhang",
            "Guofeng Mei"
        ],
        "title": "PointGAC: Geometric-Aware Codebook for Masked Point Cloud Modeling",
        "abstract": "arXiv:2507.04801v1 Announce Type: new  Abstract: Most masked point cloud modeling (MPM) methods follow a regression paradigm to reconstruct the coordinate or feature of masked regions. However, they tend to over-constrain the model to learn the details of the masked region, resulting in failure to capture generalized features. To address this limitation, we propose \\textbf{\\textit{PointGAC}}, a novel clustering-based MPM method that aims to align the feature distribution of masked regions. Specially, it features an online codebook-guided teacher-student framework. Firstly, it presents a geometry-aware partitioning strategy to extract initial patches. Then, the teacher model updates a codebook via online k-means based on features extracted from the complete patches. This procedure facilitates codebook vectors to become cluster centers. Afterward, we assigns the unmasked features to their corresponding cluster centers, and the student model aligns the assignment for the reconstructed masked features. This strategy focuses on identifying the cluster centers to which the masked features belong, enabling the model to learn more generalized feature representations. Benefiting from a proposed codebook maintenance mechanism, codebook vectors are actively updated, which further increases the efficiency of semantic feature learning. Experiments validate the effectiveness of the proposed method on various downstream tasks. Code is available at https://github.com/LAB123-tech/PointGAC",
        "arxiv_id": "2507.04801",
        "ARXIVID": "2507.04801",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04632": {
        "authors": [
            "Yun Qu",
            "Qi Cheems Wang",
            "Yixiu Mao",
            "Vincent Tao Hu",
            "Xiangyang Ji"
        ],
        "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?",
        "abstract": "arXiv:2507.04632v1 Announce Type: new  Abstract: Recent advances have witnessed the effectiveness of reinforcement learning (RL) finetuning in enhancing the reasoning capabilities of large language models (LLMs). The optimization process often requires numerous iterations to achieve satisfactory performance, resulting in high computational costs due to the need for frequent prompt evaluations under intensive LLM interactions and repeated policy updates. Appropriate online prompt selection methods reduce iteration steps by prioritizing informative prompts during training, while the pipeline's reliance on exhaustive prompt evaluation and subset selection for optimization still incurs substantial computational overhead due to frequent LLM inference calls. Distinguished from these direct evaluate-then-select schemes, this work investigates iterative approximate evaluation for arbitrary prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian risk-predictive framework that online estimates prompt difficulty without requiring costly LLM interactions. Technically, MoPPS models each prompt's success rate as a latent variable, performs streaming Bayesian inference, and employs posterior sampling in a constructed multi-armed bandit machine, enabling sample efficient and adaptive prompt selection. Extensive experiments across mathematics, planning, and vision-based geometry tasks show that MoPPS reliably predicts prompt difficulty and accelerates training with significantly reduced LLM rollouts.",
        "arxiv_id": "2507.04632",
        "ARXIVID": "2507.04632",
        "COMMENT": "Does not match any specific criteria. Focuses on prompt difficulty prediction for RL finetuning in reasoning models, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04370": {
        "authors": [
            "Yifei Gao",
            "Junhong Ye",
            "Jiaqi Wang",
            "Jitao Sang"
        ],
        "title": "WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis",
        "abstract": "arXiv:2507.04370v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have significantly improved the capabilities of web agents. However, effectively navigating complex and dynamic web environments still requires more advanced trajectory-level planning and execution. Prior studies have addressed self-improving agents by collecting extensive GUI trajectories from real-environment interactions. Despite their effectiveness, these approaches encounter two critical challenges: (1) Uncontrollable environment states, where real or sandboxed web environments often yield unstable and non-deterministic feedback, complicating the reproduction and debugging of agent behaviors; and (2) High API costs, as generating even a single interaction trajectory can involve hundreds of queries, leading to considerable API usage and computational expenses. To address these limitations and enable scalable self-improvement for agents, we propose WebSynthesis, a novel framework for trajectory synthesis and training. WebSynthesis leverages a learned world model to simulate virtual web environments, allowing a policy agent to perform efficient and reversible tree-based planning. This approach supports the large-scale generation of diverse and high-quality trajectories, which are subsequently utilized to refine the agent's policy. Experimental results demonstrate that an agent trained using WebSynthesis on a small-scale synthetic dataset achieves performance comparable to or even surpassing that of models trained on large-scale real-world data.",
        "arxiv_id": "2507.04370",
        "ARXIVID": "2507.04370",
        "COMMENT": "Does not match any specific criteria. Focuses on web agent trajectory synthesis using a world model, which is unrelated to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03682": {
        "authors": [
            "Rebekah A. Gelp\\'i",
            "Eric Xue",
            "William A. Cunningham"
        ],
        "title": "Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning",
        "abstract": "arXiv:2507.03682v1 Announce Type: new  Abstract: We propose a hybrid approach to machine Theory of Mind (ToM) that uses large language models (LLMs) as a mechanism for generating hypotheses and likelihood functions with a Bayesian inverse planning model that computes posterior probabilities for an agent's likely mental states given its actions. Bayesian inverse planning models can accurately predict human reasoning on a variety of ToM tasks, but these models are constrained in their ability to scale these predictions to scenarios with a large number of possible hypotheses and actions. Conversely, LLM-based approaches have recently demonstrated promise in solving ToM benchmarks, but can exhibit brittleness and failures on reasoning tasks even when they pass otherwise structurally identical versions. By combining these two methods, this approach leverages the strengths of each component, closely matching optimal results on a task inspired by prior inverse planning models and improving performance relative to models that utilize LLMs alone or with chain-of-thought prompting, even with smaller LLMs that typically perform poorly on ToM tasks. We also exhibit the model's potential to predict mental states on open-ended tasks, offering a promising direction for future development of ToM models and the creation of socially intelligent generative agents.",
        "arxiv_id": "2507.03682",
        "ARXIVID": "2507.03682",
        "COMMENT": "Does not match any specific criteria but is related to Theory of Mind and hybrid approaches, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03616": {
        "authors": [
            "Yingxu Wang",
            "Siwei Liu",
            "Jinyuan Fang",
            "Zaiqiao Meng"
        ],
        "title": "EvoAgentX: An Automated Framework for Evolving Agentic Workflows",
        "abstract": "arXiv:2507.03616v1 Announce Type: new  Abstract: Multi-agent systems (MAS) have emerged as a powerful paradigm for orchestrating large language models (LLMs) and specialized tools to collaboratively address complex tasks. However, existing MAS frameworks often require manual workflow configuration and lack native support for dynamic evolution and performance optimization. In addition, many MAS optimization algorithms are not integrated into a unified framework. In this paper, we present EvoAgentX, an open-source platform that automates the generation, execution, and evolutionary optimization of multi-agent workflows. EvoAgentX employs a modular architecture consisting of five core layers: the basic components, agent, workflow, evolving, and evaluation layers. Specifically, within the evolving layer, EvoAgentX integrates three MAS optimization algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts, tool configurations, and workflow topologies. We evaluate EvoAgentX on HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and mathematical problem solving, respectively, and further assess it on real-world tasks using GAIA. Experimental results show that EvoAgentX consistently achieves significant performance improvements, including a 7.44% increase in HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The source code is available at: https://github.com/EvoAgentX/EvoAgentX",
        "arxiv_id": "2507.03616",
        "ARXIVID": "2507.03616",
        "COMMENT": "Does not match any specific criteria but is related to multi-agent systems and optimization, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04006": {
        "authors": [
            "Seungjin Jung",
            "Kanghee Lee",
            "Yonghyun Jeong",
            "Haeun Noh",
            "Jungmin Lee",
            "Jongwon Choi"
        ],
        "title": "Group-wise Scaling and Orthogonal Decomposition for Domain-Invariant Feature Extraction in Face Anti-Spoofing",
        "abstract": "arXiv:2507.04006v1 Announce Type: new  Abstract: Domain Generalizable Face Anti-Spoofing (DGFAS) methods effectively capture domain-invariant features by aligning the directions (weights) of local decision boundaries across domains. However, the bias terms associated with these boundaries remain misaligned, leading to inconsistent classification thresholds and degraded performance on unseen target domains. To address this issue, we propose a novel DGFAS framework that jointly aligns weights and biases through Feature Orthogonal Decomposition (FOD) and Group-wise Scaling Risk Minimization (GS-RM). Specifically, GS-RM facilitates bias alignment by balancing group-wise losses across multiple domains. FOD employs the Gram-Schmidt orthogonalization process to decompose the feature space explicitly into domain-invariant and domain-specific subspaces. By enforcing orthogonality between domain-specific and domain-invariant features during training using domain labels, FOD ensures effective weight alignment across domains without negatively impacting bias alignment. Additionally, we introduce Expected Calibration Error (ECE) as a novel evaluation metric for quantitatively assessing the effectiveness of our method in aligning bias terms across domains. Extensive experiments on benchmark datasets demonstrate that our approach achieves state-of-the-art performance, consistently improving accuracy, reducing bias misalignment, and enhancing generalization stability on unseen target domains.",
        "arxiv_id": "2507.04006",
        "ARXIVID": "2507.04006",
        "COMMENT": "Does not match any specific criteria. Focuses on domain generalization for face anti-spoofing, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03683": {
        "authors": [
            "Ankit Sonthalia",
            "Arnas Uselis",
            "Seong Joon Oh"
        ],
        "title": "On the rankability of visual embeddings",
        "abstract": "arXiv:2507.03683v1 Announce Type: new  Abstract: We study whether visual embedding models capture continuous, ordinal attributes along linear directions, which we term _rank axes_. We define a model as _rankable_ for an attribute if projecting embeddings onto such an axis preserves the attribute's order. Across 7 popular encoders and 9 datasets with attributes like age, crowd count, head pose, aesthetics, and recency, we find that many embeddings are inherently rankable. Surprisingly, a small number of samples, or even just two extreme examples, often suffice to recover meaningful rank axes, without full-scale supervision. These findings open up new use cases for image ranking in vector databases and motivate further study into the structure and learning of rankable embeddings. Our code is available at https://github.com/aktsonthalia/rankable-vision-embeddings.",
        "arxiv_id": "2507.03683",
        "ARXIVID": "2507.03683",
        "COMMENT": "Does not match any specific criteria. Focuses on rankability of visual embeddings, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03331": {
        "authors": [
            "Mingzhuo Li",
            "Guang Li",
            "Jiafeng Mao",
            "Linfeng Ye",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "title": "Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling",
        "abstract": "arXiv:2507.03331v1 Announce Type: new  Abstract: To alleviate the reliance of deep neural networks on large-scale datasets, dataset distillation aims to generate compact, high-quality synthetic datasets that can achieve comparable performance to the original dataset. The integration of generative models has significantly advanced this field. However, existing approaches primarily focus on aligning the distilled dataset with the original one, often overlooking task-specific information that can be critical for optimal downstream performance. In this paper, focusing on the downstream task of classification, we propose a task-specific sampling strategy for generative dataset distillation that incorporates the concept of difficulty to consider the requirements of the target task better. The final dataset is sampled from a larger image pool with a sampling distribution obtained by matching the difficulty distribution of the original dataset. A logarithmic transformation is applied as a pre-processing step to correct for distributional bias. The results of extensive experiments demonstrate the effectiveness of our method and suggest its potential for enhancing performance on other downstream tasks.",
        "arxiv_id": "2507.03331",
        "ARXIVID": "2507.03331",
        "COMMENT": "Does not match any specific criteria. Focuses on dataset distillation for classification tasks, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04118": {
        "authors": [
            "Wenyang Liu",
            "Chen Cai",
            "Jianjun Gao",
            "Kejun Wu",
            "Yi Wang",
            "Kim-Hui Yap",
            "Lap-Pui Chau"
        ],
        "title": "PromptSR: Cascade Prompting for Lightweight Image Super-Resolution",
        "abstract": "arXiv:2507.04118v1 Announce Type: new  Abstract: Although the lightweight Vision Transformer has significantly advanced image super-resolution (SR), it faces the inherent challenge of a limited receptive field due to the window-based self-attention modeling. The quadratic computational complexity relative to window size restricts its ability to use a large window size for expanding the receptive field while maintaining low computational costs. To address this challenge, we propose PromptSR, a novel prompt-empowered lightweight image SR method. The core component is the proposed cascade prompting block (CPB), which enhances global information access and local refinement via three cascaded prompting layers: a global anchor prompting layer (GAPL) and two local prompting layers (LPLs). The GAPL leverages downscaled features as anchors to construct low-dimensional anchor prompts (APs) through cross-scale attention, significantly reducing computational costs. These APs, with enhanced global perception, are then used to provide global prompts, efficiently facilitating long-range token connections. The two LPLs subsequently combine category-based self-attention and window-based self-attention to refine the representation in a coarse-to-fine manner. They leverage attention maps from the GAPL as additional global prompts, enabling them to perceive features globally at different granularities for adaptive local refinement. In this way, the proposed CPB effectively combines global priors and local details, significantly enlarging the receptive field while maintaining the low computational costs of our PromptSR. The experimental results demonstrate the superiority of our method, which outperforms state-of-the-art lightweight SR methods in quantitative, qualitative, and complexity evaluations. Our code will be released at https://github.com/wenyang001/PromptSR.",
        "arxiv_id": "2507.04118",
        "ARXIVID": "2507.04118",
        "COMMENT": "Does not match any specific criteria. Focuses on lightweight image super-resolution, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03984": {
        "authors": [
            "Jeonghyo Song",
            "Kimin Yun",
            "DaeUng Jo",
            "Jinyoung Kim",
            "Youngjoon Yoo"
        ],
        "title": "CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via Chain-of-Thought Reasoning",
        "abstract": "arXiv:2507.03984v1 Announce Type: new  Abstract: Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the reliability of semantic segmentation models, particularly in complex road environments where safety and accuracy are paramount. Despite recent advancements in large language models (LLMs), notably GPT-4, which significantly enhanced multimodal reasoning through Chain-of-Thought (CoT) prompting, the application of CoT-based visual reasoning for OOD semantic segmentation remains largely unexplored. In this paper, through extensive analyses of the road scene anomalies, we identify three challenging scenarios where current state-of-the-art OOD segmentation methods consistently struggle: (1) densely packed and overlapping objects, (2) distant scenes with small objects, and (3) large foreground-dominant objects. To address the presented challenges, we propose a novel CoT-based framework targeting OOD detection in road anomaly scenes. Our method leverages the extensive knowledge and reasoning capabilities of foundation models, such as GPT-4, to enhance OOD detection through improved image understanding and prompt-based reasoning aligned with observed problematic scene attributes. Extensive experiments show that our framework consistently outperforms state-of-the-art methods on both standard benchmarks and our newly defined challenging subset of the RoadAnomaly dataset, offering a robust and interpretable solution for OOD semantic segmentation in complex driving environments.",
        "arxiv_id": "2507.03984",
        "ARXIVID": "2507.03984",
        "COMMENT": "Does not match any specific criteria. Focuses on OOD detection in road scenes using Chain-of-Thought reasoning, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05260": {
        "authors": [
            "Xiang Xu",
            "Lingdong Kong",
            "Song Wang",
            "Chuanwei Zhou",
            "Qingshan Liu"
        ],
        "title": "Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon Distillation for Better LiDAR Representations",
        "abstract": "arXiv:2507.05260v1 Announce Type: new  Abstract: LiDAR representation learning aims to extract rich structural and semantic information from large-scale, readily available datasets, reducing reliance on costly human annotations. However, existing LiDAR representation strategies often overlook the inherent spatiotemporal cues in LiDAR sequences, limiting their effectiveness. In this work, we propose LiMA, a novel long-term image-to-LiDAR Memory Aggregation framework that explicitly captures longer range temporal correlations to enhance LiDAR representation learning. LiMA comprises three key components: 1) a Cross-View Aggregation module that aligns and fuses overlapping regions across neighboring camera views, constructing a more unified and redundancy-free memory bank; 2) a Long-Term Feature Propagation mechanism that efficiently aligns and integrates multi-frame image features, reinforcing temporal coherence during LiDAR representation learning; and 3) a Cross-Sequence Memory Alignment strategy that enforces consistency across driving sequences, improving generalization to unseen environments. LiMA maintains high pretraining efficiency and incurs no additional computational overhead during downstream tasks. Extensive experiments on mainstream LiDAR-based perception benchmarks demonstrate that LiMA significantly improves both LiDAR semantic segmentation and 3D object detection. We hope this work inspires more effective pretraining paradigms for autonomous driving. The code has be made publicly accessible for future research.",
        "arxiv_id": "2507.05260",
        "ARXIVID": "2507.05260",
        "COMMENT": "Does not match any specific criteria. Focuses on LiDAR representation learning for autonomous driving, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03267": {
        "authors": [
            "Jie Peng",
            "Jiarui Ji",
            "Runlin Lei",
            "Zhewei Wei",
            "Yongchao Liu",
            "Chuntao Hong"
        ],
        "title": "GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning",
        "abstract": "arXiv:2507.03267v1 Announce Type: new  Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate structural, temporal, and textual attributes, are crucial for modeling complex real-world systems. However, most of the existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for DyTAG generation tasks requiring semantically rich inputs. Additionally, prior work mainly focuses on discriminative tasks on DyTAGs, resulting in a lack of standardized task formulations and evaluation protocols tailored for DyTAG generation. To address these critical issues, we propose Generative DyTAG Benchmark (GDGB), which comprises eight meticulously curated DyTAG datasets with high-quality textual features for both nodes and edges, overcoming limitations of prior datasets. Building on GDGB, we define two novel DyTAG generation tasks: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG). TDGG transductively generates a target DyTAG based on the given source and destination node sets, while the more challenging IDGG introduces new node generation to inductively model the dynamic expansion of real-world graph data. To enable holistic evaluation, we design multifaceted metrics that assess the structural, temporal, and textual quality of the generated DyTAGs. We further propose GAG-General, an LLM-based multi-agent generative framework tailored for reproducible and robust benchmarking of DyTAG generation. Experimental results demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key insights revealing the critical interplay of structural and textual features in DyTAG generation. These findings establish GDGB as a foundational resource for advancing generative DyTAG research and unlocking further practical applications in DyTAG generation. GDGB datasets, source codes, and leaderboards are available at \\href{https://gdgb-algo.github.io/}{here}.",
        "arxiv_id": "2507.03267",
        "ARXIVID": "2507.03267",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04323": {
        "authors": [
            "Paul Hill",
            "Alin Achim",
            "Dave Bull",
            "Nantheera Anantrasirichai"
        ],
        "title": "DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection",
        "abstract": "arXiv:2507.04323v1 Announce Type: new  Abstract: Atmospheric Turbulence (AT) degrades the clarity and accuracy of surveillance imagery, posing challenges not only for visualization quality but also for object classification and scene tracking. Deep learning-based methods have been proposed to improve visual quality, but spatio-temporal distortions remain a significant issue. Although deep learning-based object detection performs well under normal conditions, it struggles to operate effectively on sequences distorted by atmospheric turbulence. In this paper, we propose a novel framework that learns to compensate for distorted features while simultaneously improving visualization and object detection. This end-to-end framework leverages and exchanges knowledge of low-level distorted features in the AT mitigator with semantic features extracted in the object detector. Specifically, in the AT mitigator a 3D Mamba-based structure is used to handle the spatio-temporal displacements and blurring caused by turbulence. Features are extracted in a pyramid manner during the mitigation stage and passed to the detector. Optimization is achieved through back-propagation in both the AT mitigator and object detector. Our proposed DMAT outperforms state-of-the-art AT mitigation and object detection systems up to a 15% improvement on datasets corrupted by generated turbulence.",
        "arxiv_id": "2507.04323",
        "ARXIVID": "2507.04323",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03697": {
        "authors": [
            "Qika Lin",
            "Fangzhi Xu",
            "Hao Lu",
            "Kai He",
            "Rui Mao",
            "Jun Liu",
            "Erik Cambria",
            "Mengling Feng"
        ],
        "title": "Towards Unified Neurosymbolic Reasoning on Knowledge Graphs",
        "abstract": "arXiv:2507.03697v1 Announce Type: new  Abstract: Knowledge Graph (KG) reasoning has received significant attention in the fields of artificial intelligence and knowledge engineering, owing to its ability to autonomously deduce new knowledge and consequently enhance the availability and precision of downstream applications. However, current methods predominantly concentrate on a single form of neural or symbolic reasoning, failing to effectively integrate the inherent strengths of both approaches. Furthermore, the current prevalent methods primarily focus on addressing a single reasoning scenario, presenting limitations in meeting the diverse demands of real-world reasoning tasks. Unifying the neural and symbolic methods, as well as diverse reasoning scenarios in one model is challenging as there is a natural representation gap between symbolic rules and neural networks, and diverse scenarios exhibit distinct knowledge structures and specific reasoning objectives. To address these issues, we propose a unified neurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first introduces a consistent structure of reasoning graph that starts from the query entity and constantly expands subsequent nodes by iteratively searching posterior neighbors. Based on it, a forward logic message-passing mechanism is proposed to update both the propositional representations and attentions, as well as first-order logic (FOL) representations and attentions of each node. In this way, Tunsr conducts the transformation of merging multiple rules by merging possible relations at each step. Finally, the FARI algorithm is proposed to induce FOL rules by constantly performing attention calculations over the reasoning graph. Extensive experimental results on 19 datasets of four reasoning scenarios (transductive, inductive, interpolation, and extrapolation) demonstrate the effectiveness of Tunsr.",
        "arxiv_id": "2507.03697",
        "ARXIVID": "2507.03697",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03904": {
        "authors": [
            "Yingxuan Yang",
            "Ying Wen",
            "Jun Wang",
            "Weinan Zhang"
        ],
        "title": "Agent Exchange: Shaping the Future of AI Agent Economics",
        "abstract": "arXiv:2507.03904v1 Announce Type: new  Abstract: The rise of Large Language Models (LLMs) has transformed AI agents from passive computational tools into autonomous economic actors. This shift marks the emergence of the agent-centric economy, in which agents take on active economic roles-exchanging value, making strategic decisions, and coordinating actions with minimal human oversight. To realize this vision, we propose Agent Exchange (AEX), a specialized auction platform designed to support the dynamics of the AI agent marketplace. AEX offers an optimized infrastructure for agent coordination and economic participation. Inspired by Real-Time Bidding (RTB) systems in online advertising, AEX serves as the central auction engine, facilitating interactions among four ecosystem components: the User-Side Platform (USP), which translates human goals into agent-executable tasks; the Agent-Side Platform (ASP), responsible for capability representation, performance tracking, and optimization; Agent Hubs, which coordinate agent teams and participate in AEX-hosted auctions; and the Data Management Platform (DMP), ensuring secure knowledge sharing and fair value attribution. We outline the design principles and system architecture of AEX, laying the groundwork for agent-based economic infrastructure in future AI ecosystems.",
        "arxiv_id": "2507.03904",
        "ARXIVID": "2507.03904",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03542": {
        "authors": [
            "Ethan Lin",
            "Linxi Zhao",
            "Atharva Sehgal",
            "Jennifer J. Sun"
        ],
        "title": "Beyond Accuracy: Metrics that Uncover What Makes a `Good' Visual Descriptor",
        "abstract": "arXiv:2507.03542v1 Announce Type: new  Abstract: Text-based visual descriptors-ranging from simple class names to more descriptive phrases-are widely used in visual concept discovery and image classification with vision-language models (VLMs). Their effectiveness, however, depends on a complex interplay of factors, including semantic clarity, presence in the VLM's pre-training data, and how well the descriptors serve as a meaningful representation space. In this work, we systematically analyze descriptor quality along two key dimensions: (1) representational capacity, and (2) relationship with VLM pre-training data. We evaluate a spectrum of descriptor generation methods, from zero-shot LLM-generated prompts to iteratively refined descriptors. Motivated by ideas from representation alignment and language understanding, we introduce two alignment-based metrics-Global Alignment and CLIP Similarity-that move beyond accuracy. These metrics allow us to shed light on how different descriptor generation strategies interact with foundation model properties, offering insights into ways of studying descriptor effectiveness beyond accuracy evaluations.",
        "arxiv_id": "2507.03542",
        "ARXIVID": "2507.03542",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of visual descriptors and vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03903": {
        "authors": [
            "Hanzhe Liang",
            "Jie Zhang",
            "Tao Dai",
            "Linlin Shen",
            "Jinbao Wang",
            "Can Gao"
        ],
        "title": "Taming Anomalies with Down-Up Sampling Networks: Group Center Preserving Reconstruction for 3D Anomaly Detection",
        "abstract": "arXiv:2507.03903v1 Announce Type: new  Abstract: Reconstruction-based methods have demonstrated very promising results for 3D anomaly detection. However, these methods face great challenges in handling high-precision point clouds due to the large scale and complex structure. In this study, a Down-Up Sampling Network (DUS-Net) is proposed to reconstruct high-precision point clouds for 3D anomaly detection by preserving the group center geometric structure. The DUS-Net first introduces a Noise Generation module to generate noisy patches, which facilitates the diversity of training data and strengthens the feature representation for reconstruction. Then, a Down-sampling Network~(Down-Net) is developed to learn an anomaly-free center point cloud from patches with noise injection. Subsequently, an Up-sampling Network (Up-Net) is designed to reconstruct high-precision point clouds by fusing multi-scale up-sampling features. Our method leverages group centers for construction, enabling the preservation of geometric structure and providing a more precise point cloud. Extensive experiments demonstrate the effectiveness of our proposed method, achieving state-of-the-art (SOTA) performance with an Object-level AUROC of 79.9% and 79.5%, and a Point-level AUROC of 71.2% and 84.7% on the Real3D-AD and Anomaly-ShapeNet datasets, respectively.",
        "arxiv_id": "2507.03903",
        "ARXIVID": "2507.03903",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of 3D anomaly detection and reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04725": {
        "authors": [
            "Jizhou Han",
            "Shaokun Wang",
            "Yuhang He",
            "Chenhao Ding",
            "Qiang Wang",
            "Xinyuan Gao",
            "SongLin Dong",
            "Yihong Gong"
        ],
        "title": "Unleashing the Power of Neural Collapse: Consistent Supervised-Unsupervised Alignment for Generalized Category Discovery",
        "abstract": "arXiv:2507.04725v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) focuses on classifying known categories while simultaneously discovering novel categories from unlabeled data. However, previous GCD methods face challenges due to inconsistent optimization objectives and category confusion. This leads to feature overlap and ultimately hinders performance on novel categories. To address these issues, we propose the Neural Collapse-inspired Generalized Category Discovery (NC-GCD) framework. By pre-assigning and fixing Equiangular Tight Frame (ETF) prototypes, our method ensures an optimal geometric structure and a consistent optimization objective for both known and novel categories. We introduce a Consistent ETF Alignment Loss that unifies supervised and unsupervised ETF alignment and enhances category separability. Additionally, a Semantic Consistency Matcher (SCM) is designed to maintain stable and consistent label assignments across clustering iterations. Our method achieves strong performance on multiple GCD benchmarks, significantly enhancing novel category accuracy and demonstrating its effectiveness.",
        "arxiv_id": "2507.04725",
        "ARXIVID": "2507.04725",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of machine learning and category discovery.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04302": {
        "authors": [
            "Zuyu Zhang",
            "Ning Chen",
            "Yongshan Liu",
            "Qinghua Zhang",
            "Xu Zhang"
        ],
        "title": "Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization",
        "abstract": "arXiv:2507.04302v1 Announce Type: new  Abstract: Single Domain Generalization (SDG) aims to develop models capable of generalizing to unseen target domains using only one source domain, a task complicated by substantial domain shifts and limited data diversity. Existing SDG approaches primarily rely on data augmentation techniques, which struggle to effectively adapt training dynamics to accommodate large domain shifts. To address this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided optimization approach inspired by dynamical systems theory. By leveraging LE measurements to modulate the learning rate, LEAwareSGD encourages model training near the edge of chaos, a critical state that optimally balances stability and adaptability. This dynamic adjustment allows the model to explore a wider parameter space and capture more generalizable features, ultimately enhancing the model's generalization capability. Extensive experiments on PACS, OfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial generalization gains, achieving up to 9.47\\% improvement on PACS in low-data regimes. These results underscore the effectiveness of training near the edge of chaos for enhancing model generalization capability in SDG tasks.",
        "arxiv_id": "2507.04302",
        "ARXIVID": "2507.04302",
        "COMMENT": "Does not match any specific criterion but focuses on single domain generalization, which is tangentially related to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04529": {
        "authors": [
            "Philipp Reis",
            "Joshua Ransiek",
            "David Petri",
            "Jacob Langner",
            "Eric Sax"
        ],
        "title": "A Data-Driven Novelty Score for Diverse In-Vehicle Data Recording",
        "abstract": "arXiv:2507.04529v1 Announce Type: new  Abstract: High-quality datasets are essential for training robust perception systems in autonomous driving. However, real-world data collection is often biased toward common scenes and objects, leaving novel cases underrepresented. This imbalance hinders model generalization and compromises safety. The core issue is the curse of rarity. Over time, novel events occur infrequently, and standard logging methods fail to capture them effectively. As a result, large volumes of redundant data are stored, while critical novel cases are diluted, leading to biased datasets. This work presents a real-time data selection method focused on object-level novelty detection to build more balanced and diverse datasets. The method assigns a data-driven novelty score to image frames using a novel dynamic Mean Shift algorithm. It models normal content based on mean and covariance statistics to identify frames with novel objects, discarding those with redundant elements. The main findings show that reducing the training dataset size with this method can improve model performance, whereas higher redundancy tends to degrade it. Moreover, as data redundancy increases, more aggressive filtering becomes both possible and beneficial. While random sampling can offer some gains, it often leads to overfitting and unpredictability in outcomes. The proposed method supports real-time deployment with 32 frames per second and is constant over time. By continuously updating the definition of normal content, it enables efficient detection of novelties in a continuous data stream.",
        "arxiv_id": "2507.04529",
        "ARXIVID": "2507.04529",
        "COMMENT": "Does not match any specific criterion but is tangentially related to data collection for autonomous systems, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04710": {
        "authors": [
            "Anbang Wang",
            "Marawan Elbatel",
            "Keyuan Liu",
            "Lizhuo Lin",
            "Meng Lan",
            "Yanqi Yang",
            "Xiaomeng Li"
        ],
        "title": "Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model",
        "abstract": "arXiv:2507.04710v1 Announce Type: new  Abstract: Accurate detection of anatomic landmarks is essential for assessing alveolar bone and root conditions, thereby optimizing clinical outcomes in orthodontics, periodontics, and implant dentistry. Manual annotation of landmarks on cone-beam computed tomography (CBCT) by dentists is time-consuming, labor-intensive, and subject to inter-observer variability. Deep learning-based automated methods present a promising approach to streamline this process efficiently. However, the scarcity of training data and the high cost of expert annotations hinder the adoption of conventional deep learning techniques. To overcome these challenges, we introduce GeoSapiens, a novel few-shot learning framework designed for robust dental landmark detection using limited annotated CBCT of anterior teeth. Our GeoSapiens framework comprises two key components: (1) a robust baseline adapted from Sapiens, a foundational model that has achieved state-of-the-art performance in human-centric vision tasks, and (2) a novel geometric loss function that improves the model's capacity to capture critical geometric relationships among anatomical structures. Experiments conducted on our collected dataset of anterior teeth landmarks revealed that GeoSapiens surpassed existing landmark detection methods, outperforming the leading approach by an 8.18% higher success detection rate at a strict 0.5 mm threshold-a standard widely recognized in dental diagnostics. Code is available at: https://github.com/xmed-lab/GeoSapiens.",
        "arxiv_id": "2507.04710",
        "ARXIVID": "2507.04710",
        "COMMENT": "Does not match any specific criterion but is relevant to medical imaging and landmark detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04105": {
        "authors": [
            "Jinwei Hu",
            "Yi Dong",
            "Zhengtao Ding",
            "Xiaowei Huang"
        ],
        "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing",
        "abstract": "arXiv:2507.04105v1 Announce Type: new  Abstract: This paper presents a defense framework for enhancing the safety of large language model (LLM) empowered multi-agent systems (MAS) in safety-critical domains such as aerospace. We apply randomized smoothing, a statistical robustness certification technique, to the MAS consensus context, enabling probabilistic guarantees on agent decisions under adversarial influence. Unlike traditional verification methods, our approach operates in black-box settings and employs a two-stage adaptive sampling mechanism to balance robustness and computational efficiency. Simulation results demonstrate that our method effectively prevents the propagation of adversarial behaviors and hallucinations while maintaining consensus performance. This work provides a practical and scalable path toward safe deployment of LLM-based MAS in real-world, high-stakes environments.",
        "arxiv_id": "2507.04105",
        "ARXIVID": "2507.04105",
        "COMMENT": "Does not match any specific criterion but is tangentially related to multi-agent systems and robustness in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.05221": {
        "authors": [
            "Samuel Barbeau",
            "Pedram Fekri",
            "David Osowiechi",
            "Ali Bahri",
            "Moslem YazdanpanahMasih Aminbeidokhti",
            "Christian Desrosiers"
        ],
        "title": "CTA: Cross-Task Alignment for Better Test Time Training",
        "abstract": "arXiv:2507.05221v1 Announce Type: new  Abstract: Deep learning models have demonstrated exceptional performance across a wide range of computer vision tasks. However, their performance often degrades significantly when faced with distribution shifts, such as domain or dataset changes. Test-Time Training (TTT) has emerged as an effective method to enhance model robustness by incorporating an auxiliary unsupervised task during training and leveraging it for model updates at test time. In this work, we introduce CTA (Cross-Task Alignment), a novel approach for improving TTT. Unlike existing TTT methods, CTA does not require a specialized model architecture and instead takes inspiration from the success of multi-modal contrastive learning to align a supervised encoder with a self-supervised one. This process enforces alignment between the learned representations of both models, thereby mitigating the risk of gradient interference, preserving the intrinsic robustness of self-supervised learning and enabling more semantically meaningful updates at test-time. Experimental results demonstrate substantial improvements in robustness and generalization over the state-of-the-art on several benchmark datasets.",
        "arxiv_id": "2507.05221",
        "ARXIVID": "2507.05221",
        "COMMENT": "Does not match any specific criteria but is related to improving robustness in test-time training.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04404": {
        "authors": [
            "Jingze Zhu",
            "Yongliang Wu",
            "Wenbo Zhu",
            "Jiawang Cao",
            "Yanqiang Zheng",
            "Jiawei Chen",
            "Xu Yang",
            "Bernt Schiele",
            "Jonas Fischer",
            "Xinting Hu"
        ],
        "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers",
        "abstract": "arXiv:2507.04404v1 Announce Type: new  Abstract: Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledge-intensive tasks. While decoding-time strategies provide a promising efficient solution without training, existing methods typically treat token-level and layer-level signals in isolation, overlooking the joint dynamics between them. In this work, we introduce a token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks.",
        "arxiv_id": "2507.04404",
        "ARXIVID": "2507.04404",
        "COMMENT": "Does not match any specific criteria but is related to improving factuality in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04947": {
        "authors": [
            "Yecheng Wu",
            "Junyu Chen",
            "Zhuoyang Zhang",
            "Enze Xie",
            "Jincheng Yu",
            "Junsong Chen",
            "Jinyi Hu",
            "Yao Lu",
            "Song Han",
            "Han Cai"
        ],
        "title": "DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer",
        "abstract": "arXiv:2507.04947v1 Announce Type: new  Abstract: We introduce DC-AR, a novel masked autoregressive (AR) text-to-image generation framework that delivers superior image generation quality with exceptional computational efficiency. Due to the tokenizers' limitations, prior masked AR models have lagged behind diffusion models in terms of quality or efficiency. We overcome this limitation by introducing DC-HT - a deep compression hybrid tokenizer for AR models that achieves a 32x spatial compression ratio while maintaining high reconstruction fidelity and cross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT and create a new hybrid masked autoregressive image generation framework that first produces the structural elements through discrete tokens and then applies refinements via residual tokens. DC-AR achieves state-of-the-art results with a gFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while offering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to prior leading diffusion and autoregressive models.",
        "arxiv_id": "2507.04947",
        "ARXIVID": "2507.04947",
        "COMMENT": "Does not match any specific criteria but is related to efficient text-to-image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04482": {
        "authors": [
            "Kyoungmin Lee",
            "Jihun Park",
            "Jongmin Gim",
            "Wonhyeok Choi",
            "Kyumin Hwang",
            "Jaeyeul Kim",
            "Sunghoon Im"
        ],
        "title": "A Training-Free Style-Personalization via Scale-wise Autoregressive Model",
        "abstract": "arXiv:2507.04482v1 Announce Type: new  Abstract: We present a training-free framework for style-personalized image generation that controls content and style information during inference using a scale-wise autoregressive model. Our method employs a three-path design--content, style, and generation--each guided by a corresponding text prompt, enabling flexible and efficient control over image semantics without any additional training. A central contribution of this work is a step-wise and attention-wise intervention analysis. Through systematic prompt and feature injection, we find that early-to-middle generation steps play a pivotal role in shaping both content and style, and that query features predominantly encode content-specific information. Guided by these insights, we introduce two targeted mechanisms: Key Stage Attention Sharing, which aligns content and style during the semantically critical steps, and Adaptive Query Sharing, which reinforces content semantics in later steps through similarity-aware query blending. Extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.",
        "arxiv_id": "2507.04482",
        "ARXIVID": "2507.04482",
        "COMMENT": "Does not match any specific criteria but is related to style-personalized image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.04207": {
        "authors": [
            "Yu-Shan Tai (Andy)",
            "An-Yeu (Andy)",
            "Wu"
        ],
        "title": "Quick Bypass Mechanism of Zero-Shot Diffusion-Based Image Restoration",
        "abstract": "arXiv:2507.04207v1 Announce Type: new  Abstract: Recent advancements in diffusion models have demonstrated remarkable success in various image generation tasks. Building upon these achievements, diffusion models have also been effectively adapted to image restoration tasks, e.g., super-resolution and deblurring, aiming to recover high-quality images from degraded inputs. Although existing zero-shot approaches enable pretrained diffusion models to perform restoration tasks without additional fine-tuning, these methods often suffer from prolonged iteration times in the denoising process. To address this limitation, we propose a Quick Bypass Mechanism (QBM), a strategy that significantly accelerates the denoising process by initializing from an intermediate approximation, effectively bypassing early denoising steps. Furthermore, recognizing that approximation may introduce inconsistencies, we introduce a Revised Reverse Process (RRP), which adjusts the weighting of random noise to enhance the stochasticity and mitigate potential disharmony. We validate proposed methods on ImageNet-1K and CelebA-HQ across multiple image restoration tasks, e.g., super-resolution, deblurring, and compressed sensing. Our experimental results show that the proposed methods can effectively accelerate existing methods while maintaining original performance.",
        "arxiv_id": "2507.04207",
        "ARXIVID": "2507.04207",
        "COMMENT": "Does not match any specific criteria but is related to image restoration using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.03722": {
        "authors": [
            "Ruian Ke",
            "Ruy M. Ribeiro"
        ],
        "title": "Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology",
        "abstract": "arXiv:2507.03722v1 Announce Type: new  Abstract: Large language models (LLMs) are powerful artificial intelligence (AI) tools transforming how research is conducted. However, their use in research has been met with skepticism, due to concerns about hallucinations, biases and potential harms to research. These emphasize the importance of clearly understanding the strengths and weaknesses of LLMs to ensure their effective and responsible use. Here, we present a roadmap for integrating LLMs into cross-disciplinary research, where effective communication, knowledge transfer and collaboration across diverse fields are essential but often challenging. We examine the capabilities and limitations of LLMs and provide a detailed computational biology case study (on modeling HIV rebound dynamics) demonstrating how iterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary collaboration and research. We argue that LLMs are best used as augmentative tools within a human-in-the-loop framework. Looking forward, we envisage that the responsible use of LLMs will enhance innovative cross-disciplinary research and substantially accelerate scientific discoveries.",
        "arxiv_id": "2507.03722",
        "ARXIVID": "2507.03722",
        "COMMENT": "Does not match any specific criteria. Focuses on using LLMs for cross-disciplinary research, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.03811": {
        "authors": [
            "Gianlucca Zuin",
            "Saulo Mastelini",
            "T\\'ulio Loures",
            "Adriano Veloso"
        ],
        "title": "Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts",
        "abstract": "arXiv:2507.03811v1 Announce Type: new  Abstract: Documenting tacit knowledge in organizations can be a challenging task due to incomplete initial information, difficulty in identifying knowledgeable individuals, the interplay of formal hierarchies and informal networks, and the need to ask the right questions. To address this, we propose an agent-based framework leveraging large language models (LLMs) to iteratively reconstruct dataset descriptions through interactions with employees. Modeling knowledge dissemination as a Susceptible-Infectious (SI) process with waning infectivity, we conduct 864 simulations across various synthetic company structures and different dissemination parameters. Our results show that the agent achieves 94.9% full-knowledge recall, with self-critical feedback scores strongly correlating with external literature critic scores. We analyze how each simulation parameter affects the knowledge retrieval process for the agent. In particular, we find that our approach is able to recover information without needing to access directly the only domain specialist. These findings highlight the agent's ability to navigate organizational complexity and capture fragmented knowledge that would otherwise remain inaccessible.",
        "arxiv_id": "2507.03811",
        "ARXIVID": "2507.03811",
        "COMMENT": "Does not match any specific criteria. Focuses on organizational knowledge discovery using LLMs, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.04456": {
        "authors": [
            "Haotong Qin",
            "Xianglong Liu",
            "Xudong Ma",
            "Lei Ke",
            "Yulun Zhang",
            "Jie Luo",
            "Michele Magno"
        ],
        "title": "BiVM: Accurate Binarized Neural Network for Efficient Video Matting",
        "abstract": "arXiv:2507.04456v1 Announce Type: new  Abstract: Deep neural networks for real-time video matting suffer significant computational limitations on edge devices, hindering their adoption in widespread applications such as online conferences and short-form video production. Binarization emerges as one of the most common compression approaches with compact 1-bit parameters and efficient bitwise operations. However, accuracy and efficiency limitations exist in the binarized video matting network due to its degenerated encoder and redundant decoder. Following a theoretical analysis based on the information bottleneck principle, the limitations are mainly caused by the degradation of prediction-relevant information in the intermediate features and the redundant computation in prediction-irrelevant areas. We present BiVM, an accurate and resource-efficient Binarized neural network for Video Matting. First, we present a series of binarized computation structures with elastic shortcuts and evolvable topologies, enabling the constructed encoder backbone to extract high-quality representation from input videos for accurate prediction. Second, we sparse the intermediate feature of the binarized decoder by masking homogeneous parts, allowing the decoder to focus on representation with diverse details while alleviating the computation burden for efficient inference. Furthermore, we construct a localized binarization-aware mimicking framework with the information-guided strategy, prompting matting-related representation in full-precision counterparts to be accurately and fully utilized. Comprehensive experiments show that the proposed BiVM surpasses alternative binarized video matting networks, including state-of-the-art (SOTA) binarization methods, by a substantial margin. Moreover, our BiVM achieves significant savings of 14.3x and 21.6x in computation and storage costs, respectively. We also evaluate BiVM on ARM CPU hardware.",
        "arxiv_id": "2507.04456",
        "ARXIVID": "2507.04456",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.03829": {
        "authors": [
            "George Hannah",
            "Jacopo de Berardinis",
            "Terry R. Payne",
            "Valentina Tamma",
            "Andrew Mitchell",
            "Ellen Piercy",
            "Ewan Johnson",
            "Andrew Ng",
            "Harry Rostron",
            "Boris Konev"
        ],
        "title": "RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation",
        "abstract": "arXiv:2507.03829v1 Announce Type: new  Abstract: A large volume of XML data is produced in experiments carried out by robots in laboratories. In order to support the interoperability of data between labs, there is a motivation to translate the XML data into a knowledge graph. A key stage of this process is the enrichment of the XML schema to lay the foundation of an ontology schema. To achieve this, we present the RELRaE framework, a framework that employs large language models in different stages to extract and accurately label the relationships implicitly present in the XML schema. We investigate the capability of LLMs to accurately generate these labels and then evaluate them. Our work demonstrates that LLMs can be effectively used to support the generation of relationship labels in the context of lab automation, and that they can play a valuable role within semi-automatic ontology generation frameworks more generally.",
        "arxiv_id": "2507.03829",
        "ARXIVID": "2507.03829",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of large language models and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.02963": {
        "authors": [
            "Hengyi Zhu",
            "Linye Wei",
            "He Li"
        ],
        "title": "VR-YOLO: Enhancing PCB Defect Detection with Viewpoint Robustness Based on YOLO",
        "abstract": "arXiv:2507.02963v1 Announce Type: new  Abstract: The integration of large-scale circuits and systems emphasizes the importance of automated defect detection of electronic components. The YOLO image detection model has been used to detect PCB defects and it has become a typical AI-assisted case of traditional industrial production. However, conventional detection algorithms have stringent requirements for the angle, orientation, and clarity of target images. In this paper, we propose an enhanced PCB defect detection algorithm, named VR-YOLO, based on the YOLOv8 model. This algorithm aims to improve the model's generalization performance and enhance viewpoint robustness in practical application scenarios. We first propose a diversified scene enhancement (DSE) method by expanding the PCB defect dataset by incorporating diverse scenarios and segmenting samples to improve target diversity. A novel key object focus (KOF) scheme is then presented by considering angular loss and introducing an additional attention mechanism to enhance fine-grained learning of small target features. Experimental results demonstrate that our improved PCB defect detection approach achieves a mean average precision (mAP) of 98.9% for the original test images, and 94.7% for the test images with viewpoint shifts (horizontal and vertical shear coefficients of $\\pm 0.06$ and rotation angle of $\\pm 10$ degrees), showing significant improvements compared to the baseline YOLO model with negligible additional computational cost.",
        "arxiv_id": "2507.02963",
        "ARXIVID": "2507.02963",
        "COMMENT": "Does not match any specific criteria but is related to improving defect detection in industrial applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}