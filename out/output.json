{
    "2509.17664": {
        "authors": [
            "Pingyi Chen",
            "Yujing Lou",
            "Shen Cao",
            "Jinhui Guo",
            "Lubin Fan",
            "Yue Wu",
            "Lin Yang",
            "Lizhuang Ma",
            "Jieping Ye"
        ],
        "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models",
        "abstract": "arXiv:2509.17664v1 Announce Type: new  Abstract: While vision language models (VLMs) excel in 2D semantic visual understanding, their ability to quantitatively reason about 3D spatial relationships remains under-explored, due to the deficiency of 2D images' spatial representation ability. In this paper, we analyze the problem hindering VLMs' spatial understanding abilities and propose SD-VLM, a novel framework that significantly enhances fundamental spatial perception abilities of VLMs through two key contributions: (1) propose Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduce a simple depth positional encoding method strengthening VLMs' spatial awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented samples. We have trained SD-VLM, a strong generalist VLM which shows superior quantitative spatial measuring and understanding capability. SD-VLM not only achieves state-of-the-art performance on our proposed MSMU-Bench, but also shows spatial generalization abilities on other spatial understanding benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively on MSMU-Bench. Code and models are released at https://github.com/cpystan/SD-VLM.",
        "arxiv_id": "2509.17664",
        "ARXIVID": "2509.17664",
        "COMMENT": "Matches criteria 1 and 2 as it enhances spatial reasoning in vision-language models, which aligns with spatial intelligence and multimodal large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.16421": {
        "authors": [
            "Aiden Chang",
            "Celso De Melo",
            "Stephanie M. Lukin"
        ],
        "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead",
        "abstract": "arXiv:2509.16421v1 Announce Type: new  Abstract: Real-time understanding of continuous video streams is essential for intelligent agents operating in high-stakes environments, including autonomous vehicles, surveillance drones, and disaster response robots. Yet, most existing video understanding and highlight detection methods assume access to the entire video during inference, making them unsuitable for online or streaming scenarios. In particular, current models optimize for offline summarization, failing to support step-by-step reasoning needed for real-time decision-making. We introduce Aha, an autoregressive highlight detection framework that predicts the relevance of each video frame against a task described in natural language. Without accessing future video frames, Aha utilizes a multimodal vision-language model and lightweight, decoupled heads trained on a large, curated dataset of human-centric video labels. To enable scalability, we introduce the Dynamic SinkCache mechanism that achieves constant memory usage across infinite-length streams without degrading performance on standard benchmarks. This encourages the hidden representation to capture high-level task objectives, enabling effective frame-level rankings for informativeness, relevance, and uncertainty with respect to the natural language task. Aha achieves state-of-the-art (SOTA) performance on highlight detection benchmarks, surpassing even prior offline, full-context approaches and video-language models by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision). We explore Aha's potential for real-world robotics applications given a task-oriented natural language input and a continuous, robot-centric video. Both experiments demonstrate Aha's potential effectiveness as a real-time reasoning module for downstream planning and long-horizon understanding.",
        "arxiv_id": "2509.16421",
        "ARXIVID": "2509.16421",
        "COMMENT": "Matches criteria 6 as it focuses on real-time video understanding and highlight detection with novel methodologies.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.17158": {
        "authors": [
            "Pierre Andrews",
            "Amine Benhalloum",
            "Gerard Moreno-Torres Bertran",
            "Matteo Bettini",
            "Amar Budhiraja",
            "Ricardo Silveira Cabral",
            "Virginie Do",
            "Romain Froger",
            "Emilien Garreau",
            "Jean-Baptiste Gaya",
            "Hugo Lauren\\c{c}on",
            "Maxime Lecanu",
            "Kunal Malkan",
            "Dheeraj Mekala",
            "Pierre M\\'enard",
            "Gr\\'egoire Mialon",
            "Ulyana Piterbarg",
            "Mikhail Plekhanov",
            "Mathieu Rita",
            "Andrey Rusakov",
            "Thomas Scialom",
            "Vladislav Vorotilov",
            "Mengjue Wang",
            "Ian Yu"
        ],
        "title": "ARE: Scaling Up Agent Environments and Evaluations",
        "abstract": "arXiv:2509.17158v1 Announce Type: new  Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.",
        "arxiv_id": "2509.17158",
        "ARXIVID": "2509.17158",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark (Gaia2) for embodied AI and agent environments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.16972": {
        "authors": [
            "Quanzhu Niu",
            "Dengxian Gong",
            "Shihao Chen",
            "Tao Zhang",
            "Yikang Zhou",
            "Haobo Yuan",
            "Lu Qi",
            "Xiangtai Li",
            "Shunping Ji"
        ],
        "title": "The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA",
        "abstract": "arXiv:2509.16972v1 Announce Type: new  Abstract: Referring video object segmentation (RVOS) requires segmenting and tracking objects in videos conditioned on natural-language expressions, demanding fine-grained understanding of both appearance and motion. Building on Sa2VA, which couples a Multi-modal Large Language Model (MLLM) with the video segmentation model SAM2, we identify two key bottlenecks that limit segmentation performance: sparse frame sampling and reliance on a single [SEG] token for an entire video. We propose Segmentation Augmented and Selective Averaged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge (RVOS track), SaSaSa2VA achieves a $J\\&F$ of 67.45, ranking first and surpassing the runner-up by 2.80 points. This result and ablation studies demonstrate that efficient segmentation augmentation and test-time ensembling substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA repository: https://github.com/magic-research/Sa2VA.",
        "arxiv_id": "2509.16972",
        "ARXIVID": "2509.16972",
        "COMMENT": "Matches criteria 2 and 5 as it explores a Multi-modal Large Language Model (MLLM) for video object segmentation and integrates vision-language tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.18041": {
        "authors": [
            "Sahil Shah",
            "S P Sharan",
            "Harsh Goel",
            "Minkyu Choi",
            "Mustafa Munir",
            "Manvik Pasula",
            "Radu Marculescu",
            "Sandeep Chinchali"
        ],
        "title": "NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning",
        "abstract": "arXiv:2509.18041v1 Announce Type: new  Abstract: Long-Form Video Question Answering (LVQA) poses challenges beyond traditional visual question answering (VQA), which is often limited to static images or short video clips. While current vision-language models (VLMs) perform well in those settings, they struggle with complex queries in LVQA over long videos involving multi-step temporal reasoning and causality. Vanilla approaches, which sample frames uniformly and feed them to a VLM with the question, incur significant token overhead, forcing severe downsampling. As a result, the model often misses fine-grained visual structure, subtle event transitions, or key temporal cues, ultimately leading to incorrect answers. To address these limitations, recent works have explored query-adaptive frame sampling, hierarchical keyframe selection, and agent-based iterative querying. However, these methods remain fundamentally heuristic: they lack explicit temporal representations and cannot enforce or verify logical event relationships. As a result, there are no formal guarantees that the sampled context actually encodes the compositional or causal logic demanded by the question. To address these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language question into a formal temporal logic expression, constructs a video automaton from frame-level semantic propositions, and applies model checking to rigorously identify video segments satisfying the question's logical requirements. Only these logic-verified segments are submitted to the VLM, thus improving interpretability, reducing hallucinations, and enabling compositional reasoning without modifying or fine-tuning the model. Experiments on LongVideoBench and CinePile show NeuS-QA improves performance by over 10%, especially on questions involving event ordering, causality, and multi-step compositional reasoning.",
        "arxiv_id": "2509.18041",
        "ARXIVID": "2509.18041",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding with a novel neuro-symbolic reasoning approach for long-form video question answering.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.18094": {
        "authors": [
            "Ye Liu",
            "Zongyang Ma",
            "Junfu Pu",
            "Zhongang Qi",
            "Yang Wu",
            "Ying Shan",
            "Chang Wen Chen"
        ],
        "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning",
        "abstract": "arXiv:2509.18094v1 Announce Type: new  Abstract: Recent advances in Large Multi-modal Models (LMMs) have demonstrated their remarkable success as general-purpose multi-modal assistants, with particular focuses on holistic image- and video-language understanding. Conversely, less attention has been given to scaling fine-grained pixel-level understanding capabilities, where the models are expected to realize pixel-level alignment between visual signals and language semantics. Some previous studies have applied LMMs to related tasks such as region-level captioning and referring expression segmentation. However, these models are limited to performing either referring or segmentation tasks independently and fail to integrate these fine-grained perception capabilities into visual reasoning. To bridge this gap, we propose UniPixel, a large multi-modal model capable of flexibly comprehending visual prompt inputs and generating mask-grounded responses. Our model distinguishes itself by seamlessly integrating pixel-level perception with general visual understanding capabilities. Specifically, UniPixel processes visual prompts and generates relevant masks on demand, and performs subsequent reasoning conditioning on these intermediate pointers during inference, thereby enabling fine-grained pixel-level reasoning. The effectiveness of our approach has been verified on 10 benchmarks across a diverse set of tasks, including pixel-level referring/segmentation and object-centric understanding in images/videos. A novel PixelQA task that jointly requires referring, segmentation, and question answering is also designed to verify the flexibility of our method.",
        "arxiv_id": "2509.18094",
        "ARXIVID": "2509.18094",
        "COMMENT": "Matches criterion 2 as it explores a large multi-modal model (LMM) for pixel-level visual reasoning and integrates vision-language understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.16618": {
        "authors": [
            "Pengfei Hao",
            "Hongqiu Wang",
            "Shuaibo Li",
            "Zhaohu Xing",
            "Guang Yang",
            "Kaishun Wu",
            "Lei Zhu"
        ],
        "title": "Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery",
        "abstract": "arXiv:2509.16618v1 Announce Type: new  Abstract: In recent years, Visual Question Localized-Answering in robotic surgery (Surgical-VQLA) has gained significant attention for its potential to assist medical students and junior doctors in understanding surgical scenes. Recently, the rapid development of Large Language Models (LLMs) has provided more promising solutions for this task. However, current methods struggle to establish complex dependencies between text and visual details, and have difficulty perceiving the spatial information of surgical scenes. To address these challenges, we propose a novel method, Surgical-MambaLLM, which is the first to combine Mamba2 with LLM in the surgical domain, that leverages Mamba2's ability to effectively capture cross-modal dependencies and perceive spatial information in surgical scenes, thereby enhancing the LLMs' understanding of surgical images. Specifically, we propose the Cross-modal Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective multimodal fusion, with its cross-modal integration capabilities. Additionally, tailored to the geometric characteristics of surgical scenes, we design the Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the surgical images, enhancing the model's spatial understanding of the surgical scene. Extensive experiments demonstrate that our Surgical-MambaLLM model outperforms the state-of-the-art methods on the EndoVis17-VQLA and EndoVis18-VQLA datasets, significantly improving the performance of the Surgical-VQLA task.",
        "arxiv_id": "2509.16618",
        "ARXIVID": "2509.16618",
        "COMMENT": "Matches criteria 2 and 5 as it explores a multimodal large language model (MLLM) for vision-language tasks in robotic surgery, with a focus on spatial understanding and cross-modal integration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.17425": {
        "authors": [
            "Zhenliang Zhang",
            "Yuxi Wang",
            "Hongzhao Xie",
            "Shiyun Zhao",
            "Mingyuan Liu",
            "Yujie Lu",
            "Xinyi He",
            "Zhenku Cheng",
            "Yujia Peng"
        ],
        "title": "Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments",
        "abstract": "arXiv:2509.17425v1 Announce Type: new  Abstract: A key feature differentiating artificial general intelligence (AGI) from traditional AI is that AGI can perform composite tasks that require a wide range of capabilities. Although embodied agents powered by multimodal large language models (MLLMs) offer rich perceptual and interactive capabilities, it remains largely unexplored whether they can solve composite tasks. In the current work, we designed a set of composite tasks inspired by common daily activities observed in early childhood development. Within a dynamic and simulated home environment, these tasks span three core domains: object understanding, spatial intelligence, and social activity. We evaluated 17 leading proprietary and open-source MLLMs on these tasks. The results consistently showed poor performance across all three domains, indicating a substantial gap between current capabilities and general intelligence requirements. Together, our tasks offer a preliminary framework for evaluating the general capabilities of embodied agents, marking an early but significant step toward the development of embodied MLLMs and their real-world deployment.",
        "arxiv_id": "2509.17425",
        "ARXIVID": "2509.17425",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it evaluates MLLMs in a simulated home environment with tasks involving spatial intelligence and multimodal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.17079": {
        "authors": [
            "Yuhong Feng",
            "Hongtao Chen",
            "Qi Zhang",
            "Jie Chen",
            "Zhaoxi He",
            "Mingzhe Liu",
            "Jianghai Liao"
        ],
        "title": "A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion",
        "abstract": "arXiv:2509.17079v1 Announce Type: new  Abstract: Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in challenging conditions. While recent Transformer-based methods excel at capturing global context, their inherent lack of spatial inductive bias causes attention to spread to irrelevant background regions, compromising crowd localization precision. Furthermore, effectively bridging the gap between these distinct modalities remains a major hurdle. To tackle this, we propose the Dual Modulation Framework, comprising two modules: Spatially Modulated Attention (SMA), which improves crowd localization by using a learnable Spatial Decay Mask to penalize attention between distant tokens and prevent focus from spreading to the background; and Adaptive Fusion Modulation (AFM), which implements a dynamic gating mechanism to prioritize the most reliable modality for adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting datasets demonstrate the superior performance of our method compared to previous works. Code available at https://github.com/Cht2924/RGBT-Crowd-Counting.",
        "arxiv_id": "2509.17079",
        "ARXIVID": "2509.17079",
        "COMMENT": "Matches criteria 4 as it focuses on a vision foundation model for RGB-T crowd counting with novel spatial and fusion mechanisms.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17650": {
        "authors": [
            "Soroush Mahdi",
            "Fardin Ayar",
            "Ehsan Javanmardi",
            "Manabu Tsukada",
            "Mahdi Javanmardi"
        ],
        "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers",
        "abstract": "arXiv:2509.17650v1 Announce Type: new  Abstract: Streaming visual transformers like StreamVGGT achieve strong 3D perception but suffer from unbounded growth of key value (KV) memory, which limits scalability. We propose a training-free, inference-time token eviction policy that bounds memory by discarding redundant tokens while keeping the most informative ones. Our method uses significantly less memory with little to no drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from 18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under strict memory budgets, eviction enables denser frame sampling, which improves reconstruction accuracy compared to the baseline. Experiments across video depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and camera pose estimation (Sintel, TUM-dynamics) show that our approach closely matches StreamVGGT at a fraction of the memory and makes long-horizon streaming inference more practical.",
        "arxiv_id": "2509.17650",
        "ARXIVID": "2509.17650",
        "COMMENT": "Matches criteria 6 as it addresses video-based tasks like depth estimation and 3D reconstruction with memory-efficient methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17191": {
        "authors": [
            "Jinchao Ge",
            "Tengfei Cheng",
            "Biao Wu",
            "Zeyu Zhang",
            "Shiya Huang",
            "Judith Bishop",
            "Gillian Shepherd",
            "Meng Fang",
            "Ling Chen",
            "Yang Zhao"
        ],
        "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
        "abstract": "arXiv:2509.17191v1 Announce Type: new  Abstract: Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA.",
        "arxiv_id": "2509.17191",
        "ARXIVID": "2509.17191",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VaseVQA) for multimodal agents, and criterion 2 as it involves multimodal language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.16483": {
        "authors": [
            "Xujia Zhang",
            "Brendan Crowe",
            "Christoffer Heckman"
        ],
        "title": "Octree Latent Diffusion for Semantic 3D Scene Generation and Completion",
        "abstract": "arXiv:2509.16483v1 Announce Type: new  Abstract: The completion, extension, and generation of 3D semantic scenes are an interrelated set of capabilities that are useful for robotic navigation and exploration. Existing approaches seek to decouple these problems and solve them oneoff. Additionally, these approaches are often domain-specific, requiring separate models for different data distributions, e.g. indoor vs. outdoor scenes. To unify these techniques and provide cross-domain compatibility, we develop a single framework that can perform scene completion, extension, and generation in both indoor and outdoor scenes, which we term Octree Latent Semantic Diffusion. Our approach operates directly on an efficient dual octree graph latent representation: a hierarchical, sparse, and memory-efficient occupancy structure. This technique disentangles synthesis into two stages: (i) structure diffusion, which predicts binary split signals to construct a coarse occupancy octree, and (ii) latent semantic diffusion, which generates semantic embeddings decoded by a graph VAE into voxellevel semantic labels. To perform semantic scene completion or extension, our model leverages inference-time latent inpainting, or outpainting respectively. These inference-time methods use partial LiDAR scans or maps to condition generation, without the need for retraining or finetuning. We demonstrate highquality structure, coherent semantics, and robust completion from single LiDAR scans, as well as zero-shot generalization to out-of-distribution LiDAR data. These results indicate that completion-through-generation in a dual octree graph latent space is a practical and scalable alternative to regression-based pipelines for real-world robotic perception tasks.",
        "arxiv_id": "2509.16483",
        "ARXIVID": "2509.16483",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for 3D semantic scene generation and completion, relevant to robotic navigation and exploration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17107": {
        "authors": [
            "Lingzhao Kong",
            "Jiacheng Lin",
            "Siyu Li",
            "Kai Luo",
            "Zhiyong Li",
            "Kailun Yang"
        ],
        "title": "CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception",
        "abstract": "arXiv:2509.17107v1 Announce Type: new  Abstract: Collaborative perception aims to extend sensing coverage and improve perception accuracy by sharing information among multiple agents. However, due to differences in viewpoints and spatial positions, agents often acquire heterogeneous observations. Existing intermediate fusion methods primarily focus on aligning similar features, often overlooking the perceptual diversity among agents. To address this limitation, we propose CoBEVMoE, a novel collaborative perception framework that operates in the Bird's Eye View (BEV) space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In DMoE, each expert is dynamically generated based on the input features of a specific agent, enabling it to extract distinctive and reliable cues while attending to shared semantics. This design allows the fusion process to explicitly model both feature similarity and heterogeneity across agents. Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance inter-expert diversity and improve the discriminability of the fused representation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets demonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically, it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the effectiveness of expert-based heterogeneous feature modeling in multi-agent collaborative perception. The source code will be made publicly available at https://github.com/godk0509/CoBEVMoE.",
        "arxiv_id": "2509.17107",
        "ARXIVID": "2509.17107",
        "COMMENT": "Matches criterion 3 as it introduces a novel collaborative perception framework for multi-agent systems, focusing on feature fusion and heterogeneity.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.18056": {
        "authors": [
            "Yunheng Li",
            "Jing Cheng",
            "Shaoyong Jia",
            "Hangyi Kuang",
            "Shaohui Jiao",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs",
        "abstract": "arXiv:2509.18056v1 Announce Type: new  Abstract: This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1",
        "arxiv_id": "2509.18056",
        "ARXIVID": "2509.18056",
        "COMMENT": "Matches criteria 2 and 6 as it focuses on video large language models (Video LLMs) and temporal grounding tasks, with novel reinforcement fine-tuning strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17429": {
        "authors": [
            "Zhitao Zeng",
            "Guojian Yuan",
            "Junyuan Mao",
            "Yuxuan Wang",
            "Xiaoshuang Jia",
            "Yueming Jin"
        ],
        "title": "Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration",
        "abstract": "arXiv:2509.17429v1 Announce Type: new  Abstract: Accurate temporal prediction is the bridge between comprehensive scene understanding and embodied artificial intelligence. However, predicting multiple fine-grained states of a scene at multiple temporal scales is difficult for vision-language models. We formalize the Multi-Scale Temporal Prediction (MSTP) task in general and surgical scenes by decomposing multi-scale into two orthogonal dimensions: the temporal scale, forecasting states of humans and surgery at varying look-ahead intervals, and the state scale, modeling a hierarchy of states in general and surgical scenes. For example, in general scenes, states of contact relationships are finer-grained than states of spatial relationships. In surgical scenes, medium-level steps are finer-grained than high-level phases yet remain constrained by their encompassing phase. To support this unified task, we introduce the first MSTP Benchmark, featuring synchronized annotations across multiple state scales and temporal scales. We further propose a method, Incremental Generation and Multi-agent Collaboration (IG-MC), which integrates two key innovations. First, we present a plug-and-play incremental generation module that continuously synthesizes up-to-date visual previews at expanding temporal scales to inform multiple decision-making agents, keeping decisions and generated visuals synchronized and preventing performance degradation as look-ahead intervals lengthen. Second, we present a decision-driven multi-agent collaboration framework for multi-state prediction, comprising generation, initiation, and multi-state assessment agents that dynamically trigger and evaluate prediction cycles to balance global coherence and local fidelity.",
        "arxiv_id": "2509.17429",
        "ARXIVID": "2509.17429",
        "COMMENT": "Matches criterion 6 as it focuses on multi-scale temporal prediction in video understanding, introducing a novel benchmark and methodology.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17328": {
        "authors": [
            "Hongxin Li",
            "Jingran Su",
            "Jingfan Chen",
            "Zheng Ju",
            "Yuntao Chen",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents",
        "abstract": "arXiv:2509.17328v1 Announce Type: new  Abstract: Building autonomous agents that perceive and operate graphical user interfaces (GUIs) like humans has long been a vision in the field of artificial intelligence. Central to these agents is the capability for GUI interaction, which involves GUI understanding and planning capabilities. Existing methods have tried developing GUI agents based on the multi-modal comprehension ability of vision-language models (VLMs). However, the limited scenario, insufficient size, and heterogeneous action spaces hinder the progress of building generalist GUI agents. To resolve these issues, this paper proposes \\textbf{UIPro}, a novel generalist GUI agent trained with extensive multi-platform and multi-task GUI interaction data, coupled with a unified action space. We first curate a comprehensive dataset encompassing 20.6 million GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding capability, which is key to downstream GUI agent tasks. Subsequently, we establish a unified action space to harmonize heterogeneous GUI agent task datasets and produce a merged dataset to foster the action prediction ability of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's superior performance across multiple GUI task benchmarks on various platforms, highlighting the effectiveness of our approach.",
        "arxiv_id": "2509.17328",
        "ARXIVID": "2509.17328",
        "COMMENT": "Matches criterion 3 as it introduces a novel generalist GUI agent with multi-platform and multi-task interaction capabilities, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17074": {
        "authors": [
            "Qian Zhang",
            "Lin Zhang",
            "Xing Fang",
            "Mingxin Zhang",
            "Zhiyuan Wei",
            "Ran Song",
            "Wei Zhang"
        ],
        "title": "Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models",
        "abstract": "arXiv:2509.17074v1 Announce Type: new  Abstract: Visual affordance learning is crucial for robots to understand and interact effectively with the physical world. Recent advances in this field attempt to leverage pre-trained knowledge of vision-language foundation models to learn affordance properties with limited training data, providing a novel paradigm for visual affordance learning. However, these methods overlook the significance of maintaining feature alignment between visual images and language descriptions for identifying affordance areas with textual guidance, and thus may lead to suboptimal results. In this paper, we present an informative framework for text-guided affordance learning, which involves information-based constraints to achieve text-image alignment at feature level. Specifically, we design an affordance mutual information constraint that helps learn appropriate textual prompts and task-oriented visual features simultaneously by maximizing the mutual information between the features of the affordance areas in the input images and the corresponding textual prompts. In addition, we propose an object-level information constraint that maximizes the mutual information between the visual features of a given object and the text features of the category it belongs to. This enables the model to capture high-quality representations for the object, providing more reliable semantic priors for identifying affordance regions. Experimental results on the AGD20K dataset show that the proposed method outperforms existing approaches and achieves the new state-of-the-art in one-shot affordance learning.",
        "arxiv_id": "2509.17074",
        "ARXIVID": "2509.17074",
        "COMMENT": "Matches criterion 1 and 2 as it focuses on visual affordance learning using vision-language foundation models, which involves spatial reasoning and vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17684": {
        "authors": [
            "ThankGod Egbe",
            "Peng Wang",
            "Zhihao Guo",
            "Zidong Chen"
        ],
        "title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning",
        "abstract": "arXiv:2509.17684v1 Announce Type: new  Abstract: This paper evaluates DINOv3, a recent large-scale self-supervised vision backbone, for visuomotor diffusion policy learning in robotic manipulation. We investigate whether a purely self-supervised encoder can match or surpass conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under three regimes: training from scratch, frozen, and finetuned. Across four benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned diffusion policy, we find that (i) finetuned DINOv3 matches or exceeds ResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating strong transferable priors, and (iii) self-supervised features improve sample efficiency and robustness. These results support self-supervised large visual models as effective, generalizable perceptual front-ends for action diffusion policies, motivating further exploration of scalable label-free pretraining in robotic manipulation. Compared to using ResNet18 as a backbone, our approach with DINOv3 achieves up to a 10% absolute increase in test-time success rates on challenging tasks such as Can, and on-the-par performance in tasks like Lift, PushT, and Square.",
        "arxiv_id": "2509.17684",
        "ARXIVID": "2509.17684",
        "COMMENT": "Matches criterion 3. Evaluates a self-supervised large visual model (DINOv3) for robotic manipulation tasks, which aligns with embodied/robotic AI methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17522": {
        "authors": [
            "Hangzhou He",
            "Lei Zhu",
            "Kaiwen Li",
            "Xinliang Zhang",
            "Jiakui Hu",
            "Ourui Fu",
            "Zhengjian Yao",
            "Yanye Lu"
        ],
        "title": "Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models",
        "abstract": "arXiv:2509.17522v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first predicting a set of human-understandable concepts and then mapping them to labels through a simple classifier. While users can intervene in the concept space to improve predictions, traditional CBMs typically employ a fixed linear classifier over concept scores, which restricts interventions to manual value adjustments and prevents the incorporation of new concepts or domain knowledge at test time. These limitations are particularly severe in unsupervised CBMs, where concept activations are often noisy and densely activated, making user interventions ineffective. We introduce Chat-CBM, which replaces score-based classifiers with a language-based classifier that reasons directly over concept semantics. By grounding prediction in the semantic space of concepts, Chat-CBM preserves the interpretability of CBMs while enabling richer and more intuitive interventions, such as concept correction, addition or removal of concepts, incorporation of external knowledge, and high-level reasoning guidance. Leveraging the language understanding and few-shot capabilities of frozen large language models, Chat-CBM extends the intervention interface of CBMs beyond numerical editing and remains effective even in unsupervised settings. Experiments on nine datasets demonstrate that Chat-CBM achieves higher predictive performance and substantially improves user interactivity while maintaining the concept-based interpretability of CBMs.",
        "arxiv_id": "2509.17522",
        "ARXIVID": "2509.17522",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a concept bottleneck model with frozen LLMs for interactive reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.17757": {
        "authors": [
            "Hongxing Fan",
            "Lipeng Wang",
            "Haohua Chen",
            "Zehuan Huang",
            "Jiangtao Wu",
            "Lu Sheng"
        ],
        "title": "Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance",
        "abstract": "arXiv:2509.17757v1 Announce Type: new  Abstract: Amodal completion, generating invisible parts of occluded objects, is vital for applications like image editing and AR. Prior methods face challenges with data needs, generalization, or error accumulation in progressive pipelines. We propose a Collaborative Multi-Agent Reasoning Framework based on upfront collaborative reasoning to overcome these issues. Our framework uses multiple agents to collaboratively analyze occlusion relationships and determine necessary boundary expansion, yielding a precise mask for inpainting. Concurrently, an agent generates fine-grained textual descriptions, enabling Fine-Grained Semantic Guidance. This ensures accurate object synthesis and prevents the regeneration of occluders or other unwanted elements, especially within large inpainting areas. Furthermore, our method directly produces layered RGBA outputs guided by visible masks and attention maps from a Diffusion Transformer, eliminating extra segmentation. Extensive evaluations demonstrate our framework achieves state-of-the-art visual quality.",
        "arxiv_id": "2509.17757",
        "ARXIVID": "2509.17757",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates semantic guidance and image synthesis with multimodal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.16909": {
        "authors": [
            "Yijun Yuan",
            "Zhuoguang Chen",
            "Kenan Li",
            "Weibang Wang",
            "Hang Zhao"
        ],
        "title": "SLAM-Former: Putting SLAM into One Transformer",
        "abstract": "arXiv:2509.16909v1 Announce Type: new  Abstract: We present SLAM-Former, a novel neural approach that integrates full SLAM capabilities into a single transformer. Similar to traditional SLAM systems, SLAM-Former comprises both a frontend and a backend that operate in tandem. The frontend processes sequential monocular images in real-time for incremental mapping and tracking, while the backend performs global refinement to ensure a geometrically consistent result. This alternating execution allows the frontend and backend to mutually promote one another, enhancing overall system performance. Comprehensive experimental results demonstrate that SLAM-Former achieves superior or highly competitive performance compared to state-of-the-art dense SLAM methods.",
        "arxiv_id": "2509.16909",
        "ARXIVID": "2509.16909",
        "COMMENT": "Matches criterion 1 as it presents a novel transformer-based approach for spatial intelligence in SLAM systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.16727": {
        "authors": [
            "Xin Lei Lin",
            "Soroush Mehraban",
            "Abhishek Moturu",
            "Babak Taati"
        ],
        "title": "Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment",
        "abstract": "arXiv:2509.16727v1 Announce Type: new  Abstract: Automated pain assessment from facial expressions is crucial for non-communicative patients, such as those with dementia. Progress has been limited by two challenges: (i) existing datasets exhibit severe demographic and label imbalance due to ethical constraints, and (ii) current generative models cannot precisely control facial action units (AUs), facial structure, or clinically validated pain levels.   We present 3DPain, a large-scale synthetic dataset specifically designed for automated pain assessment, featuring unprecedented annotation richness and demographic diversity. Our three-stage framework generates diverse 3D meshes, textures them with diffusion models, and applies AU-driven face rigging to synthesize multi-view faces with paired neutral and pain images, AU configurations, PSPI scores, and the first dataset-level annotations of pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain expression heatmaps and 2,500 synthetic identities balanced by age, gender, and ethnicity.   We further introduce ViTPain, a Vision Transformer based cross-modal distillation framework in which a heatmap-trained teacher guides a student trained on RGB images, enhancing accuracy, interpretability, and clinical reliability. Together, 3DPain and ViTPain establish a controllable, diverse, and clinically grounded foundation for generalizable automated pain assessment.",
        "arxiv_id": "2509.16727",
        "ARXIVID": "2509.16727",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (3DPain) and a novel method (ViTPain) for embodied AI in pain assessment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.16452": {
        "authors": [
            "Son Hai Nguyen",
            "Diwei Wang",
            "Jinhyeok Jang",
            "Hyewon Seo"
        ],
        "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models",
        "abstract": "arXiv:2509.16452v1 Announce Type: new  Abstract: Accurate vision-based action recognition is crucial for developing autonomous robots that can operate safely and reliably in complex, real-world environments. In this work, we advance video-based recognition of indoor daily actions for robotic perception by leveraging vision-language models (VLMs) enriched with domain-specific knowledge. We adapt a prompt-learning framework in which class-level textual descriptions of each action are embedded as learnable prompts into a frozen pre-trained VLM backbone. Several strategies for structuring and encoding these textual descriptions are designed and evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our method, using only RGB video inputs at test time, achieves over 95\\% accuracy and outperforms state-of-the-art approaches. These results highlight the effectiveness of knowledge-augmented prompts in enabling robust action recognition with minimal supervision.",
        "arxiv_id": "2509.16452",
        "ARXIVID": "2509.16452",
        "COMMENT": "Matches criterion 2 as it explores vision-language models (VLMs) for robotic action recognition, and criterion 3 as it applies to robotic perception.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.16415": {
        "authors": [
            "Zhengri Wu",
            "Yiran Wang",
            "Yu Wen",
            "Zeyu Zhang",
            "Biao Wu",
            "Hao Tang"
        ],
        "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
        "abstract": "arXiv:2509.16415v1 Announce Type: new  Abstract: Underwater stereo depth estimation provides accurate 3D geometry for robotics tasks such as navigation, inspection, and mapping, offering metric depth from low-cost passive cameras while avoiding the scale ambiguity of monocular methods. However, existing approaches face two critical challenges: (i) parameter-efficiently adapting large vision foundation encoders to the underwater domain without extensive labeled data, and (ii) tightly fusing globally coherent but scale-ambiguous monocular priors with locally metric yet photometrically fragile stereo correspondences. To address these challenges, we propose StereoAdapter, a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo refinement module. We further introduce dynamic LoRA adaptation for efficient rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to enhance robustness under diverse underwater conditions. Comprehensive evaluations on both simulated and real-world benchmarks show improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, while real-world deployment with the BlueROV2 robot further demonstrates the consistent robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter. Website: https://aigeeksgroup.github.io/StereoAdapter.",
        "arxiv_id": "2509.16415",
        "ARXIVID": "2509.16415",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for stereo depth estimation in underwater robotics, addressing domain-specific challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.16684": {
        "authors": [
            "Qi Zhang",
            "Bin Li",
            "Antoni B. Chan",
            "Hui Huang"
        ],
        "title": "Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels",
        "abstract": "arXiv:2509.16684v1 Announce Type: new  Abstract: Multi-view crowd counting and localization fuse the input multi-views for estimating the crowd number or locations on the ground. Existing methods mainly focus on accurately predicting on the crowd shown in the input views, which neglects the problem of choosing the `best' camera views to perceive all crowds well in the scene. Besides, existing view selection methods require massive labeled views and images, and lack the ability for cross-scene settings, reducing their application scenarios. Thus, in this paper, we study the view selection issue for better scene-level multi-view crowd counting and localization results with cross-scene ability and limited label demand, instead of input-view-level results. We first propose an independent view selection method (IVS) that considers view and scene geometries in the view selection strategy and conducts the view selection, labeling, and downstream tasks independently. Based on IVS, we also put forward an active view selection method (AVS) that jointly optimizes the view selection, labeling, and downstream tasks. In AVS, we actively select the labeled views and consider both the view/scene geometries and the predictions of the downstream task models in the view selection process. Experiments on multi-view counting and localization tasks demonstrate the cross-scene and the limited label demand advantages of the proposed active view selection method (AVS), outperforming existing methods and with wider application scenarios.",
        "arxiv_id": "2509.16684",
        "ARXIVID": "2509.16684",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on spatial reasoning and view selection for multi-view crowd counting and localization.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.17627": {
        "authors": [
            "Jinshu Chen",
            "Xinghui Li",
            "Xu Bai",
            "Tianxiang Ma",
            "Pengze Zhang",
            "Zhuowei Chen",
            "Gen Li",
            "Lijie Liu",
            "Songtao Zhao",
            "Bingchuan Li",
            "Qian He"
        ],
        "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models",
        "abstract": "arXiv:2509.17627v1 Announce Type: new  Abstract: Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion and aim to resolve three key challenges: data scarcity, subject-scene equilibrium, and insertion harmonization. To address the data scarcity, we propose a new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, a novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce a simple yet effective Condition-Specific Feature Injection mechanism to distinctly inject multi-source conditions and propose a novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate a Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of a benchmark for the field, we introduce InsertBench, a comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. The code will be released.",
        "arxiv_id": "2509.17627",
        "ARXIVID": "2509.17627",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks (mask-free video insertion) and introduces a new benchmark (InsertBench).",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.17589": {
        "authors": [
            "Jun Ling",
            "Yao Qi",
            "Tao Huang",
            "Shibo Zhou",
            "Yanqin Huang",
            "Jiang Yang",
            "Ziqi Song",
            "Ying Zhou",
            "Yang Yang",
            "Heng Tao Shen",
            "Peng Wang"
        ],
        "title": "Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models",
        "abstract": "arXiv:2509.17589v1 Announce Type: new  Abstract: In this work, we address the task of table image to LaTeX code generation, with the goal of automating the reconstruction of high-quality, publication-ready tables from visual inputs. A central challenge of this task lies in accurately handling complex tables -- those with large sizes, deeply nested structures, and semantically rich or irregular cell content -- where existing methods often fail. We begin with a comprehensive analysis, identifying key challenges and highlighting the limitations of current evaluation protocols. To overcome these issues, we propose a reinforced multimodal large language model (MLLM) framework, where a pre-trained MLLM is fine-tuned on a large-scale table-to-LaTeX dataset. To further improve generation quality, we introduce a dual-reward reinforcement learning strategy based on Group Relative Policy Optimization (GRPO). Unlike standard approaches that optimize purely over text outputs, our method incorporates both a structure-level reward on LaTeX code and a visual fidelity reward computed from rendered outputs, enabling direct optimization of the visual output quality. We adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and show that our method achieves state-of-the-art performance, particularly on structurally complex tables, demonstrating the effectiveness and robustness of our approach.",
        "arxiv_id": "2509.17589",
        "ARXIVID": "2509.17589",
        "COMMENT": "Matches criterion 5 as it combines multimodal tasks (image-to-LaTeX) with large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.17329": {
        "authors": [
            "Neham Jain",
            "Andrew Jong",
            "Sebastian Scherer",
            "Ioannis Gkioulekas"
        ],
        "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction",
        "abstract": "arXiv:2509.17329v1 Announce Type: new  Abstract: Smoke in real-world scenes can severely degrade the quality of images and hamper visibility. Recent methods for image restoration either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from a video capturing multiple views of a scene. Our method uses thermal and RGB images, leveraging the fact that the reduced scattering in thermal images enables us to see through the smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene explicitly into smoke and non-smoke components. Unlike prior approaches, SmokeSeer handles a broad range of smoke densities and can adapt to temporally varying smoke. We validate our approach on synthetic data and introduce a real-world multi-view smoke dataset with RGB and thermal images. We provide open-source code and data at the project website.",
        "arxiv_id": "2509.17329",
        "ARXIVID": "2509.17329",
        "COMMENT": "Matches criterion 3 as it introduces a new method for scene reconstruction and smoke removal, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2509.17500": {
        "authors": [
            "Yujie Xie",
            "Hongyang Zhang",
            "Zhihui Liu",
            "Shihai Ruan"
        ],
        "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge",
        "abstract": "arXiv:2509.17500v1 Announce Type: new  Abstract: Large-scale Video Object Segmentation (LSVOS) addresses the challenge of accurately tracking and segmenting objects in long video sequences, where difficulties stem from object reappearance, small-scale targets, heavy occlusions, and crowded scenes. Existing approaches predominantly adopt SAM2-based frameworks with various memory mechanisms for complex video mask generation. In this report, we proposed Segment Anything with Memory Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE track of ICCV 2025, which integrates the strengths of stateof-the-art VOS models into an effective paradigm. To handle visually similar instances and long-term object disappearance in MOSE, we incorporate a long-term memorymodule for reliable object re-identification. Additionly, we adopt SAM2Long as a post-processing strategy to reduce error accumulation and enhance segmentation stability in long video sequences. Our method achieved a final performance of 0.8427 in terms of J &F in the test-set leaderboard.",
        "arxiv_id": "2509.17500",
        "ARXIVID": "2509.17500",
        "COMMENT": "Matches criteria 6 as it addresses video object segmentation challenges with novel memory mechanisms and post-processing strategies.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16748": {
        "authors": [
            "Heyuan Li",
            "Kenkun Liu",
            "Lingteng Qiu",
            "Qi Zuo",
            "Keru Zheng",
            "Zilong Dong",
            "Xiaoguang Han"
        ],
        "title": "HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis",
        "abstract": "arXiv:2509.16748v1 Announce Type: new  Abstract: Tri-plane-like representations have been widely adopted in 3D-aware GANs for head image synthesis and other 3D object/scene modeling tasks due to their efficiency. However, querying features via Cartesian coordinate projection often leads to feature entanglement, which results in mirroring artifacts. A recent work, SphereHead, attempted to address this issue by introducing spherical tri-planes based on a spherical coordinate system. While it successfully mitigates feature entanglement, SphereHead suffers from uneven mapping between the square feature maps and the spherical planes, leading to inefficient feature map utilization during rendering and difficulties in generating fine image details. Moreover, both tri-plane and spherical tri-plane representations share a subtle yet persistent issue: feature penetration across convolutional channels can cause interference between planes, particularly when one plane dominates the others. These challenges collectively prevent tri-plane-based methods from reaching their full potential. In this paper, we systematically analyze these problems for the first time and propose innovative solutions to address them. Specifically, we introduce a novel hybrid-plane (hy-plane for short) representation that combines the strengths of both planar and spherical planes while avoiding their respective drawbacks. We further enhance the spherical plane by replacing the conventional theta-phi warping with a novel near-equal-area warping strategy, which maximizes the effective utilization of the square feature map. In addition, our generator synthesizes a single-channel unified feature map instead of multiple feature maps in separate channels, thereby effectively eliminating feature penetration. With a series of technical improvements, our hy-plane representation enables our method, HyPlaneHead, to achieve state-of-the-art performance in full-head image synthesis.",
        "arxiv_id": "2509.16748",
        "ARXIVID": "2509.16748",
        "COMMENT": "Matches criterion 4 as it proposes a novel hybrid-plane representation for 3D-aware GANs, relevant to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16949": {
        "authors": [
            "Ruicong Liu",
            "Takehiko Ohkawa",
            "Tze Ho Elden Tse",
            "Mingfang Zhang",
            "Angela Yao",
            "Yoichi Sato"
        ],
        "title": "Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation",
        "abstract": "arXiv:2509.16949v1 Announce Type: new  Abstract: This paper presents RPEP, the first pre-training method for event-based 3D hand pose estimation using labeled RGB images and unpaired, unlabeled event data. Event data offer significant benefits such as high temporal resolution and low latency, but their application to hand pose estimation is still limited by the scarcity of labeled training data. To address this, we repurpose real RGB datasets to train event-based estimators. This is done by constructing pseudo-event-RGB pairs, where event data is generated and aligned with the ground-truth poses of RGB images. Unfortunately, existing pseudo-event generation techniques assume stationary objects, thus struggling to handle non-stationary, dynamically moving hands. To overcome this, RPEP introduces a novel generation strategy that decomposes hand movements into smaller, step-by-step motions. This decomposition allows our method to capture temporal changes in articulation, constructing more realistic event data for a moving hand. Additionally, RPEP imposes a motion reversal constraint, regularizing event generation using reversed motion. Extensive experiments show that our pre-trained model significantly outperforms state-of-the-art methods on real event data, achieving up to 24% improvement on EvRealHands. Moreover, it delivers strong performance with minimal labeled samples for fine-tuning, making it well-suited for practical deployment.",
        "arxiv_id": "2509.16949",
        "ARXIVID": "2509.16949",
        "COMMENT": "Matches criterion 3 as it introduces a novel pre-training method for event-based hand pose estimation, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.17593": {
        "authors": [
            "Samet Hicsonmez",
            "Abd El Rahman Shabayek",
            "Arunkumar Rathinam",
            "Djamila Aouada"
        ],
        "title": "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints",
        "abstract": "arXiv:2509.17593v1 Announce Type: new  Abstract: Object detection is essential in space applications targeting Space Domain Awareness and also applications involving relative navigation scenarios. Current deep learning models for Object Detection in space applications are often trained on synthetic data from simulators, however, the model performance drops significantly on real-world data due to the domain gap. However, domain adaptive object detection is an overlooked problem in the community. In this work, we first show the importance of domain adaptation and then explore Supervised Domain Adaptation (SDA) to reduce this gap using minimal labeled real data. We build on a recent semi-supervised adaptation method and tailor it for object detection. Our approach combines domain-invariant feature learning with a CNN-based domain discriminator and invariant risk minimization using a domain-independent regression head. To meet real-time deployment needs, we test our method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet backbone and on the more advanced Fully Convolutional One-Stage object detector (FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and SPARK. The results show up to 20-point improvements in average precision (AP) with just 250 labeled real images.",
        "arxiv_id": "2509.17593",
        "ARXIVID": "2509.17593",
        "COMMENT": "Matches criterion 3. Explores domain adaptive object detection for space applications, which aligns with embodied/robotic AI methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.17864": {
        "authors": [
            "Shi Chen",
            "Erik Sandstr\\\"om",
            "Sandro Lombardi",
            "Siyuan Li",
            "Martin R. Oswald"
        ],
        "title": "ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos",
        "abstract": "arXiv:2509.17864v1 Announce Type: new  Abstract: Achieving truly practical dynamic 3D reconstruction requires online operation, global pose and map consistency, detailed appearance modeling, and the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM methods typically merely remove the dynamic parts or require RGB-D input, while offline methods are not scalable to long video sequences, and current transformer-based feedforward methods lack global consistency and appearance details. To this end, we achieve online dynamic scene reconstruction by disentangling the static and dynamic parts within a SLAM system. The poses are tracked robustly with a novel motion masking strategy, and dynamic parts are reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph. Our method yields novel view renderings competitive to offline methods and achieves on-par tracking with state-of-the-art dynamic SLAM methods.",
        "arxiv_id": "2509.17864",
        "ARXIVID": "2509.17864",
        "COMMENT": "Matches criterion 3. Proposes a method for dynamic 3D scene reconstruction, which is relevant to embodied/robotic AI benchmarks and methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.17506": {
        "authors": [
            "Houqiang Zhong",
            "Zihan Zheng",
            "Qiang Hu",
            "Yuan Tian",
            "Ning Cao",
            "Lan Xu",
            "Xiaoyun Zhang",
            "Zhengxue Cheng",
            "Li Song",
            "Wenjun Zhang"
        ],
        "title": "4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression",
        "abstract": "arXiv:2509.17506v1 Announce Type: new  Abstract: Volumetric video has emerged as a key medium for immersive telepresence and augmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation and realistic spatial interactions. However, delivering high-quality dynamic volumetric content at scale remains challenging due to massive data volume, complex motion, and limited editability of existing representations. In this paper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework designed for scalable and editable volumetric video streaming. Our method introduces a layered representation that explicitly separates static backgrounds from dynamic foregrounds using a lookahead-based motion decomposition strategy, significantly reducing temporal redundancy and enabling selective background/foreground streaming. To capture continuous motion trajectories, we employ a multi-resolution motion estimation grid and a lightweight shared MLP, complemented by a dynamic Gaussian compensation mechanism to model emergent content. An adaptive grouping scheme dynamically inserts background keyframes to balance temporal consistency and compression efficiency. Furthermore, an entropy-aware training pipeline jointly optimizes the motion fields and Gaussian parameters under a rate-distortion (RD) objective, while employing range-based and KD-tree compression to minimize storage overhead. Extensive experiments on multiple datasets demonstrate that 4D-MoDe consistently achieves competitive reconstruction quality with an order of magnitude lower storage cost (e.g., as low as \\textbf{11.4} KB/frame) compared to state-of-the-art methods, while supporting practical applications such as background replacement and foreground-only streaming.",
        "arxiv_id": "2509.17506",
        "ARXIVID": "2509.17506",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on volumetric video compression and streaming, which involves video-based tasks and applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.17660": {
        "authors": [
            "Yikun Ma",
            "Bo Li",
            "Ying Chen",
            "Zijie Yue",
            "Shuchang Xu",
            "Jingyao Li",
            "Lei Ma",
            "Liang Zhong",
            "Duowu Zou",
            "Leiming Xu",
            "Yunshi Zhong",
            "Xiaobo Li",
            "Weiqun Ding",
            "Minmin Zhang",
            "Dongli He",
            "Zhenghong Li",
            "Ye Chen",
            "Ye Zhao",
            "Jialong Zhuo",
            "Xiaofen Wu",
            "Lisha Yi",
            "Miaojing Shi",
            "Huihui Sun"
        ],
        "title": "Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study",
        "abstract": "arXiv:2509.17660v1 Announce Type: new  Abstract: The early detection of esophagogastric junction adenocarcinoma (EGJA) is crucial for improving patient prognosis, yet its current diagnosis is highly operator-dependent. This paper aims to make the first attempt to develop an artificial intelligence (AI) foundation model-based method for both screening and staging diagnosis of EGJA using endoscopic images. In this cohort and learning study, we conducted a multicentre study across seven Chinese hospitals between December 28, 2016 and December 30, 2024. It comprises 12,302 images from 1,546 patients; 8,249 of them were employed for model training, while the remaining were divided into the held-out (112 patients, 914 images), external (230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test sets for evaluation. The proposed model employs DINOv2 (a vision foundation model) and ResNet50 (a convolutional neural network) to extract features of global appearance and local details of endoscopic images for EGJA staging diagnosis. Our model demonstrates satisfactory performance for EGJA staging diagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and 0.8956, respectively. In contrast, among representative AI models, the best one (ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test sets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on the held-out test set. Moreover, with the assistance of our model, the overall accuracy for the trainee, competent, and expert endoscopists improves from 0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our knowledge, our model is the first application of foundation models for EGJA staging diagnosis and demonstrates great potential in both diagnostic accuracy and efficiency.",
        "arxiv_id": "2509.17660",
        "ARXIVID": "2509.17660",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it applies a vision foundation model (DINOv2) for medical imaging tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.17088": {
        "authors": [
            "Jiexuan Zhang",
            "Yiheng Du",
            "Qian Wang",
            "Weiqi Li",
            "Yu Gu",
            "Jian Zhang"
        ],
        "title": "AlignedGen: Aligning Style Across Generated Images",
        "abstract": "arXiv:2509.17088v1 Announce Type: new  Abstract: Despite their generative power, diffusion models struggle to maintain style consistency across images conditioned on the same style prompt, hindering their practical deployment in creative workflows. While several training-free methods attempt to solve this, they are constrained to the U-Net architecture, which not only leads to low-quality results and artifacts like object repetition but also renders them incompatible with superior Diffusion Transformer (DiT). To address these issues, we introduce AlignedGen, a novel training-free framework that enhances style consistency across images generated by DiT models. Our work first reveals a critical insight: naive attention sharing fails in DiT due to conflicting positional signals from improper position embeddings. We introduce Shifted Position Embedding (ShiftPE), an effective solution that resolves this conflict by allocating a non-overlapping set of positional indices to each image. Building on this foundation, we develop Advanced Attention Sharing (AAS), a suite of three techniques meticulously designed to fully unleash the potential of attention sharing within the DiT. Furthermore, to broaden the applicability of our method, we present an efficient query, key, and value feature extraction algorithm, enabling our method to seamlessly incorporate external images as style references. Extensive experimental results validate that our method effectively enhances style consistency across generated images while maintaining precise text-to-image alignment.",
        "arxiv_id": "2509.17088",
        "ARXIVID": "2509.17088",
        "COMMENT": "Matches criterion 5 as it focuses on techniques combining image generation tasks and large language models (style consistency in diffusion models).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.17084": {
        "authors": [
            "Binhua Huang",
            "Nan Wang",
            "Arjun Parakash",
            "Soumyabrata Dev"
        ],
        "title": "MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors",
        "abstract": "arXiv:2509.17084v1 Announce Type: new  Abstract: Video action recognition is a fundamental task in computer vision, but state-of-the-art models are often computationally expensive and rely on extensive video pre-training. In parallel, large-scale vision-language models like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot capabilities on static images, while motion vectors (MV) provide highly efficient temporal information directly from compressed video streams. To synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple yet powerful two-stream late fusion framework for efficient video recognition. Our approach combines features from a frozen CLIP image encoder with features from a lightweight, supervised network trained on raw MV. During fusion, both backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is trained, ensuring extreme efficiency. Through comprehensive experiments on the UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy, significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%) baselines. Our work provides a new, highly efficient baseline for video understanding that effectively bridges the gap between large static models and dynamic, low-cost motion cues. Our code and models are available at https://github.com/microa/MoCLIP-Lite.",
        "arxiv_id": "2509.17084",
        "ARXIVID": "2509.17084",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding tasks (video action recognition) and proposes a novel methodology (MoCLIP-Lite).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16892": {
        "authors": [
            "Jiahe Qian",
            "Yaoyu Fang",
            "Ziqiao Weng",
            "Xinkun Wang",
            "Lee A. Cooper",
            "Bo Zhou"
        ],
        "title": "Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning",
        "abstract": "arXiv:2509.16892v1 Announce Type: new  Abstract: Spatial transcriptomics aims to connect high-resolution histology images with spatially resolved gene expression. To achieve better performance on downstream tasks such as gene expression prediction, large-scale pre-training is required to obtain generalisable representations that can bridge histology and transcriptomics across tissues, protocols, and laboratories. Existing cross-modal pre-training approaches for spatial transcriptomics rely on either gene names or expression values in isolation, which strips the gene branch of essential semantics and breaks the association between each gene and its quantitative magnitude. In addition, by restricting supervision to image-text alignment, these methods ignore intrinsic visual cues that are critical for learning robust image features. We present CoMTIP, the first Contrastive Masked Text-Image Pretraining framework that jointly learns from images, gene names, and expression values while capturing fine-grained visual context for spatial transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct occluded patches and learn context-aware image embeddings. The text branch applies a scalable Gene-Text Encoder that processes all gene sentences in parallel, enriches each gene and its numerical value with dedicated embeddings, and employs Pair-aware Adversarial Training (PAAT) to preserve correct gene-value associations. Image and text representations are aligned in a shared InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets show that CoMTIP not only surpasses previous methods on diverse downstream tasks but also achieves zero-shot gene expression prediction, a capability that existing approaches do not provide.",
        "arxiv_id": "2509.16892",
        "ARXIVID": "2509.16892",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it combines image understanding with gene expression tasks using a multimodal framework.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.17065": {
        "authors": [
            "Yao Du",
            "Jiarong Guo",
            "Xiaomeng Li"
        ],
        "title": "CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner",
        "abstract": "arXiv:2509.17065v1 Announce Type: new  Abstract: Echocardiography is a vital non-invasive modality for cardiac assessment, with left ventricular ejection fraction (LVEF) serving as a key indicator of heart function. Existing LVEF estimation methods depend on large-scale annotated video datasets, which are costly and limit adaptability across various clinical settings. Recent vision-language models for echocardiography, such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial temporal dynamics and localized cardiac structures essential for accurate diagnosis. To address these challenges, we propose CardiacCLIP, a video-based framework that enhances LVEF prediction through attention-based frame aggregation and multi-resolution input scaling. Specifically, we introduce MFL (Multi Frame Learning), a novel attention-based mechanism for selectively fusing informative frames, and EchoZoom, a multi-scale feature extraction strategy that refines spatial representations of cardiac structures. As a novel adaptation of CLIP models for few-shot echocardiogram video analysis, our approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on the EchoNet-Dynamic dataset under 1-shot setting. The code is available at https://github.com/xmed-lab/CardiacCLIP.",
        "arxiv_id": "2509.17065",
        "ARXIVID": "2509.17065",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks (echocardiogram video analysis) with novel methodologies.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.17513": {
        "authors": [
            "Zihan Zheng",
            "Zhenlong Wu",
            "Houqiang Zhong",
            "Yuan Tian",
            "Ning Cao",
            "Lan Xu",
            "Jiangchao Yao",
            "Xiaoyun Zhang",
            "Qiang Hu",
            "Wenjun Zhang"
        ],
        "title": "4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming",
        "abstract": "arXiv:2509.17513v1 Announce Type: new  Abstract: Achieving seamless viewing of high-fidelity volumetric video, comparable to 2D video experiences, remains an open challenge. Existing volumetric video compression methods either lack the flexibility to adjust quality and bitrate within a single model for efficient streaming across diverse networks and devices, or struggle with real-time decoding and rendering on lightweight mobile platforms. To address these challenges, we introduce 4DGCPro, a novel hierarchical 4D Gaussian compression framework that facilitates real-time mobile decoding and high-quality rendering via progressive volumetric video streaming in a single bitstream. Specifically, we propose a perceptually-weighted and compression-friendly hierarchical 4D Gaussian representation with motion-aware adaptive grouping to reduce temporal redundancy, preserve coherence, and enable scalable multi-level detail streaming. Furthermore, we present an end-to-end entropy-optimized training scheme, which incorporates layer-wise rate-distortion (RD) supervision and attribute-specific entropy modeling for efficient bitstream generation. Extensive experiments show that 4DGCPro enables flexible quality and multiple bitrate within a single model, achieving real-time decoding and rendering on mobile devices while outperforming existing methods in RD performance across multiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro",
        "arxiv_id": "2509.17513",
        "ARXIVID": "2509.17513",
        "COMMENT": "Matches criterion 6 as it focuses on volumetric video streaming, which is related to video understanding tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.16500": {
        "authors": [
            "Tianyi Yan",
            "Wencheng Han",
            "Xia Zhou",
            "Xueyang Zhang",
            "Kun Zhan",
            "Cheng-zhong Xu",
            "Jianbing Shen"
        ],
        "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation",
        "abstract": "arXiv:2509.16500v1 Announce Type: new  Abstract: Synthetic data is crucial for advancing autonomous driving (AD) systems, yet current state-of-the-art video generation models, despite their visual realism, suffer from subtle geometric distortions that limit their utility for downstream perception tasks. We identify and quantify this critical issue, demonstrating a significant performance gap in 3D object detection when using synthetic versus real data. To address this, we introduce Reinforcement Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion models by incorporating rewards from specialized latent-space AD perception models. Its core components include an efficient Latent-Space Windowing Optimization technique for targeted feedback during diffusion, and a Hierarchical Geometric Reward (HGR) system providing multi-level rewards for point-line-plane alignment, and scene occupancy coherence. To quantify these distortions, we propose GeoScores. Applied to models like DiVE on nuScenes, RLGF substantially reduces geometric errors (e.g., VP error by 21\\%, Depth error by 57\\%) and dramatically improves 3D object detection mAP by 12.7\\%, narrowing the gap to real-data performance. RLGF offers a plug-and-play solution for generating geometrically sound and reliable synthetic videos for AD development.",
        "arxiv_id": "2509.16500",
        "ARXIVID": "2509.16500",
        "COMMENT": "Matches criterion 6 as it focuses on video generation for autonomous driving, which is related to video understanding tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.17789": {
        "authors": [
            "Guoxi Huang",
            "Haoran Wang",
            "Zipeng Qi",
            "Wenjun Lu",
            "David Bull",
            "Nantheera Anantrasirichai"
        ],
        "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes",
        "abstract": "arXiv:2509.17789v1 Announce Type: new  Abstract: Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \\textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy.",
        "arxiv_id": "2509.17789",
        "ARXIVID": "2509.17789",
        "COMMENT": "Matches criterion 4 as it focuses on improving 3D reconstruction using vision foundation models in underwater scenes.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.17888": {
        "authors": [
            "Divya Mereddy",
            "Marcos Quinones-Grueiro",
            "Ashwin T S",
            "Eduardo Davalos",
            "Gautam Biswas",
            "Kent Etherton",
            "Tyler Davis",
            "Katelyn Kay",
            "Jill Lear",
            "Benjamin Goldberg"
        ],
        "title": "Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training",
        "abstract": "arXiv:2509.17888v1 Announce Type: new  Abstract: This study examines how Critical Care Air Transport Team (CCATT) members are trained using mixed-reality simulations that replicate the high-pressure conditions of aeromedical evacuation. Each team - a physician, nurse, and respiratory therapist - must stabilize severely injured soldiers by managing ventilators, IV pumps, and suction devices during flight. Proficient performance requires clinical expertise and cognitive skills, such as situational awareness, rapid decision-making, effective communication, and coordinated task management, all of which must be maintained under stress. Recent advances in simulation and multimodal data analytics enable more objective and comprehensive performance evaluation. In contrast, traditional instructor-led assessments are subjective and may overlook critical events, thereby limiting generalizability and consistency. However, AI-based automated and more objective evaluation metrics still demand human input to train the AI algorithms to assess complex team dynamics in the presence of environmental noise and the need for accurate re-identification in multi-person tracking. To address these challenges, we introduce a systematic, data-driven assessment framework that combines Cognitive Task Analysis (CTA) with Multimodal Learning Analytics (MMLA). We have developed a domain-specific CTA model for CCATT training and a vision-based action recognition pipeline using a fine-tuned Human-Object Interaction model, the Cascade Disentangling Network (CDN), to detect and track trainee-equipment interactions over time. These interactions automatically yield performance indicators (e.g., reaction time, task duration), which are mapped onto a hierarchical CTA model tailored to CCATT operations, enabling interpretable, domain-relevant performance evaluations.",
        "arxiv_id": "2509.17888",
        "ARXIVID": "2509.17888",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for embodied AI in mixed-reality training environments.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.16517": {
        "authors": [
            "Burak Satar",
            "Zhixin Ma",
            "Patrick A. Irawan",
            "Wilfried A. Mulyawan",
            "Jing Jiang",
            "Ee-Peng Lim",
            "Chong-Wah Ngo"
        ],
        "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding",
        "abstract": "arXiv:2509.16517v1 Announce Type: new  Abstract: Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures. In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture",
        "arxiv_id": "2509.16517",
        "ARXIVID": "2509.16517",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a benchmark for cultural reasoning in vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17476": {
        "authors": [
            "Mallikarjun B. R.",
            "Fei Yin",
            "Vikram Voleti",
            "Nikita Drobyshev",
            "Maksim Lapin",
            "Aaryaman Vasishta",
            "Varun Jampani"
        ],
        "title": "Stable Video-Driven Portraits",
        "abstract": "arXiv:2509.17476v1 Announce Type: new  Abstract: Portrait animation aims to generate photo-realistic videos from a single source image by reenacting the expression and pose from a driving video. While early methods relied on 3D morphable models or feature warping techniques, they often suffered from limited expressivity, temporal inconsistency, and poor generalization to unseen identities or large pose variations. Recent advances using diffusion models have demonstrated improved quality but remain constrained by weak control signals and architectural limitations. In this work, we propose a novel diffusion based framework that leverages masked facial regions specifically the eyes, nose, and mouth from the driving video as strong motion control cues. To enable robust training without appearance leakage, we adopt cross identity supervision. To leverage the strong prior from the pretrained diffusion model, our novel architecture introduces minimal new parameters that converge faster and help in better generalization. We introduce spatial temporal attention mechanisms that allow inter frame and intra frame interactions, effectively capturing subtle motions and reducing temporal artifacts. Our model uses history frames to ensure continuity across segments. At inference, we propose a novel signal fusion strategy that balances motion fidelity with identity preservation. Our approach achieves superior temporal consistency and accurate expression control, enabling high-quality, controllable portrait animation suitable for real-world applications.",
        "arxiv_id": "2509.17476",
        "ARXIVID": "2509.17476",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video-driven portrait animation with novel methodologies for temporal consistency and expression control.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17566": {
        "authors": [
            "Ding Shaodong",
            "Liu Ziyang",
            "Zhou Yijun",
            "Liu Tao"
        ],
        "title": "MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data",
        "abstract": "arXiv:2509.17566v1 Announce Type: new  Abstract: The automatic diagnosis of Parkinson's disease is in high clinical demand due to its prevalence and the importance of targeted treatment. Current clinical practice often relies on diagnostic biomarkers in QSM and NM-MRI images. However, the lack of large, high-quality datasets makes training diagnostic models from scratch prone to overfitting. Adapting pre-trained 3D medical models is also challenging, as the diversity of medical imaging leads to mismatches in voxel spacing and modality between pre-training and fine-tuning data. In this paper, we address these challenges by leveraging 2D vision foundation models (VFMs). Specifically, we crop multiple key ROIs from NM and QSM images, process each ROI through separate branches to compress the ROI into a token, and then combine these tokens into a unified patient representation for classification. Within each branch, we use 2D VFMs to encode axial slices of the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary segmentation head that steers the feature extraction toward specific brain nuclei. Additionally, we introduce multi-ROI supervised contrastive learning, which improves diagnostic performance by pulling together representations of patients from the same class while pushing away those from different classes. Our approach achieved first place in the MICCAI 2025 PDCADxFoundation challenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled QSM and NM-MRI scans, outperforming the second-place method by 5.5%.These results highlight the potential of 2D VFMs for clinical analysis of 3D MR images.",
        "arxiv_id": "2509.17566",
        "ARXIVID": "2509.17566",
        "COMMENT": "Matches criterion 4 as it leverages vision foundation models for medical imaging applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17816": {
        "authors": [
            "Brown Ebouky",
            "Ajad Chhatkuli",
            "Cristiano Malossi",
            "Christoph Studer",
            "Roy Assaf",
            "Andrea Bartezzaghi"
        ],
        "title": "Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training",
        "abstract": "arXiv:2509.17816v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as a central paradigm for training foundation models by leveraging large-scale unlabeled datasets, often producing representations with strong generalization capabilities. These models are typically pre-trained on general-purpose datasets such as ImageNet and subsequently adapted to various downstream tasks through finetuning. While recent advances have explored parameter-efficient strategies for adapting pre-trained models, extending SSL pre-training itself to new domains - particularly under limited data regimes and for dense prediction tasks - remains underexplored. In this work, we address the problem of adapting vision foundation models to new domains in an unsupervised and data-efficient manner, specifically targeting downstream semantic segmentation. We propose GLARE (Global Local and Regional Enforcement), a novel continual self-supervised pre-training task designed to enhance downstream segmentation performance. GLARE introduces patch-level augmentations to encourage local consistency and incorporates a regional consistency constraint that leverages spatial semantics in the data. For efficient continual pre-training, we initialize Vision Transformers (ViTs) with weights from existing SSL models and update only lightweight adapter modules - specifically UniAdapter - while keeping the rest of the backbone frozen. Experiments across multiple semantic segmentation benchmarks on different domains demonstrate that GLARE consistently improves downstream performance with minimal computational and parameter overhead.",
        "arxiv_id": "2509.17816",
        "ARXIVID": "2509.17816",
        "COMMENT": "Matches criterion 4 as it focuses on adapting vision foundation models for semantic segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.16935": {
        "authors": [
            "Lavish Ramchandani",
            "Gunjan Deotale",
            "Dev Kumar Das"
        ],
        "title": "Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification",
        "abstract": "arXiv:2509.16935v1 Announce Type: new  Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated with tumor aggressiveness and poor prognosis. Their detection remains a significant challenge due to subtle morphological cues, class imbalance, and inter-observer variability among pathologists. The MIDOG 2025 challenge introduced a dedicated track for atypical mitosis classification, enabling systematic evaluation of deep learning methods. In this study, we investigated the use of large vision foundation models, including Virchow, Virchow2, and UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We conducted extensive experiments with different LoRA ranks, as well as random and group-based data splits, to analyze robustness under varied conditions. Our best approach, Virchow with LoRA rank 8 and ensemble of three-fold cross-validation, achieved a balanced accuracy of 88.37% on the preliminary test set, ranking joint 9th in the challenge leaderboard. These results highlight the promise of foundation models with efficient adaptation strategies for the classification of atypical mitosis, while underscoring the need for improvements in specificity and domain generalization.",
        "arxiv_id": "2509.16935",
        "ARXIVID": "2509.16935",
        "COMMENT": "Matches criterion 4 as it explores parameter-efficient fine-tuning of vision foundation models for a specific application in medical imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.17044": {
        "authors": [
            "Mingqing Zhang",
            "Zhuoning Xu",
            "Peijie Wang",
            "Rongji Li",
            "Liang Wang",
            "Qiang Liu",
            "Jian Xu",
            "Xuyao Zhang",
            "Shu Wu",
            "Liang Wang"
        ],
        "title": "AgriDoctor: A Multimodal Intelligent Assistant for Agriculture",
        "abstract": "arXiv:2509.17044v1 Announce Type: new  Abstract: Accurate crop disease diagnosis is essential for sustainable agriculture and global food security. Existing methods, which primarily rely on unimodal models such as image-based classifiers and object detectors, are limited in their ability to incorporate domain-specific agricultural knowledge and lack support for interactive, language-based understanding. Recent advances in large language models (LLMs) and large vision-language models (LVLMs) have opened new avenues for multimodal reasoning. However, their performance in agricultural contexts remains limited due to the absence of specialized datasets and insufficient domain adaptation. In this work, we propose AgriDoctor, a modular and extensible multimodal framework designed for intelligent crop disease diagnosis and agricultural knowledge interaction. As a pioneering effort to introduce agent-based multimodal reasoning into the agricultural domain, AgriDoctor offers a novel paradigm for building interactive and domain-adaptive crop health solutions. It integrates five core components: a router, classifier, detector, knowledge retriever and LLMs. To facilitate effective training and evaluation, we construct AgriMM, a comprehensive benchmark comprising 400000 annotated disease images, 831 expert-curated knowledge entries, and 300000 bilingual prompts for intent-driven tool selection. Extensive experiments demonstrate that AgriDoctor, trained on AgriMM, significantly outperforms state-of-the-art LVLMs on fine-grained agricultural tasks, establishing a new paradigm for intelligent and sustainable farming applications.",
        "arxiv_id": "2509.17044",
        "ARXIVID": "2509.17044",
        "COMMENT": "Matches criterion 2 as it explores a multimodal framework integrating vision and language for agricultural applications, relevant to vision-language integration.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.17481": {
        "authors": [
            "Xingqi Wang",
            "Yiming Cui",
            "Xin Yao",
            "Shijin Wang",
            "Guoping Hu",
            "Xiaoyu Qin"
        ],
        "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding",
        "abstract": "arXiv:2509.17481v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have recently demonstrated remarkable progress, yet hallucination remains a critical barrier, particularly in chart understanding, which requires sophisticated perceptual and cognitive abilities as well as rigorous factual accuracy. While prior work has investigated hallucinations and chart comprehension independently, their intersection remains largely unexplored. To address this gap, we present ChartHal, a benchmark that features a fine-grained taxonomy of hallucination scenarios in chart understanding, along with a human-validated dataset of 1,062 samples. Our evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations on ChartHal, including proprietary models such as GPT-5 and o4-mini, which achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals that questions involving information absent from or contradictory to charts are especially likely to trigger hallucinations, underscoring the urgent need for more robust mitigation strategies. Code and data are available at https://github.com/ymcui/ChartHal .",
        "arxiv_id": "2509.17481",
        "ARXIVID": "2509.17481",
        "COMMENT": "Matches criterion 2 as it evaluates hallucination in Large Vision-Language Models (LVLMs), which is relevant to vision-language integration.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.16832": {
        "authors": [
            "Ziyang Xu",
            "Benedikt Schwab",
            "Yihui Yang",
            "Thomas H. Kolbe",
            "Christoph Holst"
        ],
        "title": "L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models",
        "abstract": "arXiv:2509.16832v1 Announce Type: new  Abstract: Accurate registration between LiDAR (Light Detection and Ranging) point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection and model refinement. However, achieving accurate LiDAR-to-Model registration at individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model, and adaptively estimating vertical translation. Experiments on three real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present.",
        "arxiv_id": "2509.16832",
        "ARXIVID": "2509.16832",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for LiDAR-to-Model registration, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17702": {
        "authors": [
            "Patrick Schmidt",
            "Vasileios Belagiannis",
            "Lazaros Nalpantidis"
        ],
        "title": "Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation",
        "abstract": "arXiv:2509.17702v1 Announce Type: new  Abstract: Autonomous robotic systems applied to new domains require an abundance of expensive, pixel-level dense labels to train robust semantic segmentation models under full supervision. This study proposes a model-agnostic Depth Edge Alignment Loss to improve Weakly Supervised Semantic Segmentation models across different datasets. The methodology generates pixel-level semantic labels from image-level supervision, avoiding expensive annotation processes. While weak supervision is widely explored in traditional computer vision, our approach adds supervision with pixel-level depth information, a modality commonly available in robotic systems. We demonstrate how our approach improves segmentation performance across datasets and models, but can also be combined with other losses for even better performance, with improvements up to +5.439, +1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC / MS COCO validation, and the HOPE static onboarding split, respectively. Our code will be made publicly available.",
        "arxiv_id": "2509.17702",
        "ARXIVID": "2509.17702",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for weakly supervised semantic segmentation in robotic systems, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17907": {
        "authors": [
            "Xiaojing Dong",
            "Weilin Huang",
            "Liang Li",
            "Yiying Li",
            "Shu Liu",
            "Tongtong Ou",
            "Shuang Ouyang",
            "Yu Tian",
            "Fengxuan Zhao"
        ],
        "title": "MEF: A Systematic Evaluation Framework for Text-to-Image Models",
        "abstract": "arXiv:2509.17907v1 Announce Type: new  Abstract: Rapid advances in text-to-image (T2I) generation have raised higher requirements for evaluation methodologies. Existing benchmarks center on objective capabilities and dimensions, but lack an application-scenario perspective, limiting external validity. Moreover, current evaluations typically rely on either ELO for overall ranking or MOS for dimension-specific scoring, yet both methods have inherent shortcomings and limited interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF), a systematic and practical approach for evaluating T2I models. First, we propose a structured taxonomy encompassing user scenarios, elements, element compositions, and text expression forms to construct the Magic-Bench-377, which supports label-level assessment and ensures a balanced coverage of both user scenarios and capabilities. On this basis, we combine ELO and dimension-specific MOS to generate model rankings and fine-grained assessments respectively. This joint evaluation method further enables us to quantitatively analyze the contribution of each dimension to user satisfaction using multivariate logistic regression. By applying MEF to current T2I models, we obtain a leaderboard and key characteristics of the leading models. We release our evaluation framework and make Magic-Bench-377 fully open-source to advance research in the evaluation of visual generative models.",
        "arxiv_id": "2509.17907",
        "ARXIVID": "2509.17907",
        "COMMENT": "Matches criterion 4 as it focuses on evaluation frameworks for visual generative models, which are related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.17544": {
        "authors": [
            "Juan Ca\\~nada",
            "Ra\\'ul Alonso",
            "Julio Molleda",
            "Fidel D\\'iez"
        ],
        "title": "A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data",
        "abstract": "arXiv:2509.17544v1 Announce Type: new  Abstract: The increasing availability of open Earth Observation (EO) and agricultural datasets holds great potential for supporting sustainable land management. However, their high technical entry barrier limits accessibility for non-expert users. This study presents an open-source conversational assistant that integrates multimodal retrieval and large language models (LLMs) to enable natural language interaction with heterogeneous agricultural and geospatial data. The proposed architecture combines orthophotos, Sentinel-2 vegetation indices, and user-provided documents through retrieval-augmented generation (RAG), allowing the system to flexibly determine whether to rely on multimodal evidence, textual knowledge, or both in formulating an answer. To assess response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional quantitative evaluation framework. Preliminary results show that the system is capable of generating clear, relevant, and context-aware responses to agricultural queries, while remaining reproducible and scalable across geographic regions. The primary contributions of this work include an architecture for fusing multimodal EO and textual knowledge sources, a demonstration of lowering the barrier to access specialized agricultural information through natural language interaction, and an open and reproducible design.",
        "arxiv_id": "2509.17544",
        "ARXIVID": "2509.17544",
        "COMMENT": "Matches criterion 2 as it integrates multimodal retrieval and large language models for geospatial and agricultural data analysis.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.17978": {
        "authors": [
            "Antoni Guasch",
            "Maria Isabel Valdez"
        ],
        "title": "The STAR-XAI Protocol: An Interactive Framework for Inducing Second-Order Agency in AI Agents",
        "abstract": "arXiv:2509.17978v1 Announce Type: new  Abstract: Current Large Reasoning Models (LRMs) exhibit significant limitations in reliability and transparency, often showing a collapse in reasoning capabilities when faced with high-complexity, long-horizon tasks. This \"illusion of thinking\" is frequently an artifact of non-agentic, black-box evaluation paradigms that fail to cultivate robust problem-solving processes. In response, we introduce The STAR-XAI Protocol (Socratic, Transparent, Agentic, Reasoning - for eXplainable Artificial Intelligence), a novel methodology for training and operating verifiably reliable AI agents. Our method reframes the human-AI interaction as a structured, Socratic dialogue, governed by an explicit and evolving rulebook, the Consciousness Transfer Package (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc strategic justification and a state-locking Checksum that prevents error accumulation, the protocol transforms a powerful but opaque LRM into a disciplined \"Clear Box\" agent. We demonstrate the efficacy of this method through an exhaustive 25-move case study in the complex strategic game \"Caps i Caps\". The agent not only solved the high-complexity puzzle but also demonstrated Second-Order Agency, identifying flaws in its own supervisor-approved plans and adapting its core integrity protocols mid-task. The STAR-XAI Protocol offers a practical pathway to creating AI agents that are not just high-performing, but also transparent, auditable, and trustworthy by design.",
        "arxiv_id": "2509.17978",
        "ARXIVID": "2509.17978",
        "COMMENT": "Does not match any specific criteria but discusses explainable AI and reasoning frameworks, which are tangentially related to embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.17567": {
        "authors": [
            "Yang Xiao",
            "Mohan Jiang",
            "Jie Sun",
            "Keyu Li",
            "Jifan Lin",
            "Yumin Zhuang",
            "Ji Zeng",
            "Shijie Xia",
            "Qishuo Hua",
            "Xuefeng Li",
            "Xiaojie Cai",
            "Tongyu Wang",
            "Yue Zhang",
            "Liming Liu",
            "Xia Wu",
            "Jinlong Hou",
            "Yuan Cheng",
            "Wenjie Li",
            "Xiang Wang",
            "Dequan Wang",
            "Pengfei Liu"
        ],
        "title": "LIMI: Less is More for Agency",
        "abstract": "arXiv:2509.17567v1 Announce Type: new  Abstract: We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.",
        "arxiv_id": "2509.17567",
        "ARXIVID": "2509.17567",
        "COMMENT": "Does not match any specific criteria but discusses agency in AI systems, which is tangentially related to embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2509.16578": {
        "authors": [
            "Wenyao Li",
            "Ran Zhang",
            "Pengyang Wang",
            "Yuanchun Zhou",
            "Pengfei Wang"
        ],
        "title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning",
        "abstract": "arXiv:2509.16578v1 Announce Type: new  Abstract: Human mobility forecasting is important for applications such as transportation planning, urban management, and personalized recommendations. However, existing methods often fail to generalize to unseen users or locations and struggle to capture dynamic intent due to limited labeled data and the complexity of mobility patterns. We propose ZHMF, a framework for zero-shot human mobility forecasting that combines a semantic enhanced retrieval and reflection mechanism with a hierarchical language model based reasoning system. The task is reformulated as a natural language question answering paradigm. Leveraging LLMs semantic understanding of user histories and context, our approach handles previously unseen prediction scenarios. We further introduce a hierarchical reflection mechanism for iterative reasoning and refinement by decomposing forecasting into an activity level planner and a location level selector, enabling collaborative modeling of long term user intentions and short term contextual preferences. Experiments on standard human mobility datasets show that our approach outperforms existing models. Ablation studies reveal the contribution of each module, and case studies illustrate how the method captures user intentions and adapts to diverse contextual scenarios.",
        "arxiv_id": "2509.16578",
        "ARXIVID": "2509.16578",
        "COMMENT": "Does not match any specific criteria but is related to general advancements in zero-shot reasoning and mobility forecasting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.16742": {
        "authors": [
            "Mohammad Beigi",
            "Ying Shen",
            "Parshin Shojaee",
            "Qifan Wang",
            "Zichao Wang",
            "Chandan Reddy",
            "Ming Jin",
            "Lifu Huang"
        ],
        "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories",
        "abstract": "arXiv:2509.16742v1 Announce Type: new  Abstract: Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \\textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \\textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \\textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",
        "arxiv_id": "2509.16742",
        "ARXIVID": "2509.16742",
        "COMMENT": "Does not match any specific criteria but is related to general advancements in reasoning optimization for language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.16630": {
        "authors": [
            "Yue Ma",
            "Zexuan Yan",
            "Hongyu Liu",
            "Hongfa Wang",
            "Heng Pan",
            "Yingqing He",
            "Junkun Yuan",
            "Ailing Zeng",
            "Chengfei Cai",
            "Heung-Yeung Shum",
            "Zhifeng Li",
            "Wei Liu",
            "Linfeng Zhang",
            "Qifeng Chen"
        ],
        "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation",
        "abstract": "arXiv:2509.16630v1 Announce Type: new  Abstract: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in https://follow-your-emoji.github.io/.",
        "arxiv_id": "2509.16630",
        "ARXIVID": "2509.16630",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and animation in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.17232": {
        "authors": [
            "Bo Liu",
            "Runlong Li",
            "Li Zhou",
            "Yan Zhou"
        ],
        "title": "DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction",
        "abstract": "arXiv:2509.17232v1 Announce Type: new  Abstract: This paper proposes a Diffusion Model-Optimized Neural Radiance Field (DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency in 3D scene reconstruction. By combining diffusion models with Transformers, DT-NeRF effectively restores details under sparse viewpoints and maintains high accuracy in complex geometric scenes. Experimental results demonstrate that DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art methods on the Matterport3D and ShapeNet datasets, particularly in metrics such as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further confirm the critical role of the diffusion and Transformer modules in the model's performance, with the removal of either module leading to a decline in performance. The design of DT-NeRF showcases the synergistic effect between modules, providing an efficient and accurate solution for 3D scene reconstruction. Future research may focus on further optimizing the model, exploring more advanced generative models and network architectures to enhance its performance in large-scale dynamic scenes.",
        "arxiv_id": "2509.17232",
        "ARXIVID": "2509.17232",
        "COMMENT": "Does not match any specific criteria but is relevant to 3D reconstruction and neural radiance fields, which are tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.17955": {
        "authors": [
            "Fan Xu",
            "Hao Wu",
            "Nan Wang",
            "Lilan Peng",
            "Kun Wang",
            "Wei Gong",
            "Xibin Zhao"
        ],
        "title": "Breaking the Discretization Barrier of Continuous Physics Simulation Learning",
        "abstract": "arXiv:2509.17955v1 Announce Type: new  Abstract: The modeling of complicated time-evolving physical dynamics from partial observations is a long-standing challenge. Particularly, observations can be sparsely distributed in a seemingly random or unstructured manner, making it difficult to capture highly nonlinear features in a variety of scientific and engineering problems. However, existing data-driven approaches are often constrained by fixed spatial and temporal discretization. While some researchers attempt to achieve spatio-temporal continuity by designing novel strategies, they either overly rely on traditional numerical methods or fail to truly overcome the limitations imposed by discretization. To address these, we propose CoPS, a purely data-driven methods, to effectively model continuous physics simulation from partial observations. Specifically, we employ multiplicative filter network to fuse and encode spatial information with the corresponding observations. Then we customize geometric grids and use message-passing mechanism to map features from original spatial domain to the customized grids. Subsequently, CoPS models continuous-time dynamics by designing multi-scale graph ODEs, while introducing a Markov-based neural auto-correction module to assist and constrain the continuous extrapolations. Comprehensive experiments demonstrate that CoPS advances the state-of-the-art methods in space-time continuous modeling across various scenarios.",
        "arxiv_id": "2509.17955",
        "ARXIVID": "2509.17955",
        "COMMENT": "Does not match any specific criteria but discusses continuous physics simulation learning, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.17050": {
        "authors": [
            "Junhao Jia",
            "Yunyou Liu",
            "Yifei Sun",
            "Huangwei Chen",
            "Feiwei Qin",
            "Changmiao Wang",
            "Yong Peng"
        ],
        "title": "Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition",
        "abstract": "arXiv:2509.17050v1 Announce Type: new  Abstract: Nonlinear manifolds are widespread in deep visual features, where Euclidean distances often fail to capture true similarity. This limitation becomes particularly severe in prototype-based interpretable fine-grained recognition, where subtle semantic distinctions are essential. To address this challenge, we propose a novel paradigm for prototype-based recognition that anchors similarity within the intrinsic geometry of deep features. Specifically, we distill the latent manifold structure of each class into a diffusion space and introduce a differentiable Nystr\\\"om interpolation, making the geometry accessible to both unseen samples and learnable prototypes. To ensure efficiency, we employ compact per-class landmark sets with periodic updates. This design keeps the embedding aligned with the evolving backbone, enabling fast and scalable inference. Extensive experiments on the CUB-200-2011 and Stanford Cars datasets show that our GeoProto framework produces prototypes focusing on semantically aligned parts, significantly outperforming Euclidean prototype networks.",
        "arxiv_id": "2509.17050",
        "ARXIVID": "2509.17050",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.17620": {
        "authors": [
            "Gregory Schroeder",
            "Mohamed Sabry",
            "Cristina Olaverri-Monreal"
        ],
        "title": "Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method",
        "abstract": "arXiv:2509.17620v1 Announce Type: new  Abstract: Estimating camera intrinsic parameters without prior scene knowledge is a fundamental challenge in computer vision. This capability is particularly important for applications such as autonomous driving and vehicle platooning, where precalibrated setups are impractical and real-time adaptability is necessary. To advance the state-of-the-art, we present a set of equations based on the calibrated trifocal tensor, enabling projective camera self-calibration from minimal image data. Our method, termed TrifocalCalib, significantly improves accuracy and robustness compared to both recent learning-based and classical approaches. Unlike many existing techniques, our approach requires no calibration target, imposes no constraints on camera motion, and simultaneously estimates both focal length and principal point. Evaluations in both procedurally generated synthetic environments and structured dataset-based scenarios demonstrate the effectiveness of our approach. To support reproducibility, we make the code publicly available.",
        "arxiv_id": "2509.17620",
        "ARXIVID": "2509.17620",
        "COMMENT": "Does not match any specific criteria but is related to camera calibration, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16456": {
        "authors": [
            "Jiahao Yu",
            "Zelei Cheng",
            "Xian Wu",
            "Xinyu Xing"
        ],
        "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
        "abstract": "arXiv:2509.16456v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly used in various domains, showing impressive potential on different tasks. Recently, reasoning LLMs have been proposed to improve the \\textit{reasoning} or \\textit{thinking} capabilities of LLMs to solve complex problems. Despite the promising results of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs still remains a significant challenge. While existing optimization methods have advanced the LLM reasoning capabilities, they often treat reasoning trajectories as a whole, without considering the underlying critical steps within the trajectory. In this paper, we introduce \\textbf{G}uided \\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that dives into the reasoning process to enable more effective improvements. GPO first identifies the `critical step' within a reasoning trajectory - a point that the model must carefully proceed to succeed at the problem. We locate the critical step by estimating the advantage function. GPO then resets the policy to the critical step, samples the new rollout and prioritizes the learning process on those rollouts. This focus allows the model to learn more effectively from pivotal moments within the reasoning process to improve the reasoning performance. We demonstrate that GPO is a general strategy that can be integrated with various optimization methods to improve reasoning performance. Besides theoretical analysis, our experiments across challenging reasoning benchmarks show that GPO can consistently and significantly enhance the performance of existing optimization methods, showcasing its effectiveness and generalizability in improving LLM reasoning by concentrating on pivotal moments within the generation process.",
        "arxiv_id": "2509.16456",
        "ARXIVID": "2509.16456",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning optimization in large language models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16561": {
        "authors": [
            "Yue Xin",
            "Chen Shen",
            "Shaotian Yan",
            "Xiaosong Yuan",
            "Yaoming Wang",
            "Xiaofeng Zhang",
            "Chenxi Huang",
            "Jieping Ye"
        ],
        "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning",
        "abstract": "arXiv:2509.16561v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of large language models (LLMs) to a large margin. However, the mechanism underlying such improvements remains unexplored. In this paper, we present \\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed \\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd} M\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a mathematically rigorous evaluation metric for quantifying component-level contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley value for mathematical expression attribution and develop an efficient stratified sampling algorithm that significantly reduces the computational complexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality \\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance analysis. Comprehensive validation across popular LLM models and diverse mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder framework exhibits a robust monotonic correlation with model performance, not only providing theoretical explanations for the empirical success of existing few-shot CoT but also establishing mathematically rigorous principles for prompt construction optimization. Furthermore, we verify the reliability of the explanation, based on which we unify the insights of previous work.",
        "arxiv_id": "2509.16561",
        "ARXIVID": "2509.16561",
        "COMMENT": "Does not match any specific criteria. Focuses on mathematical reasoning in large language models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.18076": {
        "authors": [
            "Hy Dang",
            "Tianyi Liu",
            "Zhuofeng Wu",
            "Jingfeng Yang",
            "Haoming Jiang",
            "Tao Yang",
            "Pei Chen",
            "Zhengyang Wang",
            "Helen Wang",
            "Huasheng Li",
            "Bing Yin",
            "Meng Jiang"
        ],
        "title": "Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates",
        "abstract": "arXiv:2509.18076v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated strong reasoning and tool-use capabilities, yet they often fail in real-world tool-interactions due to incorrect parameterization, poor tool selection, or misinterpretation of user intent. These issues often stem from an incomplete understanding of user goals and inadequate comprehension of tool documentation. While Chain-of-Thought (CoT) prompting has proven effective for enhancing reasoning in general contexts, our analysis reveals that free-form CoT is insufficient and sometimes counterproductive for structured function-calling tasks. To address this, we introduce a curriculum-inspired framework that leverages structured reasoning templates to guide LLMs through more deliberate step-by-step instructions for generating function callings. Experimental results show that our method reduces tool-use errors, achieving 3-12% relative improvements over strong baselines across diverse model series and approaches. Moreover, our framework enhances the robustness, interpretability, and transparency of tool-using agents, advancing the development of more reliable AI assistants for real-world applications.",
        "arxiv_id": "2509.18076",
        "ARXIVID": "2509.18076",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to improving LLM interpretability and tool use.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16839": {
        "authors": [
            "Yu Yao",
            "Jiayi Dong",
            "Ju Li",
            "Yang Yang",
            "Yilun Du"
        ],
        "title": "Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs",
        "abstract": "arXiv:2509.16839v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities not only in language generation but also in advancing scientific discovery. A growing body of work has explored ways to improve their reasoning, from self-consistency and chain-of-thought to multi-agent debate. Inspired by the dynamics of scientific committees and the \"Society of Mind,\" we introduce Roundtable Policy, a complementary inference-time reasoning framework that performs inference through the weighted consensus of multiple LLMs. Our findings indicate that this approach significantly enhances reasoning in complex heterogeneous scientific tasks and improves scientific narratives in terms of creativity, rigor, and logical coherence, while reducing hallucinations that single models are prone to. Our approach emphasizes structured and interpretable consensus rather than opaque convergence, while requiring only black-box access and uniform procedures, making it broadly applicable to multi-LLM reasoning.",
        "arxiv_id": "2509.16839",
        "ARXIVID": "2509.16839",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to reasoning and LLM advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17492": {
        "authors": [
            "Qinghua Lin",
            "Guang-Hai Liu",
            "Zuoyong Li",
            "Yang Li",
            "Yuting Jiang",
            "Xiang Wu"
        ],
        "title": "Multimodal Medical Image Classification via Synergistic Learning Pre-training",
        "abstract": "arXiv:2509.17492v1 Announce Type: new  Abstract: Multimodal pathological images are usually in clinical diagnosis, but computer vision-based multimodal image-assisted diagnosis faces challenges with modality fusion, especially in the absence of expert-annotated data. To achieve the modality fusion in multimodal images with label scarcity, we propose a novel ``pretraining + fine-tuning\" framework for multimodal semi-supervised medical image classification. Specifically, we propose a synergistic learning pretraining framework of consistency, reconstructive, and aligned learning. By treating one modality as an augmented sample of another modality, we implement a self-supervised learning pre-train, enhancing the baseline model's feature representation capability. Then, we design a fine-tuning method for multimodal fusion. During the fine-tuning stage, we set different encoders to extract features from the original modalities and provide a multimodal fusion encoder for fusion modality. In addition, we propose a distribution shift method for multimodal fusion features, which alleviates the prediction uncertainty and overfitting risks caused by the lack of labeled samples. We conduct extensive experiments on the publicly available gastroscopy image datasets Kvasir and Kvasirv2. Quantitative and qualitative results demonstrate that the proposed method outperforms the current state-of-the-art classification methods. The code will be released at: https://github.com/LQH89757/MICS.",
        "arxiv_id": "2509.17492",
        "ARXIVID": "2509.17492",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17062": {
        "authors": [
            "Cristian P\\'erez-Corral",
            "Antonio Garrido",
            "Laura Sebastia"
        ],
        "title": "From domain-landmark graph learning to problem-landmark graph generation",
        "abstract": "arXiv:2509.17062v1 Announce Type: new  Abstract: Landmarks have long played a pivotal role in automated planning, serving as crucial elements for improving the planning algorithms. The main limitation of classical landmark extraction methods is their sensitivity to specific planning tasks. This results in landmarks fully tailored to individual instances, thereby limiting their applicability across other instances of the same planning domain. We propose a novel approach that learns landmark relationships from multiple planning tasks of a planning domain. This leads to the creation of a \\textit{probabilistic lifted ordering graph}, as a structure that captures weighted abstractions of relationships between parameterized landmarks. Although these orderings are not 100\\% true (they are probabilistic), they can still be very useful in planning. Next, given a new planning task for that domain, we instantiate the relationships from that graph to this particular instance. This instantiation operates in two phases. First, it generates two graphs: the former instantiating information from the initial state and the latter from the goal state. Second, it combines these two graphs into one unified graph by searching equivalences to extract landmark orderings. We evaluate the precision and recallof the information found by our approach over well-known planning domains.",
        "arxiv_id": "2509.17062",
        "ARXIVID": "2509.17062",
        "COMMENT": "Does not match any specific criteria but involves planning and graph-based reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17427": {
        "authors": [
            "Hodaka Kawachi",
            "Jose Reinaldo Cunha Santos A. V. Silva Neto",
            "Yasushi Yagi",
            "Hajime Nagahara",
            "Tomoya Nakamura"
        ],
        "title": "Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling",
        "abstract": "arXiv:2509.17427v1 Announce Type: new  Abstract: We propose a single-snapshot depth-from-defocus (DFD) reconstruction method for coded-aperture imaging that replaces hand-crafted priors with a learned diffusion prior used purely as regularization. Our optimization framework enforces measurement consistency via a differentiable forward model while guiding solutions with the diffusion prior in the denoised image domain, yielding higher accuracy and stability than clas- sical optimization. Unlike U-Net-style regressors, our approach requires no paired defocus-RGBD training data and does not tie training to a specific camera configuration. Experiments on comprehensive simulations and a prototype camera demonstrate consistently strong RGBD reconstructions across noise levels, outperforming both U-Net baselines and a classical coded- aperture DFD method.",
        "arxiv_id": "2509.17427",
        "ARXIVID": "2509.17427",
        "COMMENT": "Does not match any specific criteria but involves depth estimation and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17431": {
        "authors": [
            "Keyu Du",
            "Jingyu Hu",
            "Haipeng Li",
            "Hao Xu",
            "Haibing Huang",
            "Chi-Wing Fu",
            "Shuaicheng Liu"
        ],
        "title": "Emergent 3D Correspondence from Neural Shape Representation",
        "abstract": "arXiv:2509.17431v1 Announce Type: new  Abstract: This paper presents a new approach to estimate accurate and robust 3D semantic correspondence with the hierarchical neural semantic representation. Our work has three key contributions. First, we design the hierarchical neural semantic representation (HNSR), which consists of a global semantic feature to capture high-level structure and multi-resolution local geometric features to preserve fine details, by carefully harnessing 3D priors from pre-trained 3D generative models. Second, we design a progressive global-to-local matching strategy, which establishes coarse semantic correspondence using the global semantic feature, then iteratively refines it with local geometric features, yielding accurate and semantically-consistent mappings. Third, our framework is training-free and broadly compatible with various pre-trained 3D generative backbones, demonstrating strong generalization across diverse shape categories. Our method also supports various applications, such as shape co-segmentation, keypoint matching, and texture transfer, and generalizes well to structurally diverse shapes, with promising results even in cross-category scenarios. Both qualitative and quantitative evaluations show that our method outperforms previous state-of-the-art techniques.",
        "arxiv_id": "2509.17431",
        "ARXIVID": "2509.17431",
        "COMMENT": "Does not match any specific criteria but is relevant to 3D semantic correspondence and neural shape representation, which are tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16590": {
        "authors": [
            "Manuel Borroto",
            "Katie Gallagher",
            "Antonio Ielo",
            "Irfan Kareem",
            "Francesco Ricca",
            "Alessandra Russo"
        ],
        "title": "Question Answering with LLMs and Learning from Answer Sets",
        "abstract": "arXiv:2509.16590v1 Announce Type: new  Abstract: Large Language Models (LLMs) excel at understanding natural language but struggle with explicit commonsense reasoning. A recent trend of research suggests that the combination of LLM with robust symbolic reasoning systems can overcome this problem on story-based question answering tasks. In this setting, existing approaches typically depend on human expertise to manually craft the symbolic component. We argue, however, that this component can also be automatically learned from examples. In this work, we introduce LLM2LAS, a hybrid system that effectively combines the natural language understanding capabilities of LLMs, the rule induction power of the Learning from Answer Sets (LAS) system ILASP, and the formal reasoning strengths of Answer Set Programming (ASP). LLMs are used to extract semantic structures from text, which ILASP then transforms into interpretable logic rules. These rules allow an ASP solver to perform precise and consistent reasoning, enabling correct answers to previously unseen questions. Empirical results outline the strengths and weaknesses of our automatic approach for learning and reasoning in a story-based question answering benchmark.",
        "arxiv_id": "2509.16590",
        "ARXIVID": "2509.16590",
        "COMMENT": "Does not match any specific criteria but is relevant to hybrid reasoning systems combining LLMs and symbolic reasoning, which is tangentially related to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17598": {
        "authors": [
            "Aiming Zhang",
            "Tianyuan Yu",
            "Liang Bai",
            "Jun Tang",
            "Yanming Guo",
            "Yirun Ruan",
            "Yun Zhou",
            "Zhihe Lu"
        ],
        "title": "COLA: Context-aware Language-driven Test-time Adaptation",
        "abstract": "arXiv:2509.17598v1 Announce Type: new  Abstract: Test-time adaptation (TTA) has gained increasing popularity due to its efficacy in addressing ``distribution shift'' issue while simultaneously protecting data privacy.   However, most prior methods assume that a paired source domain model and target domain sharing the same label space coexist, heavily limiting their applicability.   In this paper, we investigate a more general source model capable of adaptation to multiple target domains without needing shared labels.   This is achieved by using a pre-trained vision-language model (VLM), \\egno, CLIP, that can recognize images through matching with class descriptions.   While the zero-shot performance of VLMs is impressive, they struggle to effectively capture the distinctive attributes of a target domain.   To that end, we propose a novel method -- Context-aware Language-driven TTA (COLA).   The proposed method incorporates a lightweight context-aware module that consists of three key components: a task-aware adapter, a context-aware unit, and a residual connection unit for exploring task-specific knowledge, domain-specific knowledge from the VLM and prior knowledge of the VLM, respectively.   It is worth noting that the context-aware module can be seamlessly integrated into a frozen VLM, ensuring both minimal effort and parameter efficiency.   Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy to mitigate the adverse effects caused by class imbalance.   We demonstrate the effectiveness of our method not only in TTA scenarios but also in class generalisation tasks.   The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.",
        "arxiv_id": "2509.17598",
        "ARXIVID": "2509.17598",
        "COMMENT": "Does not match any specific criteria but is relevant to test-time adaptation and vision-language models, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16702": {
        "authors": [
            "Chen Liu",
            "Haitao Wu",
            "Kafeng Wang",
            "Xiaowang Zhang"
        ],
        "title": "Animalbooth: multimodal feature enhancement for animal subject personalization",
        "abstract": "arXiv:2509.16702v1 Announce Type: new  Abstract: Personalized animal image generation is challenging due to rich appearance cues and large morphological variability. Existing approaches often exhibit feature misalignment across domains, which leads to identity drift. We present AnimalBooth, a framework that strengthens identity preservation with an Animal Net and an adaptive attention module, mitigating cross domain alignment errors. We further introduce a frequency controlled feature integration module that applies Discrete Cosine Transform filtering in the latent space to guide the diffusion process, enabling a coarse to fine progression from global structure to detailed texture. To advance research in this area, we curate AnimalBench, a high resolution dataset for animal personalization. Extensive experiments show that AnimalBooth consistently outperforms strong baselines on multiple benchmarks and improves both identity fidelity and perceptual quality.",
        "arxiv_id": "2509.16702",
        "ARXIVID": "2509.16702",
        "COMMENT": "Does not match any specific criteria but is generally relevant to generative modeling and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16685": {
        "authors": [
            "Binbin Wen",
            "Yihang Wu",
            "Tareef Daqqaq",
            "Ahmad Chaddad"
        ],
        "title": "Towards a Transparent and Interpretable AI Model for Medical Image Classifications",
        "abstract": "arXiv:2509.16685v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into medicine is remarkable, offering advanced diagnostic and therapeutic possibilities. However, the inherent opacity of complex AI models presents significant challenges to their clinical practicality. This paper focuses primarily on investigating the application of explainable artificial intelligence (XAI) methods, with the aim of making AI decisions transparent and interpretable. Our research focuses on implementing simulations using various medical datasets to elucidate the internal workings of the XAI model. These dataset-driven simulations demonstrate how XAI effectively interprets AI predictions, thus improving the decision-making process for healthcare professionals. In addition to a survey of the main XAI methods and simulations, ongoing challenges in the XAI field are discussed. The study highlights the need for the continuous development and exploration of XAI, particularly from the perspective of diverse medical datasets, to promote its adoption and effectiveness in the healthcare domain.",
        "arxiv_id": "2509.16685",
        "ARXIVID": "2509.16685",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of explainable AI in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17956": {
        "authors": [
            "Lin Luo",
            "Yuri Nakao",
            "Mathieu Chollet",
            "Hiroya Inakoshi",
            "Simone Stumpf"
        ],
        "title": "\"I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment",
        "abstract": "arXiv:2509.17956v1 Announce Type: new  Abstract: Assessing fairness in artificial intelligence (AI) typically involves AI experts who select protected features, fairness metrics, and set fairness thresholds. However, little is known about how stakeholders, particularly those affected by AI outcomes but lacking AI expertise, assess fairness. To address this gap, we conducted a qualitative study with 30 stakeholders without AI expertise, representing potential decision subjects in a credit rating scenario, to examine how they assess fairness when placed in the role of deciding on features with priority, metrics, and thresholds. We reveal that stakeholders' fairness decisions are more complex than typical AI expert practices: they considered features far beyond legally protected features, tailored metrics for specific contexts, set diverse yet stricter fairness thresholds, and even preferred designing customized fairness. Our results extend the understanding of how stakeholders can meaningfully contribute to AI fairness governance and mitigation, underscoring the importance of incorporating stakeholders' nuanced fairness judgments.",
        "arxiv_id": "2509.17956",
        "ARXIVID": "2509.17956",
        "COMMENT": "Does not match any specific criterion but is tangentially relevant to fairness in AI, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16690": {
        "authors": [
            "Xiaodong Wang",
            "Zijun He",
            "Ping Wang",
            "Lishun Wang",
            "Yanan Hu",
            "Xin Yuan"
        ],
        "title": "Spectral Compressive Imaging via Chromaticity-Intensity Decomposition",
        "abstract": "arXiv:2509.16690v1 Announce Type: new  Abstract: In coded aperture snapshot spectral imaging (CASSI), the captured measurement entangles spatial and spectral information, posing a severely ill-posed inverse problem for hyperspectral images (HSIs) reconstruction. Moreover, the captured radiance inherently depends on scene illumination, making it difficult to recover the intrinsic spectral reflectance that remains invariant to lighting conditions. To address these challenges, we propose a chromaticity-intensity decomposition framework, which disentangles an HSI into a spatially smooth intensity map and a spectrally variant chromaticity cube. The chromaticity encodes lighting-invariant reflectance, enriched with high-frequency spatial details and local spectral sparsity. Building on this decomposition, we develop CIDNet, a Chromaticity-Intensity Decomposition unfolding network within a dual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral Transformer tailored to reconstruct fine-grained and sparse spectral chromaticity and a degradation-aware, spatially-adaptive noise estimation module that captures anisotropic noise across iterative stages. Extensive experiments on both synthetic and real-world CASSI datasets demonstrate that our method achieves superior performance in both spectral and chromaticity fidelity. Code and models will be publicly available.",
        "arxiv_id": "2509.16690",
        "ARXIVID": "2509.16690",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17553": {
        "authors": [
            "Congcong Ge",
            "Yachuan Liu",
            "Yixuan Tang",
            "Yifan Zhu",
            "Yaofeng Tu",
            "Yunjun Gao"
        ],
        "title": "MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances",
        "abstract": "arXiv:2509.17553v1 Announce Type: new  Abstract: In commercial systems, a pervasive requirement for automatic data preparation (ADP) is to transfer relational data from disparate sources to targets with standardized schema specifications. Previous methods rely on labor-intensive supervision signals or target table data access permissions, limiting their usage in real-world scenarios. To tackle these challenges, we propose an effective end-to-end ADP framework MontePrep, which enables training-free pipeline synthesis with zero target-instance requirements. MontePrep is formulated as an open-source large language model (LLM) powered tree-structured search problem. It consists of three pivot components, i.e., a data preparation action sandbox (DPAS), a fundamental pipeline generator (FPG), and an execution-aware pipeline optimizer (EPO). We first introduce DPAS, a lightweight action sandbox, to navigate the search-based pipeline generation. The design of DPAS circumvents exploration of infeasible pipelines. Then, we present FPG to build executable DP pipelines incrementally, which explores the predefined action sandbox by the LLM-powered Monte Carlo Tree Search. Furthermore, we propose EPO, which invokes pipeline execution results from sources to targets to evaluate the reliability of the generated pipelines in FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the search process from both efficiency and effectiveness perspectives. Extensive experimental results demonstrate the superiority of MontePrep with significant improvement against five state-of-the-art competitors.",
        "arxiv_id": "2509.17553",
        "ARXIVID": "2509.17553",
        "COMMENT": "Does not match any specific criteria but discusses data preparation using LLMs, which is tangentially relevant to your friend's general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16509": {
        "authors": [
            "Haijin Zeng",
            "Xuan Lu",
            "Yurong Zhang",
            "Yongyong Chen",
            "Jingyong Su",
            "Jie Liu"
        ],
        "title": "SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging",
        "abstract": "arXiv:2509.16509v1 Announce Type: new  Abstract: Humans learn in two complementary ways: a slow, cumulative process that builds broad, general knowledge, and a fast, on-the-fly process that captures specific experiences. Existing deep-unfolding methods for spectral compressive imaging (SCI) mirror only the slow component-relying on heavy pre-training with many unfolding stages-yet they lack the rapid adaptation needed to handle new optical configurations. As a result, they falter on out-of-distribution cameras, especially in bespoke spectral setups unseen during training. This depth also incurs heavy computation and slow inference. To bridge this gap, we introduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any deep unfolding network beyond SCI systems. During slow learning, we pre-train or reuse a priors-based backbone and distill it via imaging guidance into a compact fast-unfolding model. In the fast learning stage, lightweight adaptation modules are embedded within each block and trained self-supervised at test time via a dual-domain loss-without retraining the backbone. To the best of our knowledge, SlowFast-SCI is the first test-time adaptation-driven deep unfolding framework for efficient, self-adaptive spectral reconstruction. Its dual-stage design unites offline robustness with on-the-fly per-sample calibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB PSNR improvement on out-of-distribution data, preserved cross-domain adaptability, and a 4x faster adaptation speed. In addition, its modularity integrates with any deep-unfolding network, paving the way for self-adaptive, field-deployable imaging and expanded computational imaging modalities. Code and models are available at https://github.com/XuanLu11/SlowFast-SCI.",
        "arxiv_id": "2509.16509",
        "ARXIVID": "2509.16509",
        "COMMENT": "Does not match any specific criteria but is related to computational imaging and deep learning, which are tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17993": {
        "authors": [
            "Haoxin Yang",
            "Bangzhen Liu",
            "Xuemiao Xu",
            "Cheng Xu",
            "Yuyang Yu",
            "Zikai Huang",
            "Yi Wang",
            "Shengfeng He"
        ],
        "title": "StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models",
        "abstract": "arXiv:2509.17993v1 Announce Type: new  Abstract: The advancement of diffusion models has enhanced the realism of AI-generated content but also raised concerns about misuse, necessitating robust copyright protection and tampering localization. Although recent methods have made progress toward unified solutions, their reliance on post hoc processing introduces considerable application inconvenience and compromises forensic reliability. We propose StableGuard, a novel framework that seamlessly integrates a binary watermark into the diffusion generation process, ensuring copyright protection and tampering localization in Latent Diffusion Models through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE) by equipping a pretrained Variational Autoencoder (VAE) with a lightweight latent residual-based adapter, enabling the generation of paired watermarked and watermark-free images. These pairs, fused via random masks, create a diverse dataset for training a tampering-agnostic forensic network. To further enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic Network (MoE-GFN) that dynamically integrates holistic watermark patterns, local tampering traces, and frequency-domain cues for precise watermark verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly optimized in a self-supervised, end-to-end manner, fostering a reciprocal training between watermark embedding and forensic accuracy. Extensive experiments demonstrate that StableGuard consistently outperforms state-of-the-art methods in image fidelity, watermark verification, and tampering localization.",
        "arxiv_id": "2509.17993",
        "ARXIVID": "2509.17993",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.17049": {
        "authors": [
            "Peng Wang",
            "Yong Li",
            "Lin Zhao",
            "Xiu-Shen Wei"
        ],
        "title": "Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization",
        "abstract": "arXiv:2509.17049v1 Announce Type: new  Abstract: Fine-grained hashing has become a powerful solution for rapid and efficient image retrieval, particularly in scenarios requiring high discrimination between visually similar categories. To enable each hash bit to correspond to specific visual attributes, we propoe a novel method that harnesses learnable queries for attribute-aware hash codes learning. This method deploys a tailored set of queries to capture and represent nuanced attribute-level information within the hashing process, thereby enhancing both the interpretability and relevance of each hash bit. Building on this query-based optimization framework, we incorporate an auxiliary branch to help alleviate the challenges of complex landscape optimization often encountered with low-bit hash codes. This auxiliary branch models high-order attribute interactions, reinforcing the robustness and specificity of the generated hash codes. Experimental results on benchmark datasets demonstrate that our method generates attribute-aware hash codes and consistently outperforms state-of-the-art techniques in retrieval accuracy and robustness, especially for low-bit hash codes, underscoring its potential in fine-grained image hashing tasks.",
        "arxiv_id": "2509.17049",
        "ARXIVID": "2509.17049",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and machine learning for fine-grained image retrieval.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.16957": {
        "authors": [
            "Leiyu Wang",
            "Biao Jin",
            "Feng Huang",
            "Liqiong Chen",
            "Zhengyong Wang",
            "Xiaohai He",
            "Honggang Chen"
        ],
        "title": "MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image",
        "abstract": "arXiv:2509.16957v1 Announce Type: new  Abstract: Oriented object detection for multi-spectral imagery faces significant challenges due to differences both within and between modalities. Although existing methods have improved detection accuracy through complex network architectures, their high computational complexity and memory consumption severely restrict their performance. Motivated by the success of large kernel convolutions in remote sensing, we propose MO R-CNN, a lightweight framework for multi-spectral oriented detection featuring heterogeneous feature extraction network (HFEN), single modality supervision (SMS), and condition-based multimodal label fusion (CMLF). HFEN leverages inter-modal differences to adaptively align, merge, and enhance multi-modal features. SMS constrains multi-scale features and enables the model to learn from multiple modalities. CMLF fuses multimodal labels based on specific rules, providing the model with a more robust and consistent supervisory signal. Experiments on the DroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The source code is available at:https://github.com/Iwill-github/MORCNN.",
        "arxiv_id": "2509.16957",
        "ARXIVID": "2509.16957",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.16639": {
        "authors": [
            "Shangzhuo Xie",
            "Qianqian Yang"
        ],
        "title": "Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination",
        "abstract": "arXiv:2509.16639v1 Announce Type: new  Abstract: Point cloud analysis has evolved with diverse network architectures, while existing works predominantly focus on introducing novel structural designs. However, conventional point-based architectures - processing raw points through sequential sampling, grouping, and feature extraction layers - demonstrate underutilized potential. We notice that substantial performance gains can be unlocked through strategic module integration rather than structural modifications. In this paper, we propose the Grouping-Feature Coordination Module (GF-Core), a lightweight separable component that simultaneously regulates both grouping layer and feature extraction layer to enable more nuanced feature aggregation. Besides, we introduce a self-supervised pretraining strategy specifically tailored for point-based inputs to enhance model robustness in complex point cloud analysis scenarios. On ModelNet40 dataset, our method elevates baseline networks to 94.0% accuracy, matching advanced frameworks' performance while preserving architectural simplicity. On three variants of the ScanObjectNN dataset, we obtain improvements of 2.96%, 6.34%, and 6.32% respectively.",
        "arxiv_id": "2509.16639",
        "ARXIVID": "2509.16639",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}