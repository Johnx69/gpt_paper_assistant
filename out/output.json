{
    "2509.03800": {
        "authors": [
            "Yuheng Li",
            "Yenho Chen",
            "Yuxiang Lai",
            "Jike Zhong",
            "Vanessa Wildman",
            "Xiaofeng Yang"
        ],
        "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting",
        "abstract": "arXiv:2509.03800v1 Announce Type: new  Abstract: Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released.",
        "arxiv_id": "2509.03800",
        "ARXIVID": "2509.03800",
        "COMMENT": "This paper matches criterion 2 and 5 as it explores a vision-language model for 3D CT analysis and integrates image understanding with language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.03635": {
        "authors": [
            "Hongpei Zheng",
            "Lintao Xiang",
            "Qijun Yang",
            "Qian Lin",
            "Hujun Yin"
        ],
        "title": "Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding",
        "abstract": "arXiv:2509.03635v1 Announce Type: new  Abstract: The rapid development of Large Multimodal Models (LMMs) has led to remarkable progress in 2D visual understanding; however, extending these capabilities to 3D scene understanding remains a significant challenge. Existing approaches predominantly rely on text-only supervision, which fails to provide the geometric constraints required for learning robust 3D spatial representations. In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction Tuning framework that addresses this limitation by incorporating geometry-aware supervision directly into the training process. Our key insight is that effective 3D understanding necessitates reconstructing underlying geometric structures rather than merely describing them. Unlike existing methods that inject 3D information solely at the input level, Reg3D adopts a dual-supervision paradigm that leverages 3D geometric information both as input and as explicit learning targets. Specifically, we design complementary object-level and frame-level reconstruction tasks within a dual-encoder architecture, enforcing geometric consistency to encourage the development of spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap, ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance improvements, establishing a new training paradigm for spatially aware multimodal models.",
        "arxiv_id": "2509.03635",
        "ARXIVID": "2509.03635",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 2 (Visual and Multimodal Large Language Models) as it introduces a geometry-aware framework for 3D scene understanding in multimodal models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.03704": {
        "authors": [
            "Seth Z. Zhao",
            "Huizhi Zhang",
            "Zhaowei Li",
            "Juntong Peng",
            "Anthony Chui",
            "Zewei Zhou",
            "Zonglin Meng",
            "Hao Xiang",
            "Zhiyu Huang",
            "Fujia Wang",
            "Ran Tian",
            "Chenfeng Xu",
            "Bolei Zhou",
            "Jiaqi Ma"
        ],
        "title": "QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception",
        "abstract": "arXiv:2509.03704v1 Announce Type: new  Abstract: Cooperative perception through Vehicle-to-Everything (V2X) communication offers significant potential for enhancing vehicle perception by mitigating occlusions and expanding the field of view. However, past research has predominantly focused on improving accuracy metrics without addressing the crucial system-level considerations of efficiency, latency, and real-world deployability. Noticeably, most existing systems rely on full-precision models, which incur high computational and transmission costs, making them impractical for real-time operation in resource-constrained environments. In this paper, we introduce \\textbf{QuantV2X}, the first fully quantized multi-agent system designed specifically for efficient and scalable deployment of multi-modal, multi-agent V2X cooperative perception. QuantV2X introduces a unified end-to-end quantization strategy across both neural network models and transmitted message representations that simultaneously reduces computational load and transmission bandwidth. Remarkably, despite operating under low-bit constraints, QuantV2X achieves accuracy comparable to full-precision systems. More importantly, when evaluated under deployment-oriented metrics, QuantV2X reduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in mAP30 over full-precision baselines. Furthermore, QuantV2X scales more effectively, enabling larger and more capable models to fit within strict memory budgets. These results highlight the viability of a fully quantized multi-agent intermediate fusion system for real-world deployment. The system will be publicly released to promote research in this field: https://github.com/ucla-mobility/QuantV2X.",
        "arxiv_id": "2509.03704",
        "ARXIVID": "2509.03704",
        "COMMENT": "This paper matches criterion 3 as it introduces a new system for cooperative perception in embodied AI, focusing on efficiency and scalability.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.04123": {
        "authors": [
            "Ayan Banerjee",
            "Josep Llad\\'os",
            "Umapada Pal",
            "Anjan Dutta"
        ],
        "title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering",
        "abstract": "arXiv:2509.04123v1 Announce Type: new  Abstract: Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering.",
        "arxiv_id": "2509.04123",
        "ARXIVID": "2509.04123",
        "COMMENT": "Matches criterion 5 as it focuses on combining image generation tasks with LLMs for multi-character story generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.03897": {
        "authors": [
            "Xiaofu Chen",
            "Israfel Salazar",
            "Yova Kementchedjhieva"
        ],
        "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation",
        "abstract": "arXiv:2509.03897v1 Announce Type: new  Abstract: As interest grows in generating long, detailed image captions, standard evaluation metrics become increasingly unreliable. N-gram-based metrics though efficient, fail to capture semantic correctness. Representational Similarity (RS) metrics, designed to address this, initially saw limited use due to high computational costs, while today, despite advances in hardware, they remain unpopular due to low correlation to human judgments. Meanwhile, metrics based on large language models (LLMs) show strong correlation with human judgments, but remain too expensive for iterative use during model development.   We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS metric tailored to long image captioning. SPECS modifies CLIP with a new objective that emphasizes specificity: rewarding correct details and penalizing incorrect ones. We show that SPECS matches the performance of open-source LLM-based metrics in correlation to human judgments, while being far more efficient. This makes it a practical alternative for iterative checkpoint evaluation during image captioning model development.Our code can be found at https://github.com/mbzuai-nlp/SPECS.",
        "arxiv_id": "2509.03897",
        "ARXIVID": "2509.03897",
        "COMMENT": "This paper matches criterion 6 as it introduces a new metric for evaluating long image captions, which is relevant to video and image understanding tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.04338": {
        "authors": [
            "JiYuan Wang",
            "Chunyu Lin",
            "Lei Sun",
            "Rongying Liu",
            "Lang Nie",
            "Mingxing Li",
            "Kang Liao",
            "Xiangxiang Chu",
            "Yao Zhao"
        ],
        "title": "From Editor to Dense Geometry Estimator",
        "abstract": "arXiv:2509.04338v1 Announce Type: new  Abstract: Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning.   Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining\" their innate features, and ultimately achieve higher performance than their generative counterparts.   Based on these findings, we introduce \\textbf{FE2E}, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor's original flow matching loss into the ``consistent velocity\" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor's native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT's global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other.   Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100$\\times$ data. The project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}.",
        "arxiv_id": "2509.04338",
        "ARXIVID": "2509.04338",
        "COMMENT": "This paper matches criterion 4 as it focuses on adapting foundation models for dense geometry estimation in computer vision.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.04438": {
        "authors": [
            "Sabbir Mollah",
            "Rohit Gupta",
            "Sirnam Swetha",
            "Qingyang Liu",
            "Ahnaf Munir",
            "Mubarak Shah"
        ],
        "title": "The Telephone Game: Evaluating Semantic Drift in Unified Models",
        "abstract": "arXiv:2509.04438v1 Announce Type: new  Abstract: Employing a single, unified model (UM) for both visual understanding (image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened a new direction in Visual Language Model (VLM) research. While UMs can also support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus on the core cross-modal pair T2I and I2T, as consistency between understanding and generation is critical for downstream use. Existing evaluations consider these capabilities in isolation: FID and GenEval for T2I, and benchmarks such as MME, MMBench for I2T. These single-pass metrics do not reveal whether a model that understands a concept can also render it, nor whether meaning is preserved when cycling between image and text modalities. To address this, we introduce the Unified Consistency Framework for Unified Models (UCF-UM), a cyclic evaluation protocol that alternates I2T and T2I over multiple generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic loss; (ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii) Multi-Generation GenEval (MGG), an object-level compliance score extending GenEval. To assess generalization beyond COCO, which is widely used in training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and evaluate on seven recent models. UCF-UM reveals substantial variation in cross-modal stability: some models like BAGEL maintain semantics over many alternations, whereas others like Vila-u drift quickly despite strong single-pass scores. Our results highlight cyclic consistency as a necessary complement to standard I2T and T2I evaluations, and provide practical metrics to consistently assess unified model's cross-modal stability and strength of their shared representations. Code: https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models",
        "arxiv_id": "2509.04438",
        "ARXIVID": "2509.04438",
        "COMMENT": "This paper matches criterion 5 as it evaluates semantic drift in unified models combining image and text modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.03644": {
        "authors": [
            "Fran\\c{c}ois Olivier",
            "Zied Bouraoui"
        ],
        "title": "Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations",
        "abstract": "arXiv:2509.03644v1 Announce Type: new  Abstract: Despite significant progress in natural language understanding, Large Language Models (LLMs) remain error-prone when performing logical reasoning, often lacking the robust mental representations that enable human-like comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that grounds understanding and logical reasoning in schematic representations based on image schemas-recurring patterns derived from sensorimotor experience that structure human cognition. Our system operationalizes the spatial foundations of these cognitive structures using declarative spatial reasoning within Answer Set Programming. Through evaluation on logical deduction problems, we demonstrate that LLMs can be guided to interpret scenarios through embodied cognitive structures, that these structures can be formalized as executable programs, and that the resulting representations support effective logical reasoning with enhanced interpretability. While our current implementation focuses on spatial primitives, it establishes the computational foundation for incorporating more complex and dynamic representations.",
        "arxiv_id": "2509.03644",
        "ARXIVID": "2509.03644",
        "COMMENT": "This paper matches criterion 1 as it focuses on spatial reasoning and embodied cognitive structures for logical reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.04378": {
        "authors": [
            "Yilin Tao",
            "Jiashui Huang",
            "Huaze Xu",
            "Ling Shao"
        ],
        "title": "Aesthetic Image Captioning with Saliency Enhanced MLLMs",
        "abstract": "arXiv:2509.04378v1 Announce Type: new  Abstract: Aesthetic Image Captioning (AIC) aims to generate textual descriptions of image aesthetics, becoming a key research direction in the field of computational aesthetics. In recent years, pretrained Multimodal Large Language Models (MLLMs) have advanced rapidly, leading to a significant increase in image aesthetics research that integrates both visual and textual modalities. However, most existing studies on image aesthetics primarily focus on predicting aesthetic ratings and have shown limited application in AIC. Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods without specifically adapting MLLMs to focus on target aesthetic content. To address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal Large Language Model (ASE-MLLM), an end-to-end framework that explicitly incorporates aesthetic saliency into MLLMs. Within this framework, we introduce the Image Aesthetic Saliency Module (IASM), which efficiently and effectively extracts aesthetic saliency features from images. Additionally, we design IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency features with original image features via a cross-attention mechanism. To the best of our knowledge, ASE-MLLM is the first framework to integrate image aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments demonstrated that our approach significantly outperformed traditional methods and generic MLLMs on current mainstream AIC benchmarks, achieving state-of-the-art (SOTA) performance.",
        "arxiv_id": "2509.04378",
        "ARXIVID": "2509.04378",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) for aesthetic image captioning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.03740": {
        "authors": [
            "Taha Koleilat",
            "Hassan Rivaz",
            "Yiming Xiao"
        ],
        "title": "Singular Value Few-shot Adaptation of Vision-Language Models",
        "abstract": "arXiv:2509.03740v1 Announce Type: new  Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and \\textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \\textbf{0.04\\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.",
        "arxiv_id": "2509.03740",
        "ARXIVID": "2509.03740",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes a parameter-efficient adaptation technique for vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.04276": {
        "authors": [
            "Jianning Deng",
            "Kartic Subr",
            "Hakan Bilen"
        ],
        "title": "PAOLI: Pose-free Articulated Object Learning from Sparse-view Images",
        "abstract": "arXiv:2509.04276v1 Announce Type: new  Abstract: We present a novel self-supervised framework for learning articulated object representations from sparse-view, unposed images. Unlike prior methods that require dense multi-view observations and ground-truth camera poses, our approach operates with as few as four views per articulation and no camera supervision. To address the inherent challenges, we first reconstruct each articulation independently using recent advances in sparse-view 3D reconstruction, then learn a deformation field that establishes dense correspondences across poses. A progressive disentanglement strategy further separates static from moving parts, enabling robust separation of camera and object motion. Finally, we jointly optimize geometry, appearance, and kinematics with a self-supervised loss that enforces cross-view and cross-pose consistency. Experiments on the standard benchmark and real-world examples demonstrate that our method produces accurate and detailed articulated object representations under significantly weaker input assumptions than existing approaches.",
        "arxiv_id": "2509.04276",
        "ARXIVID": "2509.04276",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel self-supervised framework for learning articulated object representations.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.03999": {
        "authors": [
            "Han Huang",
            "Han Sun",
            "Ningzhong Liu",
            "Huiyu Zhou",
            "Jiaquan Shen"
        ],
        "title": "SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation",
        "abstract": "arXiv:2509.03999v1 Announce Type: new  Abstract: Driven by autonomous driving's demands for precise 3D perception, 3D semantic occupancy prediction has become a pivotal research topic. Unlike bird's-eye-view (BEV) methods, which restrict scene representation to a 2D plane, occupancy prediction leverages a complete 3D voxel grid to model spatial structures in all dimensions, thereby capturing semantic variations along the vertical axis. However, most existing approaches overlook height-axis information when processing voxel features. And conventional SENet-style channel attention assigns uniform weight across all height layers, limiting their ability to emphasize features at different heights. To address these limitations, we propose SliceSemOcc, a novel vertical slice based multimodal framework for 3D semantic occupancy representation. Specifically, we extract voxel features along the height-axis using both global and local vertical slices. Then, a global local fusion module adaptively reconciles fine-grained spatial details with holistic contextual information. Furthermore, we propose the SEAttention3D module, which preserves height-wise resolution through average pooling and assigns dynamic channel attention weights to each height layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy datasets verify that our method significantly enhances mean IoU, achieving especially pronounced gains on most small-object categories. Detailed ablation studies further validate the effectiveness of the proposed SliceSemOcc framework.",
        "arxiv_id": "2509.03999",
        "ARXIVID": "2509.03999",
        "COMMENT": "Matches criterion 6 as it focuses on 3D semantic occupancy prediction, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.03961": {
        "authors": [
            "Yijun Zhou",
            "Yikui Zhai",
            "Zilu Ying",
            "Tingfeng Xian",
            "Wenlve Zhou",
            "Zhiheng Zhou",
            "Xiaolin Tian",
            "Xudong Jia",
            "Hongsheng Zhang",
            "C. L. Philip Chen"
        ],
        "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection",
        "abstract": "arXiv:2509.03961v1 Announce Type: new  Abstract: Although deep learning has advanced remote sensing change detection (RSCD), most methods rely solely on image modality, limiting feature representation, change pattern modeling, and generalization especially under illumination and noise disturbances. To address this, we propose MMChange, a multimodal RSCD method that combines image and text modalities to enhance accuracy and robustness. An Image Feature Refinement (IFR) module is introduced to highlight key regions and suppress environmental noise. To overcome the semantic limitations of image features, we employ a vision language model (VLM) to generate semantic descriptions of bitemporal images. A Textual Difference Enhancement (TDE) module then captures fine grained semantic shifts, guiding the model toward meaningful changes. To bridge the heterogeneity between modalities, we design an Image Text Feature Fusion (ITFF) module that enables deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and SYSUCD demonstrate that MMChange consistently surpasses state of the art methods across multiple metrics, validating its effectiveness for multimodal RSCD. Code is available at: https://github.com/yikuizhai/MMChange.",
        "arxiv_id": "2509.03961",
        "ARXIVID": "2509.03961",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it combines image and text modalities for remote sensing change detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.04444": {
        "authors": [
            "Xin Lin",
            "Xian Ge",
            "Dizhe Zhang",
            "Zhaoliang Wan",
            "Xianshun Wang",
            "Xiangtai Li",
            "Wenjie Jiang",
            "Bo Du",
            "Dacheng Tao",
            "Ming-Hsuan Yang",
            "Lu Qi"
        ],
        "title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision",
        "abstract": "arXiv:2509.04444v1 Announce Type: new  Abstract: Driven by the demand for spatial intelligence and holistic scene perception, omnidirectional images (ODIs), which provide a complete 360\\textdegree{} field of view, are receiving growing attention across diverse applications such as virtual reality, autonomous driving, and embodied robotics. Despite their unique characteristics, ODIs exhibit remarkable differences from perspective images in geometric projection, spatial distribution, and boundary continuity, making it challenging for direct domain adaption from perspective methods. This survey reviews recent panoramic vision techniques with a particular emphasis on the perspective-to-panorama adaptation. We first revisit the panoramic imaging pipeline and projection methods to build the prior knowledge required for analyzing the structural disparities. Then, we summarize three challenges of domain adaptation: severe geometric distortions near the poles, non-uniform sampling in Equirectangular Projection (ERP), and periodic boundary continuity. Building on this, we cover 20+ representative tasks drawn from more than 300 research papers in two dimensions. On one hand, we present a cross-method analysis of representative strategies for addressing panoramic specific challenges across different tasks. On the other hand, we conduct a cross-task comparison and classify panoramic vision into four major categories: visual quality enhancement and assessment, visual understanding, multimodal understanding, and visual generation. In addition, we discuss open challenges and future directions in data, models, and applications that will drive the advancement of panoramic vision research. We hope that our work can provide new insight and forward looking perspectives to advance the development of panoramic vision technologies. Our project page is https://insta360-research-team.github.io/Survey-of-Panorama",
        "arxiv_id": "2509.04444",
        "ARXIVID": "2509.04444",
        "COMMENT": "Matches criterion 1 as it discusses spatial intelligence and panoramic vision, which is relevant for embodied agents and spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2509.04446": {
        "authors": [
            "Kiymet Akdemir",
            "Jing Shi",
            "Kushal Kafle",
            "Brian Price",
            "Pinar Yanardag"
        ],
        "title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models",
        "abstract": "arXiv:2509.04446v1 Announce Type: new  Abstract: Text-to-image diffusion models have demonstrated significant capabilities to generate diverse and detailed visuals in various domains, and story visualization is emerging as a particularly promising application. However, as their use in real-world creative domains increases, the need for providing enhanced control, refinement, and the ability to modify images post-generation in a consistent manner becomes an important challenge. Existing methods often lack the flexibility to apply fine or coarse edits while maintaining visual and narrative consistency across multiple frames, preventing creators from seamlessly crafting and refining their visual stories. To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail.",
        "arxiv_id": "2509.04446",
        "ARXIVID": "2509.04446",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on story visualization and editing using text-to-image diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2509.04159": {
        "authors": [
            "Aarush Kumbhakern",
            "Saransh Kumar Gupta",
            "Lipika Dey",
            "Partha Pratim Das"
        ],
        "title": "Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs",
        "abstract": "arXiv:2509.04159v1 Announce Type: new  Abstract: Formalizing cooking procedures remains a challenging task due to their inherent complexity and ambiguity. We introduce an extensible domain-specific language for representing recipes as directed action graphs, capturing processes, transfers, environments, concurrency, and compositional structure. Our approach enables precise, modular modeling of complex culinary workflows. Initial manual evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and suitability for future automated recipe analysis and execution. This work represents initial steps towards an action-centric ontology for cooking, using temporal graphs to enable structured machine understanding, precise interpretation, and scalable automation of culinary processes - both in home kitchens and professional culinary settings.",
        "arxiv_id": "2509.04159",
        "ARXIVID": "2509.04159",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for representing cooking procedures, which could be relevant for embodied AI applications.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.04298": {
        "authors": [
            "Yingxuan Li",
            "Jiafeng Mao",
            "Yusuke Matsui"
        ],
        "title": "Noisy Label Refinement with Semantically Reliable Synthetic Images",
        "abstract": "arXiv:2509.04298v1 Announce Type: new  Abstract: Semantic noise in image classification datasets, where visually similar categories are frequently mislabeled, poses a significant challenge to conventional supervised learning approaches. In this paper, we explore the potential of using synthetic images generated by advanced text-to-image models to address this issue. Although these high-quality synthetic images come with reliable labels, their direct application in training is limited by domain gaps and diversity constraints. Unlike conventional approaches, we propose a novel method that leverages synthetic images as reliable reference points to identify and correct mislabeled samples in noisy datasets. Extensive experiments across multiple benchmark datasets show that our approach significantly improves classification accuracy under various noise conditions, especially in challenging scenarios with semantic label noise. Additionally, since our method is orthogonal to existing noise-robust learning techniques, when combined with state-of-the-art noise-robust training methods, it achieves superior performance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100 under 70% semantic noise, and by 24% on ImageNet-100 under real-world noise conditions.",
        "arxiv_id": "2509.04298",
        "ARXIVID": "2509.04298",
        "COMMENT": "Matches criterion 4 as it focuses on improving image classification using synthetic images, which relates to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03872": {
        "authors": [
            "Nan Yang",
            "Yang Wang",
            "Zhanwen Liu",
            "Yuchao Dai",
            "Yang Liu",
            "Xiangmo Zhao"
        ],
        "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection",
        "abstract": "arXiv:2509.03872v1 Announce Type: new  Abstract: Existing RGB-Event detection methods process the low-information regions of both modalities (background in images and non-event regions in event data) uniformly during feature extraction and fusion, resulting in high computational costs and suboptimal performance. To mitigate the computational redundancy during feature extraction, researchers have respectively proposed token sparsification methods for the image and event modalities. However, these methods employ a fixed number or threshold for token selection, hindering the retention of informative tokens for samples with varying complexity. To achieve a better balance between accuracy and efficiency, we propose FocusMamba, which performs adaptive collaborative sparsification of multimodal features and efficiently integrates complementary information. Specifically, an Event-Guided Multimodal Sparsification (EGMS) strategy is designed to identify and adaptively discard low-information regions within each modality by leveraging scene content changes perceived by the event camera. Based on the sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed to effectively capture and integrate complementary features from both modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that the proposed method achieves superior performance in both accuracy and efficiency compared to existing methods. The code will be available at https://github.com/Zizzzzzzz/FocusMamba.",
        "arxiv_id": "2509.03872",
        "ARXIVID": "2509.03872",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a multimodal feature sparsification method for RGB-Event object detection.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.03883": {
        "authors": [
            "Haiwei Xue",
            "Xiangyang Luo",
            "Zhanghao Hu",
            "Xin Zhang",
            "Xunzhi Xiang",
            "Yuqin Dai",
            "Jianzhuang Liu",
            "Zhensong Zhang",
            "Minglei Li",
            "Jian Yang",
            "Fei Ma",
            "Zhiyong Wu",
            "Changpeng Yang",
            "Zonghong Dai",
            "Fei Richard Yu"
        ],
        "title": "Human Motion Video Generation: A Survey",
        "abstract": "arXiv:2509.03883v1 Announce Type: new  Abstract: Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.",
        "arxiv_id": "2509.03883",
        "ARXIVID": "2509.03883",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it is a comprehensive survey on human motion video generation.",
        "RELEVANCE": 7,
        "NOVELTY": 4
    },
    "2509.04448": {
        "authors": [
            "Zehong Yan",
            "Peng Qi",
            "Wynne Hsu",
            "Mong Li Lee"
        ],
        "title": "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection",
        "abstract": "arXiv:2509.04448v1 Announce Type: new  Abstract: Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.",
        "arxiv_id": "2509.04448",
        "ARXIVID": "2509.04448",
        "COMMENT": "Matches criterion 2 as it focuses on a vision-language model (TRUST-VL) for multimodal misinformation detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.04243": {
        "authors": [
            "Wanfu Wang",
            "Qipeng Huang",
            "Guangquan Xue",
            "Xiaobo Liang",
            "Juntao Li"
        ],
        "title": "Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding",
        "abstract": "arXiv:2509.04243v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have recently achieved significant progress in bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model introduced a zoom-in search strategy that effectively elicits active perception capabilities in VLMs, improving downstream task performance. However, enabling VLMs to reason effectively over appropriate image regions remains a core challenge in GUI grounding, particularly under high-resolution inputs and complex multi-element visual interactions. In this work, we propose LASER, a self-evolving framework that progressively endows VLMs with multi-step perception capabilities, enabling precise coordinate prediction. Specifically, our approach integrate Monte Carlo quality estimation with Intersection-over-Union (IoU)-based region quality evaluation to jointly encourage both accuracy and diversity in constructing high-quality preference data. This combination explicitly guides the model to focus on instruction-relevant key regions while adaptively allocating reasoning steps based on task complexity. Comprehensive experiments on the ScreenSpot Pro and ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new state-of-the-art (SoTA) among 7B-scale models.",
        "arxiv_id": "2509.04243",
        "ARXIVID": "2509.04243",
        "COMMENT": "Matches criterion 2 as it focuses on vision-language models (VLMs) and their application to GUI grounding tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03887": {
        "authors": [
            "Bu Jin",
            "Songen Gu",
            "Xiaotao Hu",
            "Yupeng Zheng",
            "Xiaoyang Guo",
            "Qian Zhang",
            "Xiaoxiao Long",
            "Wei Yin"
        ],
        "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction",
        "abstract": "arXiv:2509.03887v1 Announce Type: new  Abstract: In this paper, we propose OccTENS, a generative occupancy world model that enables controllable, high-fidelity long-term occupancy generation while maintaining computational efficiency. Different from visual generation, the occupancy world model must capture the fine-grained 3D geometry and dynamic evolution of the 3D scenes, posing great challenges for the generative models. Recent approaches based on autoregression (AR) have demonstrated the potential to predict vehicle movement and future occupancy scenes simultaneously from historical observations, but they typically suffer from \\textbf{inefficiency}, \\textbf{temporal degradation} in long-term generation and \\textbf{lack of controllability}. To holistically address these issues, we reformulate the occupancy world model as a temporal next-scale prediction (TENS) task, which decomposes the temporal sequence modeling problem into the modeling of spatial scale-by-scale generation and temporal scene-by-scene prediction. With a \\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and spatial relationships of occupancy sequences in a flexible and scalable way. To enhance the pose controllability, we further propose a holistic pose aggregation strategy, which features a unified sequence modeling for occupancy and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art method with both higher occupancy quality and faster inference time.",
        "arxiv_id": "2509.03887",
        "ARXIVID": "2509.03887",
        "COMMENT": "Matches criterion 3 as it introduces a novel generative occupancy world model (OccTENS) for embodied AI with a focus on 3D spatial reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03536": {
        "authors": [
            "Weizhi Chen",
            "Ziwei Wang",
            "Leyang Yang",
            "Sheng Zhou",
            "Xiaoxuan Tang",
            "Jiajun Bu",
            "Yong Li",
            "Wei Jiang"
        ],
        "title": "PG-Agent: An Agent Powered by Page Graph",
        "abstract": "arXiv:2509.03536v1 Announce Type: new  Abstract: Graphical User Interface (GUI) agents possess significant commercial and social value, and GUI agents powered by advanced multimodal large language models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI agents usually utilize sequential episodes of multi-step operations across pages as the prior GUI knowledge, which fails to capture the complex transition relationship between pages, making it challenging for the agents to deeply perceive the GUI environment and generalize to new scenarios. Therefore, we design an automated pipeline to transform the sequential episodes into page graphs, which explicitly model the graph structure of the pages that are naturally connected by actions. To fully utilize the page graphs, we further introduce Retrieval-Augmented Generation (RAG) technology to effectively retrieve reliable perception guidelines of GUI from them, and a tailored multi-agent framework PG-Agent with task decomposition strategy is proposed to be injected with the guidelines so that it can generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.",
        "arxiv_id": "2509.03536",
        "ARXIVID": "2509.03536",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (PG-Agent) for GUI agents with a focus on generalization and perception in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03903": {
        "authors": [
            "Yuanfeng Ji",
            "Dan Lin",
            "Xiyue Wang",
            "Lu Zhang",
            "Wenhui Zhou",
            "Chongjian Ge",
            "Ruihang Chu",
            "Xiaoli Yang",
            "Junhan Zhao",
            "Junsong Chen",
            "Xiangde Luo",
            "Sen Yang",
            "Jin Fang",
            "Ping Luo",
            "Ruijiang Li"
        ],
        "title": "A Generative Foundation Model for Chest Radiography",
        "abstract": "arXiv:2509.03903v1 Announce Type: new  Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for developing reliable AI models in healthcare. Substantial technical advances have been made in generative foundation models for natural images. Here we develop `ChexGen', a generative vision-language foundation model that introduces a unified framework for text-, mask-, and bounding box-guided synthesis of chest radiographs. Built upon the latent diffusion transformer architecture, ChexGen was pretrained on the largest curated chest X-ray dataset to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves accurate synthesis of radiographs through expert evaluations and quantitative metrics. We demonstrate the utility of ChexGen for training data augmentation and supervised pretraining, which led to performance improvements across disease classification, detection, and segmentation tasks using a small fraction of training data. Further, our model enables the creation of diverse patient cohorts that enhance model fairness by detecting and mitigating demographic biases. Our study supports the transformative role of generative foundation models in building more accurate, data-efficient, and equitable medical AI systems.",
        "arxiv_id": "2509.03903",
        "ARXIVID": "2509.03903",
        "COMMENT": "Matches criterion 4 as it focuses on a vision foundation model (ChexGen) for medical imaging and its applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03817": {
        "authors": [
            "Wei Yang",
            "Jesse Thomason"
        ],
        "title": "Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning",
        "abstract": "arXiv:2509.03817v1 Announce Type: new  Abstract: Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.",
        "arxiv_id": "2509.03817",
        "ARXIVID": "2509.03817",
        "COMMENT": "Does not match any specific criterion but is related to multi-agent reasoning and reinforcement learning, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.04343": {
        "authors": [
            "Maciej Besta",
            "Shriram Chandran",
            "Robert Gerstenberger",
            "Mathis Lindner",
            "Marcin Chrapek",
            "Sebastian Hermann Martschat",
            "Taraneh Ghandi",
            "Patrick Iff",
            "Hubert Niewiadomski",
            "Piotr Nyczyk",
            "J\\\"urgen M\\\"uller",
            "Torsten Hoefler"
        ],
        "title": "Psychologically Enhanced AI Agents",
        "abstract": "arXiv:2509.04343v1 Announce Type: new  Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.",
        "arxiv_id": "2509.04343",
        "ARXIVID": "2509.04343",
        "COMMENT": "This paper does not match any specific criteria but is related to psychologically enhanced AI agents, which may be tangentially interesting for embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.04007": {
        "authors": [
            "Jinyuan Li",
            "Yi Chu",
            "Yiwen Sun",
            "Mengchuan Zou",
            "Shaowei Cai"
        ],
        "title": "AutoPBO: LLM-powered Optimization for Local Search PBO Solvers",
        "abstract": "arXiv:2509.04007v1 Announce Type: new  Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling combinatorial problems through pseudo-Boolean (PB) constraints. Local search solvers have shown excellent performance in PBO solving, and their efficiency is highly dependent on their internal heuristics to guide the search. Still, their design often requires significant expert effort and manual tuning in practice. While Large Language Models (LLMs) have demonstrated potential in automating algorithm design, their application to optimizing PBO solvers remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered framework to automatically enhance PBO local search solvers. We conduct experiments on a broad range of four public benchmarks, including one real-world benchmark, a benchmark from PB competition, an integer linear programming optimization benchmark, and a crafted combinatorial benchmark, to evaluate the performance improvement achieved by AutoPBO and compare it with six state-of-the-art competitors, including two local search PBO solvers NuPBO and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates significant improvements over previous local search approaches, while maintaining competitive performance compared to state-of-the-art competitors. The results suggest that AutoPBO offers a promising approach to automating local search solver design.",
        "arxiv_id": "2509.04007",
        "ARXIVID": "2509.04007",
        "COMMENT": "This paper does not match any specific criteria but is related to optimization and algorithm design, which may be tangentially interesting for general AI research.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.04406": {
        "authors": [
            "Zanwei Zhou",
            "Taoran Yi",
            "Jiemin Fang",
            "Chen Yang",
            "Lingxi Xie",
            "Xinggang Wang",
            "Wei Shen",
            "Qi Tian"
        ],
        "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
        "abstract": "arXiv:2509.04406v1 Announce Type: new  Abstract: Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.",
        "arxiv_id": "2509.04406",
        "ARXIVID": "2509.04406",
        "COMMENT": "This paper does not directly match any specific criteria but is relevant to 3D generation and flow-based models, which may be tangentially interesting for vision-related tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.03730": {
        "authors": [
            "Pengrui Han",
            "Rafal Kocielnik",
            "Peiyang Song",
            "Ramit Debnath",
            "Dean Mobbs",
            "Anima Anandkumar",
            "R. Michael Alvarez"
        ],
        "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs",
        "abstract": "arXiv:2509.03730v1 Announce Type: new  Abstract: Personality traits have long been studied as predictors of human behavior.Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.",
        "arxiv_id": "2509.03730",
        "ARXIVID": "2509.03730",
        "COMMENT": "Does not match any specific criterion but provides insights into LLM behavior and personality, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.04310": {
        "authors": [
            "Yunbo Long",
            "Liming Xu",
            "Lukas Beckenbauer",
            "Yuhan Liu",
            "Alexandra Brintrup"
        ],
        "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation",
        "abstract": "arXiv:2509.04310v1 Announce Type: new  Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \\textit{complex}, \\textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.",
        "arxiv_id": "2509.04310",
        "ARXIVID": "2509.04310",
        "COMMENT": "Does not match any specific criterion but is tangentially related to multi-agent systems and negotiation strategies, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03626": {
        "authors": [
            "Zahra Zehtabi Sabeti Moghaddam",
            "Zeinab Dehghani",
            "Maneeha Rani",
            "Koorosh Aslansefat",
            "Bhupesh Kumar Mishra",
            "Rameez Raja Kureshi",
            "Dhavalkumar Thakker"
        ],
        "title": "Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE",
        "abstract": "arXiv:2509.03626v1 Announce Type: new  Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive progress but still produces hallucinations and unverifiable claims, limiting reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves accuracy by grounding outputs in external knowledge, especially in domains like healthcare, where precision is vital. However, RAG remains opaque and essentially a black box, heavily dependent on data quality. We developed a method-agnostic, perturbation-based framework that provides token and component-level interoperability for Graph RAG using SMILE and named it as Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing similarities, and training weighted linear surrogates, KG-SMILE identifies the graph entities and relations most influential to generated outputs, thereby making RAG more transparent. We evaluate KG-SMILE using comprehensive attribution metrics, including fidelity, faithfulness, consistency, stability, and accuracy. Our findings show that KG-SMILE produces stable, human-aligned explanations, demonstrating its capacity to balance model effectiveness with interpretability and thereby fostering greater transparency and trust in machine learning technologies.",
        "arxiv_id": "2509.03626",
        "ARXIVID": "2509.03626",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to retrieval-augmented generation and explainability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.04439": {
        "authors": [
            "Matthew Ho",
            "Chen Si",
            "Zhaoxiang Feng",
            "Fangxu Yu",
            "Zhijian Liu",
            "Zhiting Hu",
            "Lianhui Qin"
        ],
        "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory",
        "abstract": "arXiv:2509.04439v1 Announce Type: new  Abstract: While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at https://github.com/matt-seb-ho/arc_memo.",
        "arxiv_id": "2509.04439",
        "ARXIVID": "2509.04439",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to reasoning and memory in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.04043": {
        "authors": [
            "Yuchen Zhu",
            "Longxiang Yin",
            "Kai Zhao"
        ],
        "title": "Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on \"Phytium + Cambricon\"",
        "abstract": "arXiv:2509.04043v1 Announce Type: new  Abstract: In the frontier research and application of current video surveillance technology, traditional camera systems exhibit significant limitations of response delay exceeding 200 ms in dynamic scenarios due to the insufficient deep feature extraction capability of automatic recognition algorithms and the efficiency bottleneck of computing architectures, failing to meet the real-time requirements in complex scenes. To address this issue, this study proposes a heterogeneous computing architecture based on Phytium processors and Cambricon accelerator cards, constructing a UAV tracking and gazing system with millisecond-level response capability. At the hardware level, the system adopts a collaborative computing architecture of Phytium FT-2000/4 processors and MLU220 accelerator cards, enhancing computing power through multi-card parallelism. At the software level, it innovatively integrates a lightweight YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming a closed-loop control chain of \"detection-tracking-feedback\". Experimental results demonstrate that the system achieves a stable single-frame comprehensive processing delay of 50-100 ms in 1920*1080 resolution video stream processing, with a multi-scale target recognition accuracy of over 98.5%, featuring both low latency and high precision. This study provides an innovative solution for UAV monitoring and the application of domestic chips.",
        "arxiv_id": "2509.04043",
        "ARXIVID": "2509.04043",
        "COMMENT": "This paper does not match any specific criteria but is related to UAV tracking and gazing systems, which may be tangentially interesting for embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03646": {
        "authors": [
            "Haozhe Wang",
            "Qixin Xu",
            "Che Liu",
            "Junhong Wu",
            "Fangzhen Lin",
            "Wenhu Chen"
        ],
        "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning",
        "abstract": "arXiv:2509.03646v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.",
        "arxiv_id": "2509.03646",
        "ARXIVID": "2509.03646",
        "COMMENT": "Does not match any specific criterion but discusses reinforcement learning for reasoning in LLMs, which is tangentially relevant to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.04041": {
        "authors": [
            "Daniel Raggi",
            "Gem Stapleton",
            "Mateja Jamnik",
            "Aaron Stockdill",
            "Grecia Garcia Garcia",
            "Peter C-H. Cheng"
        ],
        "title": "Oruga: An Avatar of Representational Systems Theory",
        "abstract": "arXiv:2509.04041v1 Announce Type: new  Abstract: Humans use representations flexibly. We draw diagrams, change representations and exploit creative analogies across different domains. We want to harness this kind of power and endow machines with it to make them more compatible with human use. Previously we developed Representational Systems Theory (RST) to study the structure and transformations of representations. In this paper we present Oruga (caterpillar in Spanish; a symbol of transformation), an implementation of various aspects of RST. Oruga consists of a core of data structures corresponding to concepts in RST, a language for communicating with the core, and an engine for producing transformations using a method we call structure transfer. In this paper we present an overview of the core and language of Oruga, with a brief example of the kind of transformation that structure transfer can execute.",
        "arxiv_id": "2509.04041",
        "ARXIVID": "2509.04041",
        "COMMENT": "Does not match any specific criterion but is tangentially related to spatial reasoning and representation transformation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.04100": {
        "authors": [
            "Alberto Luise",
            "Michele Lombardi",
            "Florent Teichteil Koenigsbuch"
        ],
        "title": "Hybrid Reinforcement Learning and Search for Flight Trajectory Planning",
        "abstract": "arXiv:2509.04100v1 Announce Type: new  Abstract: This paper explores the combination of Reinforcement Learning (RL) and search-based path planners to speed up the optimization of flight paths for airliners, where in case of emergency a fast route re-calculation can be crucial. The fundamental idea is to train an RL Agent to pre-compute near-optimal paths based on location and atmospheric data and use those at runtime to constrain the underlying path planning solver and find a solution within a certain distance from the initial guess. The approach effectively reduces the size of the solver's search space, significantly speeding up route optimization. Although global optimality is not guaranteed, empirical results conducted with Airbus aircraft's performance models show that fuel consumption remains nearly identical to that of an unconstrained solver, with deviations typically within 1%. At the same time, computation speed can be improved by up to 50% as compared to using a conventional solver alone.",
        "arxiv_id": "2509.04100",
        "ARXIVID": "2509.04100",
        "COMMENT": "Does not match any specific criterion but is related to reinforcement learning and optimization, which may interest your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.03614": {
        "authors": [
            "Seungho Choe",
            "Xiaoli Qin",
            "Abubakr Shafique",
            "Amanda Dy",
            "Dimitri Androutsos",
            "Susan Done",
            "April Khademi"
        ],
        "title": "Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge",
        "abstract": "arXiv:2509.03614v1 Announce Type: new  Abstract: Counting mitotic figures is time-intensive for pathologists and leads to inter-observer variability. Artificial intelligence (AI) promises a solution by automatically detecting mitotic figures while maintaining decision consistency. However, AI tools are susceptible to domain shift, where a significant drop in performance can occur due to differences in the training and testing sets, including morphological diversity between organs, species, and variations in staining protocols. Furthermore, the number of mitoses is much less than the count of normal nuclei, which introduces severely imbalanced data for the detection task. In this work, we formulate mitosis detection as a pixel-level segmentation and propose a teacher-student model that simultaneously addresses mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our method is based on a UNet segmentation backbone that integrates domain generalization modules, namely contrastive representation learning and domain-adversarial training. A teacher-student strategy is employed to generate pixel-level pseudo-masks not only for annotated mitoses and hard negatives but also for normal nuclei, thereby enhancing feature discrimination and improving robustness against domain shift. For the classification task, we introduce a multi-scale CNN classifier that leverages feature maps from the segmentation model within a multi-task learning paradigm. On the preliminary test set, the algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of 0.8414 in Track 2, demonstrating the effectiveness of integrating segmentation-based detection and classification into a unified framework for robust mitosis analysis.",
        "arxiv_id": "2509.03614",
        "ARXIVID": "2509.03614",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to AI applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.03786": {
        "authors": [
            "Xinxin Wang",
            "Han Sun",
            "Ningzhong Liu",
            "Huiyu Zhou",
            "Yinan Yao"
        ],
        "title": "SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection",
        "abstract": "arXiv:2509.03786v1 Announce Type: new  Abstract: Underwater Camouflaged Object Detection (UCOD) aims to identify objects that blend seamlessly into underwater environments. This task is critically important to marine ecology. However, it remains largely underexplored and accurate identification is severely hindered by optical distortions, water turbidity, and the complex traits of marine organisms. To address these challenges, we introduce the UCOD task and present DeepCamo, a benchmark dataset designed for this domain. We also propose Semantic Localization and Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE) module and a Localization Guidance Branch (LGB) to enhance multi-scale feature representation while generating a location map enriched with global semantic information. This map guides the Multi-Scale Supervised Decoder (MSSD) to produce more accurate predictions. Experiments on our DeepCamo dataset and three benchmark COD datasets confirm SLENet's superior performance over SOTA methods, and underscore its high generality for the broader COD task.",
        "arxiv_id": "2509.03786",
        "ARXIVID": "2509.03786",
        "COMMENT": "Does not match any specific criterion but is related to underwater object detection, which is tangentially relevant to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.04344": {
        "authors": [
            "Feng-Qi Cui",
            "Zhen Lin",
            "Xinlong Rao",
            "Anyang Tong",
            "Shiyao Li",
            "Fei Wang",
            "Changlin Chen",
            "Bin Liu"
        ],
        "title": "MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition",
        "abstract": "arXiv:2509.04344v1 Announce Type: new  Abstract: Dynamic facial expression recognition (DFER) faces significant challenges due to long-tailed category distributions and complexity of spatio-temporal feature modeling. While existing deep learning-based methods have improved DFER performance, they often fail to address these issues, resulting in severe model induction bias. To overcome these limitations, we propose a novel multi-instance learning framework called MICACL, which integrates spatio-temporal dependency modeling and long-tailed contrastive learning optimization. Specifically, we design the Graph-Enhanced Instance Interaction Module (GEIIM) to capture intricate spatio-temporal between adjacent instances relationships through adaptive adjacency matrices and multiscale convolutions. To enhance instance-level feature aggregation, we develop the Weighted Instance Aggregation Network (WIAN), which dynamically assigns weights based on instance importance. Furthermore, we introduce a Multiscale Category-aware Contrastive Learning (MCCL) strategy to balance training between major and minor categories. Extensive experiments on in-the-wild datasets (i.e., DFEW and FERV39k) demonstrate that MICACL achieves state-of-the-art performance with superior robustness and generalization.",
        "arxiv_id": "2509.04344",
        "ARXIVID": "2509.04344",
        "COMMENT": "Does not match any specific criterion but is related to dynamic facial expression recognition, which is tangentially relevant to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.04273": {
        "authors": [
            "Junying Meng",
            "Gangxuan Zhou",
            "Jun Liu",
            "Weihong Guo"
        ],
        "title": "Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation",
        "abstract": "arXiv:2509.04273v1 Announce Type: new  Abstract: Despite signi cant progress in semi-supervised medical image segmentation, most existing segmentation networks overlook e ective methodological guidance for feature extraction and important prior information from   datasets. In this paper, we develop a semi-supervised medical image segmentation framework that e ectively integrates spatial regularization methods and volume priors. Speci cally, our approach integrates a strong explicit volume prior at the image scale and Threshold Dynamics spatial regularization, both derived from variational models, into the backbone segmentation network. The target region volumes for each unlabeled image are estimated by a regression network, which e ectively regularizes the backbone segmentation network through an image-scale Wasserstein distance constraint, ensuring that the class ratios in the segmentation results for each unlabeled image match those predicted by the regression network. Additionally, we design a dataset-scale Wasserstein distance loss function based on a weak implicit volume prior, which enforces that the volume distribution predicted for the unlabeled dataset is similar to that of labeled dataset. Experimental results on the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset show the superiority of the proposed method.",
        "arxiv_id": "2509.04273",
        "ARXIVID": "2509.04273",
        "COMMENT": "Does not match any specific criterion but is related to semi-supervised learning in medical imaging, which is tangentially relevant to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}