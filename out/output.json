{
    "2508.19652": {
        "authors": [
            "Zongxia Li",
            "Wenhao Yu",
            "Chengsong Huang",
            "Rui Liu",
            "Zhenwen Liang",
            "Fuxiao Liu",
            "Jingxi Che",
            "Dian Yu",
            "Jordan Boyd-Graber",
            "Haitao Mi",
            "Dong Yu"
        ],
        "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
        "abstract": "arXiv:2508.19652v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.",
        "arxiv_id": "2508.19652",
        "ARXIVID": "2508.19652",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel method for improving visual reasoning in Vision-Language Models (VLLMs) and integrates image understanding with language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.19679": {
        "authors": [
            "Qihang Ai",
            "Pi Bu",
            "Yue Cao",
            "Yingyao Wang",
            "Jihao Gu",
            "Jingxuan Xing",
            "Zekun Zhu",
            "Wei Jiang",
            "Zhicheng Zheng",
            "Jun Song",
            "Yuning Jiang",
            "Bo Zheng"
        ],
        "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning",
        "abstract": "arXiv:2508.19679v1 Announce Type: new  Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents to perceive and interact with real-world mobile environments based on human instructions. However, the current fully autonomous paradigm poses potential safety risks when model understanding or reasoning capabilities are insufficient. To address this challenge, we first introduce \\textbf{InquireBench}, a comprehensive benchmark specifically designed to evaluate mobile agents' capabilities in safe interaction and proactive inquiry with users, encompassing 5 categories and 22 sub-categories, where most existing VLM-based agents demonstrate near-zero performance. In this paper, we aim to develop an interactive system that actively seeks human confirmation at critical decision points. To achieve this, we propose \\textbf{InquireMobile}, a novel model inspired by reinforcement learning, featuring a two-stage training strategy and an interactive pre-action reasoning mechanism. Finally, our model achieves an 46.8% improvement in inquiry success rate and the best overall success rate among existing baselines on InquireBench. We will open-source all datasets, models, and evaluation codes to facilitate development in both academia and industry.",
        "arxiv_id": "2508.19679",
        "ARXIVID": "2508.19679",
        "COMMENT": "Matches criterion 1 as it introduces a benchmark (InquireBench) and a novel method (InquireMobile) for embodied agents to request human assistance, focusing on spatial intelligence and interaction.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.19850": {
        "authors": [
            "Xiaoqi Wang",
            "Yun Zhang",
            "Weisi Lin"
        ],
        "title": "Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models",
        "abstract": "arXiv:2508.19850v1 Announce Type: new  Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance degradation under adverse visual conditions. To address this, we propose a machine-centric image quality assessment (MIQA) framework that quantifies the impact of image degradations on MVS performance. We establish an MIQA paradigm encompassing the end-to-end assessment workflow. To support this, we construct a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million samples that capture distinctive degradation responses in both consistency and accuracy metrics, spanning 75 vision models, 250 degradation types, and three representative vision tasks. We further propose a region-aware MIQA (RA-MIQA) model to evaluate MVS visual quality through fine-grained spatial degradation analysis. Extensive experiments benchmark the proposed RA-MIQA against seven human visual system (HVS)-based IQA metrics and five retrained classical backbones. Results demonstrate RA-MIQA's superior performance in multiple dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on accuracy for image classification, while also revealing task-specific degradation sensitivities. Critically, HVS-based metrics prove inadequate for MVS quality prediction, while even specialized MIQA models struggle with background degradations, accuracy-oriented estimation, and subtle distortions. This study can advance MVS reliability and establish foundations for machine-centric image processing and optimization. The model and code are available at: https://github.com/XiaoqiWang/MIQA.",
        "arxiv_id": "2508.19850",
        "ARXIVID": "2508.19850",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MIQD-2.5M) and a novel method (RA-MIQA) for machine-centric image quality assessment, which is relevant to embodied or robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.20072": {
        "authors": [
            "Zhixuan Liang",
            "Yizhuo Li",
            "Tianshuo Yang",
            "Chengyue Wu",
            "Sitong Mao",
            "Liuao Pei",
            "Xiaokang Yang",
            "Jiangmiao Pang",
            "Yao Mu",
            "Ping Luo"
        ],
        "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
        "abstract": "arXiv:2508.20072v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.",
        "arxiv_id": "2508.20072",
        "ARXIVID": "2508.20072",
        "COMMENT": "Matches criterion 5 as it introduces a discrete diffusion-based action decoder for vision-language-action policies, integrating vision and language models for action generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19638": {
        "authors": [
            "Yang Li",
            "Quan Yuan",
            "Guiyang Luo",
            "Xiaoyuan Fu",
            "Rui Pan",
            "Yujia Yang",
            "Congzhang Shao",
            "Yuewen Liu",
            "Jinglin Li"
        ],
        "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception",
        "abstract": "arXiv:2508.19638v1 Announce Type: new  Abstract: Collaborative perception allows agents to enhance their perceptual capabilities by exchanging intermediate features. Existing methods typically organize these intermediate features as 2D bird's-eye-view (BEV) representations, which discard critical fine-grained 3D structural cues essential for accurate object recognition and localization. To this end, we first introduce point-level tokens as intermediate representations for collaborative perception. However, point-cloud data are inherently unordered, massive, and position-sensitive, making it challenging to produce compact and aligned point-level token sequences that preserve detailed structural information. Therefore, we present CoPLOT, a novel Collaborative perception framework that utilizes Point-Level Optimized Tokens. It incorporates a point-native processing pipeline, including token reordering, sequence modeling, and multi-agent spatial alignment. A semantic-aware token reordering module generates adaptive 1D reorderings by leveraging scene-level and token-level semantic information. A frequency-enhanced state space model captures long-range sequence dependencies across both spatial and spectral domains, improving the differentiation between foreground tokens and background clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop process, combining global agent-level correction with local token-level refinement to mitigate localization noise. Extensive experiments on both simulated and real-world datasets show that CoPLOT outperforms state-of-the-art models, with even lower communication and computation overhead. Code will be available at https://github.com/CheeryLeeyy/CoPLOT.",
        "arxiv_id": "2508.19638",
        "ARXIVID": "2508.19638",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (CoPLOT) for collaborative perception in embodied AI, focusing on spatial alignment and token optimization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19769": {
        "authors": [
            "Shu Shen",
            "C. L. Philip Chen",
            "Tong Zhang"
        ],
        "title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning",
        "abstract": "arXiv:2508.19769v1 Announce Type: new  Abstract: Multimodal learning has significantly enhanced machine learning performance but still faces numerous challenges and limitations. Imbalanced multimodal learning is one of the problems extensively studied in recent works and is typically mitigated by modulating the learning of each modality. However, we find that these methods typically hinder the dominant modality's learning to promote weaker modalities, which affects overall multimodal performance. We analyze the cause of this issue and highlight a commonly overlooked problem: optimization bias within networks. To address this, we propose Adaptive Intra-Network Modulation (AIM) to improve balanced modality learning. AIM accounts for differences in optimization state across parameters and depths within the network during modulation, achieving balanced multimodal learning without hindering either dominant or weak modalities for the first time. Specifically, AIM decouples the dominant modality's under-optimized parameters into Auxiliary Blocks and encourages reliance on these performance-degraded blocks for joint training with weaker modalities. This approach effectively prevents suppression of weaker modalities while enabling targeted optimization of under-optimized parameters to improve the dominant modality. Additionally, AIM assesses modality imbalance level across network depths and adaptively adjusts modulation strength at each depth. Experimental results demonstrate that AIM outperforms state-of-the-art imbalanced modality learning methods across multiple benchmarks and exhibits strong generalizability across different backbones, fusion strategies, and optimizers.",
        "arxiv_id": "2508.19769",
        "ARXIVID": "2508.19769",
        "COMMENT": "Matches criteria 2 as it proposes a novel method for balanced multimodal learning, which is relevant to vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.20063": {
        "authors": [
            "Peng-Hao Hsu",
            "Ke Zhang",
            "Fu-En Wang",
            "Tao Tu",
            "Ming-Feng Li",
            "Yu-Lun Liu",
            "Albert Y. C. Chen",
            "Min Sun",
            "Cheng-Hao Kuo"
        ],
        "title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations",
        "abstract": "arXiv:2508.20063v1 Announce Type: new  Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.",
        "arxiv_id": "2508.20063",
        "ARXIVID": "2508.20063",
        "COMMENT": "Matches criteria 3 as it introduces a new method for open-vocabulary 3D object detection using multi-view images, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.20096": {
        "authors": [
            "Zeyi Sun",
            "Yuhang Cao",
            "Jianze Liang",
            "Qiushi Sun",
            "Ziyu Liu",
            "Zhixiong Zhang",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Kai Chen",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning",
        "abstract": "arXiv:2508.20096v1 Announce Type: new  Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.",
        "arxiv_id": "2508.20096",
        "ARXIVID": "2508.20096",
        "COMMENT": "Matches criteria 3 as it introduces a novel compositional framework for embodied agents with a focus on reinforcement learning and planning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.19499": {
        "authors": [
            "Xiangxu Wang",
            "Tianhong Zhao",
            "Wei Tu",
            "Bowen Zhang",
            "Guanzhou Chen",
            "Jinzhou Cao"
        ],
        "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery",
        "abstract": "arXiv:2508.19499v1 Announce Type: new  Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility analysis, underpinning applications in traffic forecasting, infrastructure planning, and policy design. However, existing methods suffer from two critical limitations: (1) reliance on auxiliary features (e.g., Points of Interest, socioeconomic statistics) that are costly to collect and have limited spatial coverage; and (2) sensitivity to spatial topology, where minor index reordering of urban regions (e.g., census tract relabeling) disrupts structural coherence in generated flows. To address these challenges, we propose Sat2Flow, a latent structure-aware diffusion-based framework that generates structurally coherent OD flows using solely satellite imagery as input. Our approach introduces a multi-kernel encoder to capture diverse regional interactions and employs a permutation-aware diffusion process that aligns latent representations across different regional orderings. Through a joint contrastive training objective that bridges satellite-derived features with OD patterns, combined with equivariant diffusion training that enforces structural consistency, Sat2Flow ensures topological robustness under arbitrary regional reindexing. Experimental results on real-world urban datasets demonstrate that Sat2Flow outperforms both physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures under index permutations. Sat2Flow offers a globally scalable solution for OD flow generation in data-scarce urban environments, eliminating region-specific auxiliary data dependencies while maintaining structural invariance for robust mobility modeling.",
        "arxiv_id": "2508.19499",
        "ARXIVID": "2508.19499",
        "COMMENT": "Matches criteria 1 as it focuses on spatial intelligence by generating human flow patterns from satellite imagery, which is relevant to spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.19906": {
        "authors": [
            "Moussa Kassem Sbeyti",
            "Nadja Klein",
            "Michelle Karg",
            "Christian Wirth",
            "Sahin Albayrak"
        ],
        "title": "Streamlining the Development of Active Learning Methods in Real-World Object Detection",
        "abstract": "arXiv:2508.19906v1 Announce Type: new  Abstract: Active learning (AL) for real-world object detection faces computational and reliability challenges that limit practical deployment. Developing new AL methods requires training multiple detectors across iterations to compare against existing approaches. This creates high costs for autonomous driving datasets where the training of one detector requires up to 282 GPU hours. Additionally, AL method rankings vary substantially across validation sets, compromising reliability in safety-critical transportation systems. We introduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses these challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without requiring detector training by measuring similarity between training sets and target domains using object-level features. This enables the elimination of ineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables the selection of representative validation sets for robust evaluation. We validate our similarity-based approach on three autonomous driving datasets (KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with two detector architectures (EfficientDet, YOLOv3). This work is the first to unify AL training and evaluation strategies in object detection based on object similarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object crops, and integrates with existing AL pipelines. This provides a practical framework for deploying AL in real-world applications where computational efficiency and evaluation reliability are critical. Code is available at https://mos-ks.github.io/publications/.",
        "arxiv_id": "2508.19906",
        "ARXIVID": "2508.19906",
        "COMMENT": "Matches criterion 3 as it proposes a novel metric (OSS) to streamline active learning in object detection, addressing computational and reliability challenges in real-world applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.20066": {
        "authors": [
            "Zheng Li",
            "Yanming Guo",
            "WenZhe Liu",
            "Xueyi Zhang",
            "Zhaoyun Ding",
            "Long Xu",
            "Mingrui Lao"
        ],
        "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence",
        "abstract": "arXiv:2508.20066v1 Announce Type: new  Abstract: Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios.",
        "arxiv_id": "2508.20066",
        "ARXIVID": "2508.20066",
        "COMMENT": "Matches criterion 3 as it addresses cross-view geo-localization with a novel framework (PAUL) for handling noisy correspondences, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.19909": {
        "authors": [
            "Lechun You",
            "Zhonghua Wu",
            "Weide Liu",
            "Xulei Yang",
            "Jun Cheng",
            "Wei Zhou",
            "Bharadwaj Veeravalli",
            "Guosheng Lin"
        ],
        "title": "Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation",
        "abstract": "arXiv:2508.19909v1 Announce Type: new  Abstract: Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.",
        "arxiv_id": "2508.19909",
        "ARXIVID": "2508.19909",
        "COMMENT": "Matches criterion 4 as it leverages 2D foundation models for 3D weakly supervised segmentation, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.19305": {
        "authors": [
            "Chen Chu",
            "Cyrus Shahabi"
        ],
        "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities",
        "abstract": "arXiv:2508.19305v1 Announce Type: new  Abstract: Spatial representation learning is essential for GeoAI applications such as urban analytics, enabling the encoding of shapes, locations, and spatial relationships (topological and distance-based) of geo-entities like points, polylines, and polygons. Existing methods either target a single geo-entity type or, like Poly2Vec, decompose entities into simpler components to enable Fourier transformation, introducing high computational cost. Moreover, since the transformed space lacks geometric alignment, these methods rely on uniform, non-adaptive sampling, which blurs fine-grained features like edges and boundaries. To address these limitations, we introduce Geo2Vec, a novel method inspired by signed distance fields (SDF) that operates directly in the original space. Geo2Vec adaptively samples points and encodes their signed distances (positive outside, negative inside), capturing geometry without decomposition. A neural network trained to approximate the SDF produces compact, geometry-aware, and unified representations for all geo-entity types. Additionally, we propose a rotation-invariant positional encoding to model high-frequency spatial variations and construct a structured and robust embedding space for downstream GeoAI models. Empirical results show that Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications. Code and Data can be found at: https://github.com/chuchen2017/GeoNeuralRepresentation.",
        "arxiv_id": "2508.19305",
        "ARXIVID": "2508.19305",
        "COMMENT": "Matches criteria 1 as it focuses on spatial representation learning, which is relevant to spatial intelligence for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.19320": {
        "authors": [
            "Ming Chen",
            "Liyuan Cui",
            "Wenyuan Zhang",
            "Haoxian Zhang",
            "Yan Zhou",
            "Xiaohan Li",
            "Xiaoqiang Liu",
            "Pengfei Wan"
        ],
        "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation",
        "abstract": "arXiv:2508.19320v1 Announce Type: new  Abstract: Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.",
        "arxiv_id": "2508.19320",
        "ARXIVID": "2508.19320",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a multimodal autoregressive video generation framework integrating vision and language.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.19542": {
        "authors": [
            "Nannan Zhu",
            "Yonghao Dong",
            "Teng Wang",
            "Xueqian Li",
            "Shengjun Deng",
            "Yijia Wang",
            "Zheng Hong",
            "Tiantian Geng",
            "Guo Niu",
            "Hanyan Huang",
            "Xiongfei Yao",
            "Shuaiwei Jiao"
        ],
        "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning",
        "abstract": "arXiv:2508.19542v1 Announce Type: new  Abstract: While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs.The data and evaluation code are available at https://github.com/Hokhim2/CVBench.",
        "arxiv_id": "2508.19542",
        "ARXIVID": "2508.19542",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for cross-video reasoning tasks, which is a novel video-based task evaluation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.19705": {
        "authors": [
            "Qiang Hu",
            "Ying Zhou",
            "Gepeng Ji",
            "Nick Barnes",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "title": "FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation",
        "abstract": "arXiv:2508.19705v1 Announce Type: new  Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance between spatiotemporal modeling and domain generalization, limiting their applicability in real clinical scenarios. To embrace this challenge, we recast the VPS task as a track-by-detect paradigm that leverages the spatial contexts captured by the image polyp segmentation (IPS) model while integrating the temporal modeling capabilities of segment anything model 2 (SAM2). However, during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error accumulation, resulting in a snowball effect that compromises segmentation stability. We mitigate this issue by repurposing SAM2 as a video polyp segmenter with two training-free modules. In particular, the intra-association filtering module eliminates spatial inaccuracies originating from the detecting stage, reducing false positives. The inter-association refinement module adaptively updates the memory bank to prevent error propagation over time, enhancing temporal coherence. Both modules work synergistically to stabilize SAM2, achieving cutting-edge performance in both in-domain and out-of-domain scenarios. Furthermore, we demonstrate the robust tracking capabilities of FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential reliable clinical analysis.",
        "arxiv_id": "2508.19705",
        "ARXIVID": "2508.19705",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video polyp segmentation with novel methods for spatiotemporal modeling and domain generalization.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19699": {
        "authors": [
            "Yupeng Zhang",
            "Dezhi Zheng",
            "Ping Lu",
            "Han Zhang",
            "Lei Wang",
            "Liping xiang",
            "Cheng Luo",
            "Kaijun Deng",
            "Xiaowen Fu",
            "Linlin Shen",
            "Jinbao Wang"
        ],
        "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation",
        "abstract": "arXiv:2508.19699v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation for 3D scenes, offering both high-fidelity reconstruction and efficient rendering. However, 3DGS lacks 3D segmentation ability, which limits its applicability in tasks that require scene understanding. The identification and isolating of specific object components is crucial. To address this limitation, we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments the Gaussian representation with object label.LabelGS introduces cross-view consistent semantic masks for 3D Gaussians and employs a novel Occlusion Analysis Model to avoid overfitting occlusion during optimization, Main Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian Projection Filter to avoid Gaussian label conflict. Our approach achieves effective decoupling of Gaussian representations and refines the 3DGS optimization process through a random region sampling strategy, significantly improving efficiency. Extensive experiments demonstrate that LabelGS outperforms previous state-of-the-art methods, including Feature-3DGS, in the 3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code will be at https://github.com/garrisonz/LabelGS.",
        "arxiv_id": "2508.19699",
        "ARXIVID": "2508.19699",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on augmenting 3D Gaussian Splatting for scene segmentation, which is a novel application of a vision foundation model.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.19862": {
        "authors": [
            "Long Chen",
            "Ashiv Patel",
            "Mengyun Qiao",
            "Mohammad Yousuf Salmasi",
            "Salah A. Hammouche",
            "Vasilis Stavrinides",
            "Jasleen Nagi",
            "Soodeh Kalaie",
            "Xiao Yun Xu",
            "Wenjia Bai",
            "Declan P. O'Regan"
        ],
        "title": "Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction",
        "abstract": "arXiv:2508.19862v1 Announce Type: new  Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential for timely intervention but remains challenging due to the need to model both subtle local deformations and global anatomical changes within complex 3D geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN introduces a dual-branch architecture combining a novel local KNN-based convolutional network (KCN) to preserve fine-grained geometric details and a global graph convolutional network (GCN) to capture long-range structural context, overcoming the over-smoothing limitations of deep GCNs. A dedicated condition branch encodes clinical attributes (age, sex) and the target time interval to generate anatomically plausible, temporally controlled predictions, enabling retrospective and prospective modeling. We curated TAAMesh, a new longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive experiments demonstrate that MCMeshGAN consistently outperforms state-of-the-art baselines in both geometric accuracy and clinically important diameter estimation. This framework offers a robust step toward clinically deployable, personalized 3D disease trajectory modeling. The source code for MCMeshGAN and the baseline methods is publicly available at https://github.com/ImperialCollegeLondon/MCMeshGAN.",
        "arxiv_id": "2508.19862",
        "ARXIVID": "2508.19862",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19576": {
        "authors": [
            "Sining Zhoubian",
            "Dan Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding",
        "abstract": "arXiv:2508.19576v1 Announce Type: new  Abstract: With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We validate the effectiveness of the proposed RL paradigm through extensive experiments on coding problems. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.",
        "arxiv_id": "2508.19576",
        "ARXIVID": "2508.19576",
        "COMMENT": "Does not match any specific criteria but is related to improving reasoning in large language models, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19626": {
        "authors": [
            "Jiajun Sun",
            "Zhen Yu",
            "Siyuan Yan",
            "Jason J. Ong",
            "Zongyuan Ge",
            "Lei Zhang"
        ],
        "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model",
        "abstract": "arXiv:2508.19626v1 Announce Type: new  Abstract: Skin images from real-world clinical practice are often limited, resulting in a shortage of training data for deep-learning models. While many studies have explored skin image synthesis, existing methods often generate low-quality images and lack control over the lesion's location and type. To address these limitations, we present LF-VAR, a model leveraging quantified lesion measurement scores and lesion type labels to guide the clinically relevant and controllable synthesis of skin images. It enables controlled skin synthesis with specific lesion characteristics based on language prompts. We train a multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to encode images into discrete latent representations for structured tokenization. Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized representations facilitates image synthesis. Lesion measurement from the lesion region and types as conditional embeddings are integrated to enhance synthesis fidelity. Our method achieves the best overall FID score (average 0.74) among seven lesion types, improving upon the previous state-of-the-art (SOTA) by 6.3%. The study highlights our controllable skin synthesis model's effectiveness in generating high-fidelity, clinically relevant synthetic skin images. Our framework code is available at https://github.com/echosun1996/LF-VAR.",
        "arxiv_id": "2508.19626",
        "ARXIVID": "2508.19626",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19789": {
        "authors": [
            "Xiuchao Wu",
            "Pengfei Zhu",
            "Jiangjing Lyu",
            "Xinguo Liu",
            "Jie Guo",
            "Yanwen Guo",
            "Weiwei Xu",
            "Chengfei Lyu"
        ],
        "title": "StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation",
        "abstract": "arXiv:2508.19789v1 Announce Type: new  Abstract: Recovering material information from images has been extensively studied in computer graphics and vision. Recent works in material estimation leverage diffusion model showing promising results. However, these diffusion-based methods adopt a multi-step denoising strategy, which is time-consuming for each estimation. Such stochastic inference also conflicts with the deterministic material estimation task, leading to a high variance estimated results. In this paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view material estimation that can produce high-quality material parameters with low variance. To address the overly-smoothing problem in one-step diffusion, StableIntrinsic applies losses in pixel space, with each loss designed based on the properties of the material. Additionally, StableIntrinsic introduces a Detail Injection Network (DIN) to eliminate the detail loss caused by VAE encoding, while further enhancing the sharpness of material prediction results. The experimental results indicate that our method surpasses the current state-of-the-art techniques by achieving a $9.9\\%$ improvement in the Peak Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error (MSE) for metallic and roughness by $44.4\\%$ and $60.0\\%$, respectively.",
        "arxiv_id": "2508.19789",
        "ARXIVID": "2508.19789",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and generative modeling, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.19325": {
        "authors": [
            "Haoyang Su",
            "Jin-Yi Xiang",
            "Shaohao Rui",
            "Yifan Gao",
            "Xingyu Chen",
            "Tingxuan Yin",
            "Xiaosong Wang",
            "Lian-Ming Wu"
        ],
        "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI",
        "abstract": "arXiv:2508.19325v1 Announce Type: new  Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central challenge in cardiovascular prognosis. We present PRISM (Prompt-guided Representation Integration for Survival Modeling), a self-supervised framework that integrates visual representations from non-contrast cardiac cine magnetic resonance imaging with structured electronic health records (EHRs) for survival analysis. PRISM extracts temporally synchronized imaging features through motion-aware multi-view distillation and modulates them using medically informed textual prompts to enable fine-grained risk prediction. Across four independent clinical cohorts, PRISM consistently surpasses classical survival prediction models and state-of-the-art (SOTA) deep learning baselines under internal and external validation. Further clinical findings demonstrate that the combined imaging and EHR representations derived from PRISM provide valuable insights into cardiac risk across diverse cohorts. Three distinct imaging signatures associated with elevated MACE risk are uncovered, including lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior elevated focus during diastole. Prompt-guided attribution further identifies hypertension, diabetes, and smoking as dominant contributors among clinical and physiological EHR factors.",
        "arxiv_id": "2508.19325",
        "ARXIVID": "2508.19325",
        "COMMENT": "Does not match any specific criterion but is tangentially relevant to multimodal learning as it integrates visual representations and textual prompts for survival prediction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19791": {
        "authors": [
            "Shay Shomer Chai",
            "Wenxuan Peng",
            "Bharath Hariharan",
            "Hadar Averbuch-Elor"
        ],
        "title": "Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models",
        "abstract": "arXiv:2508.19791v1 Announce Type: new  Abstract: Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.",
        "arxiv_id": "2508.19791",
        "ARXIVID": "2508.19791",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19927": {
        "authors": [
            "Fayaz Ali",
            "Muhammad Zawish",
            "Steven Davy",
            "Radu Timofte"
        ],
        "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution",
        "abstract": "arXiv:2508.19927v1 Announce Type: new  Abstract: Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds.",
        "arxiv_id": "2508.19927",
        "ARXIVID": "2508.19927",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19864": {
        "authors": [
            "Oussama Hadjerci",
            "Antoine Letienne",
            "Mohamed Abbas Hedjazi",
            "Adel Hafiane"
        ],
        "title": "Self-supervised structured object representation learning",
        "abstract": "arXiv:2508.19864v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for learning visual representations. While recent SSL approaches achieve strong results in global image understanding, they are limited in capturing the structured representation in scenes. In this work, we propose a self-supervised approach that progressively builds structured visual representations by combining semantic grouping, instance level separation, and hierarchical structuring. Our approach, based on a novel ProtoScale module, captures visual elements across multiple spatial scales. Unlike common strategies like DINO that rely on random cropping and global embeddings, we preserve full scene context across augmented views to improve performance in dense prediction tasks. We validate our method on downstream object detection tasks using a combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results show that our method learns object centric representations that enhance supervised object detection and outperform the state-of-the-art methods, even when trained with limited annotated data and fewer fine-tuning epochs.",
        "arxiv_id": "2508.19864",
        "ARXIVID": "2508.19864",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19544": {
        "authors": [
            "Eduardo Davalos",
            "Yike Zhang",
            "Namrata Srivastava",
            "Yashvitha Thatigotla",
            "Jorge A. Salas",
            "Sara McFadden",
            "Sun-Joo Cho",
            "Amanda Goodwin",
            "Ashwin TS",
            "Gautam Biswas"
        ],
        "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization",
        "abstract": "arXiv:2508.19544v1 Announce Type: new  Abstract: With advancements in AI, new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement. To tackle these issues, we introduce We bEyeTrack, a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14. Our open-source code is available at https://github.com/RedForestAi/WebEyeTrack.",
        "arxiv_id": "2508.19544",
        "ARXIVID": "2508.19544",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19349": {
        "authors": [
            "Mahdieh Behjat Khatooni",
            "Mohsen Soryani"
        ],
        "title": "EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis",
        "abstract": "arXiv:2508.19349v1 Announce Type: new  Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative disorders worldwide. As it progresses, it leads to the deterioration of cognitive functions. Since AD is irreversible, early diagnosis is crucial for managing its progression. Mild Cognitive Impairment (MCI) represents an intermediate stage between Cognitively Normal (CN) individuals and those with AD, and is considered a transitional phase from normal cognition to Alzheimer's disease. Diagnosing MCI is particularly challenging due to the subtle differences between adjacent diagnostic categories. In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture both local and global features from MRI images. Unlike previous studies that rely on limited subsets of data, our approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in a more robust and unbiased model. This comprehensive methodology enhances the model's clinical reliability. Furthermore, fine-tuning large pretrained models often yields suboptimal results when source and target dataset domains differ. To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt the pretrained ViT model to our target domain. This method enables efficient knowledge transfer and reduces the risk of overfitting. Our model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset.",
        "arxiv_id": "2508.19349",
        "ARXIVID": "2508.19349",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19804": {
        "authors": [
            "Christian Marzahl",
            "Brian Napora"
        ],
        "title": "A bag of tricks for real-time Mitotic Figure detection",
        "abstract": "arXiv:2508.19804v1 Announce Type: new  Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.",
        "arxiv_id": "2508.19804",
        "ARXIVID": "2508.19804",
        "COMMENT": "Does not match any specific criteria but is related to computer vision applications, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.19314": {
        "authors": [
            "Mahdis Tourian (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)",
            "Sareh Rowlands (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)",
            "Remy Vandaele (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)",
            "Max Fancourt (Natural England",
            "York",
            "UK)",
            "Rebecca Mein (Natural England",
            "York",
            "UK)",
            "Hywel T. P. Williams (Centre for Environmental Intelligence",
            "University of Exeter",
            "Exeter",
            "UK",
            "Department of Computer Science",
            "Faculty of Environment",
            "Science and Economy",
            "University of Exeter",
            "Exeter",
            "UK)"
        ],
        "title": "Automated classification of natural habitats using ground-level imagery",
        "abstract": "arXiv:2508.19314v1 Announce Type: new  Abstract: Accurate classification of terrestrial habitats is critical for biodiversity conservation, ecological monitoring, and land-use planning. Several habitat classification schemes are in use, typically based on analysis of satellite imagery with validation by field ecologists. Here we present a methodology for classification of habitats based solely on ground-level imagery (photographs), offering improved validation and the ability to classify habitats at scale (for example using citizen-science imagery). In collaboration with Natural England, a public sector organisation responsible for nature conservation in England, this study develops a classification system that applies deep learning to ground-level habitat photographs, categorising each image into one of 18 classes defined by the 'Living England' framework. Images were pre-processed using resizing, normalisation, and augmentation; re-sampling was used to balance classes in the training data and enhance model robustness. We developed and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label to each photograph. Using five-fold cross-validation, the model demonstrated strong overall performance across 18 habitat classes, with accuracy and F1-scores varying between classes. Across all folds, the model achieved a mean F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or ambiguous classes scoring lower. These findings demonstrate the potential of this approach for ecological monitoring. Ground-level imagery is readily obtained, and accurate computational methods for habitat classification based on such data have many potential applications. To support use by practitioners, we also provide a simple web application that classifies uploaded images using our model.",
        "arxiv_id": "2508.19314",
        "ARXIVID": "2508.19314",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning due to its application of deep learning for habitat classification.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.19762": {
        "authors": [
            "Ahmed Emam",
            "Mohamed Elbassiouny",
            "Julius Miller",
            "Patrick Donworth",
            "Sabine Seidel",
            "Ribana Roscher"
        ],
        "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions",
        "abstract": "arXiv:2508.19762v1 Announce Type: new  Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food production and ecosystem stability, yet their populations are declining due to increasing anthropogenic and environmental stressors. To support scalable, automated pollinator monitoring, we introduce BuzzSet, a new large-scale dataset of high-resolution pollinator images collected in real agricultural field conditions. BuzzSet contains 7856 manually verified and labeled images, with over 8000 annotated instances across three classes: honeybees, bumblebees, and unidentified insects. Initial annotations were generated using a YOLOv12 model trained on external data and refined via human verification using open-source labeling tools. All images were preprocessed into 256~$\\times$~256 tiles to improve the detection of small insects. We provide strong baselines using the RF-DETR transformer-based object detector. The model achieves high F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively, with confusion matrix results showing minimal misclassification between these categories. The unidentified class remains more challenging due to label ambiguity and lower sample frequency, yet still contributes useful insights for robustness evaluation. Overall detection quality is strong, with a best mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object detection, class separation under label noise, and ecological computer vision.",
        "arxiv_id": "2508.19762",
        "ARXIVID": "2508.19762",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.20064": {
        "authors": [
            "Philippe Zhang",
            "Weili Jiang",
            "Yihao Li",
            "Jing Zhang",
            "Sarah Matta",
            "Yubo Tan",
            "Hui Lin",
            "Haoshen Wang",
            "Jiangtian Pan",
            "Hui Xu",
            "Laurent Borderie",
            "Alexandre Le Guilcher",
            "B\\'eatrice Cochener",
            "Chubin Ou",
            "Gwenol\\'e Quellec",
            "Mathieu Lamard"
        ],
        "title": "Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices",
        "abstract": "arXiv:2508.20064v1 Announce Type: new  Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments have been effective in slowing the progression of neovascular AMD, with better outcomes achieved through timely diagnosis and consistent monitoring. Tracking the progression of neovascular activity in OCT scans of patients with exudative AMD allows for the development of more personalized and effective treatment plans. This was the focus of the Monitoring Age-related Macular Degeneration Progression in Optical Coherence Tomography (MARIO) challenge, in which we participated. In Task 1, which involved classifying the evolution between two pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN network with model ensembling to further enhance the model's performance. For Task 2, which focused on predicting progression over the next three months based on current exam data, we proposed the Patch Progression Masked Autoencoder that generates an OCT for the next exam and then classifies the evolution between the current OCT and the one generated using our solution from Task 1. The results we achieved allowed us to place in the Top 10 for both tasks. Some team members are part of the same organization as the challenge organizers; therefore, we are not eligible to compete for the prize.",
        "arxiv_id": "2508.20064",
        "ARXIVID": "2508.20064",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}