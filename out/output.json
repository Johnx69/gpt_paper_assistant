{
    "2509.10813": {
        "authors": [
            "Weipeng Zhong",
            "Peizhou Cao",
            "Yichen Jin",
            "Li Luo",
            "Wenzhe Cai",
            "Jingli Lin",
            "Hanqing Wang",
            "Zhaoyang Lyu",
            "Tai Wang",
            "Bo Dai",
            "Xudong Xu",
            "Jiangmiao Pang"
        ],
        "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts",
        "abstract": "arXiv:2509.10813v1 Announce Type: new  Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce \\textbf{InternScenes}, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.",
        "arxiv_id": "2509.10813",
        "ARXIVID": "2509.10813",
        "COMMENT": "Matches criterion 3 as it introduces a new large-scale simulatable indoor scene dataset for embodied AI tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.11360": {
        "authors": [
            "Wan Xu",
            "Feng Zhu",
            "Yihan Zeng",
            "Yuanfan Guo",
            "Ming Liu",
            "Hang Xu",
            "Wangmeng Zuo"
        ],
        "title": "GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration",
        "abstract": "arXiv:2509.11360v1 Announce Type: new  Abstract: Video detailed captioning aims to generate comprehensive video descriptions to facilitate video understanding. Recently, most efforts in the video detailed captioning community have been made towards a local-to-global paradigm, which first generates local captions from video clips and then summarizes them into a global caption. However, we find this paradigm leads to less detailed and contextual-inconsistent captions, which can be attributed to (1) no mechanism to ensure fine-grained captions, and (2) weak interaction between local and global captions. To remedy the above two issues, we propose GLaVE-Cap, a Global-Local aligned framework with Vision Expert integration for Captioning, which consists of two core modules: TrackFusion enables comprehensive local caption generation, by leveraging vision experts to acquire cross-frame visual prompts, coupled with a dual-stream structure; while CaptionBridge establishes a local-global interaction, by using global context to guide local captioning, and adaptively summarizing local captions into a coherent global caption. Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark featuring 5X more queries per video than existing benchmarks, covering diverse visual dimensions to facilitate reliable evaluation. We further provide a training dataset GLaVE-1.2M containing 16K high-quality fine-grained video captions and 1.2M related question-answer pairs. Extensive experiments on four benchmarks show that our GLaVE-Cap achieves state-of-the-art performance. Besides, the ablation studies and student model analyses further validate the effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the video understanding community. The source code, model weights, benchmark, and dataset will be open-sourced.",
        "arxiv_id": "2509.11360",
        "ARXIVID": "2509.11360",
        "COMMENT": "Matches criteria 6 (Video Understanding) due to its focus on video captioning and the introduction of a new benchmark (GLaVE-Bench) and dataset (GLaVE-1.2M).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.10748": {
        "authors": [
            "Jecia Z. Y. Mao",
            "Francis X Creighton",
            "Russell H Taylor",
            "Manish Sahu"
        ],
        "title": "SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation",
        "abstract": "arXiv:2509.10748v1 Announce Type: new  Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene is crucial to enable context-aware intraoperative assistance and decision making. Current solutions remain tethered to domain-specific, supervised models that rely on labeled data and required domain-specific data to adapt to new surgical scenarios and beyond predefined label categories. Recent advances in prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot segmentation across heterogeneous medical images. However, dependence of these models on manual visual or textual cues restricts their deployment in introperative surgical settings. We introduce a speech-guided collaborative perception (SCOPE) framework that integrates reasoning capabilities of large language model (LLM) with perception capabilities of open-set VFMs to support on-the-fly segmentation, labeling and tracking of surgical instruments and anatomy in intraoperative video streams. A key component of this framework is a collaborative perception agent, which generates top candidates of VFM-generated segmentation and incorporates intuitive speech feedback from clinicians to guide the segmentation of surgical instruments in a natural human-machine collaboration paradigm. Afterwards, instruments themselves serve as interactive pointers to label additional elements of the surgical scene. We evaluated our proposed framework on a subset of publicly available Cataract1k dataset and an in-house ex-vivo skull-base dataset to demonstrate its potential to generate on-the-fly segmentation and tracking of surgical scene. Furthermore, we demonstrate its dynamic capabilities through a live mock ex-vivo experiment. This human-AI collaboration paradigm showcase the potential of developing adaptable, hands-free, surgeon-centric tools for dynamic operating-room environments.",
        "arxiv_id": "2509.10748",
        "ARXIVID": "2509.10748",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates vision foundation models with large language models for surgical scene segmentation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.11914": {
        "authors": [
            "Yiqun Yao",
            "Naitong Yu",
            "Xiang Li",
            "Xin Jiang",
            "Xuezhi Fang",
            "Wenjia Ma",
            "Xuying Meng",
            "Jing Li",
            "Aixin Sun",
            "Yequan Wang"
        ],
        "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models",
        "abstract": "arXiv:2509.11914v1 Announce Type: new  Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex models that process real-time omnimodal streams. EgoMem enables real-time models to recognize multiple users directly from raw audiovisual streams, to provide personalized response, and to maintain long-term knowledge of users' facts, preferences, and social relationships extracted from audiovisual history. EgoMem operates with three asynchronous processes: (i) a retrieval process that dynamically identifies user via face and voice, and gathers relevant context from a long-term memory; (ii) an omnimodal dialog process that generates personalized audio responses based on the retrieved context; and (iii) a memory management process that automatically detects dialog boundaries from omnimodal streams, and extracts necessary information to update the long-term memory. Unlike existing memory agents for LLMs, EgoMem relies entirely on raw audiovisual streams, making it especially suitable for lifelong, real-time, and embodied scenarios. Experimental results demonstrate that EgoMem's retrieval and memory management modules achieve over 95% accuracy on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot, the system achieves fact-consistency scores above 87% in real-time personalized dialogs, establishing a strong baseline for future research.",
        "arxiv_id": "2509.11914",
        "ARXIVID": "2509.11914",
        "COMMENT": "This paper matches criterion 3 as it introduces a novel lifelong memory agent for embodied AI scenarios, focusing on omnimodal streams and real-time processing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11817": {
        "authors": [
            "Liying Wang",
            "Xiaoli Zhang",
            "Chuanmin Jia",
            "Siwei Ma"
        ],
        "title": "MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation",
        "abstract": "arXiv:2509.11817v1 Announce Type: new  Abstract: Infrared-visible image fusion methods aim at generating fused images with good visual quality and also facilitate the performance of high-level tasks. Indeed, existing semantic-driven methods have considered semantic information injection for downstream applications. However, none of them investigates the potential for reciprocal promotion between pixel-wise image fusion and cross-modal feature fusion perception tasks from a macroscopic task-level perspective. To address this limitation, we propose a unified network for image fusion and semantic segmentation. MAFS is a parallel structure, containing a fusion sub-network and a segmentation sub-network. On the one hand, We devise a heterogeneous feature fusion strategy to enhance semantic-aware capabilities for image fusion. On the other hand, by cascading the fusion sub-network and a segmentation backbone, segmentation-related knowledge is transferred to promote feature-level fusion-based segmentation. Within the framework, we design a novel multi-stage Transformer decoder to aggregate fine-grained multi-scale fused features efficiently. Additionally, a dynamic factor based on the max-min fairness allocation principle is introduced to generate adaptive weights of two tasks and guarantee smooth training in a multi-task manner. Extensive experiments demonstrate that our approach achieves competitive results compared with state-of-the-art methods. The code is available at https://github.com/Abraham-Einstein/MAFS/.",
        "arxiv_id": "2509.11817",
        "ARXIVID": "2509.11817",
        "COMMENT": "Matches criterion 5 as it integrates image fusion and semantic segmentation tasks, combining image understanding and generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12046": {
        "authors": [
            "Zirui Zheng",
            "Takashi Isobe",
            "Tong Shen",
            "Xu Jia",
            "Jianbin Zhao",
            "Xiaomin Li",
            "Mengmeng Ge",
            "Baolu Li",
            "Qinghe Wang",
            "Dong Li",
            "Dong Zhou",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "Emad Barsoum"
        ],
        "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking",
        "abstract": "arXiv:2509.12046v1 Announce Type: new  Abstract: While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.",
        "arxiv_id": "2509.12046",
        "ARXIVID": "2509.12046",
        "COMMENT": "Matches criterion 5 as it explores layout-conditioned text-to-image generation, integrating image understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10660": {
        "authors": [
            "Nam H. Le",
            "Patrick Erickson",
            "Yanbo Zhang",
            "Michael Levin",
            "Josh Bongard"
        ],
        "title": "ZapGPT: Free-form Language Prompting for Simulated Cellular Control",
        "abstract": "arXiv:2509.10660v1 Announce Type: new  Abstract: Human language is one of the most expressive tools for conveying intent, yet most artificial or biological systems lack mechanisms to interpret or respond meaningfully to it. Bridging this gap could enable more natural forms of control over complex, decentralized systems. In AI and artificial life, recent work explores how language can specify high-level goals, but most systems still depend on engineered rewards, task-specific supervision, or rigid command sets, limiting generalization to novel instructions. Similar constraints apply in synthetic biology and bioengineering, where the locus of control is often genomic rather than environmental perturbation.   A key open question is whether artificial or biological collectives can be guided by free-form natural language alone, without task-specific tuning or carefully designed evaluation metrics. We provide one possible answer here by showing, for the first time, that simple agents' collective behavior can be guided by free-form language prompts: one AI model transforms an imperative prompt into an intervention that is applied to simulated cells; a second AI model scores how well the prompt describes the resulting cellular dynamics; and the former AI model is evolved to improve the scores generated by the latter.   Unlike previous work, our method does not require engineered fitness functions or domain-specific prompt design. We show that the evolved system generalizes to unseen prompts without retraining. By treating natural language as a control layer, the system suggests a future in which spoken or written prompts could direct computational, robotic, or biological systems to desired behaviors. This work provides a concrete step toward this vision of AI-biology partnerships, in which language replaces mathematical objective functions, fixed rules, and domain-specific programming.",
        "arxiv_id": "2509.10660",
        "ARXIVID": "2509.10660",
        "COMMENT": "Matches criterion 3. Explores the use of language prompts for controlling simulated cellular systems, which aligns with embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2509.11719": {
        "authors": [
            "Bingqing Wei",
            "Lianmin Chen",
            "Zhongyu Xia",
            "Yongtao Wang"
        ],
        "title": "HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction",
        "abstract": "arXiv:2509.11719v1 Announce Type: new  Abstract: Multi-agent trajectory prediction in autonomous driving requires a comprehensive understanding of complex social dynamics. Existing methods, however, often struggle to capture the full richness of these dynamics, particularly the co-existence of multi-scale interactions and the diverse behaviors of heterogeneous agents. To address these challenges, this paper introduces HeLoFusion, an efficient and scalable encoder for modeling heterogeneous and multi-scale agent interactions. Instead of relying on global context, HeLoFusion constructs local, multi-scale graphs centered on each agent, allowing it to effectively model both direct pairwise dependencies and complex group-wise interactions (\\textit{e.g.}, platooning vehicles or pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of agent heterogeneity through an aggregation-decomposition message-passing scheme and type-specific feature networks, enabling it to learn nuanced, type-dependent interaction patterns. This locality-focused approach enables a principled representation of multi-level social context, yielding powerful and expressive agent embeddings. On the challenging Waymo Open Motion Dataset, HeLoFusion achieves state-of-the-art performance, setting new benchmarks for key metrics including Soft mAP and minADE. Our work demonstrates that a locality-grounded architecture, which explicitly models multi-scale and heterogeneous interactions, is a highly effective strategy for advancing motion forecasting.",
        "arxiv_id": "2509.11719",
        "ARXIVID": "2509.11719",
        "COMMENT": "Matches criterion 3. Introduces a novel method for multi-agent trajectory prediction in autonomous driving, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11287": {
        "authors": [
            "Yifan Lu",
            "Ziqi Zhang",
            "Chunfeng Yuan",
            "Jun Gao",
            "Congxuan Zhang",
            "Xiaojuan Qi",
            "Bing Li",
            "Weiming Hu"
        ],
        "title": "Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations",
        "abstract": "arXiv:2509.11287v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination problems, where the model-generated responses are inconsistent with the visual inputs. Existing hallucination mitigation methods are mainly based on preference alignment and require external human annotations or auxiliary models for preference data collection, which increase costs and limit sustainable improvement. To tackle these challenges, we propose Autonomous Preference Alignment via Self-Injection (APASI), a novel and generalizable method that mitigates hallucinations without external dependencies. APASI leverages the target LVLM to self-inject hallucinations into a generated response, creating a pair of responses with varying preference levels. During the self-injection process, the dis-preferred response is generated based on three key observations of hallucinations, ensuring it simulates real hallucination patterns. This fidelity offers an accurate learning signal for hallucination mitigation. Moreover, APASI incorporates an iterative alignment training strategy combined with curriculum learning to periodically update the preference data with increasing challenge, enabling stable and continuous enhancement of the LVLM. Extensive experiments across six benchmarks show that APASI not only effectively mitigates hallucinations for three baseline models but also achieves comparable or even superior performance to alignment-based methods with external dependency, thereby demonstrating its effectiveness and generalization capability. The code is available at https://github.com/davidluciolu/APASI.",
        "arxiv_id": "2509.11287",
        "ARXIVID": "2509.11287",
        "COMMENT": "Matches criterion 2 as it addresses hallucination mitigation in large vision-language models, which is relevant to visual and multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2509.12203": {
        "authors": [
            "Zixin Yin",
            "Xili Dai",
            "Duomin Wang",
            "Xianfang Zeng",
            "Lionel M. Ni",
            "Gang Yu",
            "Heung-Yeung Shum"
        ],
        "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence",
        "abstract": "arXiv:2509.12203v1 Announce Type: new  Abstract: The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.",
        "arxiv_id": "2509.12203",
        "ARXIVID": "2509.12203",
        "COMMENT": "Matches criterion 5 as it introduces a novel drag-based editing method for multi-modal diffusion transformers, combining image editing and text guidance.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11772": {
        "authors": [
            "Diogo Mendon\\c{c}a",
            "Tiago Barros",
            "Cristiano Premebida",
            "Urbano J. Nunes"
        ],
        "title": "Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization",
        "abstract": "arXiv:2509.11772v1 Announce Type: new  Abstract: Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to operate reliably in dynamic environments. MOT ensures consistent object identity assignment and precise spatial delineation. Recent advances in foundation models, such as SAM2, have demonstrated strong zero-shot generalization for video segmentation, but their direct application to MOTS (MOT+Segmentation) remains limited by insufficient identity management and memory efficiency. This work introduces Seg2Track-SAM2, a framework that integrates pre-trained object detectors with SAM2 and a novel Seg2Track module to address track initialization, track management, and reinforcement. The proposed approach requires no fine-tuning and remains detector-agnostic. Experimental results on KITTI MOT and KITTI MOTS benchmarks show that Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth overall in both car and pedestrian classes on KITTI MOTS, while establishing a new benchmark in association accuracy (AssA). Furthermore, a sliding-window memory strategy reduces memory usage by up to 75% with negligible performance degradation, supporting deployment under resource constraints. These results confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot tracking, enhanced identity preservation, and efficient memory utilization. The code is available at https://github.com/hcmr-lab/Seg2Track-SAM2",
        "arxiv_id": "2509.11772",
        "ARXIVID": "2509.11772",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on multi-object tracking and segmentation for autonomous systems, leveraging foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11480": {
        "authors": [
            "Amir Taherin",
            "Juyi Lin",
            "Arash Akbari",
            "Arman Akbari",
            "Pu Zhao",
            "Weiwei Chen",
            "David Kaeli",
            "Yanzhi Wang"
        ],
        "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
        "abstract": "arXiv:2509.11480v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
        "arxiv_id": "2509.11480",
        "ARXIVID": "2509.11480",
        "COMMENT": "Matches criteria 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its evaluation of Vision-Language-Action models for robotic control and insights into scaling across hardware platforms.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.10704": {
        "authors": [
            "Xingchen Wan",
            "Han Zhou",
            "Ruoxi Sun",
            "Hootan Nakhost",
            "Ke Jiang",
            "Rajarishi Sinha",
            "Sercan \\\"O. Ar{\\i}k"
        ],
        "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration",
        "abstract": "arXiv:2509.10704v1 Announce Type: new  Abstract: Text-to-image (T2I) models, while offering immense creative potential, are highly reliant on human intervention, posing significant usability challenges that often necessitate manual, iterative prompt engineering over often underspecified prompts. This paper introduces Maestro, a novel self-evolving image generation system that enables T2I models to autonomously self-improve generated images through iterative evolution of prompts, using only an initial prompt. Maestro incorporates two key innovations: 1) self-critique, where specialized multimodal LLM (MLLM) agents act as 'critics' to identify weaknesses in generated images, correct for under-specification, and provide interpretable edit signals, which are then integrated by a 'verifier' agent while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge for head-to-head comparisons between iteratively generated images, eschewing problematic images, and evolving creative prompt candidates that align with user intents. Extensive experiments on complex T2I tasks using black-box models demonstrate that Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods, with effectiveness scaling with more advanced MLLM components. This work presents a robust, interpretable, and effective pathway towards self-improving T2I generation.",
        "arxiv_id": "2509.10704",
        "ARXIVID": "2509.10704",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) and their application in text-to-image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11662": {
        "authors": [
            "Feilong Chen",
            "Yijiang Liu",
            "Yi Huang",
            "Hao Wang",
            "Miren Tian",
            "Ya-Qi Yu",
            "Minghui Liao",
            "Jihao Wu"
        ],
        "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs",
        "abstract": "arXiv:2509.11662v1 Announce Type: new  Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs. Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers, which enables it to process images at their original variable resolutions. This design avoids the degradation caused by fixed-resolution tiling while preserving fine-grained details and global layouts, which is crucial for visually dense content such as complex charts and diagrams. To ensure the smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a distributed multimodal training framework tailored for Ascend NPUs. To maintain training accuracy, we implement equivalent replacements for certain operators. MindVL undergoes a three-phase training process, namely the warm-up phase, multitask training phase, and supervised instruction tuning phase, to gradually enhance its capabilities. This process starts with basic visual and multimodal pre-training, followed by large-scale multiask trainging and instruction tuning. We also adopt multimodal data packaging and hybrid parallelism techniques, which significantly improve end-to-end training speed. To further boost model performance, we specifically introduce test-time resolution search and model weight averaging. Notably, despite using about 1/10 of the training data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL in evaluations of general multimodal understanding and document/table comprehension. Beyond overall scores, MindVL also delivers leading performance in OCR assessments.",
        "arxiv_id": "2509.11662",
        "ARXIVID": "2509.11662",
        "COMMENT": "Matches criteria 2 as it introduces a multimodal large language model and discusses training strategies for vision-language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2509.11548": {
        "authors": [
            "Weiming Li",
            "Yan Shao",
            "Jing Yang",
            "Yujing Lu",
            "Ling Zhong",
            "Yuhan Wang",
            "Manni Duan"
        ],
        "title": "How Auxiliary Reasoning Unleashes GUI Grounding in VLMs",
        "abstract": "arXiv:2509.11548v1 Announce Type: new  Abstract: Graphical user interface (GUI) grounding is a fundamental task for building GUI agents. However, general vision-language models (VLMs) struggle with this task due to a lack of specific optimization. We identify a key gap in this paper: while VLMs exhibit significant latent grounding potential, as demonstrated by their performance measured by Pointing Game, they underperform when tasked with outputting explicit coordinates. To address this discrepancy, and bypass the high data and annotation costs of current fine-tuning approaches, we propose three zero-shot auxiliary reasoning methods. By providing explicit spatial cues such as axes, grids and labeled intersections as part of the input image, these methods enable VLMs to articulate their implicit spatial understanding capabilities. We evaluate these methods on four GUI grounding benchmarks across seven open-source and proprietary VLMs. The evaluation results demonstrate that the proposed methods substantially improve the performance of GUI grounding.",
        "arxiv_id": "2509.11548",
        "ARXIVID": "2509.11548",
        "COMMENT": "Matches criteria 2 as it explores GUI grounding in vision-language models with novel auxiliary reasoning methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.12201": {
        "authors": [
            "Yang Zhou",
            "Yifan Wang",
            "Jianjun Zhou",
            "Wenzheng Chang",
            "Haoyu Guo",
            "Zizun Li",
            "Kaijing Ma",
            "Xinyue Li",
            "Yating Wang",
            "Haoyi Zhu",
            "Mingyu Liu",
            "Dingning Liu",
            "Jiange Yang",
            "Zhoujie Fu",
            "Junyi Chen",
            "Chunhua Shen",
            "Jiangmiao Pang",
            "Kaipeng Zhang",
            "Tong He"
        ],
        "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
        "abstract": "arXiv:2509.12201v1 Announce Type: new  Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.",
        "arxiv_id": "2509.12201",
        "ARXIVID": "2509.12201",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark dataset for 4D world modeling, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11090": {
        "authors": [
            "Chao Chen",
            "Shunyu Yao",
            "Yuanwu He",
            "Tao Feng",
            "Ruojing Song",
            "Yuliang Guo",
            "Xinyu Huang",
            "Chenxu Wu",
            "Ren Liu",
            "Chen Feng"
        ],
        "title": "End-to-End Visual Autonomous Parking via Control-Aided Attention",
        "abstract": "arXiv:2509.11090v1 Announce Type: new  Abstract: Precise parking requires an end-to-end system where perception adaptively provides policy-relevant details-especially in critical areas where fine control decisions are essential. End-to-end learning offers a unified framework by directly mapping sensor inputs to control actions, but existing approaches lack effective synergy between perception and control. We find that transformer-based self-attention, when used alone, tends to produce unstable and temporally inconsistent spatial attention, which undermines the reliability of downstream policy decisions over time. Instead, we propose CAA-Policy, an end-to-end imitation learning system that allows control signal to guide the learning of visual attention via a novel Control-Aided Attention (CAA) mechanism. For the first time, we train such an attention module in a self-supervised manner, using backpropagated gradients from the control outputs instead of from the training loss. This strategy encourages the attention to focus on visual features that induce high variance in action outputs, rather than merely minimizing the training loss-a shift we demonstrate leads to a more robust and generalizable policy. To further enhance stability, CAA-Policy integrates short-horizon waypoint prediction as an auxiliary task, and introduces a separately trained motion prediction module to robustly track the target spot over time. Extensive experiments in the CARLA simulator show that \\titlevariable~consistently surpasses both the end-to-end learning baseline and the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy, robustness, and interpretability. Code is released at https://github.com/Joechencc/CAAPolicy.",
        "arxiv_id": "2509.11090",
        "ARXIVID": "2509.11090",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel end-to-end system for autonomous parking with control-aided attention.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.11880": {
        "authors": [
            "Carlos Celemin",
            "Joseph Brennan",
            "Pierluigi Vito Amadori",
            "Tim Bradley"
        ],
        "title": "Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning",
        "abstract": "arXiv:2509.11880v1 Announce Type: new  Abstract: This paper introduces a novel application of Supervised Contrastive Learning (SupCon) to Imitation Learning (IL), with a focus on learning more effective state representations for agents in video game environments. The goal is to obtain latent representations of the observations that capture better the action-relevant factors, thereby modeling better the cause-effect relationship from the observations that are mapped to the actions performed by the demonstrator, for example, the player jumps whenever an obstacle appears ahead. We propose an approach to integrate the SupCon loss with continuous output spaces, enabling SupCon to operate without constraints regarding the type of actions of the environment. Experiments on the 3D games Astro Bot and Returnal, and multiple 2D Atari games show improved representation quality, faster learning convergence, and better generalization compared to baseline models trained only with supervised action prediction loss functions.",
        "arxiv_id": "2509.11880",
        "ARXIVID": "2509.11880",
        "COMMENT": "This paper matches criterion 3 as it introduces a novel method (Supervised Contrastive Imitation Learning) for learning representations in video game agents, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2509.11866": {
        "authors": [
            "Meng Luo",
            "Shengqiong Wu",
            "Liqiang Jing",
            "Tianjie Ju",
            "Li Zheng",
            "Jinxiang Lai",
            "Tianlong Wu",
            "Xinya Du",
            "Jian Li",
            "Siyuan Yan",
            "Jiebo Luo",
            "William Yang Wang",
            "Hao Fei",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding",
        "abstract": "arXiv:2509.11866v1 Announce Type: new  Abstract: Recent advancements in large video models (LVMs) have significantly enhance video understanding. However, these models continue to suffer from hallucinations, producing content that conflicts with input videos. To address this issue, we propose Dr.V, a hierarchical framework covering perceptive, temporal, and cognitive levels to diagnose video hallucination by fine-grained spatial-temporal grounding. Dr.V comprises of two key components: a benchmark dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes 10k instances drawn from 4,974 videos spanning diverse tasks, each enriched with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in LVMs by systematically applying fine-grained spatial-temporal grounding at the perceptive and temporal levels, followed by cognitive level reasoning. This step-by-step pipeline mirrors human-like video comprehension and effectively identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is effective in diagnosing hallucination while enhancing interpretability and reliability, offering a practical blueprint for robust video understanding in real-world scenarios. All our data and code are available at https://github.com/Eurekaleo/Dr.V.",
        "arxiv_id": "2509.11866",
        "ARXIVID": "2509.11866",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding and hallucination detection in large video models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.11862": {
        "authors": [
            "Haodi Ma",
            "Vyom Pathak",
            "Daisy Zhe Wang"
        ],
        "title": "Bridging Vision Language Models and Symbolic Grounding for Video Question Answering",
        "abstract": "arXiv:2509.11862v1 Announce Type: new  Abstract: Video Question Answering (VQA) requires models to reason over spatial, temporal, and causal cues in videos. Recent vision language models (VLMs) achieve strong results but often rely on shallow correlations, leading to weak temporal grounding and limited interpretability. We study symbolic scene graphs (SGs) as intermediate grounding signals for VQA. SGs provide structured object-relation representations that complement VLMs holistic reasoning. We introduce SG-VLM, a modular framework that integrates frozen VLMs with scene graph grounding via prompting and visual localization. Across three benchmarks (NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM improves causal and temporal reasoning and outperforms prior baselines, though gains over strong VLMs are limited. These findings highlight both the promise and current limitations of symbolic grounding, and offer guidance for future hybrid VLM-symbolic approaches in video understanding.",
        "arxiv_id": "2509.11862",
        "ARXIVID": "2509.11862",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video question answering with symbolic grounding and vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.10995": {
        "authors": [
            "Nisha Pillai",
            "Aditi Virupakshaiah",
            "Harrison W. Smith",
            "Amanda J. Ashworth",
            "Prasanna Gowda",
            "Phillip R. Owens",
            "Adam R. Rivers",
            "Bindu Nanduri",
            "Mahalingam Ramkumar"
        ],
        "title": "Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring",
        "abstract": "arXiv:2509.10995v1 Announce Type: new  Abstract: Animal health monitoring and population management are critical aspects of wildlife conservation and livestock management that increasingly rely on automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV) based systems combined with computer vision offer promising solutions for non-invasive animal monitoring across challenging terrains, limited availability of labeled training data remains an obstacle in developing effective deep learning (DL) models for these applications. Transfer learning has emerged as a potential solution, allowing models trained on large datasets to be adapted for resource-limited scenarios such as those with limited data. However, the vast landscape of pre-trained neural network architectures makes it challenging to select optimal models, particularly for researchers new to the field. In this paper, we propose a reinforcement learning (RL)-based transfer learning framework that employs an upper confidence bound (UCB) algorithm to automatically select the most suitable pre-trained model for animal detection tasks. Our approach systematically evaluates and ranks candidate models based on their performance, streamlining the model selection process. Experimental results demonstrate that our framework achieves a higher detection rate while requiring significantly less computational time compared to traditional methods.",
        "arxiv_id": "2509.10995",
        "ARXIVID": "2509.10995",
        "COMMENT": "Matches criterion 3 as it introduces a novel RL-based transfer learning framework for embodied AI tasks, specifically animal detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.12187": {
        "authors": [
            "Johanna Karras",
            "Yingwei Li",
            "Yasamin Jafarian",
            "Ira Kemelmacher-Shlizerman"
        ],
        "title": "HoloGarment: 360{\\deg} Novel View Synthesis of In-the-Wild Garments",
        "abstract": "arXiv:2509.12187v1 Announce Type: new  Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due significant occlusions, complex human poses, and cloth deformations. Prior methods rely on synthetic 3D training data consisting of mostly unoccluded and static objects, leading to poor generalization on real-world clothing. In this paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3 images or a continuous video of a person wearing a garment and generates 360{\\deg} novel views of the garment in a canonical pose. Our key insight is to bridge the domain gap between real and synthetic data with a novel implicit training paradigm leveraging a combination of large-scale real video data and small-scale synthetic 3D data to optimize a shared garment embedding space. During inference, the shared embedding space further enables dynamic video-to-360{\\deg} NVS through the construction of a garment \"atlas\" representation by finetuning a garment embedding on a specific real-world video. The atlas captures garment-specific geometry and texture across all viewpoints, independent of body pose or motion. Extensive experiments show that HoloGarment achieves state-of-the-art performance on NVS of in-the-wild garments from images and videos. Notably, our method robustly handles challenging real-world artifacts -- such as wrinkling, pose variation, and occlusion -- while maintaining photorealism, view consistency, fine texture details, and accurate geometry. Visit our project page for additional results: https://johannakarras.github.io/HoloGarment",
        "arxiv_id": "2509.12187",
        "ARXIVID": "2509.12187",
        "COMMENT": "Matches criterion 6 as it focuses on novel view synthesis of garments from videos, which is a video understanding task.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.11959": {
        "authors": [
            "Ao Liang",
            "Youquan Liu",
            "Yu Yang",
            "Dongyue Lu",
            "Linfeng Li",
            "Lingdong Kong",
            "Huaici Zhao",
            "Wei Tsang Ooi"
        ],
        "title": "Learning to Generate 4D LiDAR Sequences",
        "abstract": "arXiv:2509.11959v1 Announce Type: new  Abstract: While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.",
        "arxiv_id": "2509.11959",
        "ARXIVID": "2509.11959",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for generating 4D LiDAR sequences, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.12155": {
        "authors": [
            "M. Bolhassani",
            "B. Veasey",
            "E. Daugherty",
            "S. Keltner",
            "N. Kumar",
            "N. Dunlap",
            "A. Amini"
        ],
        "title": "LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury",
        "abstract": "arXiv:2509.12155v1 Announce Type: new  Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of this approach, we compare LoRA with traditional full fine-tuning and inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3 and 75 mm3), centered at the treatment isocenter, in addition to different adaptation techniques for adapting the 2D LVMs for 3D data were used to determine the sensitivity of the models to spatial context. Experimental results show that LoRA achieves comparable or superior performance to traditional fine-tuning while significantly reducing computational costs and training times by requiring fewer trainable parameters.",
        "arxiv_id": "2509.12155",
        "ARXIVID": "2509.12155",
        "COMMENT": "Matches criteria 4 (Vision Foundation Models and Their Applications) due to its use of large vision models (DinoV2 and SwinV2) for medical imaging applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.12146": {
        "authors": [
            "Nishank Singla",
            "Krisztian Koos",
            "Farzin Haddadpour",
            "Amin Honarmandi Shandiz",
            "Lovish Chum",
            "Xiaojian Xu",
            "Qing Jin",
            "Erhan Bas"
        ],
        "title": "Multi Anatomy X-Ray Foundation Model",
        "abstract": "arXiv:2509.12146v1 Announce Type: new  Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.",
        "arxiv_id": "2509.12146",
        "ARXIVID": "2509.12146",
        "COMMENT": "Matches criterion 4 as it focuses on a foundation model for X-ray imaging, which is a vision foundation model with real-world applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.11301": {
        "authors": [
            "Matthias W\\\"uest",
            "Francis Engelmann",
            "Ondrej Miksik",
            "Marc Pollefeys",
            "Daniel Barath"
        ],
        "title": "UnLoc: Leveraging Depth Uncertainties for Floorplan Localization",
        "abstract": "arXiv:2509.11301v1 Announce Type: new  Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera localization within floorplans. Floorplan data is readily available, long-term persistent, and robust to changes in visual appearance. We address key limitations of recent methods, such as the lack of uncertainty modeling in depth predictions and the necessity for custom depth networks trained for each environment. We introduce a novel probabilistic model that incorporates uncertainty estimation, modeling depth predictions as explicit probability distributions. By leveraging off-the-shelf pre-trained monocular depth models, we eliminate the need to rely on per-environment-trained depth networks, enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale synthetic and real-world datasets, demonstrating significant improvements over existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$ times higher localization recall on long sequences (100 frames) and $16.7$ times higher on short ones (15 frames) than the state of the art on the challenging LaMAR HGE dataset.",
        "arxiv_id": "2509.11301",
        "ARXIVID": "2509.11301",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and localization within floorplans, relevant to spatial intelligence for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.11394": {
        "authors": [
            "Syed Talal Wasim",
            "Hamid Suleman",
            "Olga Zatsarynna",
            "Muzammal Naseer",
            "Juergen Gall"
        ],
        "title": "MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation",
        "abstract": "arXiv:2509.11394v1 Announce Type: new  Abstract: We present MixANT, a novel architecture for stochastic long-term dense anticipation of human activities. While recent State Space Models (SSMs) like Mamba have shown promise through input-dependent selectivity on three key parameters, the critical forget-gate ($\\textbf{A}$ matrix) controlling temporal memory remains static. We address this limitation by introducing a mixture of experts approach that dynamically selects contextually relevant $\\textbf{A}$ matrices based on input features, enhancing representational capacity without sacrificing computational efficiency. Extensive experiments on the 50Salads, Breakfast, and Assembly101 datasets demonstrate that MixANT consistently outperforms state-of-the-art methods across all evaluation settings. Our results highlight the importance of input-dependent forget-gate mechanisms for reliable prediction of human behavior in diverse real-world scenarios.",
        "arxiv_id": "2509.11394",
        "ARXIVID": "2509.11394",
        "COMMENT": "Matches criterion 3 as it introduces a novel architecture for long-term dense anticipation in human activity prediction, relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.11247": {
        "authors": [
            "Robert Long",
            "Rongxin Jiang",
            "Mingrui Yan"
        ],
        "title": "Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States",
        "abstract": "arXiv:2509.11247v1 Announce Type: new  Abstract: Person Re-Identification (ReID) has several challenges in real-world surveillance systems due to clothing changes (CCReID) and the need for maintaining continual learning (LReID). Previous existing methods either develop models specifically for one application, which is mostly a same-cloth (SC) setting or treat CCReID as its own separate sub-problem. In this work, we will introduce the LReID-Hybrid task with the goal of developing a model to achieve both SC and CC while learning in a continual setting. Mismatched representations and forgetting from one task to the next are significant issues, we address this with CMLReID, a CLIP-based framework composed of two novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive prompts, and also incorporates context to align richly multi-grained visual cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection (AKFP) which produces robust SC/CC prototypes through the use of a dual-path learner that aligns features with our Clothing-State-Aware Projection Loss. Experiments performed on a wide range of datasets and illustrate that CMLReID outperforms all state-of-the-art methods with strong robustness and generalization despite clothing variations and a sophisticated process of sequential learning.",
        "arxiv_id": "2509.11247",
        "ARXIVID": "2509.11247",
        "COMMENT": "Matches criteria 2 as it explores multimodal learning with a focus on person re-identification using vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.11102": {
        "authors": [
            "Nhi Kieu",
            "Kien Nguyen",
            "Arnold Wiliem",
            "Clinton Fookes",
            "Sridha Sridharan"
        ],
        "title": "Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation",
        "abstract": "arXiv:2509.11102v1 Announce Type: new  Abstract: Multimodal learning has shown significant performance boost compared to ordinary unimodal models across various domains. However, in real-world scenarios, multimodal signals are susceptible to missing because of sensor failures and adverse weather conditions, which drastically deteriorates models' operation and performance. Generative models such as AutoEncoder (AE) and Generative Adversarial Network (GAN) are intuitive solutions aiming to reconstruct missing modality from available ones. Yet, their efficacy in remote sensing semantic segmentation remains underexplored. In this paper, we first examine the limitations of existing generative approaches in handling the heterogeneity of multimodal remote sensing data. They inadequately capture semantic context in complex scenes with large intra-class and small inter-class variation. In addition, traditional generative models are susceptible to heavy dependence on the dominant modality, introducing bias that affects model robustness under missing modality conditions. To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent bias by encouraging consistency across modalities and tasks. Our method, GEMMNet, outperforms both generative baselines AE, cGAN (conditional GAN), and state-of-the-art non-generative approaches - mmformer and shaspec - on two challenging semantic segmentation remote sensing datasets (Vaihingen and Potsdam). Source code is made available.",
        "arxiv_id": "2509.11102",
        "ARXIVID": "2509.11102",
        "COMMENT": "Matches criteria 5 as it explores multimodal learning and generative modeling for handling missing modalities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.12105": {
        "authors": [
            "Bernardo Forni",
            "Gabriele Lombardi",
            "Federico Pozzi",
            "Mirco Planamente"
        ],
        "title": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation",
        "abstract": "arXiv:2509.12105v1 Announce Type: new  Abstract: Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2's pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at https://github.com/fornib/FS-SAM2",
        "arxiv_id": "2509.12105",
        "ARXIVID": "2509.12105",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it adapts the Segment Anything Model 2 for few-shot semantic segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.10710": {
        "authors": [
            "Sven Schreiber",
            "Noha Sarhan",
            "Simone Frintrop",
            "Christian Wilms"
        ],
        "title": "SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition",
        "abstract": "arXiv:2509.10710v1 Announce Type: new  Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB data or signer pose information. However, combining these modalities often results in the loss of crucial details, such as hand shape and orientation, due to imprecise representations like bounding boxes. Therefore, we propose the ISLR system SegSLR, which combines RGB and pose information through promptable zero-shot video segmentation. Given the rough localization of the hands and the signer's body from pose information, we segment the respective parts through the video to maintain all relevant shape information. Subsequently, the segmentations focus the processing of the RGB data on the most relevant body parts for ISLR. This effectively combines RGB and pose information. Our evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR outperforms state-of-the-art methods. Furthermore, ablation studies indicate that SegSLR strongly benefits from focusing on the signer's body and hands, justifying our design choices.",
        "arxiv_id": "2509.10710",
        "ARXIVID": "2509.10710",
        "COMMENT": "Matches criterion 6 as it focuses on video segmentation for sign language recognition, a video understanding task.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.11922": {
        "authors": [
            "Xilei Dai",
            "Ruotian Chen",
            "Songze Guan",
            "Wen-Tai Li",
            "Chau Yuen"
        ],
        "title": "BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning",
        "abstract": "arXiv:2509.11922v1 Announce Type: new  Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy management. However, there is a lack of flexible framework to implement RL across various control problems in building energy management. To address this gap, we propose BuildingGym, an open-source tool designed as a research-friendly and flexible framework for training RL control strategies for common challenges in building energy management. BuildingGym integrates EnergyPlus as its core simulator, making it suitable for both system-level and room-level control. Additionally, BuildingGym is able to accept external signals as control inputs instead of taking the building as a stand-alone entity. This feature makes BuildingGym applicable for more flexible environments, e.g. smart grid and EVs community. The tool provides several built-in RL algorithms for control strategy training, simplifying the process for building managers to obtain optimal control strategies. Users can achieve this by following a few straightforward steps to configure BuildingGym for optimization control for common problems in the building energy management field. Moreover, AI specialists can easily implement and test state-of-the-art control algorithms within the platform. BuildingGym bridges the gap between building managers and AI specialists by allowing for the easy configuration and replacement of RL algorithms, simulators, and control environments or problems. With BuildingGym, we efficiently set up training tasks for cooling load management, targeting both constant and dynamic cooling load management. The built-in algorithms demonstrated strong performance across both tasks, highlighting the effectiveness of BuildingGym in optimizing cooling strategies.",
        "arxiv_id": "2509.11922",
        "ARXIVID": "2509.11922",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and framework for reinforcement learning in building energy management, relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.11948": {
        "authors": [
            "Mahmoud Z. A. Wahba",
            "Sara Baldoni",
            "Federica Battisti"
        ],
        "title": "Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360{\\deg} Videos",
        "abstract": "arXiv:2509.11948v1 Announce Type: new  Abstract: The recent success of immersive applications is pushing the research community to define new approaches to process 360{\\deg} images and videos and optimize their transmission. Among these, saliency estimation provides a powerful tool that can be used to identify visually relevant areas and, consequently, adapt processing algorithms. Although saliency estimation has been widely investigated for 2D content, very few algorithms have been proposed for 360{\\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN, a saliency detection model for 360{\\deg} videos that leverages a Generative Adversarial Network with spherical convolutions. Extensive experiments were conducted using a public 360{\\deg} video saliency dataset, and the results demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately predicting saliency maps.",
        "arxiv_id": "2509.11948",
        "ARXIVID": "2509.11948",
        "COMMENT": "Matches criterion 6 as it focuses on saliency estimation in 360-degree videos, which is a video understanding task.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.12197": {
        "authors": [
            "Salma Galaaoui",
            "Eduardo Valle",
            "David Picard",
            "Nermin Samet"
        ],
        "title": "3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review",
        "abstract": "arXiv:2509.12197v1 Announce Type: new  Abstract: In this paper, we present a comprehensive review of 3D human pose estimation and human mesh recovery from in-the-wild LiDAR point clouds. We compare existing approaches across several key dimensions, and propose a structured taxonomy to classify these methods. Following this taxonomy, we analyze each method's strengths, limitations, and design choices. In addition, (i) we perform a quantitative comparison of the three most widely used datasets, detailing their characteristics; (ii) we compile unified definitions of all evaluation metrics; and (iii) we establish benchmark tables for both tasks on these datasets to enable fair comparisons and promote progress in the field. We also outline open challenges and research directions critical for advancing LiDAR-based 3D human understanding. Moreover, we maintain an accompanying webpage that organizes papers according to our taxonomy and continuously update it with new studies: https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR",
        "arxiv_id": "2509.12197",
        "ARXIVID": "2509.12197",
        "COMMENT": "Matches criteria 7 as it is a survey paper on 3D human pose and shape estimation from LiDAR point clouds.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.10620": {
        "authors": [
            "Emily Kaczmarek",
            "Justin Szeto",
            "Brennan Nichyporuk",
            "Tal Arbel"
        ],
        "title": "Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses",
        "abstract": "arXiv:2509.10620v1 Announce Type: new  Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly acquired in clinical settings to monitor a wide range of neurological conditions, including neurodegenerative disorders and stroke. While deep learning models have shown promising results analyzing 3D MRI across a number of brain imaging tasks, most are highly tailored for specific tasks with limited labeled data, and are not able to generalize across tasks and/or populations. The development of self-supervised learning (SSL) has enabled the creation of large medical foundation models that leverage diverse, unlabeled datasets ranging from healthy to diseased data, showing significant success in 2D medical imaging applications. However, even the very few foundation models for 3D brain MRI that have been developed remain limited in resolution, scope, or accessibility. In this work, we present a general, high-resolution SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on 18,759 patients (44,958 scans) from 11 publicly available datasets spanning diverse neurological diseases. We compare our model to Masked Autoencoders (MAE), as well as two supervised baselines, on four diverse downstream prediction tasks in both in-distribution and out-of-distribution settings. Our fine-tuned SimCLR model outperforms all other models across all tasks. Notably, our model still achieves superior performance when fine-tuned using only 20% of labeled training samples for predicting Alzheimer's disease. We use publicly available code and data, and release our trained model at https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly applicable and accessible foundation model for clinical brain MRI analysis.",
        "arxiv_id": "2509.10620",
        "ARXIVID": "2509.10620",
        "COMMENT": "This paper does not match any specific criteria but is tangentially related to vision foundation models (criterion 4) as it discusses a foundation model for 3D brain MRI analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11068": {
        "authors": [
            "Zan-Kai Chong",
            "Hiroyuki Ohsaki",
            "Bryan Ng"
        ],
        "title": "Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability",
        "abstract": "arXiv:2509.11068v1 Announce Type: new  Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic, multi-agent systems. This introduces a fundamental challenge in establishing computational trust, specifically how one agent can verify that another's output was genuinely produced by a claimed LLM, and not falsified or generated by a cheaper or inferior model. To address this challenge, this paper proposes a verification framework that achieves tractable asymmetric effort, where the cost to verify a computation is substantially lower than the cost to perform it. Our approach is built upon the principle of deterministic replicability, a property inherent to autoregressive models that strictly necessitates a computationally homogeneous environment where all agents operate on identical hardware and software stacks. Within this defined context, our framework enables multiple validators to probabilistically audit small, random segments of an LLM's output and it distributes the verification workload effectively. The simulations demonstrated that targeted verification can be over 12 times faster than full regeneration, with tunable parameters to adjust the detection probability. By establishing a tractable mechanism for auditable LLM systems, our work offers a foundational layer for responsible AI and serves as a cornerstone for future research into the more complex, heterogeneous multi-agent systems.",
        "arxiv_id": "2509.11068",
        "ARXIVID": "2509.11068",
        "COMMENT": "Does not match any specific criteria but is relevant to general interests in LLM verification and trust.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11624": {
        "authors": [
            "Wending Liu",
            "Siyun Liang",
            "Huy H. Nguyen",
            "Isao Echizen"
        ],
        "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting",
        "abstract": "arXiv:2509.11624v1 Announce Type: new  Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.",
        "arxiv_id": "2509.11624",
        "ARXIVID": "2509.11624",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D deepfake generation, which is not directly related to the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11035": {
        "authors": [
            "Yu Cui",
            "Hang Fu",
            "Haibin Zhang",
            "Licheng Wang",
            "Cong Zuo"
        ],
        "title": "Free-MAD: Consensus-Free Multi-Agent Debate",
        "abstract": "arXiv:2509.11035v1 Announce Type: new  Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning capabilities of large language models (LLMs). Existing MAD methods rely on multiple rounds of interaction among agents to reach consensus, and the final output is selected by majority voting in the last round. However, this consensus-based design faces several limitations. First, multiple rounds of communication increases token overhead and limits scalability. Second, due to the inherent conformity of LLMs, agents that initially produce correct responses may be influenced by incorrect ones during the debate process, causing error propagation. Third, majority voting introduces randomness and unfairness in the decision-making phase, and can degrade the reasoning performance.   To address these issues, we propose \\textsc{Free-MAD}, a novel MAD framework that eliminates the need for consensus among agents. \\textsc{Free-MAD} introduces a novel score-based decision mechanism that evaluates the entire debate trajectory rather than relying on the last round only. This mechanism tracks how each agent's reasoning evolves, enabling more accurate and fair outcomes. In addition, \\textsc{Free-MAD} reconstructs the debate phase by introducing anti-conformity, a mechanism that enables agents to mitigate excessive influence from the majority. Experiments on eight benchmark datasets demonstrate that \\textsc{Free-MAD} significantly improves reasoning performance while requiring only a single-round debate and thus reducing token costs. We also show that compared to existing MAD approaches, \\textsc{Free-MAD} exhibits improved robustness in real-world attack scenarios.",
        "arxiv_id": "2509.11035",
        "ARXIVID": "2509.11035",
        "COMMENT": "Does not match any specific criteria. Focuses on multi-agent debate frameworks for reasoning in LLMs, which is not directly related to the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12052": {
        "authors": [
            "Yuchen Deng",
            "Xiuyang Wu",
            "Hai-Tao Zheng",
            "Suiyang Zhang",
            "Yi He",
            "Yuxing Han"
        ],
        "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective",
        "abstract": "arXiv:2509.12052v1 Announce Type: new  Abstract: Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate \"Divide and Conquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.",
        "arxiv_id": "2509.12052",
        "ARXIVID": "2509.12052",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and talking-head animation, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11642": {
        "authors": [
            "Qiyuan Guan",
            "Qianfeng Yang",
            "Xiang Chen",
            "Tianyu Song",
            "Guiyue Jin",
            "Jiyu Jin"
        ],
        "title": "WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration",
        "abstract": "arXiv:2509.11642v1 Announce Type: new  Abstract: Existing all-in-one image restoration approaches, which aim to handle multiple weather degradations within a single framework, are predominantly trained and evaluated using mixed single-weather synthetic datasets. However, these datasets often differ significantly in resolution, style, and domain characteristics, leading to substantial domain gaps that hinder the development and fair evaluation of unified models. Furthermore, the lack of a large-scale, real-world all-in-one weather restoration dataset remains a critical bottleneck in advancing this field. To address these limitations, we present a real-world all-in-one adverse weather image restoration benchmark dataset, which contains image pairs captured under various weather conditions, including rain, snow, and haze, as well as diverse outdoor scenes and illumination settings. The resulting dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of task-specific, task-general, and all-in-one restoration methods on our dataset. Our dataset offers a valuable foundation for advancing robust and practical all-in-one image restoration in real-world scenarios. The dataset has been publicly released and is available at https://github.com/guanqiyuan/WeatherBench.",
        "arxiv_id": "2509.11642",
        "ARXIVID": "2509.11642",
        "COMMENT": "Does not match any specific criteria but introduces a new real-world dataset for adverse weather image restoration, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.12024": {
        "authors": [
            "Zixuan Fu",
            "Yan Ren",
            "Finn Carter",
            "Chenyue Wen",
            "Le Ku",
            "Daheng Yu",
            "Emily Davis",
            "Bo Zhang"
        ],
        "title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness",
        "abstract": "arXiv:2509.12024v1 Announce Type: new  Abstract: Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \\textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \\emph{adversarial independence} problem, theoretically guaranteeing that the model's outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models.",
        "arxiv_id": "2509.12024",
        "ARXIVID": "2509.12024",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and security in diffusion models, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11328": {
        "authors": [
            "Mingyuan Meng"
        ],
        "title": "Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency",
        "abstract": "arXiv:2509.11328v1 Announce Type: new  Abstract: Medical Image Computing (MIC) is a broad research topic covering both pixel-wise (e.g., segmentation, registration) and image-wise (e.g., classification, regression) vision tasks. Effective analysis demands models that capture both global long-range context and local subtle visual characteristics, necessitating fine-grained long-range visual dependency modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by intrinsic locality, transformers excel at long-range modeling; however, due to the high computational loads of self-attention, transformers typically cannot process high-resolution features (e.g., full-scale image features before downsampling or patch embedding) and thus face difficulties in modeling fine-grained dependency among subtle medical image details. Concurrently, Multi-layer Perceptron (MLP)-based visual models are recognized as computation/memory-efficient alternatives in modeling long-range visual dependency but have yet to be widely investigated in the MIC community. This doctoral research advances deep learning-based MIC by investigating effective long-range visual dependency modeling. It first presents innovative use of transformers for both pixel- and image-wise medical vision tasks. The focus then shifts to MLPs, pioneeringly developing MLP-based visual models to capture fine-grained long-range visual dependency in medical images. Extensive experiments confirm the critical role of long-range dependency modeling in MIC and reveal a key finding: MLPs provide feasibility in modeling finer-grained long-range dependency among higher-resolution medical features containing enriched anatomical/pathological details. This finding establishes MLPs as a superior paradigm over transformers/CNNs, consistently enhancing performance across various medical vision tasks and paving the way for next-generation medical vision backbones.",
        "arxiv_id": "2509.11328",
        "ARXIVID": "2509.11328",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.11311": {
        "authors": [
            "Bingchen Wang",
            "Zi-Yu Khoo",
            "Bryan Kian Hsiang Low"
        ],
        "title": "Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble",
        "abstract": "arXiv:2509.11311v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated promise in emulating human-like responses across a wide range of tasks. In this paper, we propose a novel alignment framework that treats LLMs as agent proxies for human survey respondents, affording a cost-effective and steerable solution to two pressing challenges in the social sciences: the rising cost of survey deployment and the growing demographic imbalance in survey response data. Drawing inspiration from the theory of revealed preference, we formulate alignment as a two-stage problem: constructing diverse agent personas called endowments that simulate plausible respondent profiles, and selecting a representative subset to approximate a ground-truth population based on observed data. To implement the paradigm, we introduce P2P, a system that steers LLM agents toward representative behavioral patterns using structured prompt engineering, entropy-based sampling, and regression-based selection. Unlike personalization-heavy approaches, our alignment approach is demographic-agnostic and relies only on aggregate survey results, offering better generalizability and parsimony. Beyond improving data efficiency in social science research, our framework offers a testbed for studying the operationalization of pluralistic alignment. We demonstrate the efficacy of our approach on real-world opinion survey datasets, showing that our aligned agent populations can reproduce aggregate response patterns with high fidelity and exhibit substantial response diversity, even without demographic conditioning.",
        "arxiv_id": "2509.11311",
        "ARXIVID": "2509.11311",
        "COMMENT": "This paper does not match any specific criteria but is tangentially related to large language models and their applications, which may be of general interest to your friend.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11220": {
        "authors": [
            "Gao Yu Lee",
            "Tanmoy Dam",
            "Md Meftahul Ferdaus",
            "Daniel Puiu Poenar",
            "Vu N. Duong"
        ],
        "title": "ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification",
        "abstract": "arXiv:2509.11220v1 Announce Type: new  Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a few data samples, has demonstrated promising and superior performances to ordinary CNN methods. While Bayesian based estimation approaches using Kullback-Leibler (KL) divergence have shown improvements, they remain vulnerable to adversarial attacks and natural noises. We introduce ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation Network that significantly advances the state-of-the-art in FSL robustness and performance. Our approach implements an adversarially and naturally robust Hellinger distance-based feature class aggregation scheme, demonstrating resilience to adversarial perturbations up to $\\epsilon=0.30$ and Gaussian noise up to $\\sigma=0.30$. The network achieves substantial improvements across benchmark datasets, including gains of 1.20\\% and 1.40\\% for 1-shot and 5-shot scenarios on miniImageNet respectively. We introduce a novel Hellinger Similarity contrastive loss function that generalizes cosine similarity contrastive loss for variational few-shot inference scenarios. Our approach also achieves superior image reconstruction quality with a FID score of 2.75, outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive experiments conducted on four few-shot benchmarked datasets verify that ANROT-HELANet's combination of Hellinger distance-based feature aggregation, attention mechanisms, and our novel loss function establishes new state-of-the-art performance while maintaining robustness against both adversarial and natural perturbations. Our code repository will be available at https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.",
        "arxiv_id": "2509.11220",
        "ARXIVID": "2509.11220",
        "COMMENT": "Does not match any specific criteria but is relevant to general machine learning interests, particularly few-shot learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11459": {
        "authors": [
            "Chen Jiang",
            "Kofi Osei",
            "Sai Deepthi Yeddula",
            "Dongji Feng",
            "Wei-Shinn Ku"
        ],
        "title": "Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction",
        "abstract": "arXiv:2509.11459v1 Announce Type: new  Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster management, and sustainable strategies. However, predicting rainfall has been challenging due to the complexity of climate systems and the heterogeneous nature of multi-source observational data, including radar, satellite imagery, and surface-level measurements. The multi-source data vary in spatial and temporal resolution, and they carry domain-specific features, making it challenging for effective integration in conventional deep learning models. Previous research has explored various machine learning techniques for weather prediction; however, most struggle with the integration of data with heterogeneous modalities. To address these limitations, we propose an Adaptive Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each expert within the model specializes in a specific modality or spatio-temporal pattern. We also incorporated a dynamic router that learns to assign inputs to the most relevant experts. Our results show that this modular design enhances predictive accuracy and interpretability. In addition to the modeling framework, we introduced an interactive web-based visualization tool that enables users to intuitively explore historical weather patterns over time and space. The tool was designed to support decision-making for stakeholders in climate-sensitive sectors. We evaluated our approach using a curated multimodal climate dataset capturing real-world conditions during Hurricane Ian in 2022. The benchmark results show that the Adaptive MoE significantly outperformed all the baselines.",
        "arxiv_id": "2509.11459",
        "ARXIVID": "2509.11459",
        "COMMENT": "Does not match any specific criteria but is relevant to general machine learning interests, particularly multimodal data integration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11638": {
        "authors": [
            "Yongzhe Lyu",
            "Yu Wu",
            "Yutian Lin",
            "Bo Du"
        ],
        "title": "IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed",
        "abstract": "arXiv:2509.11638v1 Announce Type: new  Abstract: Diffusion models have shown promising results in free-form inpainting. Recent studies based on refined diffusion samplers or novel architectural designs led to realistic results and high data consistency. However, random initialization seed (noise) adopted in vanilla diffusion process may introduce mismatched semantic information in masked regions, leading to biased inpainting results, e.g., low consistency and low coherence with the other unmasked area. To address this issue, we propose the Initial Seed refined Diffusion Model (IS-Diff), a completely training-free approach incorporating distributional harmonious seeds to produce harmonious results. Specifically, IS-Diff employs initial seeds sampled from unmasked areas to imitate the masked data distribution, thereby setting a promising direction for the diffusion procedure. Moreover, a dynamic selective refinement mechanism is proposed to detect severe unharmonious inpaintings in intermediate latent and adjust the strength of our initialization prior dynamically. We validate our method on both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet, and Places2 datasets, demonstrating its effectiveness across all metrics compared to state-of-the-art inpainting methods.",
        "arxiv_id": "2509.11638",
        "ARXIVID": "2509.11638",
        "COMMENT": "Does not match any specific criteria but is relevant to general computer vision interests, particularly inpainting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11526": {
        "authors": [
            "Wenhao Tang",
            "Sheng Huang",
            "Heng Fang",
            "Fengtao Zhou",
            "Bo Liu",
            "Qingshan Liu"
        ],
        "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis",
        "abstract": "arXiv:2509.11526v1 Announce Type: new  Abstract: Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only a small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure with a consistency constraint to explore the hard instances. Using a class-aware instance probability, MHIM-MIL employs a momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing a global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
        "arxiv_id": "2509.11526",
        "ARXIVID": "2509.11526",
        "COMMENT": "Does not match any specific criteria but is relevant to general machine learning and computer vision interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11344": {
        "authors": [
            "Huaiyuan Qin",
            "Muli Yang",
            "Siyuan Hu",
            "Peng Hu",
            "Yu Zhang",
            "Chen Gong",
            "Hongyuan Zhu"
        ],
        "title": "Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning",
        "abstract": "arXiv:2509.11344v1 Announce Type: new  Abstract: Self-supervised learning (SSL) conventionally relies on the instance consistency paradigm, assuming that different views of the same image can be treated as positive pairs. However, this assumption breaks down for non-iconic data, where different views may contain distinct objects or semantic information. In this paper, we investigate the effectiveness of SSL when instance consistency is not guaranteed. Through extensive ablation studies, we demonstrate that SSL can still learn meaningful representations even when positive pairs lack strict instance consistency. Furthermore, our analysis further reveals that increasing view diversity, by enforcing zero overlapping or using smaller crop scales, can enhance downstream performance on classification and dense prediction tasks. However, excessive diversity is found to reduce effectiveness, suggesting an optimal range for view diversity. To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to measure mutual information between views, finding that moderate EMD values correlate with improved SSL learning, providing insights for future SSL framework design. We validate our findings across a range of settings, highlighting their robustness and applicability on diverse data sources.",
        "arxiv_id": "2509.11344",
        "ARXIVID": "2509.11344",
        "COMMENT": "Does not match any specific criteria but is related to self-supervised learning and representation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11885": {
        "authors": [
            "Francis Xiatian Zhang",
            "Emile Mackute",
            "Mohammadreza Kasaei",
            "Kevin Dhaliwal",
            "Robert Thomson",
            "Mohsen Khadem"
        ],
        "title": "BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation",
        "abstract": "arXiv:2509.11885v1 Announce Type: new  Abstract: Monocular depth estimation in bronchoscopy can significantly improve real-time navigation accuracy and enhance the safety of interventions in complex, branching airways. Recent advances in depth foundation models have shown promise for endoscopic scenarios, yet these models often lack anatomical awareness in bronchoscopy, overfitting to local textures rather than capturing the global airway structure, particularly under ambiguous depth cues and poor lighting. To address this, we propose Brea-Depth, a novel framework that integrates airway-specific geometric priors into foundation model adaptation for bronchoscopic depth estimation. Our method introduces a depth-aware CycleGAN, refining the translation between real bronchoscopic images and airway geometries from anatomical data, effectively bridging the domain gap. In addition, we introduce an airway structure awareness loss to enforce depth consistency within the airway lumen while preserving smooth transitions and structural integrity. By incorporating anatomical priors, Brea-Depth enhances model generalization and yields more robust, accurate 3D airway reconstructions. To assess anatomical realism, we introduce Airway Depth Structure Evaluation, a new metric for structural consistency. We validate BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic dataset, where it outperforms existing methods in anatomical depth preservation.",
        "arxiv_id": "2509.11885",
        "ARXIVID": "2509.11885",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11711": {
        "authors": [
            "Zahra Babaiee",
            "Peyman M. Kiassari",
            "Daniela Rus",
            "Radu Grosu"
        ],
        "title": "The Quest for Universal Master Key Filters in DS-CNNs",
        "abstract": "arXiv:2509.11711v1 Announce Type: new  Abstract: A recent study has proposed the \"Master Key Filters Hypothesis\" for convolutional neural network filters. This paper extends this hypothesis by radically constraining its scope to a single set of just 8 universal filters that depthwise separable convolutional networks inherently converge to. While conventional DS-CNNs employ thousands of distinct trained filters, our analysis reveals these filters are predominantly linear shifts (ax+b) of our discovered universal set. Through systematic unsupervised search, we extracted these fundamental patterns across different architectures and datasets. Remarkably, networks initialized with these 8 unique frozen filters achieve over 80% ImageNet accuracy, and even outperform models with thousands of trainable parameters when applied to smaller datasets. The identified master key filters closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives, structures that are not only fundamental to classical image processing but also strikingly similar to receptive fields in mammalian visual systems. Our findings provide compelling evidence that depthwise convolutional layers naturally gravitate toward this fundamental set of spatial operators regardless of task or architecture. This work offers new insights for understanding generalization and transfer learning through the universal language of these master key filters.",
        "arxiv_id": "2509.11711",
        "ARXIVID": "2509.11711",
        "COMMENT": "Does not match any specific criteria but provides insights into generalization and transfer learning in CNNs, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11178": {
        "authors": [
            "Chengde Lin",
            "Xuezhu Gong",
            "Shuxue Ding",
            "Mingzhe Yang",
            "Xijun Lu",
            "Chengjun Mo"
        ],
        "title": "StegOT: Trade-offs in Steganography via Optimal Transport",
        "abstract": "arXiv:2509.11178v1 Announce Type: new  Abstract: Image hiding is often referred to as steganography, which aims to hide a secret image in a cover image of the same resolution. Many steganography models are based on genera-tive adversarial networks (GANs) and variational autoencoders (VAEs). However, most existing models suffer from mode collapse. Mode collapse will lead to an information imbalance between the cover and secret images in the stego image and further affect the subsequent extraction. To address these challenges, this paper proposes StegOT, an autoencoder-based steganography model incorporating optimal transport theory. We designed the multiple channel optimal transport (MCOT) module to transform the feature distribution, which exhibits multiple peaks, into a single peak to achieve the trade-off of information. Experiments demonstrate that we not only achieve a trade-off between the cover and secret images but also enhance the quality of both the stego and recovery images. The source code will be released on https://github.com/Rss1124/StegOT.",
        "arxiv_id": "2509.11178",
        "ARXIVID": "2509.11178",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling and image hiding, which are tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11385": {
        "authors": [
            "Akhil Padmanabha",
            "Arpit Agarwal",
            "Catherine Li",
            "Austin Williams",
            "Dinesh K. Patel",
            "Sankalp Chopkar",
            "Achu Wilson",
            "Ahmet Ozkan",
            "Wenzhen Yuan",
            "Sonal Choudhary",
            "Arash Mostaghimi",
            "Zackory Erickson",
            "Carmel Majidi"
        ],
        "title": "In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing",
        "abstract": "arXiv:2509.11385v1 Announce Type: new  Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for objective and quantitative dermatological assessment, but no portable, high-resolution device exists that has been validated and used for depth reconstruction across various body locations. We present a compact 3-D skin reconstruction probe based on GelSight tactile imaging with a custom elastic gel and a learning-based reconstruction algorithm for micron-level wrinkle height estimation. Our probe, integrated into a handheld probe with force sensing for consistent contact, achieves a mean absolute error of 12.55 micron on wrinkle-like test objects. In a study with 15 participants without skin disorders, we provide the first validated wrinkle depth metrics across multiple body regions. We further demonstrate statistically significant reductions in wrinkle height at three locations following over-the-counter moisturizer application. Our work offers a validated tool for clinical and cosmetic skin analysis, with potential applications in diagnosis, treatment monitoring, and skincare efficacy evaluation.",
        "arxiv_id": "2509.11385",
        "ARXIVID": "2509.11385",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and tactile sensing, which are tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11924": {
        "authors": [
            "Bo Cao (Department of Radiology and Nuclear Medicine",
            "Xuanwu Hospital",
            "Capital Medical University",
            "China)",
            "Fan Yu (Department of Radiology and Nuclear Medicine",
            "Xuanwu Hospital",
            "Capital Medical University",
            "China)",
            "Mengmeng Feng (Department of Radiology and Nuclear Medicine",
            "Xuanwu Hospital",
            "Capital Medical University",
            "China)",
            "SenHao Zhang (Department of Radiology and Nuclear Medicine",
            "Xuanwu Hospital",
            "Capital Medical University",
            "China)",
            "Xin Meng (Department of Radiology and Nuclear Medicine",
            "Xuanwu Hospital",
            "Capital Medical University",
            "China)",
            "Yue Zhang (Department of Radiology and Nuclear Medicine",
            "Xuanwu Hospital",
            "Capital Medical University",
            "China)",
            "Zhen Qian (Beijing United Intelligent Imaging Research Institute",
            "China)",
            "Jie Lu (Department of Radiology and Nuclear Medicine",
            "Xuanwu Hospital",
            "Capital Medical University",
            "China)"
        ],
        "title": "Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI",
        "abstract": "arXiv:2509.11924v1 Announce Type: new  Abstract: Multimodal learning has attracted much attention in recent years due to its ability to effectively utilize data features from a variety of different modalities. Diagnosing the vulnerability of atherosclerotic plaques directly from carotid 3D MRI images is relatively challenging for both radiologists and conventional 3D vision networks. In clinical practice, radiologists assess patient conditions using a multimodal approach that incorporates various imaging modalities and domain-specific expertise, paving the way for the creation of multimodal diagnostic networks. In this paper, we have developed an effective strategy to leverage radiologists' domain knowledge to automate the diagnosis of carotid plaque vulnerability through Variation inference and Multimodal knowledge Distillation (VMD). This method excels in harnessing cross-modality prior knowledge from limited image annotations and radiology reports within training data, thereby enhancing the diagnostic network's accuracy for unannotated 3D MRI images. We conducted in-depth experiments on the dataset collected in-house and verified the effectiveness of the VMD strategy we proposed.",
        "arxiv_id": "2509.11924",
        "ARXIVID": "2509.11924",
        "COMMENT": "Does not match any specific criterion but is related to multimodal learning and medical imaging, which are tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11587": {
        "authors": [
            "Haonan Shi",
            "Yubin Wang",
            "De Cheng",
            "Lingfeng He",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "title": "Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification",
        "abstract": "arXiv:2509.11587v1 Announce Type: new  Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to learn modality-invariant image features from unlabeled cross-modal person datasets by reducing the modality gap while minimizing reliance on costly manual annotations. Existing methods typically address USVI-ReID using cluster-based contrastive learning, which represents a person by a single cluster center. However, they primarily focus on the commonality of images within each cluster while neglecting the finer-grained differences among them. To address the limitation, we propose a Hierarchical Identity Learning (HIL) framework. Since each cluster may contain several smaller sub-clusters that reflect fine-grained variations among images, we generate multiple memories for each existing coarse-grained cluster via a secondary clustering. Additionally, we propose Multi-Center Contrastive Learning (MCCL) to refine representations for enhancing intra-modal clustering and minimizing cross-modal discrepancies. To further improve cross-modal matching quality, we design a Bidirectional Reverse Selection Transmission (BRST) mechanism, which establishes reliable cross-modal correspondences by performing bidirectional matching of pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB datasets demonstrate that the proposed method outperforms existing approaches. The source code is available at: https://github.com/haonanshi0125/HIL.",
        "arxiv_id": "2509.11587",
        "ARXIVID": "2509.11587",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and machine learning, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11218": {
        "authors": [
            "Johann Schmidt",
            "Sebastian Stober"
        ],
        "title": "Geometrically Constrained and Token-Based Probabilistic Spatial Transformers",
        "abstract": "arXiv:2509.11218v1 Announce Type: new  Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to geometric variability, where objects appear under arbitrary orientations, scales, and perspective distortions. While equivariant architectures address this issue, they typically require substantial computational resources and restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs) as a canonicalization tool for transformer-based vision pipelines, emphasizing their flexibility, backbone-agnostic nature, and lack of architectural constraints. We propose a probabilistic, component-wise extension that improves robustness. Specifically, we decompose affine transformations into rotation, scaling, and shearing, and regress each component under geometric constraints using a shared localization encoder. To capture uncertainty, we model each component with a Gaussian variational posterior and perform sampling-based canonicalization during inference.A novel component-wise alignment loss leverages augmentation parameters to guide spatial alignment. Experiments on challenging moth classification benchmarks demonstrate that our method consistently improves robustness compared to other STNs.",
        "arxiv_id": "2509.11218",
        "ARXIVID": "2509.11218",
        "COMMENT": "Does not closely match any specific criteria but is relevant to fine-grained visual classification and spatial reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11116": {
        "authors": [
            "Ashkan Taghipour",
            "Vahid Naghshin",
            "Benjamin Southwell",
            "Farid Boussaid",
            "Hamid Laga",
            "Mohammed Bennamoun"
        ],
        "title": "SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting",
        "abstract": "arXiv:2509.11116v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\\(\\times\\) compared to MaskGS and 5.63\\(\\times\\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.",
        "arxiv_id": "2509.11116",
        "ARXIVID": "2509.11116",
        "COMMENT": "Does not closely match any specific criteria but is relevant to computer vision and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11476": {
        "authors": [
            "Tianyao Sun",
            "Dawei Xiang",
            "Tianqi Ding",
            "Xiang Fang",
            "Yijiashun Qi",
            "Zunduo Zhao"
        ],
        "title": "Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision",
        "abstract": "arXiv:2509.11476v1 Announce Type: new  Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal perception that aims to integrate complementary structural and textural cues from different spectral domains. In this paper, we propose FusionNet, a novel end-to-end fusion framework that explicitly models inter-modality interaction and enhances task-critical regions. FusionNet introduces a modality-aware attention mechanism that dynamically adjusts the contribution of infrared and visible features based on their discriminative capacity. To achieve fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha blending module, which learns spatially-varying fusion weights in an adaptive and content-aware manner. Moreover, we formulate a target-aware loss that leverages weak ROI supervision to preserve semantic consistency in regions containing important objects (e.g., pedestrians, vehicles). Experiments on the public M3FD dataset demonstrate that FusionNet generates fused images with enhanced semantic preservation, high perceptual quality, and clear interpretability. Our framework provides a general and extensible solution for semantic-aware multi-modal image fusion, with benefits for downstream tasks such as object detection and scene understanding.",
        "arxiv_id": "2509.11476",
        "ARXIVID": "2509.11476",
        "COMMENT": "Does not match any specific criteria. Focuses on infrared and visible image fusion, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11201": {
        "authors": [
            "Yihang She",
            "Andrew Blake",
            "David Coomes",
            "Srinivasan Keshav"
        ],
        "title": "Scaling Up Forest Vision with Synthetic Data",
        "abstract": "arXiv:2509.11201v1 Announce Type: new  Abstract: Accurate tree segmentation is a key step in extracting individual tree metrics from forest laser scans, and is essential to understanding ecosystem functions in carbon cycling and beyond. Over the past decade, tree segmentation algorithms have advanced rapidly due to developments in AI. However existing, public, 3D forest datasets are not large enough to build robust tree segmentation systems. Motivated by the success of synthetic data in other domains such as self-driving, we investigate whether similar approaches can help with tree segmentation. In place of expensive field data collection and annotation, we use synthetic data during pretraining, and then require only minimal, real forest plot annotation for fine-tuning.   We have developed a new synthetic data generation pipeline to do this for forest vision tasks, integrating advances in game-engines with physics-based LiDAR simulation. As a result, we have produced a comprehensive, diverse, annotated 3D forest dataset on an unprecedented scale. Extensive experiments with a state-of-the-art tree segmentation algorithm and a popular real dataset show that our synthetic data can substantially reduce the need for labelled real data. After fine-tuning on just a single, real, forest plot of less than 0.1 hectare, the pretrained model achieves segmentations that are competitive with a model trained on the full scale real data. We have also identified critical factors for successful use of synthetic data: physics, diversity, and scale, paving the way for more robust 3D forest vision systems in the future. Our data generation pipeline and the resulting dataset are available at https://github.com/yihshe/CAMP3D.git.",
        "arxiv_id": "2509.11201",
        "ARXIVID": "2509.11201",
        "COMMENT": "Does not match any specific criteria. Focuses on synthetic data for tree segmentation, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11539": {
        "authors": [
            "Dezhen Wang",
            "Haixiang Zhao",
            "Xiang Shen",
            "Sheng Miao"
        ],
        "title": "SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection",
        "abstract": "arXiv:2509.11539v1 Announce Type: new  Abstract: Camouflaged object detection (COD) aims to segment objects that blend into their surroundings. However, most existing studies overlook the semantic differences among textual prompts of different targets as well as fine-grained frequency features. In this work, we propose a novel Semantic and Frequency Guided Network (SFGNet), which incorporates semantic prompts and frequency-domain features to capture camouflaged objects and improve boundary perception. We further design Multi-Band Fourier Module(MBFM) to enhance the ability of the network in handling complex backgrounds and blurred boundaries. In addition, we design an Interactive Structure Enhancement Block (ISEB) to ensure structural integrity and boundary details in the predictions. Extensive experiments conducted on three COD benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches. The core code of the model is available at the following link: https://github.com/winter794444/SFGNetICASSP2026.",
        "arxiv_id": "2509.11539",
        "ARXIVID": "2509.11539",
        "COMMENT": "Does not match any specific criteria. Focuses on camouflaged object detection, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11264": {
        "authors": [
            "Kerun Mi",
            "Guoliang Kang",
            "Guangyu Li",
            "Lin Zhao",
            "Tao Zhou",
            "Chen Gong"
        ],
        "title": "Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation",
        "abstract": "arXiv:2509.11264v1 Announce Type: new  Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a model from a labeled source domain to an unlabeled target domain, where the sets of potential target classes appearing at different time steps are disjoint and are subsets of the source classes. The key to solving this problem lies in avoiding catastrophic forgetting of knowledge about previous target classes during continuously mitigating the domain shift. Most previous works cumbersomely combine two technical components. On one hand, they need to store and utilize rehearsal target sample from previous time steps to avoid catastrophic forgetting; on the other hand, they perform alignment only between classes shared across domains at each time step. Consequently, the memory will continuously increase and the asymmetric alignment may inevitably result in knowledge forgetting. In this paper, we propose to mine and preserve domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task. Specifically, via using CLIP, we extract the class-agnostic properties which we name as \"attribute\". In our framework, we learn a \"key-value\" pair to represent an attribute, where the key corresponds to the visual prototype and the value is the textual prompt. We maintain two attribute dictionaries, each corresponding to a different domain. Then we perform attribute alignment across domains to mitigate the domain shift, via encouraging visual attention consistency and prediction consistency. Through attribute modeling and cross-domain alignment, we effectively reduce catastrophic knowledge forgetting while mitigating the domain shift, in a rehearsal-free way. Experiments on three CI-UDA benchmarks demonstrate that our method outperforms previous state-of-the-art methods and effectively alleviates catastrophic forgetting. Code is available at https://github.com/RyunMi/VisTA.",
        "arxiv_id": "2509.11264",
        "ARXIVID": "2509.11264",
        "COMMENT": "Does not match any specific criteria. Focuses on domain adaptation and CLIP-based attribute alignment, which is tangential to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.11355": {
        "authors": [
            "Robin Narsingh Ranabhat",
            "Longwei Wang",
            "Amit Kumar Patel",
            "KC santosh"
        ],
        "title": "Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness",
        "abstract": "arXiv:2509.11355v1 Announce Type: new  Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain vulnerable to common corruptions that humans handle with ease. A key reason for this fragility is their reliance on local texture cues rather than global object shapes -- a stark contrast to human perception. To address this, we propose two complementary regularization strategies designed to encourage shape-biased representations and enhance robustness. The first introduces an auxiliary loss that enforces feature consistency between original and low-frequency filtered inputs, discouraging dependence on high-frequency textures. The second incorporates supervised contrastive learning to structure the feature space around class-consistent, shape-relevant representations. Evaluated on the CIFAR-10-C benchmark, both methods improve corruption robustness without degrading clean accuracy. Our results suggest that loss-level regularization can effectively steer CNNs toward more shape-aware, resilient representations.",
        "arxiv_id": "2509.11355",
        "ARXIVID": "2509.11355",
        "COMMENT": "Does not match any specific criteria but is relevant to general computer vision and robustness in CNNs.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.11811": {
        "authors": [
            "Mehwish Mehmood",
            "Shahzaib Iqbal",
            "Tariq Mahmood Khan",
            "Ivor Spence",
            "Muhammad Fahim"
        ],
        "title": "LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio",
        "abstract": "arXiv:2509.11811v1 Announce Type: new  Abstract: Retinal vessel segmentation is critical for the early diagnosis of vision-threatening and systemic diseases, especially in real-world clinical settings with limited computational resources. Although significant improvements have been made in deep learning-based segmentation methods, current models still face challenges in extracting tiny vessels and suffer from high computational costs. In this study, we present LFRA-Net by incorporating focal modulation attention at the encoder-decoder bottleneck and region-aware attention in the selective skip connections. LFRA-Net is a lightweight network optimized for precise and effective retinal vascular segmentation. It enhances feature representation and regional focus by efficiently capturing local and global dependencies. LFRA-Net outperformed many state-of-the-art models while maintaining lightweight characteristics with only 0.17 million parameters, 0.66 MB memory size, and 10.50 GFLOPs. We validated it on three publicly available datasets: DRIVE, STARE, and CHASE\\_DB. It performed better in terms of Dice score (84.28\\%, 88.44\\%, and 85.50\\%) and Jaccard index (72.86\\%, 79.31\\%, and 74.70\\%) on the DRIVE, STARE, and CHASE\\_DB datasets, respectively. LFRA-Net provides an ideal ratio between segmentation accuracy and computational cost compared to existing deep learning methods, which makes it suitable for real-time clinical applications in areas with limited resources. The code can be found at https://github.com/Mehwish4593/LFRA-Net.",
        "arxiv_id": "2509.11811",
        "ARXIVID": "2509.11811",
        "COMMENT": "Does not closely match any specific criteria but is relevant to lightweight models in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.12047": {
        "authors": [
            "Haiyu Yang",
            "Enhong Liu",
            "Jennifer Sun",
            "Sumit Sharma",
            "Meike van Leerdam",
            "Sebastien Franceschini",
            "Puchun Niu",
            "Miel Hostens"
        ],
        "title": "A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset",
        "abstract": "arXiv:2509.12047v1 Announce Type: new  Abstract: Animal behavior analysis plays a crucial role in understanding animal welfare, health status, and productivity in agricultural settings. However, traditional manual observation methods are time-consuming, subjective, and limited in scalability. We present a modular pipeline that leverages open-sourced state-of-the-art computer vision techniques to automate animal behavior analysis in a group housing environment. Our approach combines state-of-the-art models for zero-shot object detection, motion-aware tracking and segmentation, and advanced feature extraction using vision transformers for robust behavior recognition. The pipeline addresses challenges including animal occlusions and group housing scenarios as demonstrated in indoor pig monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset for multiple behavioral tasks. Our temporal model achieved 94.2% overall accuracy, representing a 21.2 percentage point improvement over existing methods. The pipeline demonstrated robust tracking capabilities with 93.3% identity preservation score and 89.3% object detection precision. The modular design suggests potential for adaptation to other contexts, though further validation across species would be required. The open-source implementation provides a scalable solution for behavior monitoring, contributing to precision pig farming and welfare assessment through automated, objective, and continuous analysis.",
        "arxiv_id": "2509.12047",
        "ARXIVID": "2509.12047",
        "COMMENT": "Does not closely match any specific criteria but is relevant to computer vision applications in animal behavior analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}