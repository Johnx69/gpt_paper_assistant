{
    "2509.09154": {
        "authors": [
            "Bui Duc Manh",
            "Soumyaratna Debnath",
            "Zetong Zhang",
            "Shriram Damodaran",
            "Arvind Kumar",
            "Yueyi Zhang",
            "Lu Mi",
            "Erik Cambria",
            "Lin Wang"
        ],
        "title": "Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective",
        "abstract": "arXiv:2509.09154v1 Announce Type: new  Abstract: Recent advances in agentic AI have led to systems capable of autonomous task execution and language-based reasoning, yet their spatial reasoning abilities remain limited and underexplored, largely constrained to symbolic and sequential processing. In contrast, human spatial intelligence, rooted in integrated multisensory perception, spatial memory, and cognitive maps, enables flexible, context-aware decision-making in unstructured environments. Therefore, bridging this gap is critical for advancing Agentic Spatial Intelligence toward better interaction with the physical 3D world. To this end, we first start from scrutinizing the spatial neural models as studied in computational neuroscience, and accordingly introduce a novel computational framework grounded in neuroscience principles. This framework maps core biological functions to six essential computation modules: bio-inspired multimodal sensing, multi-sensory integration, egocentric-allocentric conversion, an artificial cognitive map, spatial memory, and spatial reasoning. Together, these modules form a perspective landscape for agentic spatial reasoning capability across both virtual and physical environments. On top, we conduct a framework-guided analysis of recent methods, evaluating their relevance to each module and identifying critical gaps that hinder the development of more neuroscience-grounded spatial reasoning modules. We further examine emerging benchmarks and datasets and explore potential application domains ranging from virtual to embodied systems, such as robotics. Finally, we outline potential research directions, emphasizing the promising roadmap that can generalize spatial reasoning across dynamic or unstructured environments. We hope this work will benefit the research community with a neuroscience-grounded perspective and a structured pathway. Our project page can be found at Github.",
        "arxiv_id": "2509.09154",
        "ARXIVID": "2509.09154",
        "COMMENT": "Matches criteria 1 (Spatial Intelligence and Embodied Agents) due to its focus on agentic spatial intelligence and neuroscience-inspired frameworks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.09680": {
        "authors": [
            "Rongyao Fang",
            "Aldrich Yu",
            "Chengqi Duan",
            "Linjiang Huang",
            "Shuai Bai",
            "Yuxuan Cai",
            "Kun Wang",
            "Si Liu",
            "Xihui Liu",
            "Hongsheng Li"
        ],
        "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark",
        "abstract": "arXiv:2509.09680v1 Announce Type: new  Abstract: The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .",
        "arxiv_id": "2509.09680",
        "ARXIVID": "2509.09680",
        "COMMENT": "Matches criterion 6 (Video Understanding) and criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a reasoning-focused text-to-image dataset and benchmark for complex reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.09595": {
        "authors": [
            "Yikang Ding",
            "Jiwen Liu",
            "Wenyuan Zhang",
            "Zekun Wang",
            "Wentao Hu",
            "Liyuan Cui",
            "Mingming Lao",
            "Yingchao Shao",
            "Hui Liu",
            "Xiaohan Li",
            "Ming Chen",
            "Xiaoqiang Liu",
            "Yu-Shen Liu",
            "Pengfei Wan"
        ],
        "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis",
        "abstract": "arXiv:2509.09595v1 Announce Type: new  Abstract: Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.",
        "arxiv_id": "2509.09595",
        "ARXIVID": "2509.09595",
        "COMMENT": "Matches criteria 5 (Integration of Image/Video and Large Language Models) due to its focus on multimodal instruction understanding and avatar animation synthesis.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.09263": {
        "authors": [
            "Chao Yuan",
            "Yang Yang",
            "Yehui Yang",
            "Zach Cheng"
        ],
        "title": "DATE: Dynamic Absolute Time Enhancement for Long Video Understanding",
        "abstract": "arXiv:2509.09263v1 Announce Type: new  Abstract: Long video understanding remains a fundamental challenge for multimodal large language models (MLLMs), particularly in tasks requiring precise temporal reasoning and event localization. Existing approaches typically adopt uniform frame sampling and rely on implicit position encodings to model temporal order. However, these methods struggle with long-range dependencies, leading to critical information loss and degraded temporal comprehension. In this paper, we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a semantically guided Temporal-Aware Similarity Sampling (TASS) strategy. Specifically, we interleave video frame embeddings with textual timestamp tokens to construct a continuous temporal reference system. We further reformulate the video sampling problem as a vision-language retrieval task and introduce a two-stage algorithm to ensure both semantic relevance and temporal coverage: enriching each query into a descriptive caption to better align with the vision feature, and sampling key event with a similarity-driven temporally regularized greedy strategy. Our method achieves remarkable improvements w.r.t. absolute time understanding and key event localization, resulting in state-of-the-art performance among 7B and 72B models on hour-long video benchmarks. Particularly, our 7B model even exceeds many 72B models on some benchmarks.",
        "arxiv_id": "2509.09263",
        "ARXIVID": "2509.09263",
        "COMMENT": "Matches criteria 6 (Video Understanding) due to its focus on long video understanding, temporal reasoning, and event localization.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.08940": {
        "authors": [
            "Lisa Dunlap",
            "Joseph E. Gonzalez",
            "Trevor Darrell",
            "Fabian Caba Heilbron",
            "Josef Sivic",
            "Bryan Russell"
        ],
        "title": "Discovering Divergent Representations between Text-to-Image Models",
        "abstract": "arXiv:2509.08940v1 Announce Type: new  Abstract: In this paper, we investigate when and how visual representations learned by two different generative models diverge. Given two text-to-image models, our goal is to discover visual attributes that appear in images generated by one model but not the other, along with the types of prompts that trigger these attribute differences. For example, \"flames\" might appear in one model's outputs when given prompts expressing strong emotions, while the other model does not produce this attribute given the same prompts. We introduce CompCon (Comparing Concepts), an evolutionary search algorithm that discovers visual attributes more prevalent in one model's output than the other, and uncovers the prompt concepts linked to these visual differences. To evaluate CompCon's ability to find diverging representations, we create an automated data generation pipeline to produce ID2, a dataset of 60 input-dependent differences, and compare our approach to several LLM- and VLM-powered baselines. Finally, we use CompCon to compare popular text-to-image models, finding divergent representations such as how PixArt depicts prompts mentioning loneliness with wet streets and Stable Diffusion 3.5 depicts African American people in media professions. Code at: https://github.com/adobe-research/CompCon",
        "arxiv_id": "2509.08940",
        "ARXIVID": "2509.08940",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it explores divergent visual representations in text-to-image models and their prompts.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.09666": {
        "authors": [
            "Zhiyuan Yan",
            "Kaiqing Lin",
            "Zongjian Li",
            "Junyan Ye",
            "Hui Han",
            "Zhendong Wang",
            "Hao Liu",
            "Bin Lin",
            "Hao Li",
            "Xue Xu",
            "Xinyan Xiao",
            "Jingdong Wang",
            "Haifeng Wang",
            "Li Yuan"
        ],
        "title": "Can Understanding and Generation Truly Benefit Together -- or Just Coexist?",
        "abstract": "arXiv:2509.09666v1 Announce Type: new  Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.",
        "arxiv_id": "2509.09666",
        "ARXIVID": "2509.09666",
        "COMMENT": "Matches criterion 5 as it explores the integration of image understanding and generation tasks with large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.09547": {
        "authors": [
            "Dohun Lee",
            "Hyeonho Jeong",
            "Jiwook Kim",
            "Duygu Ceylan",
            "Jong Chul Ye"
        ],
        "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders",
        "abstract": "arXiv:2509.09547v1 Announce Type: new  Abstract: Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/",
        "arxiv_id": "2509.09547",
        "ARXIVID": "2509.09547",
        "COMMENT": "Matches criteria 6 (Video Understanding) due to its focus on video diffusion models and video generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09555": {
        "authors": [
            "Sirui Xu",
            "Dongting Li",
            "Yucheng Zhang",
            "Xiyan Xu",
            "Qi Long",
            "Ziyin Wang",
            "Yunzhi Lu",
            "Shuchang Dong",
            "Hezi Jiang",
            "Akshat Gupta",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation",
        "abstract": "arXiv:2509.09555v1 Announce Type: new  Abstract: While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. To support continued research in this area, the dataset is publicly available at https://github.com/wzyabcas/InterAct, and will be actively maintained.",
        "arxiv_id": "2509.09555",
        "ARXIVID": "2509.09555",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark and methods for 3D human-object interaction generation, relevant for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09307": {
        "authors": [
            "Zhengzhao Lai",
            "Youbin Zheng",
            "Zhenyang Cai",
            "Haonan Lyu",
            "Jinpu Yang",
            "Hongqing Liang",
            "Yan Hu",
            "Benyou Wang"
        ],
        "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization",
        "abstract": "arXiv:2509.09307v1 Announce Type: new  Abstract: Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.",
        "arxiv_id": "2509.09307",
        "ARXIVID": "2509.09307",
        "COMMENT": "Matches criteria 2 as it evaluates multimodal LLMs on a new benchmark for materials characterization, focusing on vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09584": {
        "authors": [
            "Lingdong Kong",
            "Dongyue Lu",
            "Ao Liang",
            "Rong Li",
            "Yuhao Dong",
            "Tianshuai Hu",
            "Lai Xing Ng",
            "Wei Tsang Ooi",
            "Benoit R. Cottereau"
        ],
        "title": "Visual Grounding from Event Cameras",
        "abstract": "arXiv:2509.09584v1 Announce Type: new  Abstract: Event cameras capture changes in brightness with microsecond precision and remain reliable under motion blur and challenging illumination, offering clear advantages for modeling highly dynamic scenes. Yet, their integration with natural language understanding has received little attention, leaving a gap in multimodal perception. To address this, we introduce Talk2Event, the first large-scale benchmark for language-driven object grounding using event data. Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes, 13,458 annotated objects, and more than 30,000 carefully validated referring expressions. Each expression is enriched with four structured attributes -- appearance, status, relation to the viewer, and relation to surrounding objects -- that explicitly capture spatial, temporal, and relational cues. This attribute-centric design supports interpretable and compositional grounding, enabling analysis that moves beyond simple object recognition to contextual reasoning in dynamic environments. We envision Talk2Event as a foundation for advancing multimodal and temporally-aware perception, with applications spanning robotics, human-AI interaction, and so on.",
        "arxiv_id": "2509.09584",
        "ARXIVID": "2509.09584",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel benchmark for spatial reasoning and grounding in dynamic environments, which is relevant for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.08897": {
        "authors": [
            "Davide Caffagni",
            "Sara Sarto",
            "Marcella Cornia",
            "Lorenzo Baraldi",
            "Rita Cucchiara"
        ],
        "title": "Recurrence Meets Transformers for Universal Multimodal Retrieval",
        "abstract": "arXiv:2509.08897v1 Announce Type: new  Abstract: With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT-2",
        "arxiv_id": "2509.08897",
        "ARXIVID": "2509.08897",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it proposes a unified retrieval model for multimodal queries and documents, leveraging vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09667": {
        "authors": [
            "Zhengdi Yu",
            "Simone Foti",
            "Linguang Zhang",
            "Amy Zhao",
            "Cem Keskin",
            "Stefanos Zafeiriou",
            "Tolga Birdal"
        ],
        "title": "Geometric Neural Distance Fields for Learning Human Motion Priors",
        "abstract": "arXiv:2509.09667v1 Announce Type: new  Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to \"roll out\" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.",
        "arxiv_id": "2509.09667",
        "ARXIVID": "2509.09667",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for learning human motion priors, which is relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09676": {
        "authors": [
            "Jiahao Wang",
            "Yufeng Yuan",
            "Rujie Zheng",
            "Youtian Lin",
            "Jian Gao",
            "Lin-Zhuo Chen",
            "Yajie Bao",
            "Yi Zhang",
            "Chang Zeng",
            "Yanxi Zhou",
            "Xiaoxiao Long",
            "Hao Zhu",
            "Zhaoxiang Zhang",
            "Xun Cao",
            "Yao Yao"
        ],
        "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
        "abstract": "arXiv:2509.09676v1 Announce Type: new  Abstract: Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect \\textbf{SpatialVID}, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.",
        "arxiv_id": "2509.09676",
        "ARXIVID": "2509.09676",
        "COMMENT": "Matches criterion 3 as it introduces a new large-scale video dataset with spatial annotations, which can be used for embodied or robotic AI tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09321": {
        "authors": [
            "Hangyi Jia",
            "Yuxi Qian",
            "Hanwen Tong",
            "Xinhui Wu",
            "Lin Chen",
            "Feng Wei"
        ],
        "title": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization",
        "abstract": "arXiv:2509.09321v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have enabled the emergence of general-purpose agents for automating end-to-end machine learning (ML) workflows, including data analysis, feature engineering, model training, and competition solving. However, existing benchmarks remain limited in task coverage, domain diversity, difficulty modeling, and evaluation rigor, failing to capture the full capabilities of such agents in realistic settings. We present TAM Bench, a diverse, realistic, and structured benchmark for evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three key innovations: (1) A browser automation and LLM-based task acquisition system that automatically collects and structures ML challenges from platforms such as Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities (e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty modeling mechanism that estimates task complexity using participant counts and score dispersion, enabling scalable and objective task calibration; (3) A multi-dimensional evaluation framework incorporating performance, format compliance, constraint adherence, and task generalization. Based on 150 curated AutoML tasks, we construct three benchmark subsets of different sizes -- Lite, Medium, and Full -- designed for varying evaluation scenarios. The Lite version, with 18 tasks and balanced coverage across modalities and difficulty levels, serves as a practical testbed for daily benchmarking and comparative studies.",
        "arxiv_id": "2509.09321",
        "ARXIVID": "2509.09321",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating ML agents, which is relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09560": {
        "authors": [
            "Shulai Zhang",
            "Ao Xu",
            "Quan Chen",
            "Han Zhao",
            "Weihao Cui",
            "Ningxin Zheng",
            "Haibin Lin",
            "Xin Liu",
            "Minyi Guo"
        ],
        "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution",
        "abstract": "arXiv:2509.09560v1 Announce Type: new  Abstract: Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary \"thinking\" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.",
        "arxiv_id": "2509.09560",
        "ARXIVID": "2509.09560",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for optimizing embodied AI agents' inference pipelines.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.09254": {
        "authors": [
            "Jing Hao",
            "Yuxuan Fan",
            "Yanpeng Sun",
            "Kaixin Guo",
            "Lizhuo Lin",
            "Jinrong Yang",
            "Qi Yong H. Ai",
            "Lun M. Wong",
            "Hao Tang",
            "Kuo Feng Hung"
        ],
        "title": "Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis",
        "abstract": "arXiv:2509.09254v1 Announce Type: new  Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a 24.73% improvement. Both MMOral and OralGPT hold significant potential as a critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at https://github.com/isbrycee/OralGPT.",
        "arxiv_id": "2509.09254",
        "ARXIVID": "2509.09254",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a multimodal benchmark and instruction dataset for panoramic X-ray analysis, focusing on vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.09110": {
        "authors": [
            "Chenghao Zhang",
            "Lun Luo",
            "Si-Yuan Cao",
            "Xiaokai Bai",
            "Yuncheng Jin",
            "Zhu Yu",
            "Beinan Yu",
            "Yisen Wang",
            "Hui-Liang Shen"
        ],
        "title": "S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization",
        "abstract": "arXiv:2509.09110v1 Announce Type: new  Abstract: LiDAR-based global localization is an essential component of simultaneous localization and mapping (SLAM), which helps loop closure and re-localization. Current approaches rely on ground-truth poses obtained from GPS or SLAM odometry to supervise network training. Despite the great success of these supervised approaches, substantial cost and effort are required for high-precision ground-truth pose acquisition. In this work, we propose S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for LiDAR global localization, which eliminates the need for ground-truth poses and is highly scalable. We construct training triplets from single BEV images by leveraging the known geographic distances between keypoint-centered BEV patches. Convolutional neural network (CNN) is used to extract local features, and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce SoftCos loss to enhance learning from the generated triplets. Experimental results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves state-of-the-art performance in place recognition, loop closure, and global localization tasks, while offering scalability that would require extra effort for supervised approaches.",
        "arxiv_id": "2509.09110",
        "ARXIVID": "2509.09110",
        "COMMENT": "Matches criterion 3 as it introduces a self-supervised framework for LiDAR-based global localization, which is relevant to embodied or robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.09610": {
        "authors": [
            "Daria Laslo",
            "Efthymios Georgiou",
            "Marius George Linguraru",
            "Andreas Rauschecker",
            "Sabine Muller",
            "Catherine R. Jutzeler",
            "Sarah Bruningk"
        ],
        "title": "Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth",
        "abstract": "arXiv:2509.09610v1 Announce Type: new  Abstract: Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.",
        "arxiv_id": "2509.09610",
        "ARXIVID": "2509.09610",
        "COMMENT": "Matches criteria 6 as it focuses on video-based tasks, specifically spatio-temporal brain tumor growth prediction using guided diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.09498": {
        "authors": [
            "Haoran Xu",
            "Jiacong Hu",
            "Ke Zhang",
            "Lei Yu",
            "Yuxin Tang",
            "Xinyuan Song",
            "Yiqun Duan",
            "Lynn Ai",
            "Bill Shi"
        ],
        "title": "SEDM: Scalable Self-Evolving Distributed Memory for Agents",
        "abstract": "arXiv:2509.09498v1 Announce Type: new  Abstract: Long-term multi-agent systems inevitably generate vast amounts of trajectories and historical interactions, which makes efficient memory management essential for both performance and scalability. Existing methods typically depend on vector retrieval and hierarchical storage, yet they are prone to noise accumulation, uncontrolled memory expansion, and limited generalization across domains. To address these challenges, we present SEDM, Self-Evolving Distributed Memory, a verifiable and adaptive framework that transforms memory from a passive repository into an active, self-optimizing component. SEDM integrates verifiable write admission based on reproducible replay, a self-scheduling memory controller that dynamically ranks and consolidates entries according to empirical utility, and cross-domain knowledge diffusion that abstracts reusable insights to support transfer across heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM improves reasoning accuracy while reducing token overhead compared with strong memory baselines, and further enables knowledge distilled from fact verification to enhance multi-hop reasoning. The results highlight SEDM as a scalable and sustainable memory mechanism for open-ended multi-agent collaboration. The code will be released in the later stage of this project.",
        "arxiv_id": "2509.09498",
        "ARXIVID": "2509.09498",
        "COMMENT": "Matches criteria 3 as it introduces a novel memory management framework for multi-agent systems, which is relevant for embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.08908": {
        "authors": [
            "Rogerio Guimaraes",
            "Frank Xiao",
            "Pietro Perona",
            "Markus Marks"
        ],
        "title": "Diffusion-Based Action Recognition Generalizes to Untrained Domains",
        "abstract": "arXiv:2509.08908v1 Announce Type: new  Abstract: Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: $\\href{https://www.vision.caltech.edu/actiondiff/}{\\texttt{vision.caltech.edu/actiondiff}}$ Code: $\\href{https://github.com/frankyaoxiao/ActionDiff}{\\texttt{github.com/frankyaoxiao/ActionDiff}}$",
        "arxiv_id": "2509.08908",
        "ARXIVID": "2509.08908",
        "COMMENT": "Matches criterion 6 as it focuses on video-based action recognition tasks and explores generalization across domains.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.09064": {
        "authors": [
            "Qiuhui Chen",
            "Xuancheng Yao",
            "Huping Ye",
            "Yi Hong"
        ],
        "title": "Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models",
        "abstract": "arXiv:2509.09064v1 Announce Type: new  Abstract: Understanding 3D medical image volumes is critical in the medical field, yet existing 3D medical convolution and transformer-based self-supervised learning (SSL) methods often lack deep semantic comprehension. Recent advancements in multimodal large language models (MLLMs) provide a promising approach to enhance image understanding through text descriptions. To leverage these 2D MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a novel pretraining framework that integrates 3D image encoders with 2D MLLMs via a specially designed plane-slice-aware transformer module. Additionally, our model employs a partial optimal transport based alignment, demonstrating greater tolerance to noise introduced by potential noises in LLM-generated content. Med3DInsight introduces a new paradigm for scalable multimodal 3D medical representation learning without requiring human annotations. Extensive experiments demonstrate our state-of-the-art performance on two downstream tasks, i.e., segmentation and classification, across various public datasets with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can be seamlessly integrated into existing 3D medical image understanding networks, potentially enhancing their performance. Our source code, generated datasets, and pre-trained models will be available at https://github.com/Qybc/Med3DInsight.",
        "arxiv_id": "2509.09064",
        "ARXIVID": "2509.09064",
        "COMMENT": "Matches criterion 2 as it explores the integration of 2D multimodal large language models with 3D medical image understanding, which is a novel application of MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.09314": {
        "authors": [
            "Thuy Ngoc Nguyen",
            "Anita Williams Woolley",
            "Cleotilde Gonzalez"
        ],
        "title": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance",
        "abstract": "arXiv:2509.09314v1 Announce Type: new  Abstract: Coordinated teamwork is essential in fast-paced decision-making environments that require dynamic adaptation, often without an opportunity for explicit communication. Although implicit coordination has been extensively considered in the existing literature, the majority of work has focused on co-located, synchronous teamwork (such as sports teams) or, in distributed teams, primarily on coordination of knowledge work. However, many teams (firefighters, military, law enforcement, emergency response) must coordinate their movements in physical space without the benefit of visual cues or extensive explicit communication. This paper investigates how three dimensions of spatial coordination, namely exploration diversity, movement specialization, and adaptive spatial proximity, influence team performance in a collaborative online search and rescue task where explicit communication is restricted and team members rely on movement patterns to infer others' intentions and coordinate actions. Our metrics capture the relational aspects of teamwork by measuring spatial proximity, distribution patterns, and alignment of movements within shared environments. We analyze data from 34 four-person teams (136 participants) assigned to specialized roles in a search and rescue task. Results show that spatial specialization positively predicts performance, while adaptive spatial proximity exhibits a marginal inverted U-shaped relationship, suggesting moderate levels of adaptation are optimal. Furthermore, the temporal dynamics of these metrics differentiate high- from low-performing teams over time. These findings provide insights into implicit spatial coordination in role-based teamwork and highlight the importance of balanced adaptive strategies, with implications for training and AI-assisted team support systems.",
        "arxiv_id": "2509.09314",
        "ARXIVID": "2509.09314",
        "COMMENT": "Matches criterion 1 as it focuses on spatial intelligence and coordination in teams, which is relevant to embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.09116": {
        "authors": [
            "Junhao Xing",
            "Ryohei Miyakawa",
            "Yang Yang",
            "Xinpeng Liu",
            "Risa Shinoda",
            "Hiroaki Santo",
            "Yosuke Toda",
            "Fumio Okura"
        ],
        "title": "Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention",
        "abstract": "arXiv:2509.09116v1 Announce Type: new  Abstract: Foundation segmentation models achieve reasonable leaf instance extraction from top-view crop images without training (i.e., zero-shot). However, segmenting entire plant individuals with each consisting of multiple overlapping leaves remains challenging. This problem is referred to as a hierarchical segmentation task, typically requiring annotated training datasets, which are often species-specific and require notable human labor. To address this, we introduce ZeroPlantSeg, a zero-shot segmentation for rosette-shaped plant individuals from top-view images. We integrate a foundation segmentation model, extracting leaf instances, and a vision-language model, reasoning about plants' structures to extract plant individuals without additional training. Evaluations on datasets with multiple plant species, growth stages, and shooting environments demonstrate that our method surpasses existing zero-shot methods and achieves better cross-domain performance than supervised methods. Implementations are available at https://github.com/JunhaoXing/ZeroPlantSeg.",
        "arxiv_id": "2509.09116",
        "ARXIVID": "2509.09116",
        "COMMENT": "Matches criteria 4 as it applies foundation segmentation models to hierarchical plant segmentation, showcasing their application in computer vision.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.09172": {
        "authors": [
            "Chunxiao Li",
            "Xiaoxiao Wang",
            "Meiling Li",
            "Boming Miao",
            "Peng Sun",
            "Yunjian Zhang",
            "Xiangyang Ji",
            "Yao Zhu"
        ],
        "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios",
        "abstract": "arXiv:2509.09172v1 Announce Type: new  Abstract: With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms.",
        "arxiv_id": "2509.09172",
        "ARXIVID": "2509.09172",
        "COMMENT": "Matches criterion 7 as it introduces a comprehensive benchmark and evaluation dataset for AI-generated image detection, addressing real-world challenges.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.09365": {
        "authors": [
            "Xiaodong Wang",
            "Ping Wang",
            "Zhangyuan Li",
            "Xin Yuan"
        ],
        "title": "Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection",
        "abstract": "arXiv:2509.09365v1 Announce Type: new  Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a focus on single-pixel imaging. We begin by identifying key distinctions between PnP and diffusion models-particularly in their denoising mechanisms and sampling procedures. By decoupling the diffusion process into three interpretable stages: denoising, data consistency enforcement, and sampling, we provide a unified framework that integrates learned priors with physical forward models in a principled manner. Building upon this insight, we propose a hybrid data-consistency module that linearly combines multiple PnP-style fidelity terms. This hybrid correction is applied directly to the denoised estimate, improving measurement consistency without disrupting the diffusion sampling trajectory. Experimental results on single-pixel imaging tasks demonstrate that our method achieves better reconstruction quality.",
        "arxiv_id": "2509.09365",
        "ARXIVID": "2509.09365",
        "COMMENT": "Matches criterion 4 as it focuses on integrating diffusion models with physical forward models, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.09324": {
        "authors": [
            "Hui Li",
            "Yi You",
            "Qiqi Chen",
            "Bingfeng Zhang",
            "George Q. Huang"
        ],
        "title": "Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM",
        "abstract": "arXiv:2509.09324v1 Announce Type: new  Abstract: Generative AI evolves the execution of complex workflows in industry, where the large multimodal model empowers fashion design in the garment industry. Current generation AI models magically transform brainstorming into fancy designs easily, but the fine-grained customization still suffers from text uncertainty without professional background knowledge from end-users. Thus, we propose the Better Understanding Generation (BUG) workflow with LMM to automatically create and fine-grain customize the cloth designs from chat with image-into-prompt. Our framework unleashes users' creative potential beyond words and also lowers the barriers of clothing design/editing without further human involvement. To prove the effectiveness of our model, we propose a new FashionEdit dataset that simulates the real-world clothing design workflow, evaluated from generation similarity, user satisfaction, and quality. The code and dataset: https://github.com/detectiveli/FashionEdit.",
        "arxiv_id": "2509.09324",
        "ARXIVID": "2509.09324",
        "COMMENT": "Matches criterion 2 as it explores a multimodal large language model (LMM) for fine-grained fashion design, integrating vision and language tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.09190": {
        "authors": [
            "Hanwei Zhu",
            "Haoning Wu",
            "Zicheng Zhang",
            "Lingyu Zhu",
            "Yixuan Li",
            "Peilin Chen",
            "Shiqi Wang",
            "Chris Wei Zhou",
            "Linhan Cao",
            "Wei Sun",
            "Xiangyang Zhu",
            "Weixia Zhang",
            "Yucheng Zhu",
            "Jing Liu",
            "Dandan Zhu",
            "Guangtao Zhai",
            "Xiongkuo Min",
            "Zhichao Zhang",
            "Xinyue Li",
            "Shubo Xu",
            "Anh Dao",
            "Yifan Li",
            "Hongyuan Yu",
            "Jiaojiao Yi",
            "Yiding Tian",
            "Yupeng Wu",
            "Feiran Sun",
            "Lijuan Liao",
            "Song Jiang"
        ],
        "title": "VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results",
        "abstract": "arXiv:2509.09190v1 Announce Type: new  Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025 Workshop on Visual Quality Assessment. The challenge aims to evaluate and enhance the ability of state-of-the-art LMMs to perform open-ended and detailed reasoning about visual quality differences across multiple images. To this end, the competition introduces a novel benchmark comprising thousands of coarse-to-fine grained visual quality comparison tasks, spanning single images, pairs, and multi-image groups. Each task requires models to provide accurate quality judgments. The competition emphasizes holistic evaluation protocols, including 2AFC-based binary preference and multi-choice questions (MCQs). Around 100 participants submitted entries, with five models demonstrating the emerging capabilities of instruction-tuned LMMs on quality assessment. This challenge marks a significant step toward open-domain visual quality reasoning and comparison and serves as a catalyst for future research on interpretable and human-aligned quality evaluation systems.",
        "arxiv_id": "2509.09190",
        "ARXIVID": "2509.09190",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating visual quality comparison in large multimodal models, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.09151": {
        "authors": [
            "Lei Wang",
            "Piotr Koniusz",
            "Yongsheng Gao"
        ],
        "title": "Video Understanding by Design: How Datasets Shape Architectures and Insights",
        "abstract": "arXiv:2509.09151v1 Announce Type: new  Abstract: Video understanding has advanced rapidly, fueled by increasingly complex datasets and powerful architectures. Yet existing surveys largely classify models by task or family, overlooking the structural pressures through which datasets guide architectural evolution. This survey is the first to adopt a dataset-driven perspective, showing how motion complexity, temporal span, hierarchical composition, and multimodal richness impose inductive biases that models should encode. We reinterpret milestones, from two-stream and 3D CNNs to sequential, transformer, and multimodal foundation models, as concrete responses to these dataset-driven pressures. Building on this synthesis, we offer practical guidance for aligning model design with dataset invariances while balancing scalability and task demands. By unifying datasets, inductive biases, and architectures into a coherent framework, this survey provides both a comprehensive retrospective and a prescriptive roadmap for advancing general-purpose video understanding.",
        "arxiv_id": "2509.09151",
        "ARXIVID": "2509.09151",
        "COMMENT": "Matches criteria 7 as it is a survey paper on video understanding, synthesizing progress and challenges in the field.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2509.09232": {
        "authors": [
            "Jiesi Hu",
            "Jianfeng Cao",
            "Yanwu Yang",
            "Chenfei Ye",
            "Yixuan Zhang",
            "Hanyang Peng",
            "Ting Ma"
        ],
        "title": "Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement",
        "abstract": "arXiv:2509.09232v1 Announce Type: new  Abstract: In-context learning (ICL) offers a promising paradigm for universal medical image analysis, enabling models to perform diverse image processing tasks without retraining. However, current ICL models for medical imaging remain limited in two critical aspects: they cannot simultaneously achieve high-fidelity predictions and global anatomical understanding, and there is no unified model trained across diverse medical imaging tasks (e.g., segmentation and enhancement) and anatomical regions. As a result, the full potential of ICL in medical imaging remains underexplored. Thus, we present \\textbf{Medverse}, a universal ICL model for 3D medical imaging, trained on 22 datasets covering diverse tasks in universal image segmentation, transformation, and enhancement across multiple organs, imaging modalities, and clinical centers. Medverse employs a next-scale autoregressive in-context learning framework that progressively refines predictions from coarse to fine, generating consistent, full-resolution volumetric outputs and enabling multi-scale anatomical awareness. We further propose a blockwise cross-attention module that facilitates long-range interactions between context and target inputs while preserving computational efficiency through spatial sparsity. Medverse is extensively evaluated on a broad collection of held-out datasets covering previously unseen clinical centers, organs, species, and imaging modalities. Results demonstrate that Medverse substantially outperforms existing ICL baselines and establishes a novel paradigm for in-context learning. Code and model weights will be made publicly available. Our model are publicly available at https://github.com/jiesihu/Medverse.",
        "arxiv_id": "2509.09232",
        "ARXIVID": "2509.09232",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal learning and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.09456": {
        "authors": [
            "Yushen Xu",
            "Xiaosong Li",
            "Yuchun Wang",
            "Xiaoqi Cheng",
            "Huafeng Li",
            "Haishu Tan"
        ],
        "title": "FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model",
        "abstract": "arXiv:2509.09456v1 Announce Type: new  Abstract: Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method.",
        "arxiv_id": "2509.09456",
        "ARXIVID": "2509.09456",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.09183": {
        "authors": [
            "Jiasheng Guo",
            "Xin Gao",
            "Yuxiang Yan",
            "Guanghao Li",
            "Jian Pu"
        ],
        "title": "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection",
        "abstract": "arXiv:2509.09183v1 Announce Type: new  Abstract: Low-light Object detection is crucial for many real-world applications but remains challenging due to degraded image quality. While recent studies have shown that RAW images offer superior potential over RGB images, existing approaches either use RAW-RGB images with information loss or employ complex frameworks. To address these, we propose a lightweight and self-adaptive Image Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW images in dark environments, enabling seamless end-to-end training for object detection. Our key innovations are: (1) We deconstruct conventional ISP pipelines into sequential linear (sensor calibration) and nonlinear (tone mapping) sub-modules, recasting them as differentiable components optimized through task-driven losses. Each module is equipped with content-aware adaptability and physics-informed priors, enabling automatic RAW-to-RGB conversion aligned with detection objectives. (2) By exploiting the ISP pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that facilitates cooperation between sub-modules. Through extensive experiments on three RAW image datasets, we demonstrate that our method outperforms state-of-the-art RGB- and RAW-based detection approaches, achieving superior results with minimal parameters in challenging low-light environments.",
        "arxiv_id": "2509.09183",
        "ARXIVID": "2509.09183",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and low-light object detection, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.09130": {
        "authors": [
            "Bin Huang",
            "Kang Chen",
            "Bingxuan Li",
            "Huafeng Liu",
            "Qiegen Liu"
        ],
        "title": "ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain",
        "abstract": "arXiv:2509.09130v1 Announce Type: new  Abstract: Building large-scale foundation model for PET imaging is hindered by limited access to labeled data and insufficient computational resources. To overcome data scarcity and efficiency limitations, we propose ALL-PET, a low-resource, low-shot PET foundation model operating directly in the projection domain. ALL-PET leverages a latent diffusion model (LDM) with three key innovations. First, we design a Radon mask augmentation strategy (RMAS) that generates over 200,000 structurally diverse training samples by projecting randomized image-domain masks into sinogram space, significantly improving generalization with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism that varies mask quantity and distribution, enhancing data diversity without added model complexity. Second, we implement positive/negative mask constraints to embed strict geometric consistency, reducing parameter burden while preserving generation quality. Third, we introduce transparent medical attention (TMA), a parameter-free, geometry-driven mechanism that enhances lesion-related regions in raw projection data. Lesion-focused attention maps are derived from coarse segmentation, covering both hypermetabolic and hypometabolic areas, and projected into sinogram space for physically consistent guidance. The system supports clinician-defined ROI adjustments, ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET acquisition physics. Experimental results show ALL-PET achieves high-quality sinogram generation using only 500 samples, with performance comparable to models trained on larger datasets. ALL-PET generalizes across tasks including low-dose reconstruction, attenuation correction, delayed-frame prediction, and tracer separation, operating efficiently with memory use under 24GB.",
        "arxiv_id": "2509.09130",
        "ARXIVID": "2509.09130",
        "COMMENT": "Does not match any specific criterion but is relevant to foundation models in medical imaging, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.09210": {
        "authors": [
            "Xing Gao",
            "Zherui Huang",
            "Weiyao Lin",
            "Xiao Sun"
        ],
        "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting",
        "abstract": "arXiv:2509.09210v1 Announce Type: new  Abstract: Accurate motion prediction of surrounding agents is crucial for the safe planning of autonomous vehicles. Recent advancements have extended prediction techniques from individual agents to joint predictions of multiple interacting agents, with various strategies to address complex interactions within future motions of agents. However, these methods overlook the evolving nature of these interactions. To address this limitation, we propose a novel progressive multi-scale decoding strategy, termed ProgD, with the help of dynamic heterogeneous graph-based scenario modeling. In particular, to explicitly and comprehensively capture the evolving social interactions in future scenarios, given their inherent uncertainty, we design a progressive modeling of scenarios with dynamic heterogeneous graphs. With the unfolding of such dynamic heterogeneous graphs, a factorized architecture is designed to process the spatio-temporal dependencies within future scenarios and progressively eliminate uncertainty in future motions of multiple agents. Furthermore, a multi-scale decoding procedure is incorporated to improve on the future scenario modeling and consistent prediction of agents' future motion. The proposed ProgD achieves state-of-the-art performance on the INTERACTION multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2 multi-world forecasting benchmark.",
        "arxiv_id": "2509.09210",
        "ARXIVID": "2509.09210",
        "COMMENT": "Does not match any specific criterion but is relevant to motion forecasting and multi-agent systems, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.08972": {
        "authors": [
            "Soheil Zibakhsh Shabgahi",
            "Pedram Aghazadeh",
            "Azalia Mirhosseini",
            "Farinaz Koushanfar"
        ],
        "title": "ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models",
        "abstract": "arXiv:2509.08972v1 Announce Type: new  Abstract: The increasing reliance on generative AI models has accelerated the generation rate of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. Although prior studies have explored the causes and detection of model collapse, existing mitigation strategies remain limited.   In this paper, we identify model overconfidence in their self-generated data as a key driver of collapse. Building on this observation, we propose a confidence-aware loss function that downweights high-confidence predictions during training. We introduce a novel loss function we call Truncated Cross Entropy (TCE). We demonstrate that TCE significantly delays model collapse in recursive training.   We provide a model-agnostic framework that links the loss function design to model collapse mitigation and validate our approach both theoretically and empirically, showing that it can extend the model's fidelity interval before collapse by more than 2.3x. Finally, we show that our method generalizes across modalities. These findings suggest that the design of loss functions provides a simple yet powerful tool for preserving the quality of generative models in the era of increasing synthetic data.",
        "arxiv_id": "2509.08972",
        "ARXIVID": "2509.08972",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and statistical techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09292": {
        "authors": [
            "Weige Cai",
            "Tong Zhu",
            "Jinyi Niu",
            "Ruiqi Hu",
            "Lingyao Li",
            "Tenglong Wang",
            "Xiaowu Dai",
            "Weining Shen",
            "Liwen Zhang"
        ],
        "title": "LightAgent: Production-level Open-source Agentic AI Framework",
        "abstract": "arXiv:2509.09292v1 Announce Type: new  Abstract: With the rapid advancement of large language models (LLMs), Multi-agent Systems (MAS) have achieved significant progress in various application scenarios. However, substantial challenges remain in designing versatile, robust, and efficient platforms for agent deployment. To address these limitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic framework, effectively resolving the trade-off between flexibility and simplicity found in existing frameworks. LightAgent integrates core functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while maintaining an extremely lightweight structure. As a fully open-source solution, it seamlessly integrates with mainstream chat platforms, enabling developers to easily build self-learning agents. We have released LightAgent at \\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}",
        "arxiv_id": "2509.09292",
        "ARXIVID": "2509.09292",
        "COMMENT": "Does not match any specific criteria but is relevant to multi-agent systems and agentic AI frameworks, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09272": {
        "authors": [
            "Vaibhav Chaudhary",
            "Neha Soni",
            "Narotam Singh",
            "Amita Kapoor"
        ],
        "title": "Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs",
        "abstract": "arXiv:2509.09272v1 Announce Type: new  Abstract: Knowledge graphs, a powerful tool for structuring information through relational triplets, have recently become the new front-runner in enhancing question-answering systems. While traditional Retrieval Augmented Generation (RAG) approaches are proficient in fact-based and local context-based extraction from concise texts, they encounter limitations when addressing the thematic and holistic understanding of complex, extensive texts, requiring a deeper analysis of both text and context. This paper presents a comprehensive technical comparative study of three different methodologies for constructing knowledge graph triplets and integrating them with Large Language Models (LLMs) for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all leveraging open source technologies. We evaluate the effectiveness, feasibility, and adaptability of these methods by analyzing their capabilities, state of development, and their impact on the performance of LLM-based question answering. Experimental results indicate that while OpenIE provides the most comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning abilities among the three. We conclude with a discussion on the strengths and limitations of each method and provide insights into future directions for improving knowledge graph-based question answering.",
        "arxiv_id": "2509.09272",
        "ARXIVID": "2509.09272",
        "COMMENT": "Does not match any specific criteria but is generally relevant to the integration of knowledge graphs and LLMs, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09066": {
        "authors": [
            "Haowei Yang",
            "Yushang Zhao",
            "Sitao Min",
            "Bo Su",
            "Chao Yao",
            "Wei Xu"
        ],
        "title": "Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users",
        "abstract": "arXiv:2509.09066v1 Announce Type: new  Abstract: The cold-start user issue further compromises the effectiveness of recommender systems in limiting access to the historical behavioral information. It is an effective pipeline to optimize instructional prompts on a few-shot large language model (LLM) used in recommender tasks. We introduce a context-conditioned prompt formulation method P(u,\\ Ds)\\ \\rightarrow\\ R\\widehat, where u is a cold-start user profile, Ds is a curated support set, and R\\widehat is the predicted ranked list of items. Based on systematic experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2, GPT-4), we provide empirical evidence that optimal exemplar injection and instruction structuring can significantly improve the precision@k and NDCG scores of such models in low-data settings. The pipeline uses token-level alignments and embedding space regularization with a greater semantic fidelity. Our findings not only show that timely composition is not merely syntactic but also functional as it is in direct control of attention scales and decoder conduct through inference. This paper shows that prompt-based adaptation may be considered one of the ways to address cold-start recommendation issues in LLM-based pipelines.",
        "arxiv_id": "2509.09066",
        "ARXIVID": "2509.09066",
        "COMMENT": "Does not match any specific criterion but is tangentially related to recommendation systems and prompt optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09006": {
        "authors": [
            "Samuel Felipe dos Santos",
            "Tiago Agostinho de Almeida",
            "Jurandy Almeida"
        ],
        "title": "E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting",
        "abstract": "arXiv:2509.09006v1 Announce Type: new  Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a labeled source to an unlabeled target domain without assuming any relationship between their label sets, requiring models to classify known samples while rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet) use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization (OEM). However, this strategy treats all classifiers equally, diluting the learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet), which integrates a dynamic weighting strategy to OEM. By leveraging the closed-set classifier's predictions, E-MLNet focuses adaptation on the most relevant class boundaries for each target sample, sharpening the distinction between known and unknown classes. We conduct extensive experiments on four challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The results demonstrate that E-MLNet achieves the highest average H-scores on VisDA and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet outperforms the strong MLNet baseline in the majority of individual adaptation tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of 31 in the Open-Set DA setting -- confirming the benefits of our focused adaptation strategy.",
        "arxiv_id": "2509.09006",
        "ARXIVID": "2509.09006",
        "COMMENT": "Does not match any specific criterion but is tangentially related to domain adaptation and classification tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.08926": {
        "authors": [
            "Waqar Ahmad",
            "Evan Murphy",
            "Vladimir A. Krylov"
        ],
        "title": "Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures",
        "abstract": "arXiv:2509.08926v1 Announce Type: new  Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed.The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: https://github.com/waqar3411/Beta-SOD",
        "arxiv_id": "2509.08926",
        "ARXIVID": "2509.08926",
        "COMMENT": "Does not match any specific criterion but is tangentially related to object re-identification and statistical modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09527": {
        "authors": [
            "Jian Zhu",
            "Xin Zou",
            "Xi Wang",
            "Ning Zhang",
            "Bian Wu",
            "Yao Yang",
            "Ying Zhou",
            "Lingfang Zeng",
            "Chang Tang",
            "Cheng Luo"
        ],
        "title": "Generative Diffusion Contrastive Network for Multi-View Clustering",
        "abstract": "arXiv:2509.09527v1 Announce Type: new  Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced under the influence of deep learning. By integrating heterogeneous data from multiple views, MVC enhances clustering analysis, making multi-view fusion critical to clustering performance. However, there is a problem of low-quality data in multi-view fusion. This problem primarily arises from two reasons: 1) Certain views are contaminated by noisy data. 2) Some views suffer from missing data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF) method to address this problem. SGDF leverages a multiple generative mechanism for the multi-view feature of each sample. It is robust to low-quality data. Building on SGDF, we further present the Generative Diffusion Contrastive Network (GDCN). Extensive experiments show that GDCN achieves the state-of-the-art results in deep MVC tasks. The source code is publicly available at https://github.com/HackerHyper/GDCN.",
        "arxiv_id": "2509.09527",
        "ARXIVID": "2509.09527",
        "COMMENT": "Does not match any specific criterion but is tangentially related to generative modeling and multi-view clustering.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09085": {
        "authors": [
            "Jifeng Shen",
            "Haibo Zhan",
            "Xin Zuo",
            "Heng Fan",
            "Xiaohui Yuan",
            "Jun Li",
            "Wankou Yang"
        ],
        "title": "IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection",
        "abstract": "arXiv:2509.09085v1 Announce Type: new  Abstract: Current multispectral object detection methods often retain extraneous background or noise during feature fusion, limiting perceptual performance.To address this, we propose an innovative feature fusion framework based on cross-modal feature contrastive and screening strategy, diverging from conventional approaches. The proposed method adaptively enhances salient structures by fusing object-aware complementary cross-modal features while suppressing shared background interference.Our solution centers on two novel, specially designed modules: the Mutual Feature Refinement Module (MFRM) and the Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and inter-modal feature representations by modeling their relationships, thereby improving cross-modal alignment and discriminative power.Inspired by feedback differential amplifiers, the DFFM dynamically computes inter-modal differential features as guidance signals and feeds them back to the MFRM, enabling adaptive fusion of complementary information while suppressing common-mode noise across modalities. To enable robust feature learning, the MFRM and DFFM are integrated into a unified framework, which is formally formulated as an Iterative Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion. IRDFusion enables high-quality cross-modal fusion by progressively amplifying salient relational signals through iterative feedback, while suppressing feature noise, leading to significant performance gains.In extensive experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves state-of-the-art performance and consistently outperforms existing methods across diverse challenging scenarios, demonstrating its robustness and effectiveness. Code will be available at https://github.com/61s61min/IRDFusion.git.",
        "arxiv_id": "2509.09085",
        "ARXIVID": "2509.09085",
        "COMMENT": "Does not match any specific criterion but is related to multispectral object detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09140": {
        "authors": [
            "Dylan Peek",
            "Matthew P. Skerritt",
            "Stephan Chalup"
        ],
        "title": "Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology",
        "abstract": "arXiv:2509.09140v1 Announce Type: new  Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer contrasting approaches to inferring topological structure from data. In this study, we examine the noise robustness of a supervised neural network trained to predict Betti numbers in 2D binary images. We compare an ANN approach against a PH pipeline based on cubical complexes and the Signed Euclidean Distance Transform (SEDT), which is a widely adopted strategy for noise-robust topological analysis. Using one synthetic and two real-world datasets, we show that ANNs can outperform this PH approach under noise, likely due to their capacity to learn contextual and geometric priors from training data. Though still emerging, the use of ANNs for topology estimation offers a compelling alternative to PH under structural noise.",
        "arxiv_id": "2509.09140",
        "ARXIVID": "2509.09140",
        "COMMENT": "Does not match any specific criterion but is tangentially related to computer vision and machine learning through topology estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09118": {
        "authors": [
            "Tianlu Zheng",
            "Yifan Zhang",
            "Xiang An",
            "Ziyong Feng",
            "Kaicheng Yang",
            "Qichuan Ding"
        ],
        "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval",
        "abstract": "arXiv:2509.09118v1 Announce Type: new  Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks.",
        "arxiv_id": "2509.09118",
        "ARXIVID": "2509.09118",
        "COMMENT": "Does not match any specific criterion but is relevant to vision-language integration and representation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09245": {
        "authors": [
            "Shuocheng Li",
            "Yihao Liu",
            "Silin Du",
            "Wenxuan Zeng",
            "Zhe Xu",
            "Mengyu Zhou",
            "Yeye He",
            "Haoyu Dong",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "title": "Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search",
        "abstract": "arXiv:2509.09245v1 Announce Type: new  Abstract: Large language models (LLMs) have shown great promise in automating data science workflows, but existing models still struggle with multi-step reasoning and tool use, which limits their effectiveness on complex data analysis tasks. To address this, we propose a scalable pipeline that extracts high-quality, tool-based data analysis tasks and their executable multi-step solutions from real-world Jupyter notebooks and associated data files. Using this pipeline, we introduce NbQA, a large-scale dataset of standardized task-solution pairs that reflect authentic tool-use patterns in practical data science scenarios. To further enhance multi-step reasoning, we present Jupiter, a framework that formulates data analysis as a search problem and applies Monte Carlo Tree Search (MCTS) to generate diverse solution trajectories for value model learning. During inference, Jupiter combines the value model and node visit counts to efficiently collect executable multi-step plans with minimal search steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench, respectively-matching or surpassing GPT-4o and advanced agent frameworks. Further evaluations demonstrate improved generalization and stronger tool-use reasoning across diverse multi-step reasoning tasks.",
        "arxiv_id": "2509.09245",
        "ARXIVID": "2509.09245",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning and data analysis workflows.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09298": {
        "authors": [
            "Oh-Tae Jang",
            "Min-Gon Cho",
            "Kyung-Tae Kim"
        ],
        "title": "Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion",
        "abstract": "arXiv:2509.09298v1 Announce Type: new  Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest but also complex background clutter, including terrain reflections and speckle noise. In many cases, such clutter exhibits intensity and patterns that resemble targets, leading models to extract entangled or spurious features. Such behavior undermines the ability to form clear target representations, regardless of the classifier. To address this challenge, we propose a novel object-centric learning (OCL) framework, named SlotSAR, that disentangles target representations from background clutter in SAR images without mask annotations. SlotSAR first extracts high-level semantic features from SARATR-X and low-level scattering features from the wavelet scattering network in order to obtain complementary multi-level representations for robust target characterization. We further present a multi-level slot attention module that integrates these low- and high-level features to enhance slot-wise representation distinctiveness, enabling effective OCL. Experimental results demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery by preserving structural details compared to existing OCL methods.",
        "arxiv_id": "2509.09298",
        "ARXIVID": "2509.09298",
        "COMMENT": "Does not match any specific criterion but is relevant to object-centric learning in SAR images, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09297": {
        "authors": [
            "Spyridon Loukovitis",
            "Anastasios Arsenos",
            "Vasileios Karampinis",
            "Athanasios Voulodimos"
        ],
        "title": "Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception",
        "abstract": "arXiv:2509.09297v1 Announce Type: new  Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object detection under real-world conditions. Traditional closed-set detectors degrade significantly under domain shifts and flight data corruption, posing risks to safety-critical applications. We propose a novel, model-agnostic open-set detection framework designed specifically for embedding-based detectors. The method explicitly handles unknown object rejection while maintaining robustness against corrupted flight data. It estimates semantic uncertainty via entropy modeling in the embedding space and incorporates spectral normalization and temperature scaling to enhance open-set discrimination. We validate our approach on the challenging AOT aerial benchmark and through extensive real-world flight tests. Comprehensive ablation studies demonstrate consistent improvements over baseline methods, achieving up to a 10\\% relative AUROC gain compared to standard YOLO-based detectors. Additionally, we show that background rejection further strengthens robustness without compromising detection accuracy, making our solution particularly well-suited for reliable UAV perception in dynamic air-to-air environments.",
        "arxiv_id": "2509.09297",
        "ARXIVID": "2509.09297",
        "COMMENT": "Does not match any specific criterion but is relevant to UAV perception and open-set detection, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09143": {
        "authors": [
            "Yuiko Uchida",
            "Ren Togo",
            "Keisuke Maeda",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "title": "Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation",
        "abstract": "arXiv:2509.09143v1 Announce Type: new  Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric for 3D scenes that explicitly focuses on \"objects,\" which are fundamental units of human visual perception. Existing metrics assess overall image quality, leading to discrepancies with human perception. Inspired by neuropsychological insights, we hypothesize that human recognition of 3D scenes fundamentally involves attention to individual objects. OSIM enables object-centric evaluations by leveraging an object detection model and its feature representations to quantify the \"objectness\" of each object in the scene. Our user study demonstrates that OSIM aligns more closely with human perception compared to existing metrics. We also analyze the characteristics of OSIM using various approaches. Moreover, we re-evaluate recent 3D reconstruction and generation models under a standardized experimental setup to clarify advancements in this field. The code is available at https://github.com/Objectness-Similarity/OSIM.",
        "arxiv_id": "2509.09143",
        "ARXIVID": "2509.09143",
        "COMMENT": "Does not match any specific criterion but is relevant to evaluation metrics for 3D scenes, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09448": {
        "authors": [
            "Minhyuk Kim",
            "Seungyoon Lee",
            "Heuiseok Lim"
        ],
        "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
        "abstract": "arXiv:2509.09448v1 Announce Type: new  Abstract: The approaches that guide Large Language Models (LLMs) to emulate human reasoning during response generation have emerged as an effective method for enabling them to solve complex problems in a step-by-step manner, thereby achieving superior performance. However, most existing approaches using few-shot prompts to generate responses heavily depend on the provided examples, limiting the utilization of the model's inherent reasoning capabilities. Moreover, constructing task-specific few-shot prompts is often costly and may lead to inconsistencies across different tasks. In this work, we introduce Template-Oriented Reasoning (TORSO), which elicits the model to utilize internal reasoning abilities to generate proper responses across various tasks without the need for manually crafted few-shot examples. Our experimental results demonstrate that TORSO achieves strong performance on diverse LLMs benchmarks with reasonable rationales.",
        "arxiv_id": "2509.09448",
        "ARXIVID": "2509.09448",
        "COMMENT": "Does not match any specific criterion but is relevant to reasoning in large language models, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09672": {
        "authors": [
            "Artem Lukoianov",
            "Chenyang Yuan",
            "Justin Solomon",
            "Vincent Sitzmann"
        ],
        "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
        "abstract": "arXiv:2509.09672v1 Announce Type: new  Abstract: Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.",
        "arxiv_id": "2509.09672",
        "ARXIVID": "2509.09672",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and image statistics, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.09558": {
        "authors": [
            "Akshit Achara",
            "Esther Puyol Anton",
            "Alexander Hammers",
            "Andrew P. King"
        ],
        "title": "Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification",
        "abstract": "arXiv:2509.09558v1 Announce Type: new  Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at https://github.com/acharaakshit/ShortMR",
        "arxiv_id": "2509.09558",
        "ARXIVID": "2509.09558",
        "COMMENT": "Does not match any specific criterion but is tangentially related to the general interest area of machine learning and bias in models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}