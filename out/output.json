{
    "2509.15293": {
        "authors": [
            "Dinura Dissanayake",
            "Ahmed Heakl",
            "Omkar Thawakar",
            "Noor Ahsan",
            "Ritesh Thawkar",
            "Ketan More",
            "Jean Lahoud",
            "Rao Anwer",
            "Hisham Cholakkal",
            "Ivan Laptev",
            "Fahad Shahbaz Khan",
            "Salman Khan"
        ],
        "title": "How Good are Foundation Models in Step-by-Step Embodied Reasoning?",
        "abstract": "arXiv:2509.15293v1 Announce Type: new  Abstract: Embodied agents operating in the physical world must make decisions that are not only effective but also safe, spatially coherent, and grounded in context. While recent advances in large multimodal models (LMMs) have shown promising capabilities in visual understanding and language generation, their ability to perform structured reasoning for real-world embodied tasks remains underexplored. In this work, we aim to understand how well foundation models can perform step-by-step reasoning in embodied environments. To this end, we propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to evaluate the reasoning capabilities of LMMs in complex embodied decision-making scenarios. Our benchmark spans a diverse set of tasks that require agents to interpret multimodal observations, reason about physical constraints and safety, and generate valid next actions in natural language. We present (i) a large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation framework that disentangles perceptual grounding from action reasoning, and (iii) empirical analysis of several leading LMMs under this setting. Our benchmark includes over 1.1k samples with detailed step-by-step reasoning across 10 tasks and 8 embodiments, covering three different robot types. Our results highlight both the potential and current limitations of LMMs in embodied reasoning, pointing towards key challenges and opportunities for future research in robot intelligence. Our data and code will be made publicly available.",
        "arxiv_id": "2509.15293",
        "ARXIVID": "2509.15293",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a benchmark for step-by-step embodied reasoning and evaluates large multimodal models in embodied environments.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2509.15693": {
        "authors": [
            "Cristian Sbrolli",
            "Matteo Matteucci"
        ],
        "title": "SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions",
        "abstract": "arXiv:2509.15693v1 Announce Type: new  Abstract: The whole is greater than the sum of its parts-even in 3D-text contrastive learning. We introduce SceneForge, a novel framework that enhances contrastive alignment between 3D point clouds and text through structured multi-object scene compositions. SceneForge leverages individual 3D shapes to construct multi-object scenes with explicit spatial relations, pairing them with coherent multi-object descriptions refined by a large language model. By augmenting contrastive training with these structured, compositional samples, SceneForge effectively addresses the scarcity of large-scale 3D-text datasets, significantly enriching data complexity and diversity. We systematically investigate critical design elements, such as the optimal number of objects per scene, the proportion of compositional samples in training batches, and scene construction strategies. Extensive experiments demonstrate that SceneForge delivers substantial performance gains across multiple tasks, including zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet, as well as few-shot part segmentation on ShapeNetPart. SceneForge's compositional augmentations are model-agnostic, consistently improving performance across multiple encoder architectures. Moreover, SceneForge improves 3D visual question answering on ScanQA, generalizes robustly to retrieval scenarios with increasing scene complexity, and showcases spatial reasoning capabilities by adapting spatial configurations to align precisely with textual instructions.",
        "arxiv_id": "2509.15693",
        "ARXIVID": "2509.15693",
        "COMMENT": "Matches criterion 1 as it presents a novel framework (SceneForge) for enhancing spatial reasoning in 3D-text alignment using structured scene compositions.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.15333": {
        "authors": [
            "Yulin Wang",
            "Yang Yue",
            "Yang Yue",
            "Huanqian Wang",
            "Haojun Jiang",
            "Yizeng Han",
            "Zanlin Ni",
            "Yifan Pu",
            "Minglei Shi",
            "Rui Lu",
            "Qisen Yang",
            "Andrew Zhao",
            "Zhuofan Xia",
            "Shiji Song",
            "Gao Huang"
        ],
        "title": "Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception",
        "abstract": "arXiv:2509.15333v1 Announce Type: new  Abstract: Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.",
        "arxiv_id": "2509.15333",
        "ARXIVID": "2509.15333",
        "COMMENT": "Matches criteria 3 as it introduces AdaptiveNN, a framework for active and adaptive vision models, relevant to embodied AI and efficient visual perception.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2509.16197": {
        "authors": [
            "Yanghao Li",
            "Rui Qian",
            "Bowen Pan",
            "Haotian Zhang",
            "Haoshuo Huang",
            "Bowen Zhang",
            "Jialing Tong",
            "Haoxuan You",
            "Xianzhi Du",
            "Zhe Gan",
            "Hyunjik Kim",
            "Chao Jia",
            "Zhenbang Wang",
            "Yinfei Yang",
            "Mingfei Gao",
            "Zi-Yi Dou",
            "Wenze Hu",
            "Chang Gao",
            "Dongxu Li",
            "Philipp Dufter",
            "Zirui Wang",
            "Guoli Yin",
            "Zhengdong Zhang",
            "Chen Chen",
            "Yang Zhao",
            "Ruoming Pang",
            "Zhifeng Chen"
        ],
        "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer",
        "abstract": "arXiv:2509.16197v1 Announce Type: new  Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.",
        "arxiv_id": "2509.16197",
        "ARXIVID": "2509.16197",
        "COMMENT": "This paper matches criterion 2 as it introduces a unified multimodal LLM architecture for vision-language tasks, focusing on both understanding and generation.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2509.15566": {
        "authors": [
            "Shaojie Zhang",
            "Ruoceng Zhang",
            "Pei Fu",
            "Shaokang Wang",
            "Jiahui Yang",
            "Xin Du",
            "Shiqi Cui",
            "Bin Qin",
            "Ying Huang",
            "Zhenbo Luo",
            "Jian Luan"
        ],
        "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
        "abstract": "arXiv:2509.15566v1 Announce Type: new  Abstract: In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.",
        "arxiv_id": "2509.15566",
        "ARXIVID": "2509.15566",
        "COMMENT": "Matches criteria 3 as it introduces a biologically inspired framework for GUI agents, focusing on embodied reasoning and interaction.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.16054": {
        "authors": [
            "Jihua Peng",
            "Qianxiong Xu",
            "Yichen Liu",
            "Chenxi Liu",
            "Cheng Long",
            "Rui Zhao",
            "Ziyue Li"
        ],
        "title": "Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model",
        "abstract": "arXiv:2509.16054v1 Announce Type: new  Abstract: Group activity detection (GAD) aims to simultaneously identify group members and categorize their collective activities within video sequences. Existing deep learning-based methods develop specialized architectures (e.g., transformer networks) to model the dynamics of individual roles and semantic dependencies between individuals and groups. However, they rely solely on implicit pattern recognition from visual features and struggle with contextual reasoning and explainability. In this work, we propose LIR-GAD, a novel framework of language-instructed reasoning for GAD via Multimodal Large Language Model (MLLM). Our approach expand the original vocabulary of MLLM by introducing an activity-level  token and multiple cluster-specific  tokens. We process video frames alongside two specially designed tokens and language instructions, which are then integrated into the MLLM. The pretrained commonsense knowledge embedded in the MLLM enables the  token and  tokens to effectively capture the semantic information of collective activities and learn distinct representational features of different groups, respectively. Also, we introduce a multi-label classification loss to further enhance the  token's ability to learn discriminative semantic representations. Then, we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates MLLM's hidden embeddings corresponding to the designed tokens with visual features, significantly enhancing the performance of GAD. Both quantitative and qualitative experiments demonstrate the superior performance of our proposed method in GAD taks.",
        "arxiv_id": "2509.16054",
        "ARXIVID": "2509.16054",
        "COMMENT": "Matches criteria 2 and 6 as it explores a Multimodal Large Language Model (MLLM) for group activity detection in videos, integrating vision-language reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15435": {
        "authors": [
            "Chung-En Johnny Yu (Neil)",
            "Hsuan-Chih (Neil)",
            "Chen",
            "Brian Jalaian",
            "Nathaniel D. Bastian"
        ],
        "title": "ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models",
        "abstract": "arXiv:2509.15435v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities but remain vulnerable to hallucinations from intrinsic errors and adversarial attacks from external exploitations, limiting their reliability in real-world applications. We present ORCA, an agentic reasoning framework that improves the factual accuracy and adversarial robustness of pretrained LVLMs through test-time structured inference reasoning with a suite of small vision models (less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act loop, querying multiple visual tools with evidential questions, validating cross-model inconsistencies, and refining predictions iteratively without access to model internals or retraining. ORCA also stores intermediate reasoning traces, which supports auditable decision-making. Though designed primarily to mitigate object-level hallucinations, ORCA also exhibits emergent adversarial robustness without requiring adversarial training or defense mechanisms. We evaluate ORCA across three settings: (1) clean images on hallucination benchmarks, (2) adversarially perturbed images without defense, and (3) adversarially perturbed images with defense applied. On the POPE hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\\% to +40.67\\% across different subsets. Under adversarial perturbations on POPE, ORCA achieves an average accuracy gain of +20.11\\% across LVLMs. When combined with defense techniques on adversarially perturbed AMBER images, ORCA further improves standalone LVLM performance, with gains ranging from +1.20\\% to +48.00\\% across evaluation metrics. These results demonstrate that ORCA offers a promising path toward building more reliable and robust multimodal systems.",
        "arxiv_id": "2509.15435",
        "ARXIVID": "2509.15435",
        "COMMENT": "Matches criterion 2 as it focuses on improving hallucination and robustness in large vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15704": {
        "authors": [
            "Yuxuan Liang",
            "Xu Li",
            "Xiaolei Chen",
            "Yi Zheng",
            "Haotian Chen",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "title": "Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance",
        "abstract": "arXiv:2509.15704v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have significantly advanced multimodal understanding but still struggle with efficiently processing high-resolution images. Recent approaches partition high-resolution images into multiple sub-images, dramatically increasing the number of visual tokens and causing exponential computational overhead during inference. To address these limitations, we propose a training-free token pruning strategy, Pyramid Token Pruning (PTP), that integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided importance. Inspired by human visual attention mechanisms, PTP selectively retains more tokens from visually salient regions and further leverages textual instructions to pinpoint tokens most relevant to specific multimodal tasks. Extensive experiments across 13 diverse benchmarks demonstrate that our method substantially reduces computational overhead and inference latency with minimal performance loss.",
        "arxiv_id": "2509.15704",
        "ARXIVID": "2509.15704",
        "COMMENT": "Matches criterion 5 as it proposes a token pruning strategy for efficient processing in large vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15883": {
        "authors": [
            "Xiaosheng Long",
            "Hanyu Wang",
            "Zhentao Song",
            "Kun Luo",
            "Hongde Liu"
        ],
        "title": "RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning",
        "abstract": "arXiv:2509.15883v1 Announce Type: new  Abstract: Recent retrieval-augmented image captioning methods incorporate external knowledge to compensate for the limitations in comprehending complex scenes. However, current approaches face challenges in relation modeling: (1) the representation of semantic prompts is too coarse-grained to capture fine-grained relationships; (2) these methods lack explicit modeling of image objects and their semantic relationships. To address these limitations, we propose RACap, a relation-aware retrieval-augmented model for image captioning, which not only mines structured relation semantics from retrieval captions, but also identifies heterogeneous objects from the image. RACap effectively retrieves structured relation features that contain heterogeneous visual information to enhance the semantic consistency and relational expressiveness. Experimental results show that RACap, with only 10.8M trainable parameters, achieves superior performance compared to previous lightweight captioning models.",
        "arxiv_id": "2509.15883",
        "ARXIVID": "2509.15883",
        "COMMENT": "Matches criterion 2 as it explores a novel retrieval-augmented image captioning model, which integrates vision and language.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2509.15250": {
        "authors": [
            "Wenda Qin",
            "Andrea Burns",
            "Bryan A. Plummer",
            "Margrit Betke"
        ],
        "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning",
        "abstract": "arXiv:2509.15250v1 Announce Type: new  Abstract: Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.",
        "arxiv_id": "2509.15250",
        "ARXIVID": "2509.15250",
        "COMMENT": "Matches criterion 3 as it proposes a novel method (NAP) for improving efficiency in Vision-and-Language Navigation (VLN) tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15886": {
        "authors": [
            "Paul Julius K\\\"uhn",
            "Duc Anh Nguyen",
            "Arjan Kuijper",
            "Holger Graf",
            "Dieter Fellner",
            "Saptarshi Neil Sinha"
        ],
        "title": "RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation",
        "abstract": "arXiv:2509.15886v1 Announce Type: new  Abstract: Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.",
        "arxiv_id": "2509.15886",
        "ARXIVID": "2509.15886",
        "COMMENT": "Matches criterion 4 as it explores the application of Visual Foundation Models (VFMs) for LiDAR segmentation, adapting them for 3D perception tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15695": {
        "authors": [
            "Zhaoyang Li",
            "Zhan Ling",
            "Yuchen Zhou",
            "Hao Su"
        ],
        "title": "ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models",
        "abstract": "arXiv:2509.15695v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image caption, visual question answering, and robotics by integrating visual and textual information. However, they remain prone to errors in incongruous contexts, where objects appear unexpectedly or are absent when contextually expected. This leads to two key recognition failures: object misidentification and hallucination. To systematically examine this issue, we introduce the Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark that evaluates LVLMs in scenarios where object-context relationships deviate from expectations. ORIC employs two key strategies: (1) LLM-guided sampling, which identifies objects that are present but contextually incongruous, and (2) CLIP-guided sampling, which detects plausible yet nonexistent objects that are likely to be hallucinated, thereby creating an incongruous context. Evaluating 18 LVLMs and two open-vocabulary detection models, our results reveal significant recognition gaps, underscoring the challenges posed by contextual incongruity. This work provides critical insights into LVLMs' limitations and encourages further research on context-aware object recognition.",
        "arxiv_id": "2509.15695",
        "ARXIVID": "2509.15695",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (ORIC) for evaluating LVLMs in object recognition under incongruous contexts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15459": {
        "authors": [
            "Yiyi Liu",
            "Chunyang Liu",
            "Weiqin Jiao",
            "Bojian Wu",
            "Fashuai Li",
            "Biao Xiong"
        ],
        "title": "CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction",
        "abstract": "arXiv:2509.15459v1 Announce Type: new  Abstract: We present \\textbf{CAGE} (\\textit{Continuity-Aware edGE}) network, a \\textcolor{red}{robust} framework for reconstructing vector floorplans directly from point-cloud density maps. Traditional corner-based polygon representations are highly sensitive to noise and incomplete observations, often resulting in fragmented or implausible layouts. Recent line grouping methods leverage structural cues to improve robustness but still struggle to recover fine geometric details. To address these limitations, we propose a \\textit{native} edge-centric formulation, modeling each wall segment as a directed, geometrically continuous edge. This representation enables inference of coherent floorplan structures, ensuring watertight, topologically valid room boundaries while improving robustness and reducing artifacts. Towards this design, we develop a dual-query transformer decoder that integrates perturbed and latent queries within a denoising framework, which not only stabilizes optimization but also accelerates convergence. Extensive experiments on Structured3D and SceneCAD show that \\textbf{CAGE} achieves state-of-the-art performance, with F1 scores of 99.1\\% (rooms), 91.7\\% (corners), and 89.3\\% (angles). The method also demonstrates strong cross-dataset generalization, underscoring the efficacy of our architectural innovations. Code and pretrained models will be released upon acceptance.",
        "arxiv_id": "2509.15459",
        "ARXIVID": "2509.15459",
        "COMMENT": "Matches criteria 1 as it introduces a novel edge-centric framework for reconstructing floorplans, which involves spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15785": {
        "authors": [
            "Runjie Shao",
            "Boyu Diao",
            "Zijia An",
            "Ruiqi Liu",
            "Yongjun Xu"
        ],
        "title": "CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices",
        "abstract": "arXiv:2509.15785v1 Announce Type: new  Abstract: To meet the demands of applications like robotics and autonomous driving that require real-time responses to dynamic environments, efficient continual learning methods suitable for edge devices have attracted increasing attention. In this transition, using frozen pretrained models with prompts has become a mainstream strategy to combat catastrophic forgetting. However, this approach introduces a new critical bottleneck: plasticity loss, where the model's ability to learn new knowledge diminishes due to the frozen backbone and the limited capacity of prompt parameters. We argue that the reduction in plasticity stems from a lack of update vitality in underutilized parameters during the training process. To this end, we propose the Continual Backpropagation Prompt Network (CBPNet), an effective and parameter efficient framework designed to restore the model's learning vitality. We innovatively integrate an Efficient CBP Block that counteracts plasticity decay by adaptively reinitializing these underutilized parameters. Experimental results on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks. On Split CIFAR-100, it improves average accuracy by over 1% against a strong baseline, and on the more challenging Split ImageNet-R, it achieves a state of the art accuracy of 69.41%. This is accomplished by training additional parameters that constitute less than 0.2% of the backbone's size, validating our approach.",
        "arxiv_id": "2509.15785",
        "ARXIVID": "2509.15785",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for continual learning on edge devices, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15602": {
        "authors": [
            "Zhongyuan Bao",
            "Lejun Zhang"
        ],
        "title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?",
        "abstract": "arXiv:2509.15602v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks at rally and stroke levels and includes 2,500 human-verified questions. Evaluating 16 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results reveal substantial shortcomings and yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.",
        "arxiv_id": "2509.15602",
        "ARXIVID": "2509.15602",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for tennis video understanding and evaluates multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15882": {
        "authors": [
            "Xingmei Wang",
            "Xiaoyu Hu",
            "Chengkai Huang",
            "Ziyan Zeng",
            "Guohao Nie",
            "Quan Z. Sheng",
            "Lina Yao"
        ],
        "title": "Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration",
        "abstract": "arXiv:2509.15882v1 Announce Type: new  Abstract: Bridging 2D and 3D sensor modalities is critical for robust perception in autonomous systems. However, image-to-point cloud (I2P) registration remains challenging due to the semantic-geometric gap between texture-rich but depth-ambiguous images and sparse yet metrically precise point clouds, as well as the tendency of existing methods to converge to local optima. To overcome these limitations, we introduce CrossI2P, a self-supervised framework that unifies cross-modal learning and two-stage registration in a single end-to-end pipeline. First, we learn a geometric-semantic fused embedding space via dual-path contrastive learning, enabling annotation-free, bidirectional alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine registration paradigm: a global stage establishes superpoint-superpixel correspondences through joint intra-modal context and cross-modal interaction modeling, followed by a geometry-constrained point-level refinement for precise registration. Third, we employ a dynamic training mechanism with gradient normalization to balance losses for feature alignment, correspondence refinement, and pose estimation. Extensive experiments demonstrate that CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry benchmark and by 37.9% on nuScenes, significantly improving both accuracy and robustness.",
        "arxiv_id": "2509.15882",
        "ARXIVID": "2509.15882",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its focus on image-to-point cloud registration and spatial reasoning improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2509.15243": {
        "authors": [
            "Muhammad Imran",
            "Yugyung Lee"
        ],
        "title": "Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models",
        "abstract": "arXiv:2509.15243v1 Announce Type: new  Abstract: Recent advances in vision-language models have significantly expanded the frontiers of automated image analysis. However, applying these models in safety-critical contexts remains challenging due to the complex relationships between objects, subtle visual cues, and the heightened demand for transparency and reliability. This paper presents the Multi-Modal Explainable Learning (MMEL) framework, designed to enhance the interpretability of vision-language models while maintaining high performance. Building upon prior work in gradient-based explanations for transformer architectures (Grad-eclip), MMEL introduces a novel Hierarchical Semantic Relationship Module that enhances model interpretability through multi-scale feature processing, adaptive attention weighting, and cross-modal alignment. Our approach processes features at multiple semantic levels to capture relationships between image regions at different granularities, applying learnable layer-specific weights to balance contributions across the model's depth. This results in more comprehensive visual explanations that highlight both primary objects and their contextual relationships with improved precision. Through extensive experiments on standard datasets, we demonstrate that by incorporating semantic relationship information into gradient-based attribution maps, MMEL produces more focused and contextually aware visualizations that better reflect how vision-language models process complex scenes. The MMEL framework generalizes across various domains, offering valuable insights into model decisions for applications requiring high interpretability and reliability.",
        "arxiv_id": "2509.15243",
        "ARXIVID": "2509.15243",
        "COMMENT": "Matches criteria 2 as it focuses on enhancing interpretability in vision-language models through a novel framework.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.16017": {
        "authors": [
            "Meng Yang",
            "Fan Fan",
            "Zizhuo Li",
            "Songchu Deng",
            "Yong Ma",
            "Jiayi Ma"
        ],
        "title": "DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching",
        "abstract": "arXiv:2509.16017v1 Announce Type: new  Abstract: Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis. However, the significant appearance differences between modalities make this task challenging. Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios. Vision Foundation Model (VFM), trained on large-scale data, yields generalizable and robust feature representations adapted to data and tasks of various modalities, including multimodal matching. Thus, we propose DistillMatch, a multimodal image matching method using knowledge distillation from VFM. DistillMatch employs knowledge distillation to build a lightweight student model that extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to assist matching across modalities. To retain modality-specific information, it extracts and injects modality category information into the other modality's features, which enhances the model's understanding of cross-modal correlations. Furthermore, we design V2I-GAN to boost the model's generalization by translating visible to pseudo-infrared images for data augmentation. Experiments show that DistillMatch outperforms existing algorithms on public datasets.",
        "arxiv_id": "2509.16017",
        "ARXIVID": "2509.16017",
        "COMMENT": "This paper matches criterion 4 as it leverages vision foundation models for multimodal image matching, focusing on robust feature extraction.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.15751": {
        "authors": [
            "Zhengyang Yu",
            "Arthur Aubret",
            "Chen Yu",
            "Jochen Triesch"
        ],
        "title": "Simulated Cortical Magnification Supports Self-Supervised Object Learning",
        "abstract": "arXiv:2509.15751v1 Announce Type: new  Abstract: Recent self-supervised learning models simulate the development of semantic object representations by training on visual experience similar to that of toddlers. However, these models ignore the foveated nature of human vision with high/low resolution in the center/periphery of the visual field. Here, we investigate the role of this varying resolution in the development of object representations. We leverage two datasets of egocentric videos that capture the visual experience of humans during interactions with objects. We apply models of human foveation and cortical magnification to modify these inputs, such that the visual content becomes less distinct towards the periphery. The resulting sequences are used to train two bio-inspired self-supervised learning models that implement a time-based learning objective. Our results show that modeling aspects of foveated vision improves the quality of the learned object representations in this setting. Our analysis suggests that this improvement comes from making objects appear bigger and inducing a better trade-off between central and peripheral visual information. Overall, this work takes a step towards making models of humans' learning of visual representations more realistic and performant.",
        "arxiv_id": "2509.15751",
        "ARXIVID": "2509.15751",
        "COMMENT": "Matches criterion 1 as it investigates spatial intelligence through foveated vision in self-supervised object learning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.15750": {
        "authors": [
            "Han Ye",
            "Haofu Wang",
            "Yunchi Zhang",
            "Jiangjian Xiao",
            "Yuqiang Jin",
            "Jinyuan Liu",
            "Wen-An Zhang",
            "Uladzislau Sychou",
            "Alexander Tuzikov",
            "Vladislav Sobolevskii",
            "Valerii Zakharov",
            "Boris Sokolov",
            "Minglei Fu"
        ],
        "title": "FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion",
        "abstract": "arXiv:2509.15750v1 Announce Type: new  Abstract: Reconstructing building floor plans from point cloud data is key for indoor navigation, BIM, and precise measurements. Traditional methods like geometric algorithms and Mask R-CNN-based deep learning often face issues with noise, limited generalization, and loss of geometric details. We propose FloorSAM, a framework that integrates point cloud density maps with the Segment Anything Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using grid-based filtering, adaptive resolution projection, and image enhancement, we create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for precise room segmentation, improving reconstruction across diverse layouts. Room masks are generated via adaptive prompt points and multistage filtering, followed by joint mask and point cloud analysis for contour extraction and regularization. This produces accurate floor plans and recovers room topological relationships. Tests on Giblayout and ISPRS datasets show better accuracy, recall, and robustness than traditional methods, especially in noisy and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.",
        "arxiv_id": "2509.15750",
        "ARXIVID": "2509.15750",
        "COMMENT": "Matches criterion 1 as it focuses on spatial intelligence and embodied agents through floor plan reconstruction using LiDAR data.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2509.15578": {
        "authors": [
            "Shanghong Li",
            "Chiam Wen Qi Ruth",
            "Hong Xu",
            "Fang Liu"
        ],
        "title": "Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion",
        "abstract": "arXiv:2509.15578v1 Announce Type: new  Abstract: The rapid proliferation of short video platforms has necessitated advanced methods for detecting fake news. This need arises from the widespread influence and ease of sharing misinformation, which can lead to significant societal harm. Current methods often struggle with the dynamic and multimodal nature of short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel multimodal framework that integrates video, audio, and text data to evaluate the authenticity of short video content. HFN introduces a Decision Network that dynamically adjusts modality weights during inference and a Weighted Multi-Modal Feature Fusion module to ensure robust performance even with incomplete data. Additionally, we contribute a comprehensive dataset VESV (VEracity on Short Videos) specifically designed for short video fake news detection. Experiments conducted on the FakeTT and newly collected VESV datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over state-of-the-art methods. This work establishes a robust solution capable of effectively identifying fake news in the complex landscape of short video platforms, paving the way for more reliable and comprehensive approaches in combating misinformation.",
        "arxiv_id": "2509.15578",
        "ARXIVID": "2509.15578",
        "COMMENT": "Matches criterion 5 as it integrates multimodal data (video, audio, text) for fake news detection in short videos, showcasing techniques combining vision and language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16132": {
        "authors": [
            "Carter Sifferman",
            "Yiquan Li",
            "Yiming Li",
            "Fangzhou Mu",
            "Michael Gleicher",
            "Mohit Gupta",
            "Yin Li"
        ],
        "title": "Recovering Parametric Scenes from Very Few Time-of-Flight Pixels",
        "abstract": "arXiv:2509.16132v1 Announce Type: new  Abstract: We aim to recover the geometry of 3D parametric scenes using very few depth measurements from low-cost, commercially available time-of-flight sensors. These sensors offer very low spatial resolution (i.e., a single pixel), but image a wide field-of-view per pixel and capture detailed time-of-flight data in the form of time-resolved photon counts. This time-of-flight data encodes rich scene information and thus enables recovery of simple scenes from sparse measurements. We investigate the feasibility of using a distributed set of few measurements (e.g., as few as 15 pixels) to recover the geometry of simple parametric scenes with a strong prior, such as estimating the 6D pose of a known object. To achieve this, we design a method that utilizes both feed-forward prediction to infer scene parameters, and differentiable rendering within an analysis-by-synthesis framework to refine the scene parameter estimate. We develop hardware prototypes and demonstrate that our method effectively recovers object pose given an untextured 3D model in both simulations and controlled real-world captures, and show promising initial results for other parametric scenes. We additionally conduct experiments to explore the limits and capabilities of our imaging solution.",
        "arxiv_id": "2509.16132",
        "ARXIVID": "2509.16132",
        "COMMENT": "Matches criteria 4 as it explores recovering 3D parametric scenes using low-cost sensors, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16163": {
        "authors": [
            "Het Patel",
            "Muzammil Allie",
            "Qian Zhang",
            "Jia Chen",
            "Evangelos E. Papalexakis"
        ],
        "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
        "abstract": "arXiv:2509.16163v1 Announce Type: new  Abstract: Vision language models (VLMs) excel in multimodal understanding but are prone to adversarial attacks. Existing defenses often demand costly retraining or significant architecture changes. We introduce a lightweight defense using tensor decomposition suitable for any pre-trained VLM, requiring no retraining. By decomposing and reconstructing vision encoder representations, it filters adversarial noise while preserving meaning. Experiments with CLIP on COCO and Flickr30K show improved robustness. On Flickr30K, it restores 12.3\\% performance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On COCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%. Analysis shows Tensor Train decomposition with low rank (8-32) and low residual strength ($\\alpha=0.1-0.2$) is optimal. This method is a practical, plug-and-play solution with minimal overhead for existing VLMs.",
        "arxiv_id": "2509.16163",
        "ARXIVID": "2509.16163",
        "COMMENT": "Matches criteria 2 as it proposes a tensor decomposition method to improve robustness in vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.16170": {
        "authors": [
            "Xiaoqi Zhao",
            "Youwei Pang",
            "Chenyang Yu",
            "Lihe Zhang",
            "Huchuan Lu",
            "Shijian Lu",
            "Georges El Fakhri",
            "Xiaofeng Liu"
        ],
        "title": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation",
        "abstract": "arXiv:2509.16170v1 Announce Type: new  Abstract: Multi-modal image segmentation faces real-world deployment challenges from incomplete/corrupted modalities degrading performance. While existing methods address training-inference modality gaps via specialized per-combination models, they introduce high deployment costs by requiring exhaustive model subsets and model-modality matching. In this work, we propose a unified modality-relax segmentation network (UniMRSeg) through hierarchical self-supervised compensation (HSSC). Our approach hierarchically bridges representation gaps between complete and incomplete modalities across input, feature and output levels. % First, we adopt modality reconstruction with the hybrid shuffled-masking augmentation, encouraging the model to learn the intrinsic modality characteristics and generate meaningful representations for missing modalities through cross-modal fusion. % Next, modality-invariant contrastive learning implicitly compensates the feature space distance among incomplete-complete modality pairs. Furthermore, the proposed lightweight reverse attention adapter explicitly compensates for the weak perceptual semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid consistency constraint to ensure stable prediction under all modality combinations without large performance fluctuations. Without bells and whistles, UniMRSeg significantly outperforms the state-of-the-art methods under diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D semantic segmentation, RGB-D/T salient object segmentation. The code will be released at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.",
        "arxiv_id": "2509.16170",
        "ARXIVID": "2509.16170",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes a unified segmentation network for multi-modal image segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15638": {
        "authors": [
            "Tong Wang",
            "Xingyue Zhao",
            "Linghao Zhuang",
            "Haoyu Zhao",
            "Jiayi Yin",
            "Yuyang He",
            "Gang Yu",
            "Bo Lin"
        ],
        "title": "pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation",
        "abstract": "arXiv:2509.15638v1 Announce Type: new  Abstract: Medical image segmentation is crucial for computer-aided diagnosis, yet privacy constraints hinder data sharing across institutions. Federated learning addresses this limitation, but existing approaches often rely on lightweight architectures that struggle with complex, heterogeneous data. Recently, the Segment Anything Model (SAM) has shown outstanding segmentation capabilities; however, its massive encoder poses significant challenges in federated settings. In this work, we present the first personalized federated SAM framework tailored for heterogeneous data scenarios in medical image segmentation. Our framework integrates two key innovations: (1) a personalized strategy that aggregates only the global parameters to capture cross-client commonalities while retaining the designed L-MoE (Localized Mixture-of-Experts) component to preserve domain-specific features; and (2) a decoupled global-local fine-tuning mechanism that leverages a teacher-student paradigm via knowledge distillation to bridge the gap between the global shared model and the personalized local models, thereby mitigating overgeneralization. Extensive experiments on two public datasets validate that our approach significantly improves segmentation performance, achieves robust cross-domain adaptation, and reduces communication overhead.",
        "arxiv_id": "2509.15638",
        "ARXIVID": "2509.15638",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it adapts the Segment Anything Model (SAM) for medical image segmentation in federated learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15795": {
        "authors": [
            "Tianyang Wang",
            "Xi Xiao",
            "Gaofei Chen",
            "Hanzhang Chi",
            "Qi Zhang",
            "Guo Cheng",
            "Yingrui Ji"
        ],
        "title": "TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation",
        "abstract": "arXiv:2509.15795v1 Announce Type: new  Abstract: Segment Anything Model (SAM) has demonstrated impressive zero-shot segmentation capabilities across natural image domains, but it struggles to generalize to the unique challenges of remote sensing data, such as complex terrain, multi-scale objects, and temporal dynamics. In this paper, we introduce TASAM, a terrain and temporally-aware extension of SAM designed specifically for high-resolution remote sensing image segmentation. TASAM integrates three lightweight yet effective modules: a terrain-aware adapter that injects elevation priors, a temporal prompt generator that captures land-cover changes over time, and a multi-scale fusion strategy that enhances fine-grained object delineation. Without retraining the SAM backbone, our approach achieves substantial performance gains across three remote sensing benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and task-specific models with minimal computational overhead. Our results highlight the value of domain-adaptive augmentation for foundation models and offer a scalable path toward more robust geospatial segmentation.",
        "arxiv_id": "2509.15795",
        "ARXIVID": "2509.15795",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it extends the Segment Anything Model (SAM) for remote sensing segmentation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2509.15470": {
        "authors": [
            "Thomas Z. Li",
            "Aravind R. Krishnan",
            "Lianrui Zuo",
            "John M. Still",
            "Kim L. Sandler",
            "Fabien Maldonado",
            "Thomas A. Lasko",
            "Bennett A. Landman"
        ],
        "title": "Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture",
        "abstract": "arXiv:2509.15470v1 Announce Type: new  Abstract: The development of multimodal models for pulmonary nodule diagnosis is limited by the scarcity of labeled data and the tendency for these models to overfit on the training distribution. In this work, we leverage self-supervised learning from longitudinal and multimodal archives to address these challenges. We curate an unlabeled set of patients with CT scans and linked electronic health records from our home institution to power joint embedding predictive architecture (JEPA) pretraining. After supervised finetuning, we show that our approach outperforms an unregularized multimodal model and imaging-only model in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC), but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC). We develop a synthetic environment that characterizes the context in which JEPA may underperform. This work innovates an approach that leverages unlabeled multimodal medical archives to improve predictive models and demonstrates its advantages and limitations in pulmonary nodule diagnosis.",
        "arxiv_id": "2509.15470",
        "ARXIVID": "2509.15470",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores multimodal joint-embedding predictive architectures for medical imaging.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2509.16141": {
        "authors": [
            "Vatsal Malaviya",
            "Agneet Chatterjee",
            "Maitreya Patel",
            "Yezhou Yang",
            "Chitta Baral"
        ],
        "title": "AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models",
        "abstract": "arXiv:2509.16141v1 Announce Type: new  Abstract: Text-to-Image (T2I) models have recently achieved remarkable success in generating images from textual descriptions. However, challenges still persist in accurately rendering complex scenes where actions and interactions form the primary semantic focus. Our key observation in this work is that T2I models frequently struggle to capture nuanced and often implicit attributes inherent in action depiction, leading to generating images that lack key contextual details. To enable systematic evaluation, we introduce AcT2I, a benchmark designed to evaluate the performance of T2I models in generating images from action-centric prompts. We experimentally validate that leading T2I models do not fare well on AcT2I. We further hypothesize that this shortcoming arises from the incomplete representation of the inherent attributes and contextual dependencies in the training corpora of existing T2I models. We build upon this by developing a training-free, knowledge distillation technique utilizing Large Language Models to address this limitation. Specifically, we enhance prompts by incorporating dense information across three dimensions, observing that injecting prompts with temporal details significantly improves image generation accuracy, with our best model achieving an increase of 72%. Our findings highlight the limitations of current T2I methods in generating images that require complex reasoning and demonstrate that integrating linguistic knowledge in a systematic way can notably advance the generation of nuanced and contextually accurate images.",
        "arxiv_id": "2509.16141",
        "ARXIVID": "2509.16141",
        "COMMENT": "Matches criterion 6 as it evaluates and improves action depiction in text-to-image models, focusing on nuanced and contextually accurate image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15536": {
        "authors": [
            "Sen Wang",
            "Jingyi Tian",
            "Le Wang",
            "Zhimin Liao",
            "Jiayi Li",
            "Huaiyi Dong",
            "Kun Xia",
            "Sanping Zhou",
            "Wei Tang",
            "Hua Gang"
        ],
        "title": "SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models",
        "abstract": "arXiv:2509.15536v1 Announce Type: new  Abstract: World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \\textbf{S}cale-wise \\textbf{A}utoregression with \\textbf{M}otion \\textbf{P}r\\textbf{O}mpt (\\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.",
        "arxiv_id": "2509.15536",
        "ARXIVID": "2509.15536",
        "COMMENT": "Matches criterion 3 as it introduces a novel world model (SAMPO) for action-conditioned video prediction and model-based control.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.16087": {
        "authors": [
            "Pengteng Li",
            "Pinhao Song",
            "Wuyang Li",
            "Weiyu Guo",
            "Huizai Yao",
            "Yijie Xu",
            "Dugang Liu",
            "Hui Xiong"
        ],
        "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model",
        "abstract": "arXiv:2509.16087v1 Announce Type: new  Abstract: We introduce SEE&TREK, the first training-free prompting framework tailored to enhance the spatial understanding of Multimodal Large Language Models (MLLMS) under vision-only constraints. While prior efforts have incorporated modalities like depth or point clouds to improve spatial reasoning, purely visualspatial understanding remains underexplored. SEE&TREK addresses this gap by focusing on two core principles: increasing visual diversity and motion reconstruction. For visual diversity, we conduct Maximum Semantic Richness Sampling, which employs an off-the-shell perception model to extract semantically rich keyframes that capture scene structure. For motion reconstruction, we simulate visual trajectories and encode relative spatial positions into keyframes to preserve both spatial relations and temporal coherence. Our method is training&GPU-free, requiring only a single forward pass, and can be seamlessly integrated into existing MLLM'S. Extensive experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently boosts various MLLM S performance across diverse spatial reasoning tasks with the most +3.5% improvement, offering a promising path toward stronger spatial intelligence.",
        "arxiv_id": "2509.16087",
        "ARXIVID": "2509.16087",
        "COMMENT": "Matches criterion 1 as it enhances spatial understanding in multimodal large language models using a training-free prompting framework.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.16127": {
        "authors": [
            "Yi-Fan Zhang",
            "Haihua Yang",
            "Huanyu Zhang",
            "Yang Shi",
            "Zezhou Chen",
            "Haochen Tian",
            "Chaoyou Fu",
            "Haotian Wang",
            "Kai Wu",
            "Bo Cui",
            "Xu Wang",
            "Jianfei Pan",
            "Haotian Wang",
            "Zhang Zhang",
            "Liang Wang"
        ],
        "title": "BaseReward: A Strong Baseline for Multimodal Reward Model",
        "abstract": "arXiv:2509.16127v1 Announce Type: new  Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including \\textit{reward modeling paradigms} (e.g., Naive-RM, Critic-based RM, and Generative RM), \\textit{reward head architecture}, \\textit{training strategies}, \\textit{data curation} (covering over ten multimodal and text-only preference datasets), \\textit{backbone model} and \\textit{model scale}, and \\textit{ensemble methods}.   Based on these experimental insights, we introduce \\textbf{BaseReward}, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.",
        "arxiv_id": "2509.16127",
        "ARXIVID": "2509.16127",
        "COMMENT": "Matches criterion 2 as it focuses on multimodal reward models for aligning multimodal large language models with human preferences.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15642": {
        "authors": [
            "Fangyuan Mao",
            "Shuo Wang",
            "Jilin Mei",
            "Chen Min",
            "Shun Lu",
            "Fuyang Liu",
            "Yu Hu"
        ],
        "title": "UNIV: Unified Foundation Model for Infrared and Visible Modalities",
        "abstract": "arXiv:2509.15642v1 Announce Type: new  Abstract: The demand for joint RGB-visible and infrared perception is growing rapidly, particularly to achieve robust performance under diverse weather conditions. Although pre-trained models for RGB-visible and infrared data excel in their respective domains, they often underperform in multimodal scenarios, such as autonomous vehicles equipped with both sensors. To address this challenge, we propose a biologically inspired UNified foundation model for Infrared and Visible modalities (UNIV), featuring two key innovations. First, we introduce Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided distillation framework that mimics retinal horizontal cells' lateral inhibition, which enables effective cross-modal feature alignment while remaining compatible with any transformer-based architecture. Second, our dual-knowledge preservation mechanism emulates the retina's bipolar cell signal routing - combining LoRA adapters (2% added parameters) with synchronous distillation to prevent catastrophic forgetting, thereby replicating the retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To support cross-modal learning, we introduce the MVIP dataset, the most comprehensive visible-infrared benchmark to date. It contains 98,992 precisely aligned image pairs spanning diverse scenarios. Extensive experiments demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+ of the baseline performance on visible RGB tasks. Our code is available at https://github.com/fangyuanmao/UNIV.",
        "arxiv_id": "2509.15642",
        "ARXIVID": "2509.15642",
        "COMMENT": "Matches criterion 4 as it introduces a foundation model (UNIV) for infrared and visible modalities, focusing on cross-modal learning and real-world applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15330": {
        "authors": [
            "Min Zhang",
            "Bo Jiang",
            "Jie Zhou",
            "Yimeng Liu",
            "Xin Lin"
        ],
        "title": "CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization",
        "abstract": "arXiv:2509.15330v1 Announce Type: new  Abstract: Recent advances in pre-training vision-language models (VLMs), e.g., contrastive language-image pre-training (CLIP) methods, have shown great potential in learning out-of-distribution (OOD) representations. Despite showing competitive performance, the prompt-based CLIP methods still suffer from: i) inaccurate text descriptions, which leads to degraded accuracy and robustness, and poses a challenge for zero-shot CLIP methods. ii) limited vision-language embedding alignment, which significantly affects the generalization performance. To tackle the above issues, this paper proposes a novel Conditional Domain prompt Learning (CoDoL) method, which utilizes readily-available domain information to form prompts and improves the vision-language embedding alignment for improving OOD generalization. To capture both instance-specific and domain-specific information, we further propose a lightweight Domain Meta Network (DMN) to generate input-conditional tokens for images in each domain. Extensive experiments on four OOD benchmarks (PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed CoDoL in terms of improving the vision-language embedding alignment as well as the out-of-distribution generalization performance.",
        "arxiv_id": "2509.15330",
        "ARXIVID": "2509.15330",
        "COMMENT": "Matches criterion 2 as it improves vision-language embedding alignment and explores out-of-distribution generalization in vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15479": {
        "authors": [
            "Bj\\\"orn M\\\"oller",
            "Zhengyang Li",
            "Malte Stelzer",
            "Thomas Graave",
            "Fabian Bettels",
            "Muaaz Ataya",
            "Tim Fingscheidt"
        ],
        "title": "OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data",
        "abstract": "arXiv:2509.15479v1 Announce Type: new  Abstract: Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.",
        "arxiv_id": "2509.15479",
        "ARXIVID": "2509.15479",
        "COMMENT": "Matches criterion 6 as it focuses on video generation for automotive driving scenes, which is a video-based task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15234": {
        "authors": [
            "Hanbin Ko",
            "Gihun Cho",
            "Inhyeok Baek",
            "Donguk Kim",
            "Joonbeom Koo",
            "Changi Kim",
            "Dongheon Lee",
            "Chang Min Park"
        ],
        "title": "Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays",
        "abstract": "arXiv:2509.15234v1 Announce Type: new  Abstract: Vision-language pretraining has advanced image-text alignment, yet progress in radiology remains constrained by the heterogeneity of clinical reports, including abbreviations, impression-only notes, and stylistic variability. Unlike general-domain settings where more data often leads to better performance, naively scaling to large collections of noisy reports can plateau or even degrade model learning. We ask whether large language model (LLM) encoders can provide robust clinical representations that transfer across diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR, a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a dual-tower framework that couples this encoder with a vision backbone. LLM2VEC4CXR improves clinical text understanding over BERT-based baselines, handles abbreviations and style variation, and achieves strong clinical alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to boost retrieval accuracy and clinically oriented scores, with stronger cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M CXR studies from public and private sources with heterogeneous and noisy reports, our models demonstrate that robustness -- not scale alone -- is the key to effective multimodal learning. We release models to support further research in medical image-text representation learning.",
        "arxiv_id": "2509.15234",
        "ARXIVID": "2509.15234",
        "COMMENT": "Matches criterion 2 as it explores a vision-language model (LLM2CLIP4CXR) for image-text alignment in the medical domain.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15237": {
        "authors": [
            "Di Wen",
            "Kunyu Peng",
            "Junwei Zheng",
            "Yufan Chen",
            "Yitain Shi",
            "Jiale Wei",
            "Ruiping Liu",
            "Kailun Yang",
            "Rainer Stiefelhagen"
        ],
        "title": "MICA: Multi-Agent Industrial Coordination Assistant",
        "abstract": "arXiv:2509.15237v1 Announce Type: new  Abstract: Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at https://github.com/Kratos-Wen/MICA.",
        "arxiv_id": "2509.15237",
        "ARXIVID": "2509.15237",
        "COMMENT": "This paper aligns with criterion 3 as it introduces a multi-agent coordination assistant for industrial workflows, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15788": {
        "authors": [
            "Haotian Zhang",
            "Han Guo",
            "Keyan Chen",
            "Hao Chen",
            "Zhengxia Zou",
            "Zhenwei Shi"
        ],
        "title": "FoBa: A Foreground-Background co-Guided Method and New Benchmark for Remote Sensing Semantic Change Detection",
        "abstract": "arXiv:2509.15788v1 Announce Type: new  Abstract: Despite the remarkable progress achieved in remote sensing semantic change detection (SCD), two major challenges remain. At the data level, existing SCD datasets suffer from limited change categories, insufficient change types, and a lack of fine-grained class definitions, making them inadequate to fully support practical applications. At the methodological level, most current approaches underutilize change information, typically treating it as a post-processing step to enhance spatial consistency, which constrains further improvements in model performance. To address these issues, we construct a new benchmark for remote sensing SCD, LevirSCD. Focused on the Beijing area, the dataset covers 16 change categories and 210 specific change types, with more fine-grained class definitions (e.g., roads are divided into unpaved and paved roads). Furthermore, we propose a foreground-background co-guided SCD (FoBa) method, which leverages foregrounds that focus on regions of interest and backgrounds enriched with contextual information to guide the model collaboratively, thereby alleviating semantic ambiguity while enhancing its ability to detect subtle changes. Considering the requirements of bi-temporal interaction and spatial consistency in SCD, we introduce a Gated Interaction Fusion (GIF) module along with a simple consistency loss to further enhance the model's detection performance. Extensive experiments on three datasets (SECOND, JL1, and the proposed LevirSCD) demonstrate that FoBa achieves competitive results compared to current SOTA methods, with improvements of 1.48%, 3.61%, and 2.81% in the SeK metric, respectively. Our code and dataset are available at https://github.com/zmoka-zht/FoBa.",
        "arxiv_id": "2509.15788",
        "ARXIVID": "2509.15788",
        "COMMENT": "This paper aligns with criterion 3 as it introduces a new benchmark and method for semantic change detection in remote sensing, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15532": {
        "authors": [
            "Xianhang Ye",
            "Yiqing Li",
            "Wei Dai",
            "Miancan Liu",
            "Ziyuan Chen",
            "Zhangye Han",
            "Hongbo Min",
            "Jinkui Ren",
            "Xiantao Zhang",
            "Wen Yang",
            "Zhi Jin"
        ],
        "title": "GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents",
        "abstract": "arXiv:2509.15532v1 Announce Type: new  Abstract: Existing GUI grounding methods often struggle with fine-grained localization in high-resolution screenshots. To address this, we propose GUI-ARP, a novel framework that enables adaptive multi-stage inference. Equipped with the proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC), GUI-ARP dynamically exploits visual attention for cropping task-relevant regions and adapts its inference strategy, performing a single-stage inference for simple cases and a multi-stage analysis for more complex scenarios. This is achieved through a two-phase training pipeline that integrates supervised fine-tuning with reinforcement fine-tuning based on Group Relative Policy Optimization (GRPO). Extensive experiments demonstrate that the proposed GUI-ARP achieves state-of-the-art performance on challenging GUI grounding benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9% on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.",
        "arxiv_id": "2509.15532",
        "ARXIVID": "2509.15532",
        "COMMENT": "This paper aligns with criterion 3 as it introduces a novel method for GUI agents, which can be considered a subset of embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.15645": {
        "authors": [
            "Donghyun Lee",
            "Dawoon Jeong",
            "Jae W. Lee",
            "Hongil Yoon"
        ],
        "title": "GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading",
        "abstract": "arXiv:2509.15645v1 Announce Type: new  Abstract: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.",
        "arxiv_id": "2509.15645",
        "ARXIVID": "2509.15645",
        "COMMENT": "Does not closely match any specific criterion but is relevant to large-scale 3D Gaussian Splatting and memory-efficient training.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.16091": {
        "authors": [
            "Shen Cheng",
            "Haipeng Li",
            "Haibin Huang",
            "Xiaohong Liu",
            "Shuaicheng Liu"
        ],
        "title": "Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising",
        "abstract": "arXiv:2509.16091v1 Announce Type: new  Abstract: In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised framework for real-world image denoising. Our approach addresses two major challenges: the limitations of blind-spot networks (BSNs), which often sacrifice local detail and introduce pixel discontinuities due to spatial independence assumptions, and the difficulty of adapting diffusion models to self-supervised denoising. We propose a dual-branch diffusion framework that combines a BSN-based diffusion branch, generating semi-clean images, with a conventional diffusion branch that captures underlying noise distributions. To enable effective training without paired data, we use the BSN-based branch to guide the sampling process, capturing noise structure while preserving local details. Extensive experiments on the SIDD and DND datasets demonstrate state-of-the-art performance, establishing our method as a highly effective self-supervised solution for real-world denoising. Code and pre-trained models are released at: https://github.com/Sumching/BSGD.",
        "arxiv_id": "2509.16091",
        "ARXIVID": "2509.16091",
        "COMMENT": "Does not closely match any specific criterion but is relevant to self-supervised learning and image denoising.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15342": {
        "authors": [
            "Jiuyi Xu",
            "Qing Jin",
            "Meida Chen",
            "Andrew Feng",
            "Yang Sui",
            "Yangming Shi"
        ],
        "title": "LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition",
        "abstract": "arXiv:2509.15342v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image generation but their practical application is often hindered by the slow sampling speed. Prior efforts of improving efficiency primarily focus on compressing models or reducing the total number of denoising steps, largely neglecting the possibility to leverage multiple input resolutions in the generation process. In this work, we propose LowDiff, a novel and efficient diffusion framework based on a cascaded approach by generating increasingly higher resolution outputs. Besides, LowDiff employs a unified model to progressively refine images from low resolution to the desired resolution. With the proposed architecture design and generation techniques, we achieve comparable or even superior performance with much fewer high-resolution sampling steps. LowDiff is applicable to diffusion models in both pixel space and latent space. Extensive experiments on both conditional and unconditional generation tasks across CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our method. Results show over 50% throughput improvement across all datasets and settings while maintaining comparable or better quality. On unconditional CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1 produces high-quality samples with a FID of 4.00 and an IS of 195.06, together with substantial efficiency gains.",
        "arxiv_id": "2509.15342",
        "ARXIVID": "2509.15342",
        "COMMENT": "Does not closely match any specific criterion but is relevant to generative modeling and efficiency improvements in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15791": {
        "authors": [
            "Tan Pan",
            "Kaiyu Guo",
            "Dongli Xu",
            "Zhaorui Tan",
            "Chen Jiang",
            "Deshu Chen",
            "Xin Guo",
            "Brian C. Lovell",
            "Limei Han",
            "Yuan Cheng",
            "Mahsa Baktashmotlagh"
        ],
        "title": "Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization",
        "abstract": "arXiv:2509.15791v1 Announce Type: new  Abstract: The generalization ability of deep learning has been extensively studied in supervised settings, yet it remains less explored in unsupervised scenarios. Recently, the Unsupervised Domain Generalization (UDG) task has been proposed to enhance the generalization of models trained with prevalent unsupervised learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the challenge of distinguishing semantics from variations without category labels. Although some recent methods have employed domain labels to tackle this issue, such domain labels are often unavailable in real-world contexts. In this paper, we address these limitations by formalizing UDG as the task of learning a Minimal Sufficient Semantic Representation: a representation that (i) preserves all semantic information shared across augmented views (sufficiency), and (ii) maximally removes information irrelevant to semantics (minimality). We theoretically ground these objectives from the perspective of information theory, demonstrating that optimizing representations to achieve sufficiency and minimality directly reduces out-of-distribution risk. Practically, we implement this optimization through Minimal-Sufficient UDG (MS-UDG), a learnable model by integrating (a) an InfoNCE-based objective to achieve sufficiency; (b) two complementary components to promote minimality: a novel semantic-variation disentanglement loss and a reconstruction-based mechanism for capturing adequate variation. Empirically, MS-UDG sets a new state-of-the-art on popular unsupervised domain-generalization benchmarks, consistently outperforming existing SSL and UDG methods, without category or domain labels during representation learning.",
        "arxiv_id": "2509.15791",
        "ARXIVID": "2509.15791",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to machine learning and representation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15905": {
        "authors": [
            "David Calhas",
            "Arlindo L. Oliveira"
        ],
        "title": "Deep Feedback Models",
        "abstract": "arXiv:2509.15905v1 Announce Type: new  Abstract: Deep Feedback Models (DFMs) are a new class of stateful neural networks that combine bottom up input with high level representations over time. This feedback mechanism introduces dynamics into otherwise static architectures, enabling DFMs to iteratively refine their internal state and mimic aspects of biological decision making. We model this process as a differential equation solved through a recurrent neural network, stabilized via exponential decay to ensure convergence. To evaluate their effectiveness, we measure DFMs under two key conditions: robustness to noise and generalization with limited data. In both object recognition and segmentation tasks, DFMs consistently outperform their feedforward counterparts, particularly in low data or high noise regimes. In addition, DFMs translate to medical imaging settings, while being robust against various types of noise corruption. These findings highlight the importance of feedback in achieving stable, robust, and generalizable learning. Code is available at https://github.com/DCalhas/deep_feedback_models.",
        "arxiv_id": "2509.15905",
        "ARXIVID": "2509.15905",
        "COMMENT": "This paper does not directly match any specific criterion but discusses a novel neural network architecture with feedback mechanisms, which could be tangentially relevant to embodied agents (criterion 1).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15741": {
        "authors": [
            "Laixin Zhang",
            "Shuaibo Li",
            "Wei Ma",
            "Hongbin Zha"
        ],
        "title": "TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection",
        "abstract": "arXiv:2509.15741v1 Announce Type: new  Abstract: The rapid progress of generative models has made synthetic image detection an increasingly critical task. Most existing approaches attempt to construct a single, universal discriminative space to separate real from fake content. However, such unified spaces tend to be complex and brittle, often struggling to generalize to unseen generative patterns. In this work, we propose TrueMoE, a novel dual-routing Mixture-of-Discriminative-Experts framework that reformulates the detection task as a collaborative inference across multiple specialized and lightweight discriminative subspaces. At the core of TrueMoE is a Discriminative Expert Array (DEA) organized along complementary axes of manifold structure and perceptual granularity, enabling diverse forgery cues to be captured across subspaces. A dual-routing mechanism, comprising a granularity-aware sparse router and a manifold-aware dense router, adaptively assigns input images to the most relevant experts. Extensive experiments across a wide spectrum of generative models demonstrate that TrueMoE achieves superior generalization and robustness.",
        "arxiv_id": "2509.15741",
        "ARXIVID": "2509.15741",
        "COMMENT": "Does not match any specific criterion but is tangentially related to generative modeling through synthetic image detection.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15406": {
        "authors": [
            "Hui Xu",
            "Chi Liu",
            "Congcong Zhu",
            "Minghao Wang",
            "Youyang Qu",
            "Longxiang Gao"
        ],
        "title": "Causal Fingerprints of AI Generative Models",
        "abstract": "arXiv:2509.15406v1 Announce Type: new  Abstract: AI generative models leave implicit traces in their generated images, which are commonly referred to as model fingerprints and are exploited for source attribution. Prior methods rely on model-specific cues or synthesis artifacts, yielding limited fingerprints that may generalize poorly across different generative models. We argue that a complete model fingerprint should reflect the causality between image provenance and model traces, a direction largely unexplored. To this end, we conceptualize the \\emph{causal fingerprint} of generative models, and propose a causality-decoupling framework that disentangles it from image-specific content and style in a semantic-invariant latent space derived from pre-trained diffusion reconstruction residual. We further enhance fingerprint granularity with diverse feature representations. We validate causality by assessing attribution performance across representative GANs and diffusion models and by achieving source anonymization using counterfactual examples generated from causal fingerprints. Experiments show our approach outperforms existing methods in model attribution, indicating strong potential for forgery detection, model copyright tracing, and identity protection.",
        "arxiv_id": "2509.15406",
        "ARXIVID": "2509.15406",
        "COMMENT": "Does not match any specific criterion but is tangentially related to generative modeling through causal fingerprints for AI generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.16058": {
        "authors": [
            "Krati Saxena",
            "Federico Jurado Ruiz",
            "Guido Manzi",
            "Dianbo Liu",
            "Alex Lamb"
        ],
        "title": "Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers",
        "abstract": "arXiv:2509.16058v1 Announce Type: new  Abstract: Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.",
        "arxiv_id": "2509.16058",
        "ARXIVID": "2509.16058",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to attention mechanisms and cognitive-inspired approaches.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.15987": {
        "authors": [
            "Aur\\'elien Cecille",
            "Stefan Duffner",
            "Franck Davoine",
            "R\\'emi Agier",
            "Thibault Neveu"
        ],
        "title": "Towards Sharper Object Boundaries in Self-Supervised Depth Estimation",
        "abstract": "arXiv:2509.15987v1 Announce Type: new  Abstract: Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines.",
        "arxiv_id": "2509.15987",
        "ARXIVID": "2509.15987",
        "COMMENT": "This paper does not directly match any of the specific criteria but is tangentially related to spatial intelligence and embodied agents (criterion 1) due to its focus on depth estimation, which is a component of 3D scene understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15677": {
        "authors": [
            "Gahye Lee",
            "Hyomin Kim",
            "Gwangjin Ju",
            "Jooeun Son",
            "Hyejeong Yoon",
            "Seungyong Lee"
        ],
        "title": "Camera Splatting for Continuous View Optimization",
        "abstract": "arXiv:2509.15677v1 Announce Type: new  Abstract: We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.",
        "arxiv_id": "2509.15677",
        "ARXIVID": "2509.15677",
        "COMMENT": "Does not closely match any specific criterion but is relevant to novel view synthesis and optimization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15563": {
        "authors": [
            "Min Sun",
            "Fenghui Guo"
        ],
        "title": "DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection",
        "abstract": "arXiv:2509.15563v1 Announce Type: new  Abstract: Remote sensing change detection (RSCD) is vital for identifying land-cover changes, yet existing methods, including state-of-the-art State Space Models (SSMs), often lack explicit mechanisms to handle geometric misalignments and struggle to distinguish subtle, true changes from noise.To address this, we introduce DC-Mamba, an \"align-then-enhance\" framework built upon the ChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1) Bi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric awareness to correct spatial misalignments at the semantic feature level; and (2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to selectively amplify high-confidence change signals while suppressing noise before the final classification. This synergistic design first establishes geometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA to sharpen boundaries and enhance the visibility of small or subtle targets. Experiments show our method significantly improves performance over the strong ChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU from 0.4015 to 0.4187. The results confirm the effectiveness of our \"align-then-enhance\" strategy, offering a robust and easily deployable solution that transparently addresses both geometric and feature-level challenges in RSCD.",
        "arxiv_id": "2509.15563",
        "ARXIVID": "2509.15563",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and feature alignment in remote sensing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15608": {
        "authors": [
            "Zheng Wang",
            "Hong Liu",
            "Zheng Wang",
            "Danyi Li",
            "Min Cen",
            "Baptiste Magnier",
            "Li Liang",
            "Liansheng Wang"
        ],
        "title": "Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation",
        "abstract": "arXiv:2509.15608v1 Announce Type: new  Abstract: Survival analysis based on Whole Slide Images (WSIs) is crucial for evaluating cancer prognosis, as they offer detailed microscopic information essential for predicting patient outcomes. However, traditional WSI-based survival analysis usually faces noisy features and limited data accessibility, hindering their ability to capture critical prognostic features effectively. Although pathology reports provide rich patient-specific information that could assist analysis, their potential to enhance WSI-based survival analysis remains largely unexplored. To this end, this paper proposes a novel Report-auxiliary self-distillation (Rasa) framework for WSI-based survival analysis. First, advanced large language models (LLMs) are utilized to extract fine-grained, WSI-relevant textual descriptions from original noisy pathology reports via a carefully designed task prompt. Next, a self-distillation-based pipeline is designed to filter out irrelevant or redundant WSI features for the student model under the guidance of the teacher model's textual knowledge. Finally, a risk-aware mix-up strategy is incorporated during the training of the student model to enhance both the quantity and diversity of the training data. Extensive experiments carried out on our collected data (CRC) and public data (TCGA-BRCA) demonstrate the superior effectiveness of Rasa against state-of-the-art methods. Our code is available at https://github.com/zhengwang9/Rasa.",
        "arxiv_id": "2509.15608",
        "ARXIVID": "2509.15608",
        "COMMENT": "Does not closely match any specific criterion but is relevant to multimodal learning and generative modeling in a medical context.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15711": {
        "authors": [
            "Shuaibo Li",
            "Zhaohu Xing",
            "Hongqiu Wang",
            "Pengfei Hao",
            "Xingyu Li",
            "Zekai Liu",
            "Lei Zhu"
        ],
        "title": "Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method",
        "abstract": "arXiv:2509.15711v1 Announce Type: new  Abstract: The rapid advancement of generative AI in medical imaging has introduced both significant opportunities and serious challenges, especially the risk that fake medical images could undermine healthcare systems. These synthetic images pose serious risks, such as diagnostic deception, financial fraud, and misinformation. However, research on medical forensics to counter these threats remains limited, and there is a critical lack of comprehensive datasets specifically tailored for this field. Additionally, existing media forensic methods, which are primarily designed for natural or facial images, are inadequate for capturing the distinct characteristics and subtle artifacts of AI-generated medical images. To tackle these challenges, we introduce \\textbf{MedForensics}, a large-scale medical forensics dataset encompassing six medical modalities and twelve state-of-the-art medical generative models. We also propose \\textbf{DSKI}, a novel \\textbf{D}ual-\\textbf{S}tage \\textbf{K}nowledge \\textbf{I}nfusing detector that constructs a vision-language feature space tailored for the detection of AI-generated medical images. DSKI comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for extracting subtle forgery clues from both spatial and noise domains during training, and 2) a medical forensic retrieval module (MFRM) that boosts detection accuracy through few-shot retrieval during testing. Experimental results demonstrate that DSKI significantly outperforms both existing methods and human experts, achieving superior accuracy across multiple medical modalities.",
        "arxiv_id": "2509.15711",
        "ARXIVID": "2509.15711",
        "COMMENT": "This paper does not match any specific criterion but introduces a novel method for detecting AI-generated medical images, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15690": {
        "authors": [
            "Weixuan Sun",
            "Jucai Zhai",
            "Dengfeng Liu",
            "Xin Zhang",
            "Xiaojun Wu",
            "Qiaobo Hao",
            "AIMgroup",
            "Yang Fang",
            "Jiuyang Tang"
        ],
        "title": "CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair",
        "abstract": "arXiv:2509.15690v1 Announce Type: new  Abstract: The automated repair of C++ compilation errors presents a significant challenge, the resolution of which is critical for developer productivity. Progress in this domain is constrained by two primary factors: the scarcity of large-scale, high-fidelity datasets and the limitations of conventional supervised methods, which often fail to generate semantically correct patches.This paper addresses these gaps by introducing a comprehensive framework with three core contributions. First, we present CCrepair, a novel, large-scale C++ compilation error dataset constructed through a sophisticated generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL) paradigm guided by a hybrid reward signal, shifting the focus from mere compilability to the semantic quality of the fix. Finally, we establish the robust, two-stage evaluation system providing this signal, centered on an LLM-as-a-Judge whose reliability has been rigorously validated against the collective judgments of a panel of human experts. This integrated approach aligns the training objective with generating high-quality, non-trivial patches that are both syntactically and semantically correct. The effectiveness of our approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct model achieved performance comparable to a Qwen2.5-14B-Instruct model, validating the efficiency of our training paradigm. Our work provides the research community with a valuable new dataset and a more effective paradigm for training and evaluating robust compilation repair models, paving the way for more practical and reliable automated programming assistants.",
        "arxiv_id": "2509.15690",
        "ARXIVID": "2509.15690",
        "COMMENT": "This paper does not match any specific criterion but introduces a novel reinforcement learning framework for C++ compilation repair, which is tangentially related to machine learning methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.16022": {
        "authors": [
            "Xihong Yang",
            "Siwei Wang",
            "Jiaqi Jin",
            "Fangdi Wang",
            "Tianrui Liu",
            "Yueming Jin",
            "Xinwang Liu",
            "En Zhu",
            "Kunlun He"
        ],
        "title": "Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence",
        "abstract": "arXiv:2509.16022v1 Announce Type: new  Abstract: Multi-view clustering (MVC) aims to explore the common clustering structure across multiple views. Many existing MVC methods heavily rely on the assumption of view consistency, where alignments for corresponding samples across different views are ordered in advance. However, real-world scenarios often present a challenge as only partial data is consistently aligned across different views, restricting the overall clustering performance. In this work, we consider the model performance decreasing phenomenon caused by data order shift (i.e., from fully to partially aligned) as a generalized multi-view clustering problem. To tackle this problem, we design a causal multi-view clustering network, termed CauMVC. We adopt a causal modeling approach to understand multi-view clustering procedure. To be specific, we formulate the partially aligned data as an intervention and multi-view clustering with partially aligned data as an post-intervention inference. However, obtaining invariant features directly can be challenging. Thus, we design a Variational Auto-Encoder for causal learning by incorporating an encoder from existing information to estimate the invariant features. Moreover, a decoder is designed to perform the post-intervention inference. Lastly, we design a contrastive regularizer to capture sample correlations. To the best of our knowledge, this paper is the first work to deal generalized multi-view clustering via causal learning. Empirical experiments on both fully and partially aligned data illustrate the strong generalization and effectiveness of CauMVC.",
        "arxiv_id": "2509.16022",
        "ARXIVID": "2509.16022",
        "COMMENT": "This paper does not match any specific criterion but discusses a novel causal learning approach for multi-view clustering, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15635": {
        "authors": [
            "Pan Tang",
            "Shixiang Tang",
            "Huanqi Pu",
            "Zhiqing Miao",
            "Zhixing Wang"
        ],
        "title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents",
        "abstract": "arXiv:2509.15635v1 Announce Type: new  Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: https://github.com/tangpan360/MicroRCA-Agent.",
        "arxiv_id": "2509.15635",
        "ARXIVID": "2509.15635",
        "COMMENT": "Does not match any specific criterion but is tangentially related to multimodal systems through root cause analysis using large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15706": {
        "authors": [
            "Chi Yang",
            "Fu Wang",
            "Xiaofei Yang",
            "Hao Huang",
            "Weijia Cao",
            "Xiaowen Chu"
        ],
        "title": "SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark",
        "abstract": "arXiv:2509.15706v1 Announce Type: new  Abstract: Cloud phase profiles are critical for numerical weather prediction (NWP), as they directly affect radiative transfer and precipitation processes. In this study, we present a benchmark dataset and a baseline framework for transforming multimodal satellite observations into detailed 3D cloud phase structures, aiming toward operational cloud phase profile retrieval and future integration with NWP systems to improve cloud microphysics parameterization. The multimodal observations consist of (1) high--spatiotemporal--resolution, multi-band visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites, and (2) accurate vertical cloud phase profiles from spaceborne lidar (CALIOP\\slash CALIPSO) and radar (CPR\\slash CloudSat). The dataset consists of synchronized image--profile pairs across diverse cloud regimes, defining a supervised learning task: given VIS/TIR patches, predict the corresponding 3D cloud phase structure. We adopt SGMAGNet as the main model and compare it with several baseline architectures, including UNet variants and SegNet, all designed to capture multi-scale spatial patterns. Model performance is evaluated using standard classification metrics, including Precision, Recall, F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior performance in cloud phase reconstruction, particularly in complex multi-layer and boundary transition regions. Quantitatively, SGMAGNet attains a Precision of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617, significantly outperforming all baselines across these key metrics.",
        "arxiv_id": "2509.15706",
        "ARXIVID": "2509.15706",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to computer vision through 3D cloud phase structure reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15688": {
        "authors": [
            "Johann Schmidt",
            "Sebastian Stober",
            "Joachim Denzler",
            "Paul Bodesheim"
        ],
        "title": "Saccadic Vision for Fine-Grained Visual Classification",
        "abstract": "arXiv:2509.15688v1 Announce Type: new  Abstract: Fine-grained visual classification (FGVC) requires distinguishing between visually similar categories through subtle, localized features - a task that remains challenging due to high intra-class variability and limited inter-class differences. Existing part-based methods often rely on complex localization networks that learn mappings from pixel to sample space, requiring a deep understanding of image content while limiting feature utility for downstream tasks. In addition, sampled points frequently suffer from high spatial redundancy, making it difficult to quantify the optimal number of required parts. Inspired by human saccadic vision, we propose a two-stage process that first extracts peripheral features (coarse view) and generates a sample map, from which fixation patches are sampled and encoded in parallel using a weight-shared encoder. We employ contextualized selective attention to weigh the impact of each fixation patch before fusing peripheral and focus representations. To prevent spatial collapse - a common issue in part-based methods - we utilize non-maximum suppression during fixation sampling to eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks (CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method achieves comparable performance to state-of-the-art approaches while consistently outperforming our baseline encoder.",
        "arxiv_id": "2509.15688",
        "ARXIVID": "2509.15688",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to fine-grained visual classification and attention mechanisms.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15784": {
        "authors": [
            "Xiang Chen",
            "Fengting Zhang",
            "Qinghao Liu",
            "Min Liu",
            "Kun Wu",
            "Yaonan Wang",
            "Hang Zhang"
        ],
        "title": "Ideal Registration? Segmentation is All You Need",
        "abstract": "arXiv:2509.15784v1 Announce Type: new  Abstract: Deep learning has revolutionized image registration by its ability to handle diverse tasks while achieving significant speed advantages over conventional approaches. Current approaches, however, often employ globally uniform smoothness constraints that fail to accommodate the complex, regionally varying deformations characteristic of anatomical motion. To address this limitation, we propose SegReg, a Segmentation-driven Registration framework that implements anatomically adaptive regularization by exploiting region-specific deformation patterns. Our SegReg first decomposes input moving and fixed images into anatomically coherent subregions through segmentation. These localized domains are then processed by the same registration backbone to compute optimized partial deformation fields, which are subsequently integrated into a global deformation field. SegReg achieves near-perfect structural alignment (98.23% Dice on critical anatomies) using ground-truth segmentation, and outperforms existing methods by 2-12% across three clinical registration scenarios (cardiac, abdominal, and lung images) even with automatic segmentation. Our SegReg demonstrates a near-linear dependence of registration accuracy on segmentation quality, transforming the registration challenge into a segmentation problem. The source code will be released upon manuscript acceptance.",
        "arxiv_id": "2509.15784",
        "ARXIVID": "2509.15784",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to image registration and segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15805": {
        "authors": [
            "Tianyang Wang",
            "Xi Xiao",
            "Gaofei Chen",
            "Xiaoying Liao",
            "Guo Cheng",
            "Yingrui Ji"
        ],
        "title": "Boosting Active Learning with Knowledge Transfer",
        "abstract": "arXiv:2509.15805v1 Announce Type: new  Abstract: Uncertainty estimation is at the core of Active Learning (AL). Most existing methods resort to complex auxiliary models and advanced training fashions to estimate uncertainty for unlabeled data. These models need special design and hence are difficult to train especially for domain tasks, such as Cryo-Electron Tomography (cryo-ET) classification in computational biology. To address this challenge, we propose a novel method using knowledge transfer to boost uncertainty estimation in AL. Specifically, we exploit the teacher-student mode where the teacher is the task model in AL and the student is an auxiliary model that learns from the teacher. We train the two models simultaneously in each AL cycle and adopt a certain distance between the model outputs to measure uncertainty for unlabeled data. The student model is task-agnostic and does not rely on special training fashions (e.g. adversarial), making our method suitable for various tasks. More importantly, we demonstrate that data uncertainty is not tied to concrete value of task loss but closely related to the upper-bound of task loss. We conduct extensive experiments to validate the proposed method on classical computer vision tasks and cryo-ET challenges. The results demonstrate its efficacy and efficiency.",
        "arxiv_id": "2509.15805",
        "ARXIVID": "2509.15805",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to machine learning and uncertainty estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.15768": {
        "authors": [
            "Herve Goeau",
            "Vincent Espitalier",
            "Pierre Bonnet",
            "Alexis Joly"
        ],
        "title": "Overview of PlantCLEF 2024: multi-species plant identification in vegetation plot images",
        "abstract": "arXiv:2509.15768v1 Announce Type: new  Abstract: Plot images are essential for ecological studies, enabling standardized sampling, biodiversity assessment, long-term monitoring and remote, large-scale surveys. Plot images are typically fifty centimetres or one square meter in size, and botanists meticulously identify all the species found there. The integration of AI could significantly improve the efficiency of specialists, helping them to extend the scope and coverage of ecological studies. To evaluate advances in this regard, the PlantCLEF 2024 challenge leverages a new test set of thousands of multi-label images annotated by experts and covering over 800 species. In addition, it provides a large training set of 1.7 million individual plant images as well as state-of-the-art vision transformer models pre-trained on this data. The task is evaluated as a (weakly-labeled) multi-label classification task where the aim is to predict all the plant species present on a high-resolution plot image (using the single-label training data). In this paper, we provide an detailed description of the data, the evaluation methodology, the methods and models employed by the participants and the results achieved.",
        "arxiv_id": "2509.15768",
        "ARXIVID": "2509.15768",
        "COMMENT": "This paper does not match any specific criterion but discusses a benchmark for plant identification, which is tangentially related to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}