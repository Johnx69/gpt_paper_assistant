{
    "2507.15130": {
        "authors": [
            "Ce Zhang",
            "Yale Song",
            "Ruta Desai",
            "Michael Louis Iuzzolino",
            "Joseph Tighe",
            "Gedas Bertasius",
            "Satwik Kottur"
        ],
        "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction",
        "abstract": "arXiv:2507.15130v1 Announce Type: new  Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user actions required to achieve a specified goal based on a video showing the user's progress. Although recent advances in multimodal large language models (MLLMs) have shown promising results in video understanding, long-horizon visual planning remains a challenging problem. We identify two challenges in training large MLLMs for video-based planning tasks: (1) scarcity of procedural annotations, limiting the model's ability to learn procedural task dynamics effectively, and (2) inefficiency of next-token prediction objective to explicitly capture the structured action space for visual planning when compared to free-form, natural language. To tackle data scarcity, we introduce Auxiliary Task Augmentation. We design and train our model on auxiliary tasks relevant to long-horizon video-based planning (e.g., goal prediction) to augment the model's planning ability. To more explicitly model the structured action space unique to visual planning tasks, we leverage Multi-token Prediction, extending traditional next-token prediction by using multiple heads to predict multiple future tokens during training. Our approach, VideoPlan, achieves state-of-the-art VPA performance on the COIN and CrossTask datasets, surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3 future actions. We further extend our method to the challenging Ego4D Long-term Action Anticipation task, and show that it is on par with the state-of-the-art approaches despite not using specialized egocentric features. Code will be made available.",
        "arxiv_id": "2507.15130",
        "ARXIVID": "2507.15130",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video-based tasks like visual planning and long-horizon action prediction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15824": {
        "authors": [
            "Enes Sanli",
            "Baris Sarper Tezcan",
            "Aykut Erdem",
            "Erkut Erdem"
        ],
        "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models",
        "abstract": "arXiv:2507.15824v1 Announce Type: new  Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis of visually compelling and temporally coherent videos from natural language. However, these models often fall short in basic physical commonsense, producing outputs that violate intuitive expectations around causality, object behavior, and tool use. Addressing this gap, we present PhysVidBench, a benchmark designed to evaluate the physical reasoning capabilities of T2V systems. The benchmark includes 383 carefully curated prompts, emphasizing tool use, material properties, and procedural interactions, and domains where physical plausibility is crucial. For each prompt, we generate videos using diverse state-of-the-art models and adopt a three-stage evaluation pipeline: (1) formulate grounded physics questions from the prompt, (2) caption the generated video with a vision-language model, and (3) task a language model to answer several physics-involved questions using only the caption. This indirect strategy circumvents common hallucination issues in direct video-based evaluation. By highlighting affordances and tool-mediated actions, areas overlooked in current T2V evaluations, PhysVidBench provides a structured, interpretable framework for assessing physical commonsense in generative video models.",
        "arxiv_id": "2507.15824",
        "ARXIVID": "2507.15824",
        "COMMENT": "Matches criterion 6 as it introduces a benchmark for evaluating physical commonsense understanding in video generation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15249": {
        "authors": [
            "Yanbing Zhang",
            "Zhe Wang",
            "Qin Zhou",
            "Mengping Yang"
        ],
        "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers",
        "abstract": "arXiv:2507.15249v1 Announce Type: new  Abstract: In light of recent breakthroughs in text-to-image (T2I) generation, particularly with diffusion transformers (DiT), subject-driven technologies are increasingly being employed for high-fidelity customized production that preserves subject identity from reference inputs, enabling thrilling design workflows and engaging entertainment. Existing alternatives typically require either per-subject optimization via trainable text embeddings or training specialized encoders for subject feature extraction on large-scale datasets. Such dependencies on training procedures fundamentally constrain their practical applications. More importantly, current methodologies fail to fully leverage the inherent zero-shot potential of modern diffusion transformers (e.g., the Flux series) for authentic subject-driven synthesis. To bridge this gap, we propose FreeCus, a genuinely training-free framework that activates DiT's capabilities through three key innovations: 1) We introduce a pivotal attention sharing mechanism that captures the subject's layout integrity while preserving crucial editing flexibility. 2) Through a straightforward analysis of DiT's dynamic shifting, we propose an upgraded variant that significantly improves fine-grained feature extraction. 3) We further integrate advanced Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic representations. Extensive experiments reflect that our method successfully unlocks DiT's zero-shot ability for consistent subject synthesis across diverse contexts, achieving state-of-the-art or comparable results compared to approaches that require additional training. Notably, our framework demonstrates seamless compatibility with existing inpainting pipelines and control modules, facilitating more compelling experiences. Our code is available at: https://github.com/Monalissaa/FreeCus.",
        "arxiv_id": "2507.15249",
        "ARXIVID": "2507.15249",
        "COMMENT": "Matches criterion 5 as it integrates Multimodal Large Language Models (MLLMs) with diffusion transformers for subject-driven synthesis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14675": {
        "authors": [
            "Yuchen Duan",
            "Zhe Chen",
            "Yusong Hu",
            "Weiyun Wang",
            "Shenglong Ye",
            "Botian Shi",
            "Lewei Lu",
            "Qibin Hou",
            "Tong Lu",
            "Hongsheng Li",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
        "abstract": "arXiv:2507.14675v1 Announce Type: new  Abstract: Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets. While current retrieval-augmented generation (RAG) methods offer partial solutions, they suffer from issues, such as fragmented retrieval contexts, multi-stage error accumulation, and extra time costs of retrieval. In this work, we present a high-quality document-level dataset, Doc-750K, designed to support in-depth understanding of multimodal documents. This dataset includes diverse document structures, extensive cross-page dependencies, and real question-answer pairs derived from the original documents. Building on the dataset, we develop a native multimodal model, Docopilot, which can accurately handle document-level dependencies without relying on RAG. Experiments demonstrate that Docopilot achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding. Data, code, and models are released at https://github.com/OpenGVLab/Docopilot",
        "arxiv_id": "2507.14675",
        "ARXIVID": "2507.14675",
        "COMMENT": "Matches criterion 2 as it focuses on multimodal large language models and their application to document-level understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14501": {
        "authors": [
            "Jiahui Zhang",
            "Yuelei Li",
            "Anpei Chen",
            "Muyu Xu",
            "Kunhao Liu",
            "Jianyuan Wang",
            "Xiao-Xiao Long",
            "Hanxue Liang",
            "Zexiang Xu",
            "Hao Su",
            "Christian Theobalt",
            "Christian Rupprecht",
            "Andrea Vedaldi",
            "Hanspeter Pfister",
            "Shijian Lu",
            "Fangneng Zhan"
        ],
        "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey",
        "abstract": "arXiv:2507.14501v1 Announce Type: new  Abstract: 3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.",
        "arxiv_id": "2507.14501",
        "ARXIVID": "2507.14501",
        "COMMENT": "Matches criterion 7 as it is a comprehensive survey on feed-forward 3D reconstruction and view synthesis, synthesizing the state of the art.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2507.15509": {
        "authors": [
            "Lei Chen",
            "Xuanle Zhao",
            "Zhixiong Zeng",
            "Jing Huang",
            "Yufeng Zhong",
            "Lin Ma"
        ],
        "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner",
        "abstract": "arXiv:2507.15509v1 Announce Type: new  Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based on reinforcement learning fine-tuning has received widespread attention from the community. Previous R1-Style methods mainly focus on mathematical reasoning and code intelligence. It is of great research significance to verify their advantages on more general multimodal data. Chart is an important multimodal data type with rich information, which brings important research challenges in complex reasoning. In this work, we introduce Chart-R1, a chart-domain vision-language model with reinforcement learning fine-tuning to enable complex chart reasoning. To support Chart-R1, we first propose a novel programmatic data synthesis technology to generate high-quality step-by-step chart reasoning data covering single- and multi-subcharts, which makes up for the lack of reasoning data in the chart domain. Then we develop a two-stage training strategy: Chart-COT with step-by-step chain-of-thought supervision, and Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims to decompose complex chart reasoning tasks into fine-grained, understandable subtasks through step-by-step supervision, which lays a good foundation for improving the reasoning level of reinforcement learning. Chart-RFT utilize the typical group relative policy optimization strategy, in which a relatively soft reward is adopted for numerical response to emphasize the numerical sensitivity in the chart domain. We conduct extensive experiments on open-source benchmarks and self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental results show that Chart-R1 has significant advantages compared to chart-domain methods, even comparable to open/closed source large-scale models (\\emph{e.g., GPT-4o, Claude-3.5}).",
        "arxiv_id": "2507.15509",
        "ARXIVID": "2507.15509",
        "COMMENT": "Matches criterion 5 as it focuses on integrating chart-based visual understanding with large language models for reasoning tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.15803": {
        "authors": [
            "Danhui Chen",
            "Ziquan Liu",
            "Chuxi Yang",
            "Dan Wang",
            "Yan Yan",
            "Yi Xu",
            "Xiangyang Ji"
        ],
        "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction",
        "abstract": "arXiv:2507.15803v1 Announce Type: new  Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in.",
        "arxiv_id": "2507.15803",
        "ARXIVID": "2507.15803",
        "COMMENT": "Matches criterion 4 as it focuses on foundation models in computer vision (Segment Anything Model) and their application to semi-supervised semantic segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.14520": {
        "authors": [
            "Xinyi Chen",
            "Yifei Yuan",
            "Jiaang Li",
            "Serge Belongie",
            "Maarten de Rijke",
            "Anders S{\\o}gaard"
        ],
        "title": "What if Othello-Playing Language Models Could See?",
        "abstract": "arXiv:2507.14520v1 Announce Type: new  Abstract: Language models are often said to face a symbol grounding problem. While some argue that world understanding can emerge from text alone, others suggest grounded learning is more efficient. We explore this through Othello, where the board state defines a simplified, rule-based world. Building on prior work, we introduce VISOTHELLO, a multi-modal model trained on move histories and board images. Using next-move prediction, we compare it to mono-modal baselines and test robustness to semantically irrelevant perturbations. We find that multi-modal training improves both performance and the robustness of internal representations. These results suggest that grounding language in visual input helps models infer structured world representations.",
        "arxiv_id": "2507.14520",
        "ARXIVID": "2507.14520",
        "COMMENT": "Matches criterion 2 as it explores a multi-modal model combining visual and language inputs for structured world representations.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.14500": {
        "authors": [
            "Zhiyuan Hua",
            "Dehao Yuan",
            "Cornelia Ferm\\\"uller"
        ],
        "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow",
        "abstract": "arXiv:2507.14500v1 Announce Type: new  Abstract: This paper introduces a robust framework for motion segmentation and egomotion estimation using event-based normal flow, tailored specifically for neuromorphic vision sensors. In contrast to traditional methods that rely heavily on optical flow or explicit depth estimation, our approach exploits the sparse, high-temporal-resolution event data and incorporates geometric constraints between normal flow, scene structure, and inertial measurements. The proposed optimization-based pipeline iteratively performs event over-segmentation, isolates independently moving objects via residual analysis, and refines segmentations using hierarchical clustering informed by motion similarity and temporal consistency. Experimental results on the EVIMO2v2 dataset validate that our method achieves accurate segmentation and translational motion estimation without requiring full optical flow computation. This approach demonstrates significant advantages at object boundaries and offers considerable potential for scalable, real-time robotic and navigation applications.",
        "arxiv_id": "2507.14500",
        "ARXIVID": "2507.14500",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for motion segmentation and egomotion estimation tailored for robotic and navigation applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.15724": {
        "authors": [
            "Guoxuan Xia",
            "Harleen Hanspal",
            "Petru-Daniel Tudosiu",
            "Shifeng Zhang",
            "Sarah Parisot"
        ],
        "title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers",
        "abstract": "arXiv:2507.15724v1 Announce Type: new  Abstract: Enabling image generation models to be spatially controlled is an important area of research, empowering users to better generate images according to their own fine-grained specifications via e.g. edge maps, poses. Although this task has seen impressive improvements in recent times, a focus on rapidly producing stronger models has come at the cost of detailed and fair scientific comparison. Differing training data, model architectures and generation paradigms make it difficult to disentangle the factors contributing to performance. Meanwhile, the motivations and nuances of certain approaches become lost in the literature. In this work, we aim to provide clear takeaways across generation paradigms for practitioners wishing to develop transformer-based systems for spatially-controlled generation, clarifying the literature and addressing knowledge gaps. We perform controlled experiments on ImageNet across diffusion-based/flow-based and autoregressive (AR) models. First, we establish control token prefilling as a simple, general and performant baseline approach for transformers. We then investigate previously underexplored sampling time enhancements, showing that extending classifier-free guidance to control, as well as softmax truncation, have a strong impact on control-generation consistency. Finally, we re-clarify the motivation of adapter-based approaches, demonstrating that they mitigate \"forgetting\" and maintain generation quality when trained on limited downstream data, but underperform full training in terms of generation-control consistency. Code will be released upon publication.",
        "arxiv_id": "2507.15724",
        "ARXIVID": "2507.15724",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it investigates transformer-based spatially-controlled image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.14705": {
        "authors": [
            "Sai Wang",
            "Senthilnathan Subramanian",
            "Mudit Sahni",
            "Praneeth Gone",
            "Lingjie Meng",
            "Xiaochen Wang",
            "Nicolas Ferradas Bertoli",
            "Tingxian Cheng",
            "Jun Xu"
        ],
        "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents",
        "abstract": "arXiv:2507.14705v1 Announce Type: new  Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive behaviour that quickly renders static benchmarks and ad-hoc manual testing obsolete.   We present Neo, a configurable, multi-agent framework that automates realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly. Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn.   Applied to a production-grade Seller Financial Assistant chatbot, Neo (i) uncovered edge-case failures across five attack categories with a 3.3% break rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered 10-12X higher throughput, generating 180 coherent test questions in around 45 mins versus 16h of human effort. Beyond security probing, Neo's stochastic policies balanced topic coverage and conversational depth, yielding broader behavioural exploration than manually crafted scripts.   Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent interfaces, state controller and feedback loops are model-agnostic and extensible to richer factual-grounding and policy-compliance checks. We release the framework to facilitate reproducible, high-fidelity testing of emerging agentic systems.",
        "arxiv_id": "2507.14705",
        "ARXIVID": "2507.14705",
        "COMMENT": "Matches criterion 3 as it introduces a multi-agent framework for scalable testing of LLM-based agents, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.14935": {
        "authors": [
            "Hai Huang",
            "Yan Xia",
            "Shulei Wang",
            "Hanting Wang",
            "Minghui Fang",
            "Shengpeng Ji",
            "Sashuai Zhou",
            "Tao Jin",
            "Zhou Zhao"
        ],
        "title": "Open-set Cross Modal Generalization via Multimodal Unified Representation",
        "abstract": "arXiv:2507.14935v1 Announce Type: new  Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments by proposing the more challenging Open-set Cross Modal Generalization (OSCMG) task. This task evaluates multimodal unified representations in open-set conditions, addressing the limitations of prior closed-set cross-modal evaluations. OSCMG requires not only cross-modal knowledge transfer but also robust generalization to unseen classes within new modalities, a scenario frequently encountered in real-world applications. Existing multimodal unified representation work lacks consideration for open-set environments. To tackle this, we propose MICU, comprising two key components: Fine-Coarse Masked multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI enhances multimodal alignment by applying contrastive learning at both holistic semantic and temporal levels, incorporating masking to enhance generalization. CUJP enhances feature diversity and model uncertainty by integrating modality-agnostic feature selection with self-supervised learning, thereby strengthening the model's ability to handle unknown categories in open-set tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the effectiveness of our approach. The code is available at https://github.com/haihuangcode/CMG.",
        "arxiv_id": "2507.14935",
        "ARXIVID": "2507.14935",
        "COMMENT": "Matches criterion 2 as it explores multimodal unified representations and cross-modal generalization, which aligns with vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.15094": {
        "authors": [
            "Mengya Xu",
            "Rulin Zhou",
            "An Wang",
            "Chaoyang Lyu",
            "Zhen Li",
            "Ning Zhong",
            "Hongliang Ren"
        ],
        "title": "BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking",
        "abstract": "arXiv:2507.15094v1 Announce Type: new  Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses significant risks, demanding precise, real-time localization and continuous monitoring of the bleeding source for effective hemostatic intervention. In particular, endoscopists have to repeatedly flush to clear blood, allowing only milliseconds to identify bleeding sources, an inefficient process that prolongs operations and elevates patient risks. However, current Artificial Intelligence (AI) methods primarily focus on bleeding region segmentation, overlooking the critical need for accurate bleeding source detection and temporal tracking in the challenging ESD environment, which is marked by frequent visual obstructions and dynamic scene changes. This gap is widened by the lack of specialized datasets, hindering the development of robust AI-assisted guidance systems. To address these challenges, we introduce BleedOrigin-Bench, the first comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated bleeding sources across 106,222 frames from 44 procedures, supplemented with 39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6 challenging clinical scenarios. We also present BleedOrigin-Net, a novel dual-stage detection-tracking framework for the bleeding source localization in ESD procedures, addressing the complete workflow from bleeding onset detection to continuous spatial tracking. We compare with widely-used object detection models (YOLOv11/v12), multimodal large language models, and point tracking methods. Extensive evaluation demonstrates state-of-the-art performance, achieving 96.85% frame-level accuracy ($\\pm\\leq8$ frames) for bleeding onset detection, 70.24% pixel-level accuracy ($\\leq100$ px) for initial source detection, and 96.11% pixel-level accuracy ($\\leq100$ px) for point tracking.",
        "arxiv_id": "2507.15094",
        "ARXIVID": "2507.15094",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (BleedOrigin-Bench) and a novel method (BleedOrigin-Net) for embodied AI in a medical context.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.15728": {
        "authors": [
            "Wenqi Ouyang",
            "Zeqi Xiao",
            "Danni Yang",
            "Yifan Zhou",
            "Shuai Yang",
            "Lei Yang",
            "Jianlou Si",
            "Xingang Pan"
        ],
        "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation",
        "abstract": "arXiv:2507.15728v1 Announce Type: new  Abstract: Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ .",
        "arxiv_id": "2507.15728",
        "ARXIVID": "2507.15728",
        "COMMENT": "Matches criterion 6 as it introduces a novel methodology for long video generation, addressing challenges in video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.15330": {
        "authors": [
            "Hammad Atta",
            "Muhammad Zeeshan Baig",
            "Yasir Mehmood",
            "Nadeem Shahzad",
            "Ken Huang",
            "Muhammad Aziz Ul Haq",
            "Muhammad Awais",
            "Kamal Ahmed"
        ],
        "title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI",
        "abstract": "arXiv:2507.15330v1 Announce Type: new  Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic AI systems. Unlike traditional adversarial external threats such as prompt injection, these failures originate internally, arising from memory starvation, planner recursion, context flooding, and output suppression. These systemic weaknesses lead to silent agent drift, logic collapse, and persistent hallucinations over time. To address this class of failures, we introduce the Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain 10), a lifecycle-aware defense framework defined by a six-stage cognitive degradation lifecycle. The framework includes seven runtime controls (QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger proactive mitigation through fallback routing, starvation detection, and memory integrity enforcement. Drawing from cognitive neuroscience, we map agentic architectures to human analogs, enabling early detection of fatigue, starvation, and role collapse. By introducing a formal lifecycle and real-time mitigation controls, this work establishes Cognitive Degradation as a critical new class of AI system vulnerability and proposes the first cross-platform defense model for resilient agentic behavior.",
        "arxiv_id": "2507.15330",
        "ARXIVID": "2507.15330",
        "COMMENT": "Matches criterion 1 as it addresses spatial intelligence and embodied agents by introducing a framework for mitigating cognitive degradation in agentic AI systems.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2507.15540": {
        "authors": [
            "Syed Ahmed Mahmood",
            "Ali Shah Ali",
            "Umer Ahmed",
            "Fawad Javed Fateh",
            "M. Zeeshan Zia",
            "Quoc-Huy Tran"
        ],
        "title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport",
        "abstract": "arXiv:2507.15540v1 Announce Type: new  Abstract: We study the problem of self-supervised procedure learning, which discovers key steps and establishes their order from a set of unlabeled procedural videos. Previous procedure learning methods typically learn frame-to-frame correspondences between videos before determining key steps and their order. However, their performance often suffers from order variations, background/redundant frames, and repeated actions. To overcome these challenges, we propose a self-supervised procedure learning framework, which utilizes a fused Gromov-Wasserstein optimal transport formulation with a structural prior for computing frame-to-frame mapping between videos. However, optimizing exclusively for the above temporal alignment term may lead to degenerate solutions, where all frames are mapped to a small cluster in the embedding space and hence every video is associated with only one key step. To address that limitation, we further integrate a contrastive regularization term, which maps different frames to different points in the embedding space, avoiding the collapse to trivial solutions. Finally, we conduct extensive experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e., ProceL and CrossTask) benchmarks to demonstrate superior performance by our approach against previous methods, including OPEL which relies on a traditional Kantorovich optimal transport formulation with an optimality prior.",
        "arxiv_id": "2507.15540",
        "ARXIVID": "2507.15540",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on self-supervised procedure learning from procedural videos, which involves video-based tasks like key step discovery and temporal alignment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15225": {
        "authors": [
            "Yichi Zhou",
            "Jianqiu Zhao",
            "Yongxin Zhang",
            "Bohan Wang",
            "Siran Wang",
            "Luoxin Chen",
            "Jiahui Wang",
            "Haowei Chen",
            "Allan Jie",
            "Xinbo Zhang",
            "Haocheng Wang",
            "Luong Trung",
            "Rong Ye",
            "Phan Nhat Hoang",
            "Huishuai Zhang",
            "Peng Sun",
            "Hang Li"
        ],
        "title": "Solving Formal Math Problems by Decomposition and Iterative Reflection",
        "abstract": "arXiv:2507.15225v1 Announce Type: new  Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success in intelligence, performing comparably to human experts on complex reasoning tasks such as coding and mathematical reasoning. However, generating formal proofs in specialized languages like Lean 4 remains a significant challenge for these models, limiting their application in complex theorem proving and automated verification. Current approaches typically require specializing models through fine-tuning on dedicated formal corpora, incurring high costs for data collection and training. In this work, we introduce \\textbf{Delta Prover}, an agent-based framework that orchestrates the interaction between a general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages the reflection and reasoning capabilities of general-purpose LLMs to interactively construct formal proofs in Lean 4, circumventing the need for model specialization. At its core, the agent integrates two novel, interdependent components: an algorithmic framework for reflective decomposition and iterative proof repair, and a custom Domain-Specific Language (DSL) built upon Lean 4 for streamlined subproblem management. \\textbf{Delta Prover achieves a state-of-the-art 95.9\\% success rate on the miniF2F-test benchmark, surpassing all existing approaches, including those requiring model specialization.} Furthermore, Delta Prover exhibits a significantly stronger test-time scaling law compared to standard Best-of-N proof strategies. Crucially, our findings demonstrate that general-purpose LLMs, when guided by an effective agentic structure, possess substantial untapped theorem-proving capabilities. This presents a computationally efficient alternative to specialized models for robust automated reasoning in formal environments.",
        "arxiv_id": "2507.15225",
        "ARXIVID": "2507.15225",
        "COMMENT": "This paper does not match any specific criteria but introduces a novel framework for theorem proving, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 8
    },
    "2507.14686": {
        "authors": [
            "Chen Cai",
            "Tianyi Liu",
            "Jianjun Gao",
            "Wenyang Liu",
            "Kejun Wu",
            "Ruoyu Wang",
            "Yi Wang",
            "Soo Chin Liew"
        ],
        "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition",
        "abstract": "arXiv:2507.14686v1 Announce Type: new  Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot abilities but struggle with complex Grounded Situation Recognition (GSR) and are resource-intensive for edge device deployment. Meanwhile, conventional GSR models often lack generalization ability, falling short in recognizing unseen and rare situations. In this paper, we exploit transferring knowledge from a teacher MLLM to a small GSR model to enhance its generalization and zero-shot abilities, thereby introducing the task of Open-vocabulary Grounded Situation Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt Distillation (MIPD), a novel framework that distills enriched multimodal knowledge from the foundation model, enabling the student Ov-GSR model to recognize unseen situations and be better aware of rare situations. Specifically, the MIPD framework first leverages the LLM-based Judgmental Rationales Generator (JRG) to construct positive and negative glimpse and gaze rationales enriched with contextual semantic information. The proposed scene-aware and instance-perception prompts are then introduced to align rationales with visual information from the MLLM teacher via the Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively capturing holistic and perceptual multimodal knowledge. Finally, the aligned multimodal knowledge is distilled into the student Ov-GSR model, providing a stronger foundation for generalization that enhances situation understanding, bridges the gap between seen and unseen scenarios, and mitigates prediction bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving superior performance on seen, rare, and unseen situations, and further demonstrate improved unseen detection on the HICO-DET dataset.",
        "arxiv_id": "2507.14686",
        "ARXIVID": "2507.14686",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models and their application to open-vocabulary situation recognition.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15428": {
        "authors": [
            "Jiaao Li",
            "Kaiyuan Li",
            "Chen Gao",
            "Yong Li",
            "Xinlei Chen"
        ],
        "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent",
        "abstract": "arXiv:2507.15428v1 Announce Type: new  Abstract: Egomotion videos are first-person recordings where the view changes continuously due to the agent's movement. As they serve as the primary visual input for embodied AI agents, making egomotion video reasoning more efficient is therefore essential for real-world deployment. Recent advances in vision-language models have enabled strong multimodal reasoning capabilities, but their computational cost remains prohibitive for long, redundant video inputs. Existing token pruning methods, typically designed for third-person videos, fail to leverage the spatiotemporal continuity and motion constraints inherent in egomotion settings. To address this, we propose EgoPrune, a training-free token pruning method tailored for egomotion video reasoning. EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR for temporally efficient sampling; Perspective-Aware Redundancy Filtering (PARF), which aligns visual tokens using perspective transformations and removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token selector that jointly considers visual-text relevance and intra-frame diversity. Experiments on two egomotion video benchmarks show that EgoPrune consistently outperforms prior training-free methods across various pruning ratios while significantly reducing FLOPs, memory usage, and latency. Moreover, we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB edge device, demonstrating its real-world efficiency and suitability for on-device egomotion video reasoning.",
        "arxiv_id": "2507.15428",
        "ARXIVID": "2507.15428",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for token pruning in egomotion video reasoning for embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15365": {
        "authors": [
            "Fatemeh Saleh",
            "Sadegh Aliakbarian",
            "Charlie Hewitt",
            "Lohit Petikam",
            "Xiao-Xian",
            "Antonio Criminisi",
            "Thomas J. Cashman",
            "Tadas Baltru\\v{s}aitis"
        ],
        "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data",
        "abstract": "arXiv:2507.15365v1 Announce Type: new  Abstract: The state of the art in human-centric computer vision achieves high accuracy and robustness across a diverse range of tasks. The most effective models in this domain have billions of parameters, thus requiring extremely large datasets, expensive training regimes, and compute-intensive inference. In this paper, we demonstrate that it is possible to train models on much smaller but high-fidelity synthetic datasets, with no loss in accuracy and higher efficiency. Using synthetic training data provides us with excellent levels of detail and perfect labels, while providing strong guarantees for data provenance, usage rights, and user consent. Procedural data synthesis also provides us with explicit control on data diversity, that we can use to address unfairness in the models we train. Extensive quantitative assessment on real input images demonstrates accuracy of our models on three dense prediction tasks: depth estimation, surface normal estimation, and soft foreground segmentation. Our models require only a fraction of the cost of training and inference when compared with foundational models of similar accuracy. Our human-centric synthetic dataset and trained models are available at https://aka.ms/DAViD.",
        "arxiv_id": "2507.15365",
        "ARXIVID": "2507.15365",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their applications, specifically using synthetic data for human-centric tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15569": {
        "authors": [
            "Xiaoyi Bao",
            "Chenwei Xie",
            "Hao Tang",
            "Tingyu Weng",
            "Xiaofeng Wang",
            "Yun Zheng",
            "Xingang Wang"
        ],
        "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding",
        "abstract": "arXiv:2507.15569v1 Announce Type: new  Abstract: In recent years, the introduction of Multi-modal Large Language Models (MLLMs) into video understanding tasks has become increasingly prevalent. However, how to effectively integrate temporal information remains a critical research focus. Traditional approaches treat spatial and temporal information separately. Due to issues like motion blur, it is challenging to accurately represent the spatial information of rapidly moving objects. This can lead to temporally important regions being underemphasized during spatial feature extraction, which in turn hinders accurate spatio-temporal interaction and video understanding. To address this limitation, we propose an innovative video representation method called Dynamic-Image (DynImg). Specifically, we introduce a set of non-key frames as temporal prompts to highlight the spatial areas containing fast-moving objects. During the process of visual feature extraction, these prompts guide the model to pay additional attention to the fine-grained spatial features corresponding to these regions. Moreover, to maintain the correct sequence for DynImg, we employ a corresponding 4D video Rotary Position Embedding. This retains both the temporal and spatial adjacency of DynImg, helping MLLM understand the spatio-temporal order within this combined format. Experimental evaluations reveal that DynImg surpasses the state-of-the-art methods by approximately 2% across multiple video understanding benchmarks, proving the effectiveness of our temporal prompts in enhancing video comprehension.",
        "arxiv_id": "2507.15569",
        "ARXIVID": "2507.15569",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a novel video representation method for multi-modal video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15492": {
        "authors": [
            "Rakesh John Amala Arokia Nathan",
            "Matthias Gessner",
            "Nurullah \\\"Ozkan",
            "Marius Bock",
            "Mohamed Youssef",
            "Maximilian Mews",
            "Bj\\\"orn Piltz",
            "Ralf Berger",
            "Oliver Bimber"
        ],
        "title": "An aerial color image anomaly dataset for search missions in complex forested terrain",
        "abstract": "arXiv:2507.15492v1 Announce Type: new  Abstract: After a family murder in rural Germany, authorities failed to locate the suspect in a vast forest despite a massive search. To aid the search, a research aircraft captured high-resolution aerial imagery. Due to dense vegetation obscuring small clues, automated analysis was ineffective, prompting a crowd-search initiative. This effort produced a unique dataset of labeled, hard-to-detect anomalies under occluded, real-world conditions. It can serve as a benchmark for improving anomaly detection approaches in complex forest environments, supporting manhunts and rescue operations. Initial benchmark tests showed existing methods performed poorly, highlighting the need for context-aware approaches. The dataset is openly accessible for offline processing. An additional interactive web interface supports online viewing and dynamic growth by allowing users to annotate and submit new findings.",
        "arxiv_id": "2507.15492",
        "ARXIVID": "2507.15492",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new dataset for anomaly detection in forested terrain.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15035": {
        "authors": [
            "Zhijun Zeng",
            "Youjia Zheng",
            "Hao Hu",
            "Zeyuan Dong",
            "Yihang Zheng",
            "Xinliang Liu",
            "Jinzhuo Wang",
            "Zuoqiang Shi",
            "Linfeng Zhang",
            "Yubing Li",
            "He Sun"
        ],
        "title": "OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography",
        "abstract": "arXiv:2507.15035v1 Announce Type: new  Abstract: Accurate and efficient simulation of wave equations is crucial in computational wave imaging applications, such as ultrasound computed tomography (USCT), which reconstructs tissue material properties from observed scattered waves. Traditional numerical solvers for wave equations are computationally intensive and often unstable, limiting their practical applications for quasi-real-time image reconstruction. Neural operators offer an innovative approach by accelerating PDE solving using neural networks; however, their effectiveness in realistic imaging is limited because existing datasets oversimplify real-world complexity. In this paper, we present OpenBreastUS, a large-scale wave equation dataset designed to bridge the gap between theoretical equations and practical imaging applications. OpenBreastUS includes 8,000 anatomically realistic human breast phantoms and over 16 million frequency-domain wave simulations using real USCT configurations. It enables a comprehensive benchmarking of popular neural operators for both forward simulation and inverse imaging tasks, allowing analysis of their performance, scalability, and generalization capabilities. By offering a realistic and extensive dataset, OpenBreastUS not only serves as a platform for developing innovative neural PDE solvers but also facilitates their deployment in real-world medical imaging problems. For the first time, we demonstrate efficient in vivo imaging of the human breast using neural operator solvers.",
        "arxiv_id": "2507.15035",
        "ARXIVID": "2507.15035",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark dataset for wave imaging in medical applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.14393": {
        "authors": [
            "Humza Sami",
            "Mubashir ul Islam",
            "Pierre-Emmanuel Gaillardon",
            "Valerio Tenace"
        ],
        "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation",
        "abstract": "arXiv:2507.14393v1 Announce Type: new  Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward in language model capabilities, aiming to tackle increasingly sophisticated tasks with unprecedented efficiency and accuracy. However, despite their impressive performance, recent studies have highlighted how current reasoning models frequently fail to generalize to novel, unseen problems, often resorting to memorized solutions rather than genuine inferential reasoning. Such behavior underscores a critical limitation in modern LRMs, i.e., their tendency toward overfitting, which in turn results in poor generalization in problem-solving capabilities.   In this paper, we introduce Nexus Architect, an enhanced iteration of our multi-agent system framework, Nexus, equipped with a novel automated workflow synthesis mechanism. Given a user's prompt and a small set of representative examples, the Architect autonomously generates a tailored reasoning workflow by selecting suitable strategies, tool integrations, and adversarial techniques for a specific problem class. Furthermore, the Architect includes an iterative prompt refinement mechanism that fine-tunes agents' system prompts to maximize performance and improve the generalization capabilities of the system.   We empirically evaluate Nexus Architect by employing an off-the-shelf, non-reasoning model on a custom dataset of challenging logical questions and compare its performance against state-of-the-art LRMs. Results show that Nexus Architect consistently outperforms existing solutions, achieving up to a 66% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against Claude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.",
        "arxiv_id": "2507.14393",
        "ARXIVID": "2507.14393",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a multi-agent system with reasoning capabilities, which could be relevant to spatial reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.14680": {
        "authors": [
            "Xinheng Lyu",
            "Yuci Liang",
            "Wenting Chen",
            "Meidan Ding",
            "Jiaqi Yang",
            "Guolin Huang",
            "Daokun Zhang",
            "Xiangjian He",
            "Linlin Shen"
        ],
        "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis",
        "abstract": "arXiv:2507.14680v1 Announce Type: new  Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel tissue analysis across various pathological tasks. While recent advancements in multi-modal large language models (MLLMs) allow multi-task WSI analysis through natural language, they often underperform compared to task-specific models. Collaborative multi-agent systems have emerged as a promising solution to balance versatility and accuracy in healthcare, yet their potential remains underexplored in pathology-specific domains. To address these issues, we propose WSI-Agents, a novel collaborative multi-agent system for multi-modal WSI analysis. WSI-Agents integrates specialized functional agents with robust task allocation and verification mechanisms to enhance both task-specific accuracy and multi-task versatility through three components: (1) a task allocation module assigning tasks to expert agents using a model zoo of patch and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through internal consistency checks and external validation using pathology knowledge bases and domain-specific models, and (3) a summary module synthesizing the final summary with visual interpretation maps. Extensive experiments on multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs and medical agent frameworks across diverse tasks.",
        "arxiv_id": "2507.14680",
        "ARXIVID": "2507.14680",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it discusses multi-modal large language models (MLLMs) for WSI analysis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.15602": {
        "authors": [
            "Zihui Gao",
            "Jia-Wang Bian",
            "Guosheng Lin",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting",
        "abstract": "arXiv:2507.15602v1 Announce Type: new  Abstract: Surface reconstruction and novel view rendering from sparse-view images are challenging. Signed Distance Function (SDF)-based methods struggle with fine details, while 3D Gaussian Splatting (3DGS)-based approaches lack global geometry coherence. We propose a novel hybrid method that combines the strengths of both approaches: SDF captures coarse geometry to enhance 3DGS-based rendering, while newly rendered images from 3DGS refine the details of SDF for accurate surface reconstruction. As a result, our method surpasses state-of-the-art approaches in surface reconstruction and novel view synthesis on the DTU and MobileBrick datasets. Code will be released at https://github.com/Gaozihui/SurfaceSplat.",
        "arxiv_id": "2507.15602",
        "ARXIVID": "2507.15602",
        "COMMENT": "Does not match any specific criteria. Focuses on surface reconstruction and novel view rendering, which are not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.15243": {
        "authors": [
            "Naeem Paeedeh",
            "Mahardhika Pratama",
            "Wolfgang Mayer",
            "Jimmy Cao",
            "Ryszard Kowlczyk"
        ],
        "title": "Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation",
        "abstract": "arXiv:2507.15243v1 Announce Type: new  Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model pre-trained with DINO combined with a prototypical classifier outperforms the latest SOTA methods. A crucial limitation that needs to be overcome is that updating too many parameters of the transformers leads to overfitting due to the scarcity of labeled samples. To address this challenge, we propose a new concept, Coalescent Projection (CP), as an effective successor to soft prompts. Additionally, we propose a novel pseudo-class generation method combined with Self-Supervised Transformations (SSTs) that relies solely on the base domain to prepare the network for encountering unseen samples from different domains. The proposed method exhibits its effectiveness in comprehensive experiments on the extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published at https://github.com/Naeem-Paeedeh/CPLSR.",
        "arxiv_id": "2507.15243",
        "ARXIVID": "2507.15243",
        "COMMENT": "This paper does not match any specific criteria but addresses cross-domain few-shot learning, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.15851": {
        "authors": [
            "Lingyu Li",
            "Yang Yao",
            "Yixu Wang",
            "Chubo Li",
            "Yan Teng",
            "Yingchun Wang"
        ],
        "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition",
        "abstract": "arXiv:2507.15851v1 Announce Type: new  Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain cognitive patterns similar to those of humans that are not directly specified in training data. This study investigates this phenomenon by focusing on temporal cognition in LLMs. Leveraging the similarity judgment task, we find that larger models spontaneously establish a subjective temporal reference point and adhere to the Weber-Fechner law, whereby the perceived distance logarithmically compresses as years recede from this reference point. To uncover the mechanisms behind this behavior, we conducted multiple analyses across neuronal, representational, and informational levels. We first identify a set of temporal-preferential neurons and find that this group exhibits minimal activation at the subjective reference point and implements a logarithmic coding scheme convergently found in biological systems. Probing representations of years reveals a hierarchical construction process, where years evolve from basic numerical values in shallow layers to abstract temporal orientation in deep layers. Finally, using pre-trained embedding models, we found that the training corpus itself possesses an inherent, non-linear temporal structure, which provides the raw material for the model's internal construction. In discussion, we propose an experientialist perspective for understanding these findings, where the LLMs' cognition is viewed as a subjective construction of the external world by its internal representational system. This nuanced perspective implies the potential emergence of alien cognitive frameworks that humans cannot intuitively predict, pointing toward a direction for AI alignment that focuses on guiding internal constructions. Our code is available at https://TheOtherMind.github.io.",
        "arxiv_id": "2507.15851",
        "ARXIVID": "2507.15851",
        "COMMENT": "This paper does not match any specific criteria but explores cognitive patterns in LLMs, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.15758": {
        "authors": [
            "Xingyu Wu",
            "Yuchen Yan",
            "Shangke Lyu",
            "Linjuan Wu",
            "Yiwen Qiu",
            "Yongliang Shen",
            "Weiming Lu",
            "Jian Shao",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization",
        "abstract": "arXiv:2507.15758v1 Announce Type: new  Abstract: Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.",
        "arxiv_id": "2507.15758",
        "ARXIVID": "2507.15758",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of reasoning and optimization in large models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.15690": {
        "authors": [
            "Hung Nguyen",
            "Runfa Li",
            "An Le",
            "Truong Nguyen"
        ],
        "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting",
        "abstract": "arXiv:2507.15690v1 Announce Type: new  Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in reconstructing high-quality novel views, as it often overfits to the widely-varying high-frequency (HF) details of the sparse training views. While frequency regularization can be a promising approach, its typical reliance on Fourier transforms causes difficult parameter tuning and biases towards detrimental HF learning. We propose DWTGS, a framework that rethinks frequency regularization by leveraging wavelet-space losses that provide additional spatial supervision. Specifically, we supervise only the low-frequency (LF) LL subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband in a self-supervised manner. Experiments across benchmarks show that DWTGS consistently outperforms Fourier-based counterparts, as this LF-centric strategy improves generalization and reduces HF hallucinations.",
        "arxiv_id": "2507.15690",
        "ARXIVID": "2507.15690",
        "COMMENT": "Does not match any specific criteria but is related to novel view synthesis and frequency regularization, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14811": {
        "authors": [
            "Jiaji Zhang",
            "Ruichao Sun",
            "Hailiang Zhao",
            "Jiaju Wu",
            "Peng Chen",
            "Hao Li",
            "Xinkui Zhao",
            "Kingsum Chow",
            "Gang Xiong",
            "Lin Ye",
            "Shuiguang Deng"
        ],
        "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models",
        "abstract": "arXiv:2507.14811v1 Announce Type: new  Abstract: Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost, with post-training quantization (PTQ) being particularly appealing due to its compatibility with pre-trained models without requiring retraining or training data. However, existing PTQ methods for diffusion models often rely on architecture-specific heuristics that limit their generalizability and hinder integration with industrial deployment pipelines. To address these limitations, we propose SegQuant, a unified quantization framework that adaptively combines complementary techniques to enhance cross-model versatility. SegQuant consists of a segment-aware, graph-based quantization strategy (SegLinear) that captures structural semantics and spatial heterogeneity, along with a dual-scale quantization scheme (DualScale) that preserves polarity-asymmetric activations, which is crucial for maintaining visual fidelity in generated outputs. SegQuant is broadly applicable beyond Transformer-based diffusion models, achieving strong performance while ensuring seamless compatibility with mainstream deployment tools.",
        "arxiv_id": "2507.14811",
        "ARXIVID": "2507.14811",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and computational efficiency, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14481": {
        "authors": [
            "Yujia Tong",
            "Jingling Yuan",
            "Tian Zhang",
            "Jianquan Liu",
            "Chuang Hu"
        ],
        "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning",
        "abstract": "arXiv:2507.14481v1 Announce Type: new  Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers (ViTs) without requiring access to data, allowing for the deployment of ViTs on devices with limited resources. In DFQ, the quantization model must be calibrated using synthetic samples, making the quality of these synthetic samples crucial. Existing methods fail to fully capture and balance the global and local features within the samples, resulting in limited synthetic data quality. Moreover, we have found that during inference, there is a significant difference in the distributions of intermediate layer activations between the quantized and full-precision models. These issues lead to a severe performance degradation of the quantized model. To address these problems, we propose a pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT). Specifically, we synthesize samples in order of increasing difficulty, effectively enhancing the quality of synthetic data. During the calibration and inference stage, we introduce the activation correction matrix for the quantized model to align the intermediate layer activations with those of the full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves remarkable superiority over existing DFQ methods and its performance is on par with models quantized through real data. For example, the performance of DeiT-T with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our method eliminates the need for fine-tuning, which not only reduces computational overhead but also lowers the deployment barriers for edge devices. This characteristic aligns with the principles of Green Learning by improving energy efficiency and facilitating real-world applications in resource-constrained environments.",
        "arxiv_id": "2507.14481",
        "ARXIVID": "2507.14481",
        "COMMENT": "Does not match any specific criteria. Focuses on data-free quantization for vision transformers, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15855": {
        "authors": [
            "Yichen Huang",
            "Lin F. Yang"
        ],
        "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025",
        "abstract": "arXiv:2507.15855v1 Announce Type: new  Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging problems requiring deep insight, creativity, and formal reasoning. While Large Language Models (LLMs) perform well on mathematical benchmarks like AIME, they struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly released IMO 2025 problems, avoiding data contamination. With pipeline design and prompt engineering, 5 (out of 6) problems are solved correctly (up to a caveat discussed below), highlighting the importance of finding the optimal way of using powerful models.",
        "arxiv_id": "2507.15855",
        "ARXIVID": "2507.15855",
        "COMMENT": "This paper does not match any specific criteria but discusses mathematical problem-solving with LLMs, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14447": {
        "authors": [
            "Guancheng Zeng",
            "Xueyi Chen",
            "Jiawang Hu",
            "Shaohua Qi",
            "Yaxuan Mao",
            "Zhantao Wang",
            "Yifan Nie",
            "Shuang Li",
            "Qiuyang Feng",
            "Pengxu Qiu",
            "Yujia Wang",
            "Wenqiang Han",
            "Linyan Huang",
            "Gang Li",
            "Jingjing Mo",
            "Haowen Hu"
        ],
        "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise",
        "abstract": "arXiv:2507.14447v1 Announce Type: new  Abstract: The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. To address this, this paper introduces Routine, a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an accuracy increase to 88.2% on scenario-specific evaluations, indicating improved adherence to execution plans. In addition, we employed Routine-based distillation to create a scenario-specific, multi-step tool-calling dataset. Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%, approaching GPT-4o's performance. These results highlight Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing model adaptability to new scenarios. Our experimental results demonstrate that Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.",
        "arxiv_id": "2507.14447",
        "ARXIVID": "2507.14447",
        "COMMENT": "This paper does not match any specific criteria but discusses a framework for enterprise agent systems, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15793": {
        "authors": [
            "Ghassen Baklouti",
            "Julio Silva-Rodr\\'iguez",
            "Jose Dolz",
            "Houda Bahig",
            "Ismail Ben Ayed"
        ],
        "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation",
        "abstract": "arXiv:2507.15793v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is increasingly attracting interest in medical imaging due to its effectiveness and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA) is a notable approach based on the assumption that the adaptation inherently occurs in a low-dimensional subspace. While it has shown good performance, its implementation requires a fixed and unalterable rank, which might be challenging to select given the unique complexities and requirements of each medical imaging downstream task. Inspired by advancements in natural image processing, we introduce a novel approach for medical image segmentation that dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank representation of the trainable weight matrices as a singular value decomposition, we introduce an l_1 sparsity regularizer to the loss function, and tackle it with a proximal optimizer. The regularizer could be viewed as a penalty on the decomposition rank. Hence, its minimization enables to find task-adapted ranks automatically. Our method is evaluated in a realistic few-shot fine-tuning setting, where we compare it first to the standard LoRA and then to several other PEFT methods across two distinguishable tasks: base organs and novel organs. Our extensive experiments demonstrate the significant performance improvements driven by our method, highlighting its efficiency and robustness against suboptimal rank initialization. Our code is publicly available: https://github.com/ghassenbaklouti/ARENA",
        "arxiv_id": "2507.15793",
        "ARXIVID": "2507.15793",
        "COMMENT": "Does not closely match any specific criterion but is relevant to parameter-efficient fine-tuning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14897": {
        "authors": [
            "Renxi Wang",
            "Rifo Ahmad Genadi",
            "Bilal El Bouardi",
            "Yongxin Wang",
            "Fajri Koto",
            "Zhengzhong Liu",
            "Timothy Baldwin",
            "Haonan Li"
        ],
        "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents",
        "abstract": "arXiv:2507.14897v1 Announce Type: new  Abstract: Language model (LM) agents have gained significant attention for their ability to autonomously complete tasks through interactions with environments, tools, and APIs. LM agents are primarily built with prompt engineering or supervised finetuning. At the same time, reinforcement learning (RL) has been explored to enhance LM's capabilities, such as reasoning and factuality. However, the combination of the LM agents and reinforcement learning (Agent-RL) remains underexplored and lacks systematic study. To this end, we built AgentFly, a scalable and extensible Agent-RL framework designed to empower LM agents with a variety of RL algorithms. Our framework supports multi-turn interactions by adapting traditional RL methods with token-level masking. It features a decorator-based interface for defining tools and reward functions, enabling seamless extension and ease of use. To support high-throughput training, we implement asynchronous execution of tool calls and reward computations, and design a centralized resource management system for scalable environment coordination. We also provide a suite of prebuilt tools and environments, demonstrating the framework's effectiveness through successful agent training across multiple tasks.",
        "arxiv_id": "2507.14897",
        "ARXIVID": "2507.14897",
        "COMMENT": "Does not closely match any specific criterion but is relevant to reinforcement learning and language model agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15628": {
        "authors": [
            "Shanjiang Tang",
            "Rui Huang",
            "Hsinyu Luo",
            "Chunjiang Wang",
            "Ce Yu",
            "Yusen Li",
            "Hao Fu",
            "Chao Sun",
            "and Jian Xiao"
        ],
        "title": "A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications",
        "abstract": "arXiv:2507.15628v1 Announce Type: new  Abstract: The explosive growth of video data in recent years has brought higher demands for video analytics, where accuracy and efficiency remain the two primary concerns. Deep neural networks (DNNs) have been widely adopted to ensure accuracy; however, improving their efficiency in video analytics remains an open challenge. Different from existing surveys that make summaries of DNN-based video mainly from the accuracy optimization aspect, in this survey, we aim to provide a thorough review of optimization techniques focusing on the improvement of the efficiency of DNNs in video analytics. We organize existing methods in a bottom-up manner, covering multiple perspectives such as hardware support, data processing, operational deployment, etc. Finally, based on the optimization framework and existing works, we analyze and discuss the problems and challenges in the performance optimization of DNN-based video analytics.",
        "arxiv_id": "2507.15628",
        "ARXIVID": "2507.15628",
        "COMMENT": "Matches criterion 7 as it is a survey paper on efficiency optimization techniques for DNN-based video analytics.",
        "RELEVANCE": 5,
        "NOVELTY": 3
    },
    "2507.14334": {
        "authors": [
            "Hui Yang",
            "Jiaoyan Chen",
            "Yuan He",
            "Yongsheng Gao",
            "Ian Horrocks"
        ],
        "title": "Language Models as Ontology Encoders",
        "abstract": "arXiv:2507.14334v1 Announce Type: new  Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent complex knowledge and support semantic reasoning have been widely adopted across various domains such as healthcare and bioinformatics. Recently, ontology embeddings have gained wide attention due to its potential to infer plausible new knowledge and approximate complex reasoning. However, existing methods face notable limitations: geometric model-based embeddings typically overlook valuable textual information, resulting in suboptimal performance, while the approaches that incorporate text, which are often based on language models, fail to preserve the logical structure. In this work, we propose a new ontology embedding method OnT, which tunes a Pretrained Language Model (PLM) via geometric modeling in a hyperbolic space for effectively incorporating textual labels and simultaneously preserving class hierarchies and other logical relationships of Description Logic EL. Extensive experiments on four real-world ontologies show that OnT consistently outperforms the baselines including the state-of-the-art across both tasks of prediction and inference of axioms. OnT also demonstrates strong potential in real-world applications, indicated by its robust transfer learning abilities and effectiveness in real cases of constructing a new ontology from SNOMED CT. Data and code are available at https://github.com/HuiYang1997/OnT.",
        "arxiv_id": "2507.14334",
        "ARXIVID": "2507.14334",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of language models and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15064": {
        "authors": [
            "Shuyuan Tu",
            "Zhen Xing",
            "Xintong Han",
            "Zhi-Qi Cheng",
            "Qi Dai",
            "Chong Luo",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "title": "StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation",
        "abstract": "arXiv:2507.15064v1 Announce Type: new  Abstract: Current diffusion models for human image animation often struggle to maintain identity (ID) consistency, especially when the reference image and driving video differ significantly in body size or position. We introduce StableAnimator++, the first ID-preserving video diffusion framework with learnable pose alignment, capable of generating high-quality videos conditioned on a reference image and a pose sequence without any post-processing. Building upon a video diffusion model, StableAnimator++ contains carefully designed modules for both training and inference, striving for identity consistency. In particular, StableAnimator++ first uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition (SVD). These matrices align the driven poses with the reference image, mitigating misalignment to a great extent. StableAnimator++ then computes image and face embeddings using off-the-shelf encoders, refining the face embeddings via a global content-aware Face Encoder. To further maintain ID, we introduce a distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment. During the inference stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process, guiding the diffusion trajectory for enhanced facial fidelity. Experiments on benchmarks show the effectiveness of StableAnimator++ both qualitatively and quantitatively.",
        "arxiv_id": "2507.15064",
        "ARXIVID": "2507.15064",
        "COMMENT": "Does not closely match any specific criterion but is relevant to the general interest area of generative modeling and video-based tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14787": {
        "authors": [
            "Xi Xiao",
            "Aristeidis Tsaris",
            "Anika Tabassum",
            "John Lagergren",
            "Larry M. York",
            "Tianyang Wang",
            "Xiao Wang"
        ],
        "title": "FOCUS: Fused Observation of Channels for Unveiling Spectra",
        "abstract": "arXiv:2507.14787v1 Announce Type: new  Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous wavelength bands, making it a powerful tool in biology, agriculture, and environmental monitoring. However, interpreting Vision Transformers (ViTs) in this setting remains largely unexplored due to two key challenges: (1) existing saliency methods struggle to capture meaningful spectral cues, often collapsing attention onto the class token, and (2) full-spectrum ViTs are computationally prohibitive for interpretability, given the high-dimensional nature of HSI data. We present FOCUS, the first framework that enables reliable and efficient spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core components: class-specific spectral prompts that guide attention toward semantically meaningful wavelength groups, and a learnable [SINK] token trained with an attraction loss to absorb noisy or redundant attention. Together, these designs make it possible to generate stable and interpretable 3D saliency maps and spectral importance curves in a single forward pass, without any gradient backpropagation or backbone modification. FOCUS improves band-level IoU by 15 percent, reduces attention collapse by over 40 percent, and produces saliency results that align closely with expert annotations. With less than 1 percent parameter overhead, our method makes high-resolution ViT interpretability practical for real-world hyperspectral applications, bridging a long-standing gap between black-box modeling and trustworthy HSI decision-making.",
        "arxiv_id": "2507.14787",
        "ARXIVID": "2507.14787",
        "COMMENT": "Does not match any specific criterion but is related to hyperspectral imaging and interpretability, which are tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15686": {
        "authors": [
            "Wenjie Huang",
            "Qi Yang",
            "Shuting Xia",
            "He Huang",
            "Zhu Li",
            "Yiling Xu"
        ],
        "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression",
        "abstract": "arXiv:2507.15686v1 Announce Type: new  Abstract: Existing AI-based point cloud compression methods struggle with dependence on specific training data distributions, which limits their real-world deployment. Implicit Neural Representation (INR) methods solve the above problem by encoding overfitted network parameters to the bitstream, resulting in more distribution-agnostic results. However, due to the limitation of encoding time and decoder size, current INR based methods only consider lossy geometry compression. In this paper, we propose the first INR based lossless point cloud geometry compression method called Lossless Implicit Neural Representations for Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we design a group of point clouds level coding framework with an effective network initialization strategy, which can reduce around 60% encoding time. A lightweight coding network based on multiscale SparseConv, consisting of scale context extraction, child node prediction, and model compression modules, is proposed to realize fast inference and compact decoder size. Experimental results show that our method consistently outperforms traditional and AI-based methods: for example, with the convergence time in the MVUB dataset, our method reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and 21.95% compared to SparsePCGC. Our project can be seen on https://huangwenjie2023.github.io/LINR-PCGC/.",
        "arxiv_id": "2507.15686",
        "ARXIVID": "2507.15686",
        "COMMENT": "Does not match any specific criterion but is related to point cloud compression, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.15743": {
        "authors": [
            "Elahe Vedadi",
            "David Barrett",
            "Natalie Harris",
            "Ellery Wulczyn",
            "Shashir Reddy",
            "Roma Ruparel",
            "Mike Schaekermann",
            "Tim Strother",
            "Ryutaro Tanno",
            "Yash Sharma",
            "Jihyeon Lee",
            "C\\'ian Hughes",
            "Dylan Slack",
            "Anil Palepu",
            "Jan Freyberg",
            "Khaled Saab",
            "Valentin Li\\'evin",
            "Wei-Hung Weng",
            "Tao Tu",
            "Yun Liu",
            "Nenad Tomasev",
            "Kavita Kulkarni",
            "S. Sara Mahdavi",
            "Kelvin Guu",
            "Jo\\\"elle Barral",
            "Dale R. Webster",
            "James Manyika",
            "Avinatan Hassidim",
            "Katherine Chou",
            "Yossi Matias",
            "Pushmeet Kohli",
            "Adam Rodman",
            "Vivek Natarajan",
            "Alan Karthikesalingam",
            "David Stutz"
        ],
        "title": "Towards physician-centered oversight of conversational diagnostic AI",
        "abstract": "arXiv:2507.15743v1 Announce Type: new  Abstract: Recent work has demonstrated the promise of conversational AI systems for diagnostic dialogue. However, real-world assurance of patient safety means that providing individual diagnoses and treatment plans is considered a regulated activity by licensed professionals. Furthermore, physicians commonly oversee other team members in such activities, including nurse practitioners (NPs) or physician assistants/associates (PAs). Inspired by this, we propose a framework for effective, asynchronous oversight of the Articulate Medical Intelligence Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent system that performs history taking within guardrails, abstaining from individualized medical advice. Afterwards, g-AMIE conveys assessments to an overseeing primary care physician (PCP) in a clinician cockpit interface. The PCP provides oversight and retains accountability of the clinical decision. This effectively decouples oversight from intake and can thus happen asynchronously. In a randomized, blinded virtual Objective Structured Clinical Examination (OSCE) of text consultations with asynchronous oversight, we compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across 60 scenarios, g-AMIE outperformed both groups in performing high-quality intake, summarizing cases, and proposing diagnoses and management plans for the overseeing PCP to review. This resulted in higher quality composite decisions. PCP oversight of g-AMIE was also more time-efficient than standalone PCP consultations in prior work. While our study does not replicate existing clinical practices and likely underestimates clinicians' capabilities, our results demonstrate the promise of asynchronous oversight as a feasible paradigm for diagnostic AI systems to operate under expert human oversight for enhancing real-world care.",
        "arxiv_id": "2507.15743",
        "ARXIVID": "2507.15743",
        "COMMENT": "Does not match any specific criterion but is tangentially related to embodied AI through its focus on diagnostic AI systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.14432": {
        "authors": [
            "Han Gong",
            "Qiyue Li",
            "Zhi Liu",
            "Hao Zhou",
            "Peng Yuan Zhou",
            "Zhu Li",
            "Jie Li"
        ],
        "title": "Adaptive 3D Gaussian Splatting Video Streaming",
        "abstract": "arXiv:2507.14432v1 Announce Type: new  Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the quality of volumetric video representation. Meanwhile, in contrast to conventional volumetric video, 3DGS video poses significant challenges for streaming due to its substantially larger data volume and the heightened complexity involved in compression and transmission. To address these issues, we introduce an innovative framework for 3DGS volumetric video streaming. Specifically, we design a 3DGS video construction method based on the Gaussian deformation field. By employing hybrid saliency tiling and differentiated quality modeling of 3DGS video, we achieve efficient data compression and adaptation to bandwidth fluctuations while ensuring high transmission quality. Then we build a complete 3DGS video streaming system and validate the transmission performance. Through experimental evaluation, our method demonstrated superiority over existing approaches in various aspects, including video quality, compression effectiveness, and transmission rate.",
        "arxiv_id": "2507.14432",
        "ARXIVID": "2507.14432",
        "COMMENT": "Does not match any specific criteria but is related to volumetric video streaming and compression, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.15578": {
        "authors": [
            "Gabriele Inzerillo",
            "Diego Valsesia",
            "Aniello Fiengo",
            "Enrico Magli"
        ],
        "title": "Compress-Align-Detect: onboard change detection from unregistered images",
        "abstract": "arXiv:2507.15578v1 Announce Type: new  Abstract: Change detection from satellite images typically incurs a delay ranging from several hours up to days because of latency in downlinking the acquired images and generating orthorectified image products at the ground stations; this may preclude real- or near real-time applications. To overcome this limitation, we propose shifting the entire change detection workflow onboard satellites. This requires to simultaneously solve challenges in data storage, image registration and change detection with a strict complexity constraint. In this paper, we present a novel and efficient framework for onboard change detection that addresses the aforementioned challenges in an end-to-end fashion with a deep neural network composed of three interlinked submodules: (1) image compression, tailored to minimize onboard data storage resources; (2) lightweight co-registration of non-orthorectified multi-temporal image pairs; and (3) a novel temporally-invariant and computationally efficient change detection model. This is the first approach in the literature combining all these tasks in a single end-to-end framework with the constraints dictated by onboard processing. Experimental results compare each submodule with the current state-of-the-art, and evaluate the performance of the overall integrated system in realistic setting on low-power hardware. Compelling change detection results are obtained in terms of F1 score as a function of compression rate, sustaining a throughput of 0.7 Mpixel/s on a 15W accelerator.",
        "arxiv_id": "2507.15578",
        "ARXIVID": "2507.15578",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and efficient change detection, which aligns with your friend's general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.14697": {
        "authors": [
            "Zhiwei Zhang",
            "Zi Ye",
            "Yibin Wen",
            "Shuai Yuan",
            "Haohuan Fu",
            "Jianxi Huang",
            "Juepeng Zheng"
        ],
        "title": "GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset",
        "abstract": "arXiv:2507.14697v1 Announce Type: new  Abstract: Agricultural parcels serve as basic units for conducting agricultural practices and applications, which is vital for land ownership registration, food security assessment, soil erosion monitoring, etc. However, existing agriculture parcel extraction studies only focus on mid-resolution mapping or regular plain farmlands while lacking representation of complex terraced terrains due to the demands of precision agriculture.In this paper, we introduce a more fine-grained terraced parcel dataset named GTPBD (Global Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset covering major worldwide terraced regions with more than 200,000 complex terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution images with three-level labels, including pixel-level boundary labels, mask labels, and parcel labels. It covers seven major geographic zones in China and transcontinental climatic regions around the world.Compared to the existing datasets, the GTPBD dataset brings considerable challenges due to the: (1) terrain diversity; (2) complex and irregular parcel objects; and (3) multiple domain styles. Our proposed GTPBD dataset is suitable for four different tasks, including semantic segmentation, edge detection, terraced parcel extraction, and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the GTPBD dataset on eight semantic segmentation methods, four edge extraction methods, three parcel extraction methods, and five UDA methods, along with a multi-dimensional evaluation framework integrating pixel-level and object-level metrics. GTPBD fills a critical gap in terraced remote sensing research, providing a basic infrastructure for fine-grained agricultural terrain analysis and cross-scenario knowledge transfer.",
        "arxiv_id": "2507.14697",
        "ARXIVID": "2507.14697",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.15520": {
        "authors": [
            "Hanting Li",
            "Fei Zhou",
            "Xin Sun",
            "Yang Hua",
            "Jungong Han",
            "Liang-Jie Zhang"
        ],
        "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement",
        "abstract": "arXiv:2507.15520v1 Announce Type: new  Abstract: Recent Transformer-based low-light enhancement methods have made promising progress in recovering global illumination. However, they still struggle with non-uniform lighting scenarios, such as backlit and shadow, appearing as over-exposure or inadequate brightness restoration. To address this challenge, we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer) framework that enables accurate illumination restoration. Specifically, we propose a dynamic integral image representation to model the spatially-varying illumination, and further construct a novel Spatially-Adaptive Integral Illumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which leverages the illumination to calibrate the lightness-relevant features toward visual-pleased illumination enhancement. Extensive experiments on five standard low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our SAIGFormer significantly outperforms state-of-the-art methods in both quantitative and qualitative metrics. In particular, our method achieves superior performance in non-uniform illumination enhancement while exhibiting strong generalization capabilities across multiple datasets. Code is available at https://github.com/LHTcode/SAIGFormer.git.",
        "arxiv_id": "2507.15520",
        "ARXIVID": "2507.15520",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.14879": {
        "authors": [
            "Rizhao Fan",
            "Tianfang Ma",
            "Zhigen Li",
            "Ning An",
            "Jian Cheng"
        ],
        "title": "Region-aware Depth Scale Adaptation with Sparse Measurements",
        "abstract": "arXiv:2507.14879v1 Announce Type: new  Abstract: In recent years, the emergence of foundation models for depth prediction has led to remarkable progress, particularly in zero-shot monocular depth estimation. These models generate impressive depth predictions; however, their outputs are often in relative scale rather than metric scale. This limitation poses challenges for direct deployment in real-world applications. To address this, several scale adaptation methods have been proposed to enable foundation models to produce metric depth. However, these methods are typically costly, as they require additional training on new domains and datasets. Moreover, fine-tuning these models often compromises their original generalization capabilities, limiting their adaptability across diverse scenes. In this paper, we introduce a non-learning-based approach that leverages sparse depth measurements to adapt the relative-scale predictions of foundation models into metric-scale depth. Our method requires neither retraining nor fine-tuning, thereby preserving the strong generalization ability of the original foundation models while enabling them to produce metric depth. Experimental results demonstrate the effectiveness of our approach, high-lighting its potential to bridge the gap between relative and metric depth without incurring additional computational costs or sacrificing generalization ability.",
        "arxiv_id": "2507.14879",
        "ARXIVID": "2507.14879",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.15765": {
        "authors": [
            "Feng-Qi Cui",
            "Anyang Tong",
            "Jinyang Huang",
            "Jie Zhang",
            "Dan Guo",
            "Zhi Liu",
            "Meng Wang"
        ],
        "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization",
        "abstract": "arXiv:2507.15765v1 Announce Type: new  Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in affective computing and human-computer interaction. Although existing methods achieve comparable performance, they inevitably suffer from performance degradation under sample heterogeneity caused by multi-source data and individual expression variability. To address these challenges, we propose a novel framework, called Heterogeneity-aware Distributional Framework (HDF), and design two plug-and-play modules to enhance time-frequency modeling and mitigate optimization imbalance caused by hard samples. Specifically, the Time-Frequency Distributional Attention Module (DAM) captures both temporal consistency and frequency robustness through a dual-branch attention design, improving tolerance to sequence inconsistency and visual style shifts. Then, based on gradient sensitivity and information bottleneck principles, an adaptive optimization module Distribution-aware Scaling Module (DSM) is introduced to dynamically balance classification and contrastive losses, enabling more stable and discriminative representation learning. Extensive experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF significantly improves both recognition accuracy and robustness. Our method achieves superior weighted average recall (WAR) and unweighted average recall (UAR) while maintaining strong generalization across diverse and imbalanced scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.",
        "arxiv_id": "2507.15765",
        "ARXIVID": "2507.15765",
        "COMMENT": "Does not match any specific criterion but is related to dynamic facial expression recognition, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.15085": {
        "authors": [
            "Peirong Zhang",
            "Haowei Xu",
            "Jiaxin Zhang",
            "Guitao Xu",
            "Xuhan Zheng",
            "Zhenhua Yang",
            "Junle Liu",
            "Yuyi Zhang",
            "Lianwen Jin"
        ],
        "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR",
        "abstract": "arXiv:2507.15085v1 Announce Type: new  Abstract: Text image is a unique and crucial information medium that integrates visual aesthetics and linguistic semantics in modern e-society. Due to their subtlety and complexity, the generation of text images represents a challenging and evolving frontier in the image generation field. The recent surge of specialized image generators (\\emph{e.g.}, Flux-series) and unified generative models (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a natural question: can they master the intricacies of text image generation and editing? Motivated by this, we assess current state-of-the-art generative models' capabilities in terms of text image generation and editing. We incorporate various typical optical character recognition (OCR) tasks into our evaluation and broaden the concept of text-based generation tasks into OCR generative tasks. We select 33 representative tasks and categorize them into five categories: document, handwritten text, scene text, artistic text, and complex \\& layout-rich text. For comprehensive evaluation, we examine six models across both closed-source and open-source domains, using tailored, high-quality image inputs and prompts. Through this evaluation, we draw crucial observations and identify the weaknesses of current generative models for OCR tasks. We argue that photorealistic text image generation and editing should be internalized as foundational skills into general-domain generative models, rather than being delegated to specialized solutions, and we hope this empirical analysis can provide valuable insights for the community to achieve this goal. This evaluation is online and will be continuously updated at our GitHub repository.",
        "arxiv_id": "2507.15085",
        "ARXIVID": "2507.15085",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling and OCR tasks, which are tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}