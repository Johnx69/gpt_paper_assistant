{
    "2507.08801": {
        "authors": [
            "Hangjie Yuan",
            "Weihua Chen",
            "Jun Cen",
            "Hu Yu",
            "Jingyun Liang",
            "Shuning Chang",
            "Zhihui Lin",
            "Tao Feng",
            "Pengwei Liu",
            "Jiazheng Xing",
            "Hao Luo",
            "Jiasheng Tang",
            "Fan Wang",
            "Yi Yang"
        ],
        "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective",
        "abstract": "arXiv:2507.08801v1 Announce Type: new  Abstract: Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.",
        "arxiv_id": "2507.08801",
        "ARXIVID": "2507.08801",
        "COMMENT": "Matches criteria 2 and 5 as it explores autoregressive video generation with a unified model perspective, focusing on multimodal spatiotemporal data and integration of video understanding with LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.08306": {
        "authors": [
            "Inclusion AI",
            ":",
            "Fudong Wang",
            "Jiajia Liu",
            "Jingdong Chen",
            "Jun Zhou",
            "Kaixiang Ji",
            "Lixiang Ru",
            "Qingpei Guo",
            "Ruobing Zheng",
            "Tianqi Li",
            "Yi Yuan",
            "Yifan Mao",
            "Yuting Xiao",
            "Ziping Ma"
        ],
        "title": "M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning",
        "abstract": "arXiv:2507.08306v1 Announce Type: new  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly through Reinforcement Learning with Verifiable Rewards (RLVR), have significantly enhanced their reasoning abilities. However, a critical gap persists: these models struggle with dynamic spatial interactions, a capability essential for real-world applications. To bridge this gap, we introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals. This combination of curated data and advanced training allows M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks, showcasing superior performance in both general and spatial reasoning domains.",
        "arxiv_id": "2507.08306",
        "ARXIVID": "2507.08306",
        "COMMENT": "Matches criteria 2 as it focuses on enhancing MLLMs with unified general and spatial reasoning, relevant to multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.08772": {
        "authors": [
            "Shaocong Dong",
            "Lihe Ding",
            "Xiao Chen",
            "Yaokun Li",
            "Yuxin Wang",
            "Yucheng Wang",
            "Qi Wang",
            "Jaehyeok Kim",
            "Chenjian Gao",
            "Zhanpeng Huang",
            "Zibin Wang",
            "Tianfan Xue",
            "Dan Xu"
        ],
        "title": "From One to More: Contextual Part Latents for 3D Generation",
        "abstract": "arXiv:2507.08772v1 Announce Type: new  Abstract: Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.",
        "arxiv_id": "2507.08772",
        "ARXIVID": "2507.08772",
        "COMMENT": "Matches criteria 5 as it focuses on part-aware 3D generation with contextual part latents, integrating 3D understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2507.08607": {
        "authors": [
            "Shuang Cui",
            "Jinglin Xu",
            "Yi Li",
            "Xiongxin Tang",
            "Jiangmeng Li",
            "Jiahuan Zhou",
            "Fanjiang Xu",
            "Fuchun Sun",
            "Hui Xiong"
        ],
        "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language Models via Gaussian Discriminant Analysis",
        "abstract": "arXiv:2507.08607v1 Announce Type: new  Abstract: Vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition but degrade significantly under \\textit{temporally evolving distribution shifts} common in real-world scenarios (e.g., gradual illumination or seasonal changes). Existing continual test-time adaptation (CTTA) methods are typically built around sudden and severe distribution shifts and neglect temporal continuity, leading to three core defects: limited memory cache restricts long-range distribution modeling, causing catastrophic forgetting; entropy-based confidence becomes unreliable under temporal drift, worsening error accumulation; and static visual representations misalign with evolving inputs. We formalize this practical problem as \\textit{Continual-Temporal Test-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over time. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation framework that enforces temporally consistent predictions and dynamically aligns visual representations. Specifically, BayesTTA incrementally estimates class-conditional Gaussian mixture distributions without storing raw data, adaptively selects covariance structures through statistical hypothesis testing, and performs calibrated inference using Gaussian discriminant analysis (GDA). These calibrated predictions supervise self-paced adaptation of normalization layers, ensuring efficient and stable representation alignment. We establish a comprehensive CT-TTA benchmark across four temporally evolving datasets and further evaluate generalization on ten standard TTA datasets. Extensive experiments show that BayesTTA consistently outperforms state-of-the-art methods, achieving significant gains while maintaining efficiency. Code is available at \\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
        "arxiv_id": "2507.08607",
        "ARXIVID": "2507.08607",
        "COMMENT": "This paper closely matches criterion 2 as it explores vision-language models (VLMs) and proposes a novel adaptation framework for temporally evolving distribution shifts, which aligns with advancements in VLLMs and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.08410": {
        "authors": [
            "Shijun Yang",
            "Xiang Zhang",
            "Wanqing Zhao",
            "Hangzai Luo",
            "Sheng Zhong",
            "Jinye Peng",
            "Jianping Fan"
        ],
        "title": "Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models",
        "abstract": "arXiv:2507.08410v1 Announce Type: new  Abstract: Prompt learning facilitates the efficient adaptation of Vision-Language Models (VLMs) to various downstream tasks. However, it faces two significant challenges: (1) inadequate modeling of class embedding distributions for unseen instances, leading to suboptimal generalization on novel classes; (2) prevailing methodologies predominantly confine cross-modal alignment to the final output layer of vision and text encoders, which fundamentally limits their capacity to preserve topological consistency with pre-trained multi-modal embedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance Conditional Prompt Learning), a novel paradigm designed for conditional prompt generation. MuGCP leverages Multi-modal Large Language Models (MLLMs) as conditional prompt learners to adaptively generate Semantic Conditional Prompts (SCP) that incorporate rich, fine-grained high-level semantic knowledge for image instances. To ensure effective alignment and interaction across the multi-modal space of Vision-Language Models (VLMs), we introduce the Attention Mutual-Guidance (AMG) module, which facilitates interactions between visual and semantic information. Through mutual guidance, the AMG module generates Visual Conditional Prompts (VCP), enhancing the model's performance in multi-modal tasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that integrates SCP and VCP with contextual prompts, ensuring seamless coordination among the different prompts and enhancing the modeling of class embeddings and instance-specific knowledge. Our MuGCP outperforms existing state-of-the-art methods on 14 different datasets. The code will be made available after publication.",
        "arxiv_id": "2507.08410",
        "ARXIVID": "2507.08410",
        "COMMENT": "Matches criteria 2 and 5 as it explores multimodal large language models and techniques for vision-language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.08307": {
        "authors": [
            "Kui Jiang",
            "Shiyu Liu",
            "Junjun Jiang",
            "Xin Yang",
            "Hongxun Yang",
            "Xiaopeng Fan"
        ],
        "title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation",
        "abstract": "arXiv:2507.08307v1 Announce Type: new  Abstract: Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating optimization.Specifically, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction accuracy.Meanwhile, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video generation.Experiments across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is https://m2dao-talker.github.io/M2DAO-Talk.github.io",
        "arxiv_id": "2507.08307",
        "ARXIVID": "2507.08307",
        "COMMENT": "Matches criteria 6 as it focuses on video-based tasks, specifically audio-driven talking head generation, with novel methodologies for motion decoupling and optimization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.08400": {
        "authors": [
            "Yongjian Zhang",
            "Longguang Wang",
            "Kunhong Li",
            "Ye Zhang",
            "Yun Wang",
            "Liang Lin",
            "Yulan Guo"
        ],
        "title": "PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models",
        "abstract": "arXiv:2507.08400v1 Announce Type: new  Abstract: This work presents PanMatch, a versatile foundation model for robust correspondence matching. Unlike previous methods that rely on task-specific architectures and domain-specific fine-tuning to support tasks like stereo matching, optical flow or feature matching, our key insight is that any two-frame correspondence matching task can be addressed within a 2D displacement estimation framework using the same model weights. Such a formulation eliminates the need for designing specialized unified architectures or task-specific ensemble models. Instead, it achieves multi-task integration by endowing displacement estimation algorithms with unprecedented generalization capabilities. To this end, we highlight the importance of a robust feature extractor applicable across multiple domains and tasks, and propose the feature transformation pipeline that leverage all-purpose features from Large Vision Models to endow matching baselines with zero-shot cross-view matching capabilities. Furthermore, we assemble a cross-domain dataset with near 1.8 million samples from stereo matching, optical flow, and feature matching domains to pretrain PanMatch. We demonstrate the versatility of PanMatch across a wide range of domains and downstream tasks using the same model weights. Our model outperforms UniMatch and Flow-Anything on cross-task evaluations, and achieves comparable performance to most state-of-the-art task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch presents unprecedented zero-shot performance in abnormal scenarios, such as rainy day and satellite imagery, where most existing robust algorithms fail to yield meaningful results.",
        "arxiv_id": "2507.08400",
        "ARXIVID": "2507.08400",
        "COMMENT": "Matches criterion 4 as it focuses on a vision foundation model (PanMatch) for robust correspondence matching across multiple tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.08137": {
        "authors": [
            "Hyungjun Doh",
            "Dong In Lee",
            "Seunggeun Chi",
            "Pin-Hao Huang",
            "Kwonjoon Lee",
            "Sangpil Kim",
            "Karthik Ramani"
        ],
        "title": "Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction",
        "abstract": "arXiv:2507.08137v1 Announce Type: new  Abstract: We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques.",
        "arxiv_id": "2507.08137",
        "ARXIVID": "2507.08137",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for reconstructing dynamic human-object interactions, addressing occlusions and temporal inconsistencies in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.08548": {
        "authors": [
            "Alen Adamyan",
            "Tom\\'a\\v{s} \\v{C}\\'i\\v{z}ek",
            "Matej Straka",
            "Klara Janouskova",
            "Martin Schmid"
        ],
        "title": "SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2",
        "abstract": "arXiv:2507.08548v1 Announce Type: new  Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.",
        "arxiv_id": "2507.08548",
        "ARXIVID": "2507.08548",
        "COMMENT": "Matches criteria 3 as it introduces a novel reinforcement learning approach for memory control in embodied AI systems.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.08743": {
        "authors": [
            "Rei Tamaru",
            "Pei Li",
            "Bin Ran"
        ],
        "title": "Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection",
        "abstract": "arXiv:2507.08743v1 Announce Type: new  Abstract: Digital Twins (DT) have the potential to transform traffic management and operations by creating dynamic, virtual representations of transportation systems that sense conditions, analyze operations, and support decision-making. A key component for DT of the transportation system is dynamic roadway geometry sensing. However, existing approaches often rely on static maps or costly sensors, limiting scalability and adaptability. Additionally, large-scale DTs that collect and analyze data from multiple sources face challenges in privacy, communication, and computational efficiency. To address these challenges, we introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated Twin), a unified framework that combines real-time lane detection, DT synchronization, and federated meta-learning. At the core of Geo-ORBIT is GeoLane, a lightweight lane detection model that learns lane geometries from vehicle trajectory data using roadside cameras. We extend this model through Meta-GeoLane, which learns to personalize detection parameters for local entities, and FedMeta-GeoLane, a federated learning strategy that ensures scalable and privacy-preserving adaptation across roadside deployments. Our system is integrated with CARLA and SUMO to create a high-fidelity DT that renders highway scenarios and captures traffic flows in real-time. Extensive experiments across diverse urban scenes show that FedMeta-GeoLane consistently outperforms baseline and meta-learning approaches, achieving lower geometric error and stronger generalization to unseen locations while drastically reducing communication overhead. This work lays the foundation for flexible, context-aware infrastructure modeling in DTs. The framework is publicly available at https://github.com/raynbowy23/FedMeta-GeoLane.git.",
        "arxiv_id": "2507.08743",
        "ARXIVID": "2507.08743",
        "COMMENT": "Matches criteria 3 as it introduces a new framework for scene-adaptive lane geometry detection, relevant to embodied/robotic AI benchmarks and methods.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.08441": {
        "authors": [
            "Anlin Zheng",
            "Xin Wen",
            "Xuanyang Zhang",
            "Chuofan Ma",
            "Tiancai Wang",
            "Gang Yu",
            "Xiangyu Zhang",
            "Xiaojuan Qi"
        ],
        "title": "Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation",
        "abstract": "arXiv:2507.08441v1 Announce Type: new  Abstract: Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community.",
        "arxiv_id": "2507.08441",
        "ARXIVID": "2507.08441",
        "COMMENT": "Matches criteria 4 as it focuses on vision foundation models and their applications, specifically as visual tokenizers for image generation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2507.08367": {
        "authors": [
            "Yuki Yoshihara",
            "Linjing Jiang",
            "Nihan Karatas",
            "Hitoshi Kanamori",
            "Asuka Harada",
            "Takahiro Tanaka"
        ],
        "title": "Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment",
        "abstract": "arXiv:2507.08367v1 Announce Type: new  Abstract: This study investigates the potential of a multimodal large language model (LLM), specifically ChatGPT-4o, to perform human-like interpretations of traffic scenes using static dashcam images. Herein, we focus on three judgment tasks relevant to elderly driver assessments: evaluating traffic density, assessing intersection visibility, and recognizing stop signs recognition. These tasks require contextual reasoning rather than simple object detection. Using zero-shot, few-shot, and multi-shot prompting strategies, we evaluated the performance of the model with human annotations serving as the reference standard. Evaluation metrics included precision, recall, and F1-score. Results indicate that prompt design considerably affects performance, with recall for intersection visibility increasing from 21.7% (zero-shot) to 57.0% (multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In stop-sign detection, the model demonstrated high precision (up to 86.3%) but a lower recall (approximately 76.7%), indicating a conservative response tendency. Output stability analysis revealed that humans and the model faced difficulties interpreting structurally ambiguous scenes. However, the model's explanatory texts corresponded with its predictions, enhancing interpretability. These findings suggest that, with well-designed prompts, LLMs hold promise as supportive tools for scene-level driving risk assessments. Future studies should explore scalability using larger datasets, diverse annotators, and next-generation model architectures for elderly driver assessments.",
        "arxiv_id": "2507.08367",
        "ARXIVID": "2507.08367",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models for traffic scene understanding, focusing on vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.08344": {
        "authors": [
            "Jihao Gu",
            "Fei Wang",
            "Kun Li",
            "Yanyan Wei",
            "Zhiliang Wu",
            "Dan Guo"
        ],
        "title": "MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion",
        "abstract": "arXiv:2507.08344v1 Announce Type: new  Abstract: In this paper, we present MM-Gesture, the solution developed by our team HFUT-VUT, which ranked 1st in the micro-gesture classification track of the 3rd MiGA Challenge at IJCAI 2025, achieving superior performance compared to previous state-of-the-art methods. MM-Gesture is a multimodal fusion framework designed specifically for recognizing subtle and short-duration micro-gestures (MGs), integrating complementary cues from joint, limb, RGB video, Taylor-series video, optical-flow video, and depth video modalities. Utilizing PoseConv3D and Video Swin Transformer architectures with a novel modality-weighted ensemble strategy, our method further enhances RGB modality performance through transfer learning pre-trained on the larger MA-52 dataset. Extensive experiments on the iMiGUE benchmark, including ablation studies across different modalities, validate the effectiveness of our proposed approach, achieving a top-1 accuracy of 73.213%.",
        "arxiv_id": "2507.08344",
        "ARXIVID": "2507.08344",
        "COMMENT": "Matches criteria 6 as it focuses on video-based tasks, specifically micro-gesture recognition using multimodal fusion.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.08380": {
        "authors": [
            "Sen Wang",
            "Shao Zeng",
            "Tianjun Gu",
            "Zhizhong Zhang",
            "Ruixin Zhang",
            "Shouhong Ding",
            "Jingyun Zhang",
            "Jun Wang",
            "Xin Tan",
            "Yuan Xie",
            "Lizhuang Ma"
        ],
        "title": "From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning",
        "abstract": "arXiv:2507.08380v1 Announce Type: new  Abstract: Low-level enhancement and high-level visual understanding in low-light vision have traditionally been treated separately. Low-light enhancement improves image quality for downstream tasks, but existing methods rely on physical or geometric priors, limiting generalization. Evaluation mainly focuses on visual quality rather than downstream performance. Low-light visual understanding, constrained by scarce labeled data, primarily uses task-specific domain adaptation, which lacks scalability. To address these challenges, we build a generalized bridge between low-light enhancement and low-light understanding, which we term Generalized Enhancement For Understanding (GEFU). This paradigm improves both generalization and scalability. To address the diverse causes of low-light degradation, we leverage pretrained generative diffusion models to optimize images, achieving zero-shot generalization performance. Building on this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF). Specifically, to overcome text prompt limitations, we introduce an illumination-aware image prompt to explicitly guide image generation and propose a cycle-attention adapter to maximize its semantic potential. To mitigate semantic degradation in unsupervised training, we propose caption and reflectance consistency to learn high-level semantics and image-level spatial semantics. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art methods in traditional image quality and GEFU tasks including classification, detection, and semantic segmentation.",
        "arxiv_id": "2507.08380",
        "ARXIVID": "2507.08380",
        "COMMENT": "Matches criteria 4 as it focuses on leveraging foundation models for low-light vision tasks and improving their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.08448": {
        "authors": [
            "Wei Zhang",
            "Yihang Wu",
            "Songhua Li",
            "Wenjie Ma",
            "Xin Ma",
            "Qiang Li",
            "Qi Wang"
        ],
        "title": "Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT",
        "abstract": "arXiv:2507.08448v1 Announce Type: new  Abstract: 3D reconstruction, which aims to recover the dense three-dimensional structure of a scene, is a cornerstone technology for numerous applications, including augmented/virtual reality, autonomous driving, and robotics. While traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo (MVS) achieve high precision through iterative optimization, they are limited by complex workflows, high computational cost, and poor robustness in challenging scenarios like texture-less regions. Recently, deep learning has catalyzed a paradigm shift in 3D reconstruction. A new family of models, exemplified by DUSt3R, has pioneered a feed-forward approach. These models employ a unified deep network to jointly infer camera poses and dense geometry directly from an Unconstrained set of images in a single forward pass. This survey provides a systematic review of this emerging domain. We begin by dissecting the technical framework of these feed-forward models, including their Transformer-based correspondence modeling, joint pose and geometry regression mechanisms, and strategies for scaling from two-view to multi-view scenarios. To highlight the disruptive nature of this new paradigm, we contrast it with both traditional pipelines and earlier learning-based methods like MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation metrics. Finally, we discuss the technology's broad application prospects and identify key future challenges and opportunities, such as model accuracy and scalability, and handling dynamic scenes.",
        "arxiv_id": "2507.08448",
        "ARXIVID": "2507.08448",
        "COMMENT": "Matches criteria 7 as it is a survey paper on feed-forward 3D reconstruction, synthesizing the state of the art in this area.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.08044": {
        "authors": [
            "Debasmit Das",
            "Hyoungwoo Park",
            "Munawar Hayat",
            "Seokeon Choi",
            "Sungrack Yun",
            "Fatih Porikli"
        ],
        "title": "ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints",
        "abstract": "arXiv:2507.08044v1 Announce Type: new  Abstract: Foundation models are pre-trained on large-scale datasets and subsequently fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT) techniques like low-rank adapters (LoRA). In most previous works, LoRA weight matrices are randomly initialized with a fixed rank across all attachment points. In this paper, we improve convergence and final performance of LoRA fine-tuning, using our proposed data-driven weight initialization method, ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift problem where we use multiple constraints relating the pre-training and fine-tuning activations. By reformulating these constraints, we obtain a closed-form estimate of LoRA weights that depends on pre-training weights and fine-tuning activation vectors and hence requires no training during initialization. This weight estimate is decomposed to initialize the up and down matrices with proposed flexibility of variable ranks. With the proposed initialization method, we fine-tune on downstream tasks such as image generation, image classification and image understanding. Both quantitative and qualitative results demonstrate that CNTLoRA outperforms standard and data-driven weight initialization methods. Extensive analyses and ablations further elucidate the design choices of our framework, providing an optimal recipe for faster convergence and enhanced performance.",
        "arxiv_id": "2507.08044",
        "ARXIVID": "2507.08044",
        "COMMENT": "Does not match any specific criteria. Focuses on weight initialization for LoRA fine-tuning, which is not directly related to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.08520": {
        "authors": [
            "Yufei Zheng",
            "Wenjun Wang",
            "Wenjun Gan",
            "Jiawei Liu"
        ],
        "title": "Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification",
        "abstract": "arXiv:2507.08520v1 Announce Type: new  Abstract: Occluded person re-identification aims to retrieve holistic images based on occluded ones. Existing methods often rely on aligning visible body parts, applying occlusion augmentation, or complementing missing semantics using holistic images. However, they face challenges in handling diverse occlusion scenarios not seen during training and the issue of feature contamination from holistic images. To address these limitations, we propose Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation (OGFR), which simultaneously mitigates these challenges. OGFR adopts a teacher-student distillation architecture that effectively incorporates diverse occlusion patterns into feature representation while transferring the purified discriminative holistic knowledge from the holistic to the occluded branch through reinforced knowledge distillation. Specifically, an Occlusion-Aware Vision Transformer is designed to leverage learnable occlusion pattern embeddings to explicitly model such diverse occlusion types, thereby guiding occlusion-aware robust feature representation. Moreover, we devise a Feature Erasing and Purification Module within the holistic branch, in which an agent is employed to identify low-quality patch tokens of holistic images that contain noisy negative information via deep reinforcement learning, and substitute these patch tokens with learnable embedding tokens to avoid feature contamination and further excavate identity-related discriminative clues. Afterward, with the assistance of knowledge distillation, the student branch effectively absorbs the purified holistic knowledge to precisely learn robust representation regardless of the interference of occlusions.",
        "arxiv_id": "2507.08520",
        "ARXIVID": "2507.08520",
        "COMMENT": "Does not match any specific criteria. Focuses on occluded person re-identification, which is not directly related to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.08683": {
        "authors": [
            "Debashis Gupta",
            "Aditi Golder",
            "Rongkhun Zhu",
            "Kangning Cui",
            "Wei Tang",
            "Fan Yang",
            "Ovidiu Csillik",
            "Sarra Alaqahtani",
            "V. Paul Pauca"
        ],
        "title": "MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing",
        "abstract": "arXiv:2507.08683v1 Announce Type: new  Abstract: Contrastive learning (CL) has emerged as a powerful paradigm for learning transferable representations without the reliance on large labeled datasets. Its ability to capture intrinsic similarities and differences among data samples has led to state-of-the-art results in computer vision tasks. These strengths make CL particularly well-suited for Earth System Observation (ESO), where diverse satellite modalities such as optical and SAR imagery offer naturally aligned views of the same geospatial regions. However, ESO presents unique challenges, including high inter-class similarity, scene clutter, and ambiguous boundaries, which complicate representation learning -- especially in low-label, multi-label settings. Existing CL frameworks often focus on intra-modality self-supervision or lack mechanisms for multi-label alignment and semantic precision across modalities. In this work, we introduce MoSAiC, a unified framework that jointly optimizes intra- and inter-modality contrastive learning with a multi-label supervised contrastive loss. Designed specifically for multi-modal satellite imagery, MoSAiC enables finer semantic disentanglement and more robust representation learning across spectrally similar and spatially complex classes. Experiments on two benchmark datasets, BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both fully supervised and self-supervised baselines in terms of accuracy, cluster coherence, and generalization in low-label and high-class-overlap scenarios.",
        "arxiv_id": "2507.08683",
        "ARXIVID": "2507.08683",
        "COMMENT": "Does not match any specific criteria but is relevant to multimodal learning and contrastive learning in remote sensing.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.08375": {
        "authors": [
            "Alexandra Malyugina",
            "Yini Li",
            "Joanne Lin",
            "Nantheera Anantrasirichai"
        ],
        "title": "Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques",
        "abstract": "arXiv:2507.08375v1 Announce Type: new  Abstract: Video restoration and enhancement are critical not only for improving visual quality, but also as essential pre-processing steps to boost the performance of a wide range of downstream computer vision tasks. This survey presents a comprehensive review of video restoration and enhancement techniques with a particular focus on unsupervised approaches. We begin by outlining the most common video degradations and their underlying causes, followed by a review of early conventional and deep learning methods-based, highlighting their strengths and limitations. We then present an in-depth overview of unsupervised methods, categorise by their fundamental approaches, including domain translation, self-supervision signal design and blind spot or noise-based methods. We also provide a categorization of loss functions employed in unsupervised video restoration and enhancement, and discuss the role of paired synthetic datasets in enabling objective evaluation. Finally, we identify key challenges and outline promising directions for future research in this field.",
        "arxiv_id": "2507.08375",
        "ARXIVID": "2507.08375",
        "COMMENT": "Matches criterion 7 as it is a survey paper on video restoration and enhancement techniques, synthesizing the state of the art.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2507.08776": {
        "authors": [
            "Zhengqing Wang",
            "Yuefan Wu",
            "Jiacheng Chen",
            "Fuyang Zhang",
            "Yasutaka Furukawa"
        ],
        "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering",
        "abstract": "arXiv:2507.08776v1 Announce Type: new  Abstract: This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed.",
        "arxiv_id": "2507.08776",
        "ARXIVID": "2507.08776",
        "COMMENT": "Does not match any specific criteria but is relevant to neural rendering and efficient representation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.08340": {
        "authors": [
            "Jia-Xuan Jiang",
            "Jiashuai Liu",
            "Hongtao Wu",
            "Yifeng Wu",
            "Zhong Wang",
            "Qi Bi",
            "Yefeng Zheng"
        ],
        "title": "Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement",
        "abstract": "arXiv:2507.08340v1 Announce Type: new  Abstract: Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG",
        "arxiv_id": "2507.08340",
        "ARXIVID": "2507.08340",
        "COMMENT": "Does not match any specific criteria but is relevant to multimodal learning in a medical context.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.08216": {
        "authors": [
            "Rodrigo Castellano Ontiveros",
            "Francesco Giannini",
            "Marco Gori",
            "Giuseppe Marra",
            "Michelangelo Diligenti"
        ],
        "title": "Grounding Methods for Neural-Symbolic AI",
        "abstract": "arXiv:2507.08216v1 Announce Type: new  Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to process the input entities, while relying on a reasoner based on First-Order Logic to represent and process more complex relationships among the entities. A fundamental role for these methods is played by the process of logic grounding, which determines the relevant substitutions for the logic rules using a (sub)set of entities. Some NeSy methods use an exhaustive derivation of all possible substitutions, preserving the full expressive power of the logic knowledge. This leads to a combinatorial explosion in the number of ground formulas to consider and, therefore, strongly limits their scalability. Other methods rely on heuristic-based selective derivations, which are generally more computationally efficient, but lack a justification and provide no guarantees of preserving the information provided to and returned by the reasoner. Taking inspiration from multi-hop symbolic reasoning, this paper proposes a parametrized family of grounding methods generalizing classic Backward Chaining. Different selections within this family allow us to obtain commonly employed grounding methods as special cases, and to control the trade-off between expressiveness and scalability of the reasoner. The experimental results show that the selection of the grounding criterion is often as important as the NeSy method itself.",
        "arxiv_id": "2507.08216",
        "ARXIVID": "2507.08216",
        "COMMENT": "Does not match any specific criteria. Focuses on grounding methods for neural-symbolic AI, which is not directly related to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.08735": {
        "authors": [
            "Anna Rosenberg",
            "John Kennedy",
            "Zohar Keidar",
            "Yehoshua Y. Zeevi",
            "Guy Gilboa"
        ],
        "title": "Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study",
        "abstract": "arXiv:2507.08735v1 Announce Type: new  Abstract: Solving computer vision problems through machine learning, one often encounters lack of sufficient training data. To mitigate this we propose the use of ensembles of weak learners based on spectral total-variation (STV) features (Gilboa 2014). The features are related to nonlinear eigenfunctions of the total-variation subgradient and can characterize well textures at various scales. It was shown (Burger et-al 2016) that, in the one-dimensional case, orthogonal features are generated, whereas in two-dimensions the features are empirically lowly correlated. Ensemble learning theory advocates the use of lowly correlated weak learners. We thus propose here to design ensembles using learners based on STV features. To show the effectiveness of this paradigm we examine a hard real-world medical imaging problem: the predictive value of computed tomography (CT) data for high uptake in positron emission tomography (PET) for patients suspected of skeletal metastases. The database consists of 457 scans with 1524 unique pairs of registered CT and PET slices. Our approach is compared to deep-learning methods and to Radiomics features, showing STV learners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and Radiomics (AUC=0.79). We observe that fine STV scales in CT images are especially indicative for the presence of high uptake in PET.",
        "arxiv_id": "2507.08735",
        "ARXIVID": "2507.08735",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and ensemble learning for medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.08655": {
        "authors": [
            "Zach Eidex",
            "Mojtaba Safari",
            "Tonghe Wang",
            "Vanessa Wildman",
            "David S. Yu",
            "Hui Mao",
            "Erik Middlebrooks",
            "Aparna Kesewala",
            "Xiaofeng Yang"
        ],
        "title": "Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an Efficient Transformer Model",
        "abstract": "arXiv:2507.08655v1 Announce Type: new  Abstract: Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over standard clinical field strengths (1.5T, 3T). However, 7T scanners are costly, scarce, and introduce additional challenges such as susceptibility artifacts. We propose an efficient transformer-based model (7T-Restormer) to synthesize 7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods: Our model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding 7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128 slices) were randomly divided into 105 (25; 80) training cases (19,204 slices), 19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145 slices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans, respectively. The synthetic 7T T1 maps were compared against the ResViT and ResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/- 4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs, and 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using 10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter ResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter ResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T (0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus was superior to single-field strategies. Restricting the model to 1.5T increased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely on 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We propose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T and 3T T1W scans with higher quality than existing state-of-the-art methods. Our approach makes the benefits of 7T MRI more accessible to standard clinical workflows.",
        "arxiv_id": "2507.08655",
        "ARXIVID": "2507.08655",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and transformer-based models for MRI synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.08205": {
        "authors": [
            "Ken C. L. Wong",
            "Hongzhi Wang",
            "Tanveer Syeda-Mahmood"
        ],
        "title": "HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation",
        "abstract": "arXiv:2507.08205v1 Announce Type: new  Abstract: In medical image segmentation, convolutional neural networks (CNNs) and transformers are dominant. For CNNs, given the local receptive fields of convolutional layers, long-range spatial correlations are captured through consecutive convolutions and pooling. However, as the computational cost and memory footprint can be prohibitively large, 3D models can only afford fewer layers than 2D models with reduced receptive fields and abstract levels. For transformers, although long-range correlations can be captured by multi-head attention, its quadratic complexity with respect to input size is computationally demanding. Therefore, either model may require input size reduction to allow more filters and layers for better segmentation. Nevertheless, given their discrete nature, models trained with patch-wise training or image downsampling may produce suboptimal results when applied on higher resolutions. To address this issue, here we propose the resolution-robust HNOSeg-XS architecture. We model image segmentation by learnable partial differential equations through the Fourier neural operator which has the zero-shot super-resolution property. By replacing the Fourier transform by the Hartley transform and reformulating the problem in the frequency domain, we created the HNOSeg-XS model, which is resolution robust, fast, memory efficient, and extremely parameter efficient. When tested on the BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS showed its superior resolution robustness with fewer than 34.7k model parameters. It also achieved the overall best inference time (< 0.24 s) and memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer models.",
        "arxiv_id": "2507.08205",
        "ARXIVID": "2507.08205",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and efficient segmentation methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}