{
    "2506.23329": {
        "authors": [
            "Parker Liu",
            "Chenxin Li",
            "Zhengxin Li",
            "Yipeng Wu",
            "Wuyang Li",
            "Zhiqin Yang",
            "Zhenyuan Zhang",
            "Yunlong Lin",
            "Sirui Han",
            "Brandon Y. Feng"
        ],
        "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering",
        "abstract": "arXiv:2506.23329v1 Announce Type: new  Abstract: Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This \"understanding-by-creating\" approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide a comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating.",
        "arxiv_id": "2506.23329",
        "ARXIVID": "2506.23329",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a benchmark for vision-language agents to demonstrate scene understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.22979": {
        "authors": [
            "Jie Liu",
            "Jiayi Shen",
            "Pan Zhou",
            "Jan-Jakob Sonke",
            "Efstratios Gavves"
        ],
        "title": "Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation",
        "abstract": "arXiv:2506.22979v1 Announce Type: new  Abstract: Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a segmentation model to novel classes with only a few annotated examples while maintaining performance on base classes. Recently, pretrained vision-language models (VLMs) such as CLIP have been leveraged in GFSS to improve generalization on novel classes through multi-modal prototypes learning. However, existing prototype-based methods are inherently deterministic, limiting the adaptability of learned prototypes to diverse samples, particularly for novel classes with scarce annotations. To address this, we propose FewCLIP, a probabilistic prototype calibration framework over multi-modal prototypes from the pretrained CLIP, thus providing more adaptive prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype calibration mechanism, which refines frozen textual prototypes with learnable visual calibration prototypes, leading to a more discriminative and adaptive representation. Furthermore, unlike deterministic prototype learning techniques, FewCLIP introduces distribution regularization over these calibration prototypes. This probabilistic formulation ensures structured and uncertainty-aware prototype learning, effectively mitigating overfitting to limited novel class data while enhancing generalization. Extensive experimental results on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed FewCLIP significantly outperforms state-of-the-art approaches across both GFSS and class-incremental setting. The code is available at https://github.com/jliu4ai/FewCLIP.",
        "arxiv_id": "2506.22979",
        "ARXIVID": "2506.22979",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores vision-language models for few-shot semantic segmentation.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.23120": {
        "authors": [
            "Zhenhua Ning",
            "Zhuotao Tian",
            "Shaoshuai Shi",
            "Guangming Lu",
            "Daojing He",
            "Wenjie Pei",
            "Li Jiang"
        ],
        "title": "Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation",
        "abstract": "arXiv:2506.23120v1 Announce Type: new  Abstract: Recent advances in point cloud perception have demonstrated remarkable progress in scene understanding through vision-language alignment leveraging large language models (LLMs). However, existing methods may still encounter challenges in handling complex instructions that require accurate spatial reasoning, even if the 3D point cloud data provides detailed spatial cues such as size and position for identifying the targets. To tackle this issue, we propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based segmentation framework. The framework emulates human cognitive processes by decomposing spatial reasoning into two sequential stages: first identifying relevant elements, then processing instructions guided by their associated visual priors. Furthermore, acknowledging the inadequacy of existing datasets in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based segmentation dataset comprising 25,185 training samples and 3,966 validation samples with precise annotations. Both quantitative and qualitative experiments demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud perception with stronger spatial reasoning capabilities, and we hope that they can serve as a new baseline and benchmark for future work.",
        "arxiv_id": "2506.23120",
        "ARXIVID": "2506.23120",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on enhancing spatial reasoning in multimodal large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.23468": {
        "authors": [
            "Xuan Yao",
            "Junyu Gao",
            "Changsheng Xu"
        ],
        "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments",
        "abstract": "arXiv:2506.23468v1 Announce Type: new  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at \\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.",
        "arxiv_id": "2506.23468",
        "ARXIVID": "2506.23468",
        "COMMENT": "Matches criteria 1 and 3 as it focuses on spatial reasoning and introduces a novel framework for vision-and-language navigation in continuous environments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.23219": {
        "authors": [
            "Jie Feng",
            "Shengyuan Wang",
            "Tianhui Liu",
            "Yanxin Xi",
            "Yong Li"
        ],
        "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding",
        "abstract": "arXiv:2506.23219v1 Announce Type: new  Abstract: Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.",
        "arxiv_id": "2506.23219",
        "ARXIVID": "2506.23219",
        "COMMENT": "Matches criteria 1 and 2 as it introduces UrbanLLaVA, a multi-modal large language model with spatial reasoning capabilities for urban intelligence tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.23044": {
        "authors": [
            "Guo-Hua Wang",
            "Shanshan Zhao",
            "Xinjie Zhang",
            "Liangfu Cao",
            "Pengxin Zhan",
            "Lunhao Duan",
            "Shiyin Lu",
            "Minghao Fu",
            "Xiaohao Chen",
            "Jianshan Zhao",
            "Yang Li",
            "Qing-Guo Chen"
        ],
        "title": "Ovis-U1 Technical Report",
        "abstract": "arXiv:2506.23044v1 Announce Type: new  Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.",
        "arxiv_id": "2506.23044",
        "ARXIVID": "2506.23044",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a multimodal model for understanding, generation, and editing.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.23115": {
        "authors": [
            "Haonan Chen",
            "Hong Liu",
            "Yuping Luo",
            "Liang Wang",
            "Nan Yang",
            "Furu Wei",
            "Zhicheng Dou"
        ],
        "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings",
        "abstract": "arXiv:2506.23115v1 Announce Type: new  Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.",
        "arxiv_id": "2506.23115",
        "ARXIVID": "2506.23115",
        "COMMENT": "Matches criterion 2 as it proposes a framework for improving multimodal embeddings, integrating vision and language.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23858": {
        "authors": [
            "Jianzong Wu",
            "Liang Hou",
            "Haotian Yang",
            "Xin Tao",
            "Ye Tian",
            "Pengfei Wan",
            "Di Zhang",
            "Yunhai Tong"
        ],
        "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
        "abstract": "arXiv:2506.23858v1 Announce Type: new  Abstract: The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.",
        "arxiv_id": "2506.23858",
        "ARXIVID": "2506.23858",
        "COMMENT": "Matches criterion 6 as it introduces a novel sparse attention mechanism for video diffusion models, advancing video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23049": {
        "authors": [
            "Leander Melroy Maben",
            "Gayathri Ganesh Lakshmy",
            "Srijith Radhakrishnan",
            "Siddhant Arora",
            "Shinji Watanabe"
        ],
        "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks",
        "abstract": "arXiv:2506.23049v1 Announce Type: new  Abstract: Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.",
        "arxiv_id": "2506.23049",
        "ARXIVID": "2506.23049",
        "COMMENT": "Matches criterion 1 as it discusses reasoning and tool use in embodied agents for voice-driven tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.24121": {
        "authors": [
            "Sisi Dai",
            "Xinxin Su",
            "Boyan Wan",
            "Ruizhen Hu",
            "Kai Xu"
        ],
        "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation",
        "abstract": "arXiv:2506.24121v1 Announce Type: new  Abstract: Recent advancements in diffusion generative models significantly advanced image, video, and 3D content creation from user-provided text prompts. However, the challenging problem of dynamic 3D content generation (text-to-4D) with diffusion guidance remains largely unexplored. In this paper, we introduce TextMesh4D, a novel framework for high-quality text-to-4D generation. Our approach leverages per-face Jacobians as a differentiable mesh representation and decomposes 4D generation into two stages: static object creation and dynamic motion synthesis. We further propose a flexibility-rigidity regularization term to stabilize Jacobian optimization under video diffusion priors, ensuring robust geometric performance. Experiments demonstrate that TextMesh4D achieves state-of-the-art results in terms of temporal consistency, structural fidelity, and visual realism. Moreover, TextMesh4D operates with a low GPU memory overhead-requiring only a single 24GB GPU-offering a cost-effective yet high-quality solution for text-driven 4D mesh generation. The code will be released to facilitate future research in text-to-4D generation.",
        "arxiv_id": "2506.24121",
        "ARXIVID": "2506.24121",
        "COMMENT": "Matches criterion 5 as it combines text-to-4D mesh generation with video diffusion priors, integrating image/video understanding with LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23690": {
        "authors": [
            "Shuai Tan",
            "Biao Gong",
            "Yujie Wei",
            "Shiwei Zhang",
            "Zhuoxin Liu",
            "Dandan Zheng",
            "Jingdong Chen",
            "Yan Wang",
            "Hao Ouyang",
            "Kecheng Zheng",
            "Yujun Shen"
        ],
        "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation",
        "abstract": "arXiv:2506.23690v1 Announce Type: new  Abstract: Diffusion-based video motion customization facilitates the acquisition of human motion representations from a few video samples, while achieving arbitrary subjects transfer through precise textual conditioning. Existing approaches often rely on semantic-level alignment, expecting the model to learn new motion concepts and combine them with other entities (e.g., ''cats'' or ''dogs'') to produce visually appealing results. However, video data involve complex spatio-temporal patterns, and focusing solely on semantics cause the model to overlook the visual complexity of motion. Conversely, tuning only the visual representation leads to semantic confusion in representing the intended action. To address these limitations, we propose SynMotion, a new motion-customized video generation model that jointly leverages semantic guidance and visual adaptation. At the semantic level, we introduce the dual-embedding semantic comprehension mechanism which disentangles subject and motion representations, allowing the model to learn customized motion features while preserving its generative capabilities for diverse subjects. At the visual level, we integrate parameter-efficient motion adapters into a pre-trained video generation model to enhance motion fidelity and temporal coherence. Furthermore, we introduce a new embedding-specific training strategy which \\textbf{alternately optimizes} subject and motion embeddings, supported by the manually constructed Subject Prior Video (SPV) training dataset. This strategy promotes motion specificity while preserving generalization across diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark with diverse motion patterns. Experimental results across both T2V and I2V settings demonstrate that \\method outperforms existing baselines. Project page: https://lucaria-academy.github.io/SynMotion/",
        "arxiv_id": "2506.23690",
        "ARXIVID": "2506.23690",
        "COMMENT": "Matches criterion 5 as it combines video understanding tasks with generative modeling using semantic-visual adaptation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23549": {
        "authors": [
            "Huai-Chih Wang",
            "Hsiang-Chun Chuang",
            "Hsi-Chun Cheng",
            "Dai-Jie Wu",
            "Shao-Hua Sun"
        ],
        "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers",
        "abstract": "arXiv:2506.23549v1 Announce Type: new  Abstract: Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios.",
        "arxiv_id": "2506.23549",
        "ARXIVID": "2506.23549",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for coordination in multi-agent systems, relevant to spatial intelligence and embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23580": {
        "authors": [
            "Yawen Zou",
            "Guang Li",
            "Duo Su",
            "Zi Wang",
            "Jun Yu",
            "Chao Zhang"
        ],
        "title": "Dataset Distillation via Vision-Language Category Prototype",
        "abstract": "arXiv:2506.23580v1 Announce Type: new  Abstract: Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source large language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/",
        "arxiv_id": "2506.23580",
        "ARXIVID": "2506.23580",
        "COMMENT": "Matches criteria 5 as it integrates vision-language methods into dataset distillation, combining image and language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23590": {
        "authors": [
            "Qiming Li",
            "Zekai Ye",
            "Xiaocheng Feng",
            "Weihong Zhong",
            "Libo Qin",
            "Ruihan Chen",
            "Baohang Li",
            "Kui Jiang",
            "Yaowei Wang",
            "Ting Liu",
            "Bing Qin"
        ],
        "title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
        "abstract": "arXiv:2506.23590v1 Announce Type: new  Abstract: Although Large Vision-Language Models (LVLMs) have demonstrated powerful capabilities in interpreting visual information, they frequently produce content that deviates from visual information, leading to object hallucination. To tackle this, recent works mostly depend on expensive manual annotations and training cost, or significantly increase inference time. In this work, we observe that LVLMs' attention to visual information is significantly stronger when answering caption queries compared to non-caption queries. Inspired by this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a training-free, plug-and-play hallucination mitigation method that leverages the attention activation pattern in response to caption queries to enhance LVLMs' visual perception capability. Extensive experimental results across four benchmarks covering both discriminative and generative tasks, demonstrate that CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only with minimal additional inference cost.",
        "arxiv_id": "2506.23590",
        "ARXIVID": "2506.23590",
        "COMMENT": "Matches criteria 2 as it addresses hallucination in large vision-language models, improving their visual perception capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2506.22930": {
        "authors": [
            "Yiwei He",
            "Xiangtai Li",
            "Zhenglin Huang",
            "Yi Dong",
            "Hao Fei",
            "Jiangning Zhang",
            "Baoyuan Wu",
            "Guangliang Cheng"
        ],
        "title": "Towards Explainable Bilingual Multimodal Misinformation Detection and Localization",
        "abstract": "arXiv:2506.22930v1 Announce Type: new  Abstract: The increasing realism of multimodal content has made misinformation more subtle and harder to detect, especially in news media where images are frequently paired with bilingual (e.g., Chinese-English) subtitles. Such content often includes localized image edits and cross-lingual inconsistencies that jointly distort meaning while remaining superficially plausible. We introduce BiMi, a bilingual multimodal framework that jointly performs region-level localization, cross-modal and cross-lingual consistency detection, and natural language explanation for misinformation analysis. To support generalization, BiMi integrates an online retrieval module that supplements model reasoning with up-to-date external context. We further release BiMiBench, a large-scale and comprehensive benchmark constructed by systematically editing real news images and subtitles, comprising 104,000 samples with realistic manipulations across visual and linguistic modalities. To enhance interpretability, we apply Group Relative Policy Optimization (GRPO) to improve explanation quality, marking the first use of GRPO in this domain. Extensive experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore, advancing state-of-the-art performance in realistic, multilingual misinformation detection. Code, models, and datasets will be released.",
        "arxiv_id": "2506.22930",
        "ARXIVID": "2506.22930",
        "COMMENT": "Matches criteria 2 as it focuses on bilingual multimodal misinformation detection, integrating vision and language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23785": {
        "authors": [
            "Yongjian Wu",
            "Yang Zhou",
            "Jiya Saiyin",
            "Bingzheng Wei",
            "Yan Xu"
        ],
        "title": "Visual Textualization for Image Prompted Object Detection",
        "abstract": "arXiv:2506.23785v1 Announce Type: new  Abstract: We propose VisTex-OVLM, a novel image prompted object detection method that introduces visual textualization -- a process that projects a few visual exemplars into the text feature space to enhance Object-level Vision-Language Models' (OVLMs) capability in detecting rare categories that are difficult to describe textually and nearly absent from their pre-training data, while preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM leverages multi-scale textualizing blocks and a multi-stage fusion strategy to integrate visual information from visual exemplars, generating textualized visual tokens that effectively guide OVLMs alongside text prompts. Unlike previous methods, our method maintains the original architecture of OVLM, maintaining its generalization capabilities while enhancing performance in few-shot settings. VisTex-OVLM demonstrates superior performance across open-set datasets which have minimal overlap with OVLM's pre-training data and achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO. The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.",
        "arxiv_id": "2506.23785",
        "ARXIVID": "2506.23785",
        "COMMENT": "Matches criteria 5 as it combines image understanding tasks with object detection and vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22554": {
        "authors": [
            "Vasu Agrawal",
            "Akinniyi Akinyemi",
            "Kathryn Alvero",
            "Morteza Behrooz",
            "Julia Buffalini",
            "Fabio Maria Carlucci",
            "Joy Chen",
            "Junming Chen",
            "Zhang Chen",
            "Shiyang Cheng",
            "Praveen Chowdary",
            "Joe Chuang",
            "Antony D'Avirro",
            "Jon Daly",
            "Ning Dong",
            "Mark Duppenthaler",
            "Cynthia Gao",
            "Jeff Girard",
            "Martin Gleize",
            "Sahir Gomez",
            "Hongyu Gong",
            "Srivathsan Govindarajan",
            "Brandon Han",
            "Sen He",
            "Denise Hernandez",
            "Yordan Hristov",
            "Rongjie Huang",
            "Hirofumi Inaguma",
            "Somya Jain",
            "Raj Janardhan",
            "Qingyao Jia",
            "Christopher Klaiber",
            "Dejan Kovachev",
            "Moneish Kumar",
            "Hang Li",
            "Yilei Li",
            "Pavel Litvin",
            "Wei Liu",
            "Guangyao Ma",
            "Jing Ma",
            "Martin Ma",
            "Xutai Ma",
            "Lucas Mantovani",
            "Sagar Miglani",
            "Sreyas Mohan",
            "Louis-Philippe Morency",
            "Evonne Ng",
            "Kam-Woh Ng",
            "Tu Anh Nguyen",
            "Amia Oberai",
            "Benjamin Peloquin",
            "Juan Pino",
            "Jovan Popovic",
            "Omid Poursaeed",
            "Fabian Prada",
            "Alice Rakotoarison",
            "Alexander Richard",
            "Christophe Ropers",
            "Safiyyah Saleem",
            "Vasu Sharma",
            "Alex Shcherbyna",
            "Jia Shen",
            "Jie Shen",
            "Anastasis Stathopoulos",
            "Anna Sun",
            "Paden Tomasello",
            "Tuan Tran",
            "Arina Turkatenko",
            "Bo Wan",
            "Chao Wang",
            "Jeff Wang",
            "Mary Williamson",
            "Carleigh Wood",
            "Tao Xiang",
            "Yilin Yang",
            "Julien Yao",
            "Chen Zhang",
            "Jiemin Zhang",
            "Xinyue Zhang",
            "Jason Zheng",
            "Pavlo Zhyzheria",
            "Jan Zikes",
            "Michael Zollhoefer"
        ],
        "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset",
        "abstract": "arXiv:2506.22554v1 Announce Type: new  Abstract: Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.",
        "arxiv_id": "2506.22554",
        "ARXIVID": "2506.22554",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it discusses dyadic audiovisual motion modeling and integrates speech from an LLM.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23982": {
        "authors": [
            "Ruiyang Hao",
            "Bowen Jing",
            "Haibao Yu",
            "Zaiqing Nie"
        ],
        "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving",
        "abstract": "arXiv:2506.23982v1 Announce Type: new  Abstract: While personalization has been explored in traditional autonomous driving systems, it remains largely overlooked in end-to-end autonomous driving (E2EAD), despite its growing prominence. This gap is critical, as user-aligned behavior is essential for trust, comfort, and widespread adoption of autonomous vehicles. A core challenge is the lack of large-scale real-world datasets annotated with diverse and fine-grained driving preferences, hindering the development and evaluation of personalized E2EAD models. In this work, we present the first large-scale real-world dataset enriched with annotations capturing diverse driving preferences, establishing a foundation for personalization in E2EAD. We extract static environmental features from real-world road topology and infer dynamic contextual cues using a fine-tuned visual language model (VLM), enabling consistent and fine-grained scenario construction. Based on these scenarios, we derive objective preference annotations through behavioral distribution analysis and rule-based heuristics. To address the inherent subjectivity of driving style, we further employ the VLM to generate subjective annotations by jointly modeling scene semantics and driver behavior. Final high-quality labels are obtained through a human-in-the-loop verification process that fuses both perspectives. Building on this dataset, we propose the first benchmark for evaluating personalized E2EAD models. We assess several state-of-the-art models with and without preference conditioning, demonstrating that incorporating personalized preferences results in behavior more aligned with human driving. Our work lays the foundation for personalized E2EAD by providing a standardized platform to systematically integrate human preferences into data-driven E2EAD systems, catalyzing future research in human-centric autonomy.",
        "arxiv_id": "2506.23982",
        "ARXIVID": "2506.23982",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for personalized end-to-end autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22800": {
        "authors": [
            "Sicong Du",
            "Jiarun Liu",
            "Qifeng Chen",
            "Hao-Xiang Chen",
            "Tai-Jiang Mu",
            "Sheng Yang"
        ],
        "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors",
        "abstract": "arXiv:2506.22800v1 Announce Type: new  Abstract: A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version incorporating reviewer suggestions will be updated soon.)",
        "arxiv_id": "2506.22800",
        "ARXIVID": "2506.22800",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel framework for expansive driving scene reconstruction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.24086": {
        "authors": [
            "Bingfan Zhu",
            "Biao Jiang",
            "Sunyi Wang",
            "Shixiang Tang",
            "Tao Chen",
            "Linjie Luo",
            "Youyi Zheng",
            "Xin Chen"
        ],
        "title": "MotionGPT3: Human Motion as a Second Modality",
        "abstract": "arXiv:2506.24086v1 Announce Type: new  Abstract: Though recent advances in multimodal models have demonstrated strong capabilities and opportunities in unified understanding and generation, the development of unified motion-language models remains underexplored. To enable such models with high-fidelity human motion, two core challenges must be addressed. The first is the reconstruction gap between the continuous motion modality and discrete representation in an autoregressive manner, and the second is the degradation of language intelligence during unified training. Inspired by the mixture of experts, we propose MotionGPT3, a bimodal motion-language model that treats human motion as a second modality, decoupling motion modeling via separate model parameters and enabling both effective cross-modal interaction and efficient multimodal scaling training. To preserve language intelligence, the text branch retains the original structure and parameters of the pretrained language model, while a new motion branch is integrated via a shared attention mechanism, enabling bidirectional information flow between two modalities. We first employ a motion Variational Autoencoder (VAE) to encode raw human motion into latent representations. Based on this continuous latent space, the motion branch predicts motion latents directly from intermediate hidden states using a diffusion head, bypassing discrete tokenization. Extensive experiments show that our approach achieves competitive performance on both motion understanding and generation tasks while preserving strong language capabilities, establishing a unified bimodal motion diffusion framework within an autoregressive manner.",
        "arxiv_id": "2506.24086",
        "ARXIVID": "2506.24086",
        "COMMENT": "Matches criterion 2 as it explores a unified motion-language model, treating human motion as a second modality, relevant to multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23275": {
        "authors": [
            "Chengyou Jia",
            "Xin Shen",
            "Zhuohang Dang",
            "Zhuohang Dang",
            "Changliang Xia",
            "Weijia Wu",
            "Xinyu Zhang",
            "Hangwei Qian",
            "Ivor W. Tsang",
            "Minnan Luo"
        ],
        "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation",
        "abstract": "arXiv:2506.23275v1 Announce Type: new  Abstract: Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce $\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free framework that maximally leverages pretrained Diffusion Transformers' in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. Visit our project in https://chengyou-jia.github.io/T2IS-Home.",
        "arxiv_id": "2506.23275",
        "ARXIVID": "2506.23275",
        "COMMENT": "Matches criterion 5 as it introduces a framework for generating and evaluating image sets based on text instructions, combining image understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.24113": {
        "authors": [
            "Kaiwen Zhang",
            "Zhenyu Tang",
            "Xiaotao Hu",
            "Xingang Pan",
            "Xiaoyang Guo",
            "Yuan Liu",
            "Jingwei Huang",
            "Li Yuan",
            "Qian Zhang",
            "Xiao-Xiao Long",
            "Xun Cao",
            "Wei Yin"
        ],
        "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
        "abstract": "arXiv:2506.24113v1 Announce Type: new  Abstract: Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.",
        "arxiv_id": "2506.24113",
        "ARXIVID": "2506.24113",
        "COMMENT": "Matches criteria 3 as it introduces a novel autoregressive diffusion world model for autonomous driving, integrating motion planning and visual modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.23361": {
        "authors": [
            "Yuanhao Cai",
            "He Zhang",
            "Xi Chen",
            "Jinbo Xing",
            "Yiwei Hu",
            "Yuqian Zhou",
            "Kai Zhang",
            "Zhifei Zhang",
            "Soo Ye Kim",
            "Tianyu Wang",
            "Yulun Zhang",
            "Xiaokang Yang",
            "Zhe Lin",
            "Alan Yuille"
        ],
        "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions",
        "abstract": "arXiv:2506.23361v1 Announce Type: new  Abstract: Existing feedforward subject-driven video customization methods mainly study single-subject scenarios due to the difficulty of constructing multi-subject training data pairs. Another challenging problem that how to use the signals such as depth, mask, camera, and text prompts to control and edit the subject in the customized video is still less explored. In this paper, we first propose a data construction pipeline, VideoCus-Factory, to produce training data pairs for multi-subject customization from raw videos without labels and control signals such as depth-to-video and mask-to-video pairs. Based on our constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with image editing data to enable instructive editing for the subject in the customized video. Then we propose a diffusion Transformer framework, OmniVCus, with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned Embedding (TAE). LE enables inference with more subjects by using the training subjects to activate more frame embeddings. TAE encourages the generation process to extract guidance from temporally aligned control signals by assigning the same frame embeddings to the control and noise tokens. Experiments demonstrate that our method significantly surpasses state-of-the-art methods in both quantitative and qualitative evaluations. Video demos are at our project page: https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released at https://github.com/caiyuanhao1998/Open-OmniVCus",
        "arxiv_id": "2506.23361",
        "ARXIVID": "2506.23361",
        "COMMENT": "Matches criteria 5 as it combines video understanding tasks with multimodal control conditions and large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.22636": {
        "authors": [
            "Sotirios Panagiotis Chytas",
            "Miso Choi",
            "Hyunwoo J. Kim",
            "Vikas Singh"
        ],
        "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models",
        "abstract": "arXiv:2506.22636v1 Announce Type: new  Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and reasoning with both visual and language data. But these models make mistakes. A common finding -- similar to LLMs -- is their tendency to hallucinate, i.e., generate plausible sounding text which is not grounded in the visual input, or at worst, is contradictory. A growing consensus attributes this behavior to an over-reliance on language -- especially as the generation progresses, the model suffers from a ``fading memory effect'' with respect to the provided visual input. We study mechanisms by which this behavior can be controlled. Specifically, using ideas from geometric algebra and relational compositions, we propose the addition of a small, trainable module (named ReCo) on top of any VLM -- no other modification is needed. We show that such a lightweight module is able to mitigate the fading memory effect on three of the most widely used VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on multiple benchmarks. Additionally, we show that our module can be combined with many of the other approaches for reducing hallucination where we achieve improved results for each one.",
        "arxiv_id": "2506.22636",
        "ARXIVID": "2506.22636",
        "COMMENT": "Matches criterion 2 as it explores methods to mitigate hallucinations in Vision-Language Models (VLLMs) and introduces a novel module (ReCo) for improving performance.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.23135": {
        "authors": [
            "Yu Shang",
            "Xin Zhang",
            "Yinzhou Tang",
            "Lei Jin",
            "Chen Gao",
            "Wei Wu",
            "Yong Li"
        ],
        "title": "RoboScape: Physics-informed Embodied World Model",
        "abstract": "arXiv:2506.23135v1 Announce Type: new  Abstract: World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.",
        "arxiv_id": "2506.23135",
        "ARXIVID": "2506.23135",
        "COMMENT": "Matches criterion 3 as it introduces a physics-informed embodied world model for robotic scenarios, which is a novel method in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.24096": {
        "authors": [
            "Antoine Gu\\'edon",
            "Diego Gomez",
            "Nissim Maruani",
            "Bingchen Gong",
            "George Drettakis",
            "Maks Ovsjanikov"
        ],
        "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction",
        "abstract": "arXiv:2506.24096v1 Announce Type: new  Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.",
        "arxiv_id": "2506.24096",
        "ARXIVID": "2506.24096",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for surface reconstruction in 3D scenes, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.23263": {
        "authors": [
            "Lei-lei Li",
            "Jianwu Fang",
            "Junbin Xiao",
            "Shanmin Pang",
            "Hongkai Yu",
            "Chen Lv",
            "Jianru Xue",
            "Tat-Seng Chua"
        ],
        "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis",
        "abstract": "arXiv:2506.23263v1 Announce Type: new  Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial for the safety of self-driving cars, and synthesizing causal-entity reflected accident videos can facilitate the capability test to respond to unaffordable accidents in reality. However, incorporating causal relations as seen in real-world videos into synthetic videos remains challenging. This work argues that precisely identifying the accident participants and capturing their related behaviors are of critical importance. In this regard, we propose a novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic accident videos. To enable causal entity grounding in video diffusion, Causal-VidSyn leverages the cause descriptions and driver fixations to identify the accident participants and behaviors, facilitated by accident reason answering and gaze-conditioned selection modules. To support Causal-VidSyn, we further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M frames of fixations) in driving accident scenarios. Extensive experiments show that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms of frame quality and causal sensitivity in various tasks, including accident video editing, normal-to-accident video diffusion, and text-to-video generation.",
        "arxiv_id": "2506.23263",
        "ARXIVID": "2506.23263",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding with a novel diffusion model for egocentric traffic accident video synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.22604": {
        "authors": [
            "David Porfirio",
            "Vincent Hsiao",
            "Morgan Fine-Morris",
            "Leslie Smith",
            "Laura M. Hiatt"
        ],
        "title": "Bootstrapping Human-Like Planning via LLMs",
        "abstract": "arXiv:2506.22604v1 Announce Type: new  Abstract: Robot end users increasingly require accessible means of specifying tasks for robots to perform. Two common end-user programming paradigms include drag-and-drop interfaces and natural language programming. Although natural language interfaces harness an intuitive form of human communication, drag-and-drop interfaces enable users to meticulously and precisely dictate the key actions of the robot's task. In this paper, we investigate the degree to which both approaches can be combined. Specifically, we construct a large language model (LLM)-based pipeline that accepts natural language as input and produces human-like action sequences as output, specified at a level of granularity that a human would produce. We then compare these generated action sequences to another dataset of hand-specified action sequences. Although our results reveal that larger models tend to outperform smaller ones in the production of human-like action sequences, smaller models nonetheless achieve satisfactory performance.",
        "arxiv_id": "2506.22604",
        "ARXIVID": "2506.22604",
        "COMMENT": "Matches criteria 1 as it explores human-like planning for embodied agents using large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.23440": {
        "authors": [
            "Mahesh Bhosale",
            "Abdul Wasi",
            "Yuanhao Zhai",
            "Yunjie Tian",
            "Samuel Border",
            "Nan Xi",
            "Pinaki Sarder",
            "Junsong Yuan",
            "David Doermann",
            "Xuan Gong"
        ],
        "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions",
        "abstract": "arXiv:2506.23440v1 Announce Type: new  Abstract: Diffusion-based generative models have shown promise in synthesizing histopathology images to address data scarcity caused by privacy constraints. Diagnostic text reports provide high-level semantic descriptions, and masks offer fine-grained spatial structures essential for representing distinct morphological regions. However, public datasets lack paired text and mask data for the same histopathological images, limiting their joint use in image generation. This constraint restricts the ability to fully exploit the benefits of combining both modalities for enhanced control over semantics and spatial details. To overcome this, we propose PathDiff, a diffusion framework that effectively learns from unpaired mask-text data by integrating both modalities into a unified conditioning space. PathDiff allows precise control over structural and contextual features, generating high-quality, semantically accurate images. PathDiff also improves image fidelity, text-image alignment, and faithfulness, enhancing data augmentation for downstream tasks like nuclei segmentation and classification. Extensive experiments demonstrate its superiority over existing methods.",
        "arxiv_id": "2506.23440",
        "ARXIVID": "2506.23440",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates text and mask conditions for histopathology image synthesis.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.23434": {
        "authors": [
            "Tianran Liu",
            "Shengwen Zhao",
            "Nicholas Rhinehart"
        ],
        "title": "Towards foundational LiDAR world models with efficient latent flow matching",
        "abstract": "arXiv:2506.23434v1 Announce Type: new  Abstract: LiDAR-based world models offer more structured and geometry-aware representations than their image-based counterparts. However, existing LiDAR world models are narrowly trained; each model excels only in the domain for which it was built. Can we develop LiDAR world models that exhibit strong transferability across multiple domains? We conduct the first systematic domain transfer study across three demanding scenarios: (i) outdoor to indoor generalization, (ii) sparse-beam \\& dense-beam adaptation, and (iii) non-semantic to semantic transfer. Given different amounts of fine-tuning data, our experiments show that a single pre-trained model can achieve up to 11% absolute improvement (83\\% relative) over training from scratch and outperforms training from scratch in 30/36 of our comparisons. This transferability of dynamic learning significantly reduces the reliance on manually annotated data for semantic occupancy forecasting: our method exceed the previous semantic occupancy forecasting models with only 5% of the labeled training data required by prior models. We also observed inefficiencies of current LiDAR world models, mainly through their under-compression of LiDAR data and inefficient training objectives. To address this, we propose a latent conditional flow matching (CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy using only half the training data and a compression ratio 6 times higher than that of prior methods. Our model achieves SOTA performance on future-trajectory-conditioned semantic occupancy forecasting while being 23x more computationally efficient (a 28x FPS speedup); and achieves SOTA performance on semantic occupancy forecasting while being 2x more computationally efficient (a 1.1x FPS speedup).",
        "arxiv_id": "2506.23434",
        "ARXIVID": "2506.23434",
        "COMMENT": "Matches criterion 3 as it introduces a LiDAR-based world model with strong transferability, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.24063": {
        "authors": [
            "Deng Li",
            "Aming Wu",
            "Yang Li",
            "Yaowei Wang",
            "Yahong Han"
        ],
        "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios",
        "abstract": "arXiv:2506.24063v1 Announce Type: new  Abstract: In practice, environments constantly change over time and space, posing significant challenges for object detectors trained based on a closed-set assumption, i.e., training and test data share the same distribution. To this end, continual test-time adaptation has attracted much attention, aiming to improve detectors' generalization by fine-tuning a few specific parameters, e.g., BatchNorm layers. However, based on a small number of test images, fine-tuning certain parameters may affect the representation ability of other fixed parameters, leading to performance degradation. Instead, we explore a new mechanism, i.e., converting the fine-tuning process to a specific-parameter generation. Particularly, we first design a dual-path LoRA-based domain-aware adapter that disentangles features into domain-invariant and domain-specific components, enabling efficient adaptation. Additionally, a conditional diffusion-based parameter generation mechanism is presented to synthesize the adapter's parameters based on the current environment, preventing the optimization from getting stuck in local optima. Finally, we propose a class-centered optimal transport alignment method to mitigate catastrophic forgetting. Extensive experiments conducted on various continuous domain adaptive object detection tasks demonstrate the effectiveness. Meanwhile, visualization results show that the representation extracted by the generated parameters can capture more object-related information and strengthen the generalization ability.",
        "arxiv_id": "2506.24063",
        "ARXIVID": "2506.24063",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for continual adaptation in object detection for dynamic environments.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.23856": {
        "authors": [
            "Ji Zhang",
            "Shihan Wu",
            "Lianli Gao",
            "Jingkuan Song",
            "Nicu Sebe",
            "Heng Tao Shen"
        ],
        "title": "A Closer Look at Conditional Prompt Tuning for Vision-Language Models",
        "abstract": "arXiv:2506.23856v1 Announce Type: new  Abstract: Despite the great promise of Prompt Tuning (PT) in adapting large Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better tuned to a base task, their ability to generalize to new tasks diminishes. Recent work on conditional PT addresses this problem by replacing static prompts with dynamic Visual Image Information (VII)-conditioned prompts, improving the model's generalization to new tasks to some extent. In this work, we first identify a critical issue with existing conditional PT methods: using VII as the \"condition\" of prompts yields suboptimal performance, and even random noise-conditioned prompts can outperform the VII-conditioned counterparts. On further analysis, we find that learning dynamic prompts conditioned on Textual Class Information (TCI) is the key to solving the BNT problem. Motivated by this, we then propose Class-adaptive Prompt Tuning (CaPT), which enables fast adaptation of tuned models to new classes by learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be used as a plugin to mitigate the BNT problem for existing unconditional PT schemes. Extensive experiments on 11 datasets show that CaPT consistently improves the performance of five strong unconditional PT baselines with negligible additional computational cost. Additionally, by integrating CaPT with our recently proposed DePT framework, we devise a new conditional PT approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art conditional PT scheme by 3.49%, averaged over the 11 datasets. Code: https://github.com/Koorye/CaPT.",
        "arxiv_id": "2506.23856",
        "ARXIVID": "2506.23856",
        "COMMENT": "Matches criterion 2 as it focuses on improving prompt tuning for Vision-Language Models (VLPMs) and introduces a novel method (CaPT) to address the Base-New Tradeoff problem.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.24119": {
        "authors": [
            "Bo Liu",
            "Leon Guertler",
            "Simon Yu",
            "Zichen Liu",
            "Penghui Qi",
            "Daniel Balcells",
            "Mickel Liu",
            "Cheston Tan",
            "Weiyan Shi",
            "Min Lin",
            "Wee Sun Lee",
            "Natasha Jaques"
        ],
        "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
        "abstract": "arXiv:2506.24119v1 Announce Type: new  Abstract: Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.",
        "arxiv_id": "2506.24119",
        "ARXIVID": "2506.24119",
        "COMMENT": "Matches criterion 1 as it focuses on reasoning development in multi-agent reinforcement learning, relevant to embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22900": {
        "authors": [
            "Mai A. Shaaban",
            "Tausifa Jan Saleem",
            "Vijay Ram Papineni",
            "Mohammad Yaqub"
        ],
        "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering",
        "abstract": "arXiv:2506.22900v1 Announce Type: new  Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical decision-making by providing contextually rich answers to image-based queries. Although vision-language models (VLMs) are widely used for this task, they often generate factually incorrect answers. Retrieval-augmented generation addresses this challenge by providing information from external sources, but risks retrieving irrelevant context, which can degrade the reasoning capabilities of VLMs. Re-ranking retrievals, as introduced in existing approaches, enhances retrieval relevance by focusing on query-text alignment. However, these approaches neglect the visual or multimodal context, which is particularly crucial for medical diagnosis. We propose MOTOR, a novel multimodal retrieval and re-ranking approach that leverages grounded captions and optimal transport. It captures the underlying relationships between the query and the retrieved context based on textual and visual information. Consequently, our approach identifies more clinically relevant contexts to augment the VLM input. Empirical analysis and human expert evaluation demonstrate that MOTOR achieves higher accuracy on MedVQA datasets, outperforming state-of-the-art methods by an average of 6.45%. Code is available at https://github.com/BioMedIA-MBZUAI/MOTOR.",
        "arxiv_id": "2506.22900",
        "ARXIVID": "2506.22900",
        "COMMENT": "Matches criterion 2 as it explores multimodal retrieval and re-ranking for medical visual question answering, integrating vision and language.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.24044": {
        "authors": [
            "Sicong Jiang",
            "Zilin Huang",
            "Kangan Qian",
            "Ziang Luo",
            "Tianze Zhu",
            "Yang Zhong",
            "Yihong Tang",
            "Menglin Kong",
            "Yunlong Wang",
            "Siwen Jiao",
            "Hao Ye",
            "Zihao Sheng",
            "Xin Zhao",
            "Tuopu Wen",
            "Zheng Fu",
            "Sikai Chen",
            "Kun Jiang",
            "Diange Yang",
            "Seongjin Choi",
            "Lijun Sun"
        ],
        "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "abstract": "arXiv:2506.24044v1 Announce Type: new  Abstract: The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.",
        "arxiv_id": "2506.24044",
        "ARXIVID": "2506.24044",
        "COMMENT": "Matches criterion 7 as it is a survey paper on vision-language-action models, synthesizing the state of the art in a relevant area.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23618": {
        "authors": [
            "Zhongdao Wang",
            "Guodongfang Zhao",
            "Jingjing Ren",
            "Bailan Feng",
            "Shifeng Zhang",
            "Wenbo Li"
        ],
        "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them",
        "abstract": "arXiv:2506.23618v1 Announce Type: new  Abstract: Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image SR show surprising fine details.",
        "arxiv_id": "2506.23618",
        "ARXIVID": "2506.23618",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding through video super-resolution with novel methodologies.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2506.23689": {
        "authors": [
            "Zihao Liu",
            "Xinhang Sui",
            "Yueran Song",
            "Siwen Wang"
        ],
        "title": "Pok\\'eAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red",
        "abstract": "arXiv:2506.23689v1 Announce Type: new  Abstract: We introduce Pok\\'eAI, the first text-based, multi-agent large language model (LLM) framework designed to autonomously play and progress through Pok\\'emon Red. Our system consists of three specialized agents-Planning, Execution, and Critique-each with its own memory bank, role, and skill set. The Planning Agent functions as the central brain, generating tasks to progress through the game. These tasks are then delegated to the Execution Agent, which carries them out within the game environment. Upon task completion, the Critique Agent evaluates the outcome to determine whether the objective was successfully achieved. Once verification is complete, control returns to the Planning Agent, forming a closed-loop decision-making system.   As a preliminary step, we developed a battle module within the Execution Agent. Our results show that the battle AI achieves an average win rate of 80.8% across 50 wild encounters, only 6% lower than the performance of an experienced human player. Furthermore, we find that a model's battle performance correlates strongly with its LLM Arena score on language-related tasks, indicating a meaningful link between linguistic ability and strategic reasoning. Finally, our analysis of gameplay logs reveals that each LLM exhibits a unique playstyle, suggesting that individual models develop distinct strategic behaviors.",
        "arxiv_id": "2506.23689",
        "ARXIVID": "2506.23689",
        "COMMENT": "Matches criterion 1 as it involves spatial intelligence and embodied agents with a novel multi-agent system for playing Pokemon Red.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22881": {
        "authors": [
            "Fumiya Uchiyama",
            "Rintaro Yanagi",
            "Shohei Taniguchi",
            "Shota Takashiro",
            "Masahiro Suzuki",
            "Hirokatsu Kataoka",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "title": "How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings",
        "abstract": "arXiv:2506.22881v1 Announce Type: new  Abstract: Contrastive learning has the capacity to model multimodal probability distributions by embedding and aligning visual representations with semantics from captions. This approach enables the estimation of relational semantic similarity; however, it remains unclear whether it can also represent absolute semantic informativeness. In this work, we introduce a semantic informativeness metric for an image calculated from text samples via a contrastive learning model; similarly, the informativeness of a text is calculated from image samples. We propose a redefinition of the concept of Information Gain, a concept previously explored in natural language processing, extending its application to the domains of vision and language. Our metric quantifies how conditioning on an image distorts the distribution of associated texts, and vice versa for text conditioning on image distributions. In OpenCLIP's empirical results, we observe that images with the lowest Information Gain scores often correspond to placeholder icons such as \"image not found.\" Furthermore, we propose to measure a norm-based metric of the embedding to estimate the Information Gain, following the theoretical results for Skip-Gram with Negative Sampling (SGNS) word embedding. Information Gain can be measured using either CLIP or SigLIP, and the results demonstrate a strong correlation with a coefficient of determination ranging from 0.98 to 1.00. After obtaining the mean and the covariance of the sample embedding, the computational cost of this method is independent of the sample size, and it is compatible with publicly available, open-weight models.",
        "arxiv_id": "2506.22881",
        "ARXIVID": "2506.22881",
        "COMMENT": "Matches criteria 2 as it explores contrastive learning embeddings and their semantic informativeness, which is relevant to vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22720": {
        "authors": [
            "Jinghao Wang",
            "Zhang Li",
            "Zi Wang",
            "Banglei Guan",
            "Yang Shang",
            "Qifeng Yu"
        ],
        "title": "Deterministic Object Pose Confidence Region Estimation",
        "abstract": "arXiv:2506.22720v1 Announce Type: new  Abstract: 6D pose confidence region estimation has emerged as a critical direction, aiming to perform uncertainty quantification for assessing the reliability of estimated poses. However, current sampling-based approach suffers from critical limitations that severely impede their practical deployment: 1) the sampling speed significantly decreases as the number of samples increases. 2) the derived confidence regions are often excessively large. To address these challenges, we propose a deterministic and efficient method for estimating pose confidence regions. Our approach uses inductive conformal prediction to calibrate the deterministically regressed Gaussian keypoint distributions into 2D keypoint confidence regions. We then leverage the implicit function theorem to propagate these keypoint confidence regions directly into 6D pose confidence regions. This method avoids the inefficiency and inflated region sizes associated with sampling and ensembling. It provides compact confidence regions that cover the ground-truth poses with a user-defined confidence level. Experimental results on the LineMOD Occlusion and SPEED datasets show that our method achieves higher pose estimation accuracy with reduced computational time. For the same coverage rate, our method yields significantly smaller confidence region volumes, reducing them by up to 99.9\\% for rotations and 99.8\\% for translations. The code will be available soon.",
        "arxiv_id": "2506.22720",
        "ARXIVID": "2506.22720",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for pose confidence region estimation, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23810": {
        "authors": [
            "Mahshid Shiri",
            "Cigdem Beyan",
            "Vittorio Murino"
        ],
        "title": "MadCLIP: Few-shot Medical Anomaly Detection with CLIP",
        "abstract": "arXiv:2506.23810v1 Announce Type: new  Abstract: An innovative few-shot anomaly detection approach is presented, leveraging the pre-trained CLIP model for medical data, and adapting it for both image-level anomaly classification (AC) and pixel-level anomaly segmentation (AS). A dual-branch design is proposed to separately capture normal and abnormal features through learnable adapters in the CLIP vision encoder. To improve semantic alignment, learnable text prompts are employed to link visual features. Furthermore, SigLIP loss is applied to effectively handle the many-to-one relationship between images and unpaired text prompts, showcasing its adaptation in the medical field for the first time. Our approach is validated on multiple modalities, demonstrating superior performance over existing methods for AC and AS, in both same-dataset and cross-dataset evaluations. Unlike prior work, it does not rely on synthetic data or memory banks, and an ablation study confirms the contribution of each component. The code is available at https://github.com/mahshid1998/MadCLIP.",
        "arxiv_id": "2506.23810",
        "ARXIVID": "2506.23810",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it adapts the CLIP model for medical anomaly detection, showcasing its application in a specific domain.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23835": {
        "authors": [
            "Ziwei Chen",
            "Ziling Liu",
            "Zitong Huang",
            "Mingqi Gao",
            "Feng Zheng"
        ],
        "title": "Refine Any Object in Any Scene",
        "abstract": "arXiv:2506.23835v1 Announce Type: new  Abstract: Viewpoint missing of objects is common in scene reconstruction, as camera paths typically prioritize capturing the overall scene structure rather than individual objects. This makes it highly challenging to achieve high-fidelity object-level modeling while maintaining accurate scene-level representation. Addressing this issue is critical for advancing downstream tasks requiring detailed object understanding and appearance modeling. In this paper, we introduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement framework that leverages 3D generative priors to recover fine-grained object geometry and appearance under missing views. Starting from substituting degraded objects with proxies, via a 3D generative model with strong 3D understanding, RAISE progressively refines geometry and texture by aligning each proxy to its degraded counterpart in 7-DOF pose, followed by correcting spatial and appearance inconsistencies via registration-constrained enhancement. This two-stage refinement ensures the high-fidelity geometry and appearance of the original object in unseen views while maintaining consistency in spatial positioning, observed geometry, and appearance. Extensive experiments on challenging benchmarks show that RAISE significantly outperforms state-of-the-art methods in both novel view synthesis and geometry completion tasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.",
        "arxiv_id": "2506.23835",
        "ARXIVID": "2506.23835",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on refining 3D object geometry and appearance in scene reconstruction, which involves spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23150": {
        "authors": [
            "Xinyue Liang",
            "Zhiyuan Ma",
            "Lingchen Sun",
            "Yanjun Guo",
            "Lei Zhang"
        ],
        "title": "AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation",
        "abstract": "arXiv:2506.23150v1 Announce Type: new  Abstract: Single-image-to-3D models typically follow a sequential generation and reconstruction workflow. However, intermediate multi-view images synthesized by pre-trained generation models often lack cross-view consistency (CVC), significantly degrading 3D reconstruction performance. While recent methods attempt to refine CVC by feeding reconstruction results back into the multi-view generator, these approaches struggle with noisy and unstable reconstruction outputs that limit effective CVC improvement. We introduce AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D generation through distribution alignment rather than relying on strict regression losses. Our key insight is to align both generated and reconstructed multi-view distributions toward the ground-truth multi-view distribution, establishing a principled foundation for improved CVC. Observing that generated images exhibit weak CVC while reconstructed images display strong CVC due to explicit rendering, we propose a soft-hard alignment strategy with distinct objectives for generation and reconstruction models. This approach not only enhances generation quality but also dramatically accelerates inference to as few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC, seamlessly integrates various multi-view generation models with 3D reconstruction models. Extensive experiments demonstrate the effectiveness and efficiency of AlignCVC for single-image-to-3D generation.",
        "arxiv_id": "2506.23150",
        "ARXIVID": "2506.23150",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on improving cross-view consistency for single-image-to-3D generation, which is relevant to spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23783": {
        "authors": [
            "Shiao Wang",
            "Ju Huang",
            "Qingchuan Ma",
            "Jinfeng Gao",
            "Chunyi Xu",
            "Xiao Wang",
            "Lan Chen",
            "Bo Jiang"
        ],
        "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking",
        "abstract": "arXiv:2506.23783v1 Announce Type: new  Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust object tracking has garnered increasing attention in recent years. However, most existing multimodal tracking algorithms depend heavily on high-complexity Vision Transformer architectures for feature extraction and fusion across modalities. This not only leads to substantial computational overhead but also limits the effectiveness of cross-modal interactions. In this paper, we propose an efficient RGB-Event object tracking framework based on the linear-complexity Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a lightweight Prompt Generator that utilizes embedded features from each modality, together with a shared prompt pool, to dynamically generate modality-specific learnable prompt vectors. These prompts, along with the modality-specific embedded features, are then fed into a Vision Mamba-based FEMamba backbone, which facilitates prompt-guided feature extraction, cross-modal interaction, and fusion in a unified manner. Finally, the fused representations are passed to the tracking head for accurate target localization. Extensive experimental evaluations on multiple RGB-Event tracking benchmarks, including short-term COESOT dataset and long-term datasets, i.e., FE108 and FELT V2, demonstrate the superior performance and efficiency of the proposed tracking framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/Mamba_FETrack",
        "arxiv_id": "2506.23783",
        "ARXIVID": "2506.23783",
        "COMMENT": "Matches criterion 3 as it proposes a novel RGB-Event object tracking framework, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23481": {
        "authors": [
            "Xian Zhang",
            "Xiang Cheng"
        ],
        "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks",
        "abstract": "arXiv:2506.23481v1 Announce Type: new  Abstract: Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly enhanced their reasoning capabilities, enabling a wide range of intelligent applications. However, these advancements also raise critical concerns regarding privacy and ethics. MLLMs are now capable of inferring the geographic location of images -- such as those shared on social media or captured from street views -- based solely on visual content, thereby posing serious risks of privacy invasion, including doxxing, surveillance, and other security threats.   Methods: This study provides a comprehensive analysis of existing geolocation techniques based on MLLMs. It systematically reviews relevant litera-ture and evaluates the performance of state-of-the-art visual reasoning models on geolocation tasks, particularly in identifying the origins of street view imagery.   Results: Empirical evaluation reveals that the most advanced visual large models can successfully localize the origin of street-level imagery with up to $49\\%$ accuracy within a 1-kilometer radius. This performance underscores the models' powerful capacity to extract and utilize fine-grained geographic cues from visual data.   Conclusions: Building on these findings, the study identifies key visual elements that contribute to suc-cessful geolocation, such as text, architectural styles, and environmental features. Furthermore, it discusses the potential privacy implications associated with MLLM-enabled geolocation and discuss several technical and policy-based coun-termeasures to mitigate associated risks. Our code and dataset are available at https://github.com/zxyl1003/MLLM-Geolocation-Evaluation.",
        "arxiv_id": "2506.23481",
        "ARXIVID": "2506.23481",
        "COMMENT": "Matches criterion 2 as it evaluates geolocation capabilities of Multimodal Large Language Models (MLLMs), which aligns with vision\u2013language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22509": {
        "authors": [
            "Hang Xu",
            "Jie Huang",
            "Linjiang Huang",
            "Dong Li",
            "Yidi Liu",
            "Feng Zhao"
        ],
        "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment",
        "abstract": "arXiv:2506.22509v1 Announce Type: new  Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which enhances the dense prediction model's performance when tested on its unseen domain. Recently, with the development of Diffusion-based Dense Prediction (DDP) models, the exploration of DA designs tailored to this framework is worth exploring, since the diffusion model is effective in modeling the distribution transformation that comprises domain information. In this work, we propose a training-free mechanism for DDP frameworks, endowing them with DA capabilities. Our motivation arises from the observation that the exposure bias (e.g., noise statistics bias) in diffusion brings domain shift, and different domains in conditions of DDP models can also be effectively captured by the noise prediction statistics. Based on this, we propose a training-free Domain Noise Alignment (DNA) approach, which alleviates the variations of noise statistics to domain changes during the diffusion sampling process, thereby achieving domain adaptation. Specifically, when the source domain is available, we directly adopt the DNA method to achieve domain adaptation by aligning the noise statistics of the target domain with those of the source domain. For the more challenging source-free DA, inspired by the observation that regions closer to the source domain exhibit higher confidence meeting variations of sampling noise, we utilize the statistics from the high-confidence regions progressively to guide the noise statistic adjustment during the sampling process. Notably, our method demonstrates the effectiveness of enhancing the DA capability of DDP models across four common dense prediction tasks. Code is available at \\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.",
        "arxiv_id": "2506.22509",
        "ARXIVID": "2506.22509",
        "COMMENT": "Matches criteria 4 as it focuses on domain adaptation for diffusion-based dense prediction models in computer vision.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.22890": {
        "authors": [
            "Senkang Hu",
            "Yihang Tao",
            "Guowen Xu",
            "Xinyuan Qian",
            "Yiqin Deng",
            "Xianhao Chen",
            "Sam Tak Wu Kwong",
            "Yuguang Fang"
        ],
        "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems",
        "abstract": "arXiv:2506.22890v1 Announce Type: new  Abstract: Collaborative Perception (CP) has been shown to be a promising technique for multi-agent autonomous driving and multi-agent robotic systems, where multiple agents share their perception information to enhance the overall perception performance and expand the perception range. However, in CP, an ego agent needs to receive messages from its collaborators, which makes it vulnerable to attacks from malicious agents. To address this critical issue, we propose a unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which is a tailored defense mechanism for CP deployed by each agent to accurately detect and eliminate malicious agents in its collaboration network. Our key idea is to enable CP to reach a consensus rather than a conflict against an ego agent's perception results. Based on this idea, we first develop a probability-agnostic sample consensus (PASAC) method to effectively sample a subset of the collaborators and verify the consensus without prior probabilities of malicious agents. Furthermore, we define collaborative consistency loss (CCLoss) for object detection task and bird's eye view (BEV) segmentation task to capture the discrepancy between an ego agent and its collaborators, which is used as a verification criterion for consensus. In addition, we propose online adaptive threshold via dual sliding windows to dynamically adjust the threshold for consensus verification and ensure the reliability of the systems in dynamic environments. Finally, we conduct extensive experiments and demonstrate the effectiveness of our framework. Code will be released at https://github.com/CP-Security/CP-Guard",
        "arxiv_id": "2506.22890",
        "ARXIVID": "2506.22890",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for detecting malicious agents in multi-agent embodied perception systems.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.23513": {
        "authors": [
            "Zixun Fang",
            "Kai Zhu",
            "Zhiheng Liu",
            "Yu Liu",
            "Wei Zhai",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models",
        "abstract": "arXiv:2506.23513v1 Announce Type: new  Abstract: Panoramic video generation aims to synthesize 360-degree immersive videos, holding significant importance in the fields of VR, world models, and spatial intelligence. Existing works fail to synthesize high-quality panoramic videos due to the inherent modality gap between panoramic data and perspective data, which constitutes the majority of the training data for modern diffusion models. In this paper, we propose a novel framework utilizing pretrained perspective video models for generating panoramic videos. Specifically, we design a novel panorama representation named ViewPoint map, which possesses global spatial continuity and fine-grained visual details simultaneously. With our proposed Pano-Perspective attention mechanism, the model benefits from pretrained perspective priors and captures the panoramic spatial correlations of the ViewPoint map effectively. Extensive experiments demonstrate that our method can synthesize highly dynamic and spatially consistent panoramic videos, achieving state-of-the-art performance and surpassing previous methods.",
        "arxiv_id": "2506.23513",
        "ARXIVID": "2506.23513",
        "COMMENT": "Matches criterion 6 as it focuses on panoramic video generation, a video understanding task, with novel methodologies.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.23711": {
        "authors": [
            "Haoyang Chen",
            "Dongfang Sun",
            "Caoyuan Ma",
            "Shiqin Wang",
            "Kewei Zhang",
            "Zheng Wang",
            "Zhixiang Wang"
        ],
        "title": "Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion",
        "abstract": "arXiv:2506.23711v1 Announce Type: new  Abstract: We propose Subjective Camera, a human-as-imaging-device paradigm that reconstructs real-world scenes from mental impressions through synergistic use of verbal descriptions and progressive rough sketches. This approach overcomes dual limitations of language ambiguity and sketch abstraction by treating the user's drawing sequence as priors, effectively translating subjective perceptual expectations into photorealistic images.   Existing approaches face three fundamental barriers: (1) user-specific subjective input biases, (2) huge modality gap between planar sketch and 3D priors in diffusion, and (3) sketch quality-sensitive performance degradation. Current solutions either demand resource-intensive model adaptation or impose impractical requirements on sketch precision.   Our framework addresses these challenges through concept-sequential generation. (1) We establish robust appearance priors through text-reward optimization, and then implement sequence-aware disentangled generation that processes concepts in sketching order; these steps accommodate user-specific subjective expectation in a train-free way. (2) We employ latent optimization that effectively bridges the modality gap between planar sketches and 3D priors in diffusion. (3) Our hierarchical reward-guided framework enables the use of rough sketches without demanding artistic expertise. Comprehensive evaluation across diverse datasets demonstrates that our approach achieves state-of-the-art performance in maintaining both semantic and spatial coherence.",
        "arxiv_id": "2506.23711",
        "ARXIVID": "2506.23711",
        "COMMENT": "Matches criterion 5 as it integrates image understanding and generation tasks with language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.23352": {
        "authors": [
            "Shunsuke Yasuki",
            "Taiki Miyanishi",
            "Nakamasa Inoue",
            "Shuhei Kurita",
            "Koya Sakamoto",
            "Daichi Azuma",
            "Masato Taki",
            "Yutaka Matsuo"
        ],
        "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields",
        "abstract": "arXiv:2506.23352v1 Announce Type: new  Abstract: The advancement of 3D language fields has enabled intuitive interactions with 3D scenes via natural language. However, existing approaches are typically limited to small-scale environments, lacking the scalability and compositional reasoning capabilities necessary for large, complex urban settings. To overcome these limitations, we propose GeoProg3D, a visual programming framework that enables natural language-driven interactions with city-scale high-fidelity 3D scenes. GeoProg3D consists of two key components: (i) a Geography-aware City-scale 3D Language Field (GCLF) that leverages a memory-efficient hierarchical 3D model to handle large-scale data, integrated with geographic information for efficiently filtering vast urban spaces using directional cues, distance measurements, elevation data, and landmark references; and (ii) Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as area segmentation and object detection. Our framework employs large language models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate GCLF, effectively supporting diverse geographic vision tasks. To assess performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive benchmark dataset containing 952 query-answer pairs across five challenging tasks: grounding, spatial reasoning, comparison, counting, and measurement. Experiments demonstrate that GeoProg3D significantly outperforms existing 3D language fields and vision-language models across multiple tasks. To our knowledge, GeoProg3D is the first framework enabling compositional geographic reasoning in high-fidelity city-scale 3D environments via natural language. The code is available at https://snskysk.github.io/GeoProg3D/.",
        "arxiv_id": "2506.23352",
        "ARXIVID": "2506.23352",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning and compositional visual reasoning in city-scale 3D environments.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.24127": {
        "authors": [
            "Matthew Gwilliam",
            "Roy Zhang",
            "Namitha Padmanabhan",
            "Hongyang Du",
            "Abhinav Shrivastava"
        ],
        "title": "How to Design and Train Your Implicit Neural Representation for Video Compression",
        "abstract": "arXiv:2506.24127v1 Announce Type: new  Abstract: Implicit neural representation (INR) methods for video compression have recently achieved visual quality and compression ratios that are competitive with traditional pipelines. However, due to the need for per-sample network training, the encoding speeds of these methods are too slow for practical adoption. We develop a library to allow us to disentangle and review the components of methods from the NeRV family, reframing their performance in terms of not only size-quality trade-offs, but also impacts on training time. We uncover principles for effective video INR design and propose a state-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When all methods are given equal training time (equivalent to 300 NeRV epochs) for 7 different UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared to the best-performing alternative for each video in our NeRV library. We then tackle the encoding speed issue head-on by investigating the viability of hyper-networks, which predict INR weights from video inputs, to disentangle training from encoding to allow for real-time encoding. We propose masking the weights of the predicted INR during training to allow for variable, higher quality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at 0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by 0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar speeds. Our project website is available at https://mgwillia.github.io/vinrb/ and our code is available at https://github.com/mgwillia/vinrb.",
        "arxiv_id": "2506.24127",
        "ARXIVID": "2506.24127",
        "COMMENT": "Matches criterion 6 as it focuses on video compression using implicit neural representations, which is relevant to video understanding tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.23308": {
        "authors": [
            "Yiming Huang",
            "Long Bai",
            "Beilei Cui",
            "Yanheng Li",
            "Tong Chen",
            "Jie Wang",
            "Jinlin Wu",
            "Zhen Lei",
            "Hongbin Liu",
            "Hongliang Ren"
        ],
        "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting",
        "abstract": "arXiv:2506.23308v1 Announce Type: new  Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.",
        "arxiv_id": "2506.23308",
        "ARXIVID": "2506.23308",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for endoscopic scene reconstruction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.23009": {
        "authors": [
            "Jian Chen",
            "Wenye Ma",
            "Penghang Liu",
            "Wei Wang",
            "Tengwei Song",
            "Ming Li",
            "Chenguang Wang",
            "Ruiyi Zhang",
            "Changyou Chen"
        ],
        "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models",
        "abstract": "arXiv:2506.23009v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable visual reasoning abilities in natural images, text-rich documents, and graphic designs. However, their ability to interpret music sheets remains underexplored. To bridge this gap, we introduce MusiXQA, the first comprehensive dataset for evaluating and advancing MLLMs in music sheet understanding. MusiXQA features high-quality synthetic music sheets generated via MusiXTeX, with structured annotations covering note pitch and duration, chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks. Through extensive evaluations, we reveal significant limitations of current state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant performance gains over GPT-based methods. The proposed dataset and model establish a foundation for future advances in MLLMs for music sheet understanding. Code, data, and model will be released upon acceptance.",
        "arxiv_id": "2506.23009",
        "ARXIVID": "2506.23009",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models for music sheet understanding, which is a novel application of MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23863": {
        "authors": [
            "Jiahao Ma",
            "Lei Wang",
            "Miaomiao liu",
            "David Ahmedt-Aristizabal",
            "Chuong Nguyen"
        ],
        "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction",
        "abstract": "arXiv:2506.23863v1 Announce Type: new  Abstract: Multi-view 3D reconstruction remains a core challenge in computer vision. Recent methods, such as DUST3R and its successors, directly regress pointmaps from image pairs without relying on known scene geometry or camera parameters. However, the performance of these models is constrained by the diversity and scale of available training data. In this work, we introduce Puzzles, a data augmentation strategy that synthesizes an unbounded volume of high-quality posed video-depth data from a single image or video clip. By simulating diverse camera trajectories and realistic scene geometry through targeted image transformations, Puzzles significantly enhances data variety. Extensive experiments show that integrating Puzzles into existing video-based 3D reconstruction pipelines consistently boosts performance without modifying the underlying network architecture. Notably, models trained on only ten percent of the original data augmented with Puzzles still achieve accuracy comparable to those trained on the full dataset. Code is available at https://jiahao-ma.github.io/puzzles/.",
        "arxiv_id": "2506.23863",
        "ARXIVID": "2506.23863",
        "COMMENT": "Matches criterion 6 as it introduces a novel data augmentation strategy for video-based 3D reconstruction, which is relevant to video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.24039": {
        "authors": [
            "Shubhabrata Mukherjee",
            "Jack Lang",
            "Obeen Kwon",
            "Iryna Zenyuk",
            "Valerie Brogden",
            "Adam Weber",
            "Daniela Ushizima"
        ],
        "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data",
        "abstract": "arXiv:2506.24039v1 Announce Type: new  Abstract: Zero-shot and prompt-based technologies capitalized on using frequently occurring images to transform visual reasoning tasks, which explains why such technologies struggle with valuable yet scarce scientific image sets. In this work, we propose Zenesis, a comprehensive no-code interactive platform designed to minimize barriers posed by data readiness for scientific images. We develop lightweight multi-modal adaptation techniques that enable zero-shot operation on raw scientific data, along with human-in-the-loop refinement and heuristic-based temporal enhancement options. We demonstrate the performance of our approach through comprehensive comparison and validation on challenging Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded membranes. Zenesis significantly outperforms baseline methods, achieving an average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results mark a substantial improvement over traditional methods like Otsu thresholding and even advanced models like Segment Anything Model (SAM) when used in isolation. Our results demonstrate that Zenesis is a powerful tool for scientific applications, particularly in fields where high-quality annotated datasets are unavailable, accelerating accurate analysis of experimental imaging.",
        "arxiv_id": "2506.24039",
        "ARXIVID": "2506.24039",
        "COMMENT": "Matches criterion 4 as it focuses on foundation models for segmentation tasks, particularly in scientific imaging, which is a real-world application of vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23577": {
        "authors": [
            "Yanning Hou",
            "Yanran Ruan",
            "Junfa Li",
            "Shanshan Wang",
            "Jianfeng Qiu",
            "Ke Xu"
        ],
        "title": "StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection",
        "abstract": "arXiv:2506.23577v1 Announce Type: new  Abstract: Enhancing the alignment between text and image features in the CLIP model is a critical challenge in zero-shot industrial anomaly detection tasks. Recent studies predominantly utilize specific category prompts during pretraining, which can cause overfitting to the training categories and limit model generalization. To address this, we propose a method that transforms category names through multicategory name stacking to create stacked prompts, forming the basis of our StackCLIP model. Our approach introduces two key components. The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts by stacking semantically analogous categories, while utilizing multi-object textual feature fusion to amplify discriminative anomalies among similar objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific linear layers tailored for each stack cluster and adaptively integrates them based on the attributes of test categories. These modules work together to deliver superior training speed, stability, and convergence, significantly boosting anomaly segmentation performance. Additionally, our stacked prompt framework offers robust generalization across classification tasks. To further improve performance, we introduce the Regulating Prompt Learning (RPL) module, which leverages the generalization power of stacked prompts to refine prompt learning, elevating results in anomaly detection classification tasks. Extensive testing on seven industrial anomaly detection datasets demonstrates that our method achieves state-of-the-art performance in both zero-shot anomaly detection and segmentation tasks.",
        "arxiv_id": "2506.23577",
        "ARXIVID": "2506.23577",
        "COMMENT": "Matches criterion 2 as it explores a novel method for enhancing text-image alignment in CLIP models, which is relevant to Visual and Multimodal Large Language Models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23538": {
        "authors": [
            "Yuhao Huang",
            "Yueyue Xu",
            "Haoran Dou",
            "Jiaxiao Deng",
            "Xin Yang",
            "Hongyu Zheng",
            "Dong Ni"
        ],
        "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound",
        "abstract": "arXiv:2506.23538v1 Announce Type: new  Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.",
        "arxiv_id": "2506.23538",
        "ARXIVID": "2506.23538",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it proposes a system for plane localization and anomaly diagnosis in 3D ultrasound, leveraging diffusion and reinforcement learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.22718": {
        "authors": [
            "Jun-Jee Chao",
            "Qingyuan Jiang",
            "Volkan Isler"
        ],
        "title": "Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians",
        "abstract": "arXiv:2506.22718v1 Announce Type: new  Abstract: Part segmentation and motion estimation are two fundamental problems for articulated object motion analysis. In this paper, we present a method to solve these two problems jointly from a sequence of observed point clouds of a single articulated object. The main challenge in our problem setting is that the point clouds are not assumed to be generated by a fixed set of moving points. Instead, each point cloud in the sequence could be an arbitrary sampling of the object surface at that particular time step. Such scenarios occur when the object undergoes major occlusions, or if the dataset is collected using measurements from multiple sensors asynchronously. In these scenarios, methods that rely on tracking point correspondences are not appropriate. We present an alternative approach based on a compact but effective representation where we represent the object as a collection of simple building blocks modeled as 3D Gaussians. We parameterize the Gaussians with time-dependent rotations, translations, and scales that are shared across all time steps. With our representation, part segmentation can be achieved by building correspondences between the observed points and the Gaussians. Moreover, the transformation of each point across time can be obtained by following the poses of the assigned Gaussian (even when the point is not observed). Experiments show that our method outperforms existing methods that solely rely on finding point correspondences. Additionally, we extend existing datasets to emulate real-world scenarios by considering viewpoint occlusions. We further demonstrate that our method is more robust to missing points as compared to existing approaches on these challenging datasets, even when some parts are completely occluded in some time-steps. Notably, our part segmentation performance outperforms the state-of-the-art method by 13% on point clouds with occlusions.",
        "arxiv_id": "2506.22718",
        "ARXIVID": "2506.22718",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a method for part segmentation and motion estimation using dynamic 3D Gaussians, addressing challenges in articulated object motion analysis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.22908": {
        "authors": [
            "Yuzhu Wang",
            "Manni Duan",
            "Shu Kong"
        ],
        "title": "Attention to Burstiness: Low-Rank Bilinear Prompt Tuning",
        "abstract": "arXiv:2506.22908v1 Announce Type: new  Abstract: Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique that adapts a pre-trained vision Transformer (ViT) by learning a small set of parameters in the input space, known as prompts. In VPT, we uncover ``burstiness'' in the values arising from the interaction of image patch embeddings, and the key and query projectors within Transformer's self-attention module. Furthermore, the values of patch embeddings and the key and query projectors exhibit Laplacian and hyper-Laplacian distribution, respectively. Intuitively, these non-Gaussian distributions pose challenges for learning prompts. To address this, we propose whitening these data, de-correlating them and equalizing their variance towards more Gaussian before learning prompts. We derive the whitening matrix over random image patch embeddings and ViT's key and query projectors, and multiply it with the prompt to be learned in a bilinear manner. Surprisingly, this method significantly accelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on the CUB dataset; interestingly, it learns ``bursty prompts''. Extending the bilinear model which is known to introduce burstiness, we present a compact, low-rank version by learning two smaller matrices whose multiplication yields the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT). Extensive experiments across multiple benchmark datasets demonstrate that BPT methods not only outperform various VPT methods but also reduce parameter count and computation overhead.",
        "arxiv_id": "2506.22908",
        "ARXIVID": "2506.22908",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes a novel bilinear prompt tuning method for vision transformers, improving parameter efficiency and accuracy.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.22531": {
        "authors": [
            "Prasen Kumar Sharma",
            "Neeraj Matiyali",
            "Siddharth Srivastava",
            "Gaurav Sharma"
        ],
        "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation",
        "abstract": "arXiv:2506.22531v1 Announce Type: new  Abstract: We introduce \\textit{Preserve Anything}, a novel method for controlled image synthesis that addresses key limitations in object preservation and semantic consistency in text-to-image (T2I) generation. Existing approaches often fail (i) to preserve multiple objects with fidelity, (ii) maintain semantic alignment with prompts, or (iii) provide explicit control over scene composition. To overcome these challenges, the proposed method employs an N-channel ControlNet that integrates (i) object preservation with size and placement agnosticism, color and detail retention, and artifact elimination, (ii) high-resolution, semantically consistent backgrounds with accurate shadows, lighting, and prompt adherence, and (iii) explicit user control over background layouts and lighting conditions. Key components of our framework include object preservation and background guidance modules, enforcing lighting consistency and a high-frequency overlay module to retain fine details while mitigating unwanted artifacts. We introduce a benchmark dataset consisting of 240K natural images filtered for aesthetic quality and 18K 3D-rendered synthetic images with metadata such as lighting, camera angles, and object relationships. This dataset addresses the deficiencies of existing benchmarks and allows a complete evaluation. Empirical results demonstrate that our method achieves state-of-the-art performance, significantly improving feature-space fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining competitive aesthetic quality. We also conducted a user study to demonstrate the efficacy of the proposed work on unseen benchmark and observed a remarkable improvement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of prompt alignment, photorealism, the presence of AI artifacts, and natural aesthetics over existing works.",
        "arxiv_id": "2506.22531",
        "ARXIVID": "2506.22531",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a method for controlled image synthesis with object preservation, integrating text and image understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23801": {
        "authors": [
            "Ce Wang",
            "Wanjie Sun"
        ],
        "title": "Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors",
        "abstract": "arXiv:2506.23801v1 Announce Type: new  Abstract: Super-resolution (SR) techniques can enhance the spatial resolution of remote sensing images by utilizing low-resolution (LR) images to reconstruct high-resolution (HR) images, enabling more efficient large-scale earth observation applications. While single-image super-resolution (SISR) methods have shown progress, reference-based super-resolution (RefSR) offers superior performance by incorporating historical HR images alongside current LR observations. However, existing RefSR methods struggle with real-world complexities, such as cross-sensor resolution gap and significant land cover changes, often leading to under-generation or over-reliance on reference image. To address these challenges, we propose CRefDiff, a novel controllable reference-based diffusion model for real-world remote sensing image SR. To address the under-generation problem, CRefDiff is built upon the pretrained Stable Diffusion model, leveraging its powerful generative prior to produce accurate structures and textures. To mitigate over-reliance on the reference, we introduce a dual-branch fusion mechanism that adaptively integrates both local and global information from the reference image. Moreover, this novel dual-branch design enables reference strength control during inference, enhancing interactivity and flexibility of the model. Finally, a strategy named Better Start is proposed to significantly reduce the number of denoising steps, thereby accelerating the inference process. To support further research, we introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land cover changes and significant temporal gaps. Extensive experiments on Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across various metrics and improves downstream tasks such as scene classification and semantic segmentation.",
        "arxiv_id": "2506.23801",
        "ARXIVID": "2506.23801",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on super-resolution for remote sensing images, leveraging generative diffusion priors and addressing real-world challenges.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23543": {
        "authors": [
            "Hui Li",
            "Baoyou Chen",
            "Liwei Zhang",
            "Jiaye Li",
            "Jingdong Wang",
            "Siyu Zhu"
        ],
        "title": "Pyramidal Patchification Flow for Visual Generation",
        "abstract": "arXiv:2506.23543v1 Announce Type: new  Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations to token representations through linear projections, to adjust the number of tokens input to DiT blocks and thus the computation cost. Instead of a single patch size for all the timesteps, we introduce a Pyramidal Patchification Flow (PPFlow) approach: Large patch sizes are used for high noise timesteps and small patch sizes for low noise timesteps; Linear projections are learned for each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow, our approach operates over full latent representations other than pyramid representations, and adopts the normal denoising process without requiring the renoising trick. We demonstrate the effectiveness of our approach through two training manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$) inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with slightly lower training FLOPs and similar image generation performance. Training from pretrained normal DiTs achieves even better performance with small training time. The code and checkpoint are at https://github.com/fudan-generative-vision/PPFlow.",
        "arxiv_id": "2506.23543",
        "ARXIVID": "2506.23543",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a pyramidal patchification approach for diffusion transformers, improving efficiency and performance.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23418": {
        "authors": [
            "Parham Rezaei",
            "Arash Marioriyad",
            "Mahdieh Soleymani Baghshah",
            "Mohammad Hossein Rohban"
        ],
        "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models",
        "abstract": "arXiv:2506.23418v1 Announce Type: new  Abstract: Despite the ability of text-to-image models to generate high-quality, realistic, and diverse images, they face challenges in compositional generation, often struggling to accurately represent details specified in the input prompt. A prevalent issue in compositional generation is the misalignment of spatial relationships, as models often fail to faithfully generate images that reflect the spatial configurations specified between objects in the input prompts. To address this challenge, we propose a novel probabilistic framework for modeling the relative spatial positioning of objects in a scene, leveraging the concept of Probability of Superiority (PoS). Building on this insight, we make two key contributions. First, we introduce a novel evaluation metric, PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D spatial relationships between text and image, with improved adherence to human judgment. Second, we propose PoS-based Generation (PSG), an inference-time method that improves the alignment of 2D and 3D spatial relationships in T2I models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based reward function that can be utilized in two distinct ways: (1) as a gradient-based guidance mechanism applied to the cross-attention maps during the denoising steps, or (2) as a search-based strategy that evaluates a set of initial noise vectors to select the best one. Extensive experiments demonstrate that the PSE metric exhibits stronger alignment with human judgment compared to traditional center-based metrics, providing a more nuanced and reliable measure of complex spatial relationship accuracy in text-image alignment. Furthermore, PSG significantly enhances the ability of text-to-image models to generate images with specified spatial configurations, outperforming state-of-the-art methods across multiple evaluation metrics and benchmarks.",
        "arxiv_id": "2506.23418",
        "ARXIVID": "2506.23418",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it proposes a probabilistic framework for spatial relationship alignment in text-to-image models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23104": {
        "authors": [
            "Jihun Kim",
            "Hoyong Kwon",
            "Hyeokjun Kweon",
            "Wooseong Jeong",
            "Kuk-Jin Yoon"
        ],
        "title": "DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation",
        "abstract": "arXiv:2506.23104v1 Announce Type: new  Abstract: Interactive segmentation (IS) allows users to iteratively refine object boundaries with minimal cues, such as positive and negative clicks. While the Segment Anything Model (SAM) has garnered attention in the IS community for its promptable segmentation capabilities, it often struggles in specialized domains or when handling complex scenarios (e.g., camouflaged or multi-part objects). To overcome these challenges, we propose DC-TTA, a novel test-time adaptation (TTA) framework that adapts SAM on a per-sample basis by leveraging user interactions as supervision. Instead of forcing a single model to incorporate all user clicks at once, DC-TTA partitions the clicks into more coherent subsets, each processed independently via TTA with a separated model. This Divide-and-Conquer strategy reduces conflicts among diverse cues and enables more localized updates. Finally, we merge the adapted models to form a unified predictor that integrates the specialized knowledge from each subset. Experimental results across various benchmarks demonstrate that DC-TTA significantly outperforms SAM's zero-shot results and conventional TTA methods, effectively handling complex tasks such as camouflaged object segmentation with fewer interactions and improved accuracy.",
        "arxiv_id": "2506.23104",
        "ARXIVID": "2506.23104",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel test-time adaptation framework for interactive segmentation, addressing challenges in specialized domains.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23532": {
        "authors": [
            "Jefferson Hernandez",
            "Ruozhen He",
            "Guha Balakrishnan",
            "Alexander C. Berg",
            "Vicente Ordonez"
        ],
        "title": "GViT: Representing Images as Gaussians for Visual Recognition",
        "abstract": "arXiv:2506.23532v1 Announce Type: new  Abstract: We introduce GVIT, a classification framework that abandons conventional pixel or patch grid input representations in favor of a compact set of learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose positions, scales, orientations, colors, and opacities are optimized jointly with a ViT classifier trained on top of these representations. We reuse the classifier gradients as constructive guidance, steering the Gaussians toward class-salient regions while a differentiable renderer optimizes an image reconstruction loss. We demonstrate that by 2D Gaussian input representations coupled with our GVIT guidance, using a relatively standard ViT architecture, closely matches the performance of a traditional patch-based ViT, reaching a 76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.",
        "arxiv_id": "2506.23532",
        "ARXIVID": "2506.23532",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on a novel representation for visual recognition using Gaussian representations with a ViT architecture.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23844": {
        "authors": [
            "Hang Su",
            "Jun Luo",
            "Chang Liu",
            "Xiao Yang",
            "Yichi Zhang",
            "Yinpeng Dong",
            "Jun Zhu"
        ],
        "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
        "abstract": "arXiv:2506.23844v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of autonomous AI agents capable of perceiving, reasoning, and acting in dynamic, open-ended environments. These large-model agents mark a paradigm shift from static inference systems to interactive, memory-augmented entities. While these capabilities significantly expand the functional scope of AI, they also introduce qualitatively novel security risks - such as memory poisoning, tool misuse, reward hacking, and emergent misalignment - that extend beyond the threat models of conventional systems or standalone LLMs. In this survey, we first examine the structural foundations and key capabilities that underpin increasing levels of agent autonomy, including long-term memory retention, modular tool use, recursive planning, and reflective reasoning. We then analyze the corresponding security vulnerabilities across the agent stack, identifying failure modes such as deferred decision hazards, irreversible tool chains, and deceptive behaviors arising from internal state drift or value misalignment. These risks are traced to architectural fragilities that emerge across perception, cognition, memory, and action modules. To address these challenges, we systematically review recent defense strategies deployed at different autonomy layers, including input sanitization, memory lifecycle control, constrained decision-making, structured tool invocation, and introspective reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a unified cognitive framework grounded in Constrained Markov Decision Processes (CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation, and joint reward-risk optimization to enable principled, proactive safety across the agent's decision-making loop.",
        "arxiv_id": "2506.23844",
        "ARXIVID": "2506.23844",
        "COMMENT": "Matches criteria 7 as it is a survey paper on security risks in large model-based agents, relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2506.23157": {
        "authors": [
            "Hanyu Zhou",
            "Haonan Wang",
            "Haoyue Liu",
            "Yuxing Duan",
            "Luxin Yan",
            "Gim Hee Lee"
        ],
        "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene",
        "abstract": "arXiv:2506.23157v1 Announce Type: new  Abstract: High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.",
        "arxiv_id": "2506.23157",
        "ARXIVID": "2506.23157",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks and introduces a novel spatiotemporal disentanglement framework.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23523": {
        "authors": [
            "Tuong Do",
            "Binh X. Nguyen",
            "Quang D. Tran",
            "Erman Tjiputra",
            "Te-Chuan Chiu",
            "Anh Nguyen"
        ],
        "title": "Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving",
        "abstract": "arXiv:2506.23523v1 Announce Type: new  Abstract: Traditional vision-based autonomous driving systems often face difficulties in navigating complex environments when relying solely on single-image inputs. To overcome this limitation, incorporating temporal data such as past image frames or steering sequences, has proven effective in enhancing robustness and adaptability in challenging scenarios. While previous high-performance methods exist, they often rely on resource-intensive fusion networks, making them impractical for training and unsuitable for federated learning. To address these challenges, we propose lightweight temporal transformer decomposition, a method that processes sequential image frames and temporal steering data by breaking down large attention maps into smaller matrices. This approach reduces model complexity, enabling efficient weight updates for convergence and real-time predictions while leveraging temporal information to enhance autonomous driving performance. Intensive experiments on three datasets demonstrate that our method outperforms recent approaches by a clear margin while achieving real-time performance. Additionally, real robot experiments further confirm the effectiveness of our method.",
        "arxiv_id": "2506.23523",
        "ARXIVID": "2506.23523",
        "COMMENT": "Matches criterion 3 as it proposes a lightweight temporal transformer for autonomous driving, relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23207": {
        "authors": [
            "Zhen Tan",
            "Xieyuanli Chen",
            "Lei Feng",
            "Yangbing Ge",
            "Shuaifeng Zhi",
            "Jiaxiong Liu",
            "Dewen Hu"
        ],
        "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints",
        "abstract": "arXiv:2506.23207v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.",
        "arxiv_id": "2506.23207",
        "ARXIVID": "2506.23207",
        "COMMENT": "Matches criterion 3 as it introduces a novel SLAM system with robust tracking and mapping improvements for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23606": {
        "authors": [
            "Zhengkang Xiang",
            "Zizhao Li",
            "Amir Khodabandeh",
            "Kourosh Khoshelham"
        ],
        "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion",
        "abstract": "arXiv:2506.23606v1 Announce Type: new  Abstract: Lidar point cloud synthesis based on generative models offers a promising solution to augment deep learning pipelines, particularly when real-world data is scarce or lacks diversity. By enabling flexible object manipulation, this synthesis approach can significantly enrich training datasets and enhance discriminative models. However, existing methods focus on unconditional lidar point cloud generation, overlooking their potential for real-world applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar Diffusion Model that employs latent alignment to enable robust semantic-to-lidar synthesis. By directly operating in the native lidar space and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art performance in generating high-fidelity lidar point clouds guided by semantic labels. Moreover, we propose the first diffusion-based lidar translation framework based on SG-LDM, which enables cross-domain translation as a domain adaptation strategy to enhance downstream perception performance. Systematic experiments demonstrate that SG-LDM significantly outperforms existing lidar diffusion models and the proposed lidar translation framework further improves data augmentation performance in the downstream lidar segmentation task.",
        "arxiv_id": "2506.23606",
        "ARXIVID": "2506.23606",
        "COMMENT": "Matches criterion 3 as it introduces a novel diffusion-based lidar translation framework for embodied AI applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.23467": {
        "authors": [
            "Chenlang Yi",
            "Zizhan Xiong",
            "Qi Qi",
            "Xiyuan Wei",
            "Girish Bathla",
            "Ching-Long Lin",
            "Bobak Jack Mortazavi",
            "Tianbao Yang"
        ],
        "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays",
        "abstract": "arXiv:2506.23467v1 Announce Type: new  Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis.",
        "arxiv_id": "2506.23467",
        "ARXIVID": "2506.23467",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores fairness in CLIP models, which are a type of vision-language model.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2506.22902": {
        "authors": [
            "Yiling Xu",
            "Yujie Zhang",
            "Shuting Xia",
            "Kaifa Yang",
            "He Huang",
            "Ziyu Shan",
            "Wenjie Huang",
            "Qi Yang",
            "Le Yang"
        ],
        "title": "Point Cloud Compression and Objective Quality Assessment: A Survey",
        "abstract": "arXiv:2506.22902v1 Announce Type: new  Abstract: The rapid growth of 3D point cloud data, driven by applications in autonomous driving, robotics, and immersive environments, has led to criticals demand for efficient compression and quality assessment techniques. Unlike traditional 2D media, point clouds present unique challenges due to their irregular structure, high data volume, and complex attributes. This paper provides a comprehensive survey of recent advances in point cloud compression (PCC) and point cloud quality assessment (PCQA), emphasizing their significance for real-time and perceptually relevant applications. We analyze a wide range of handcrafted and learning-based PCC algorithms, along with objective PCQA metrics. By benchmarking representative methods on emerging datasets, we offer detailed comparisons and practical insights into their strengths and limitations. Despite notable progress, challenges such as enhancing visual fidelity, reducing latency, and supporting multimodal data remain. This survey outlines future directions, including hybrid compression frameworks and advanced feature extraction strategies, to enable more efficient, immersive, and intelligent 3D applications.",
        "arxiv_id": "2506.22902",
        "ARXIVID": "2506.22902",
        "COMMENT": "Matches criterion 7 as it is a survey paper on point cloud compression and quality assessment, synthesizing the state of the art in this area of computer vision.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2506.23382": {
        "authors": [
            "Vikram Rangarajan",
            "Shishira Maiya",
            "Max Ehrlich",
            "Abhinav Shrivastava"
        ],
        "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders",
        "abstract": "arXiv:2506.23382v1 Announce Type: new  Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video compression by learning per-video optimized functions, but their adoption is crippled by impractically slow encoding times. Existing attempts to accelerate INR encoding often sacrifice reconstruction quality or crucial coordinate-level control essential for adaptive streaming and transcoding. We introduce SIEDD (Shared-Implicit Encoder with Discrete Decoders), a novel architecture that fundamentally accelerates INR encoding without these compromises. SIEDD first rapidly trains a shared, coordinate-based encoder on sparse anchor frames to efficiently capture global, low-frequency video features. This encoder is then frozen, enabling massively parallel training of lightweight, discrete decoders for individual frame groups, further expedited by aggressive coordinate-space sampling. This synergistic design delivers a remarkable 20-30X encoding speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while maintaining competitive reconstruction quality and compression ratios. Critically, SIEDD retains full coordinate-based control, enabling continuous resolution decoding and eliminating costly transcoding. Our approach significantly advances the practicality of high-fidelity neural video compression, demonstrating a scalable and efficient path towards real-world deployment. Our codebase is available at https://github.com/VikramRangarajan/SIEDD .",
        "arxiv_id": "2506.23382",
        "ARXIVID": "2506.23382",
        "COMMENT": "Does not match any specific criterion but discusses video compression using implicit neural representations.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23908": {
        "authors": [
            "Andr\\'as Gy\\\"orgy",
            "Tor Lattimore",
            "Nevena Lazi\\'c",
            "Csaba Szepesv\\'ari"
        ],
        "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence",
        "abstract": "arXiv:2506.23908v1 Announce Type: new  Abstract: Sound deductive reasoning -- the ability to derive new knowledge from existing facts and rules -- is an indisputably desirable aspect of general intelligence. Despite the major advances of AI systems in areas such as math and science, especially since the introduction of transformer architectures, it is well-documented that even the most advanced frontier systems regularly and consistently falter on easily-solvable deductive reasoning tasks. Hence, these systems are unfit to fulfill the dream of achieving artificial general intelligence capable of sound deductive reasoning. We argue that their unsound behavior is a consequence of the statistical learning approach powering their development. To overcome this, we contend that to achieve reliable deductive reasoning in learning-based AI systems, researchers must fundamentally shift from optimizing for statistical performance against distributions on reasoning problems and algorithmic tasks to embracing the more ambitious exact learning paradigm, which demands correctness on all inputs. We argue that exact learning is both essential and possible, and that this ambitious objective should guide algorithm design.",
        "arxiv_id": "2506.23908",
        "ARXIVID": "2506.23908",
        "COMMENT": "Does not match any specific criterion but discusses general AI reasoning and learning paradigms.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23205": {
        "authors": [
            "Dequan Kong",
            "Zhe Zhu",
            "Honghua Chen",
            "Mingqiang Wei"
        ],
        "title": "BridgeShape: Latent Diffusion Schr\\\"odinger Bridge for 3D Shape Completion",
        "abstract": "arXiv:2506.23205v1 Announce Type: new  Abstract: Existing diffusion-based 3D shape completion methods typically use a conditional paradigm, injecting incomplete shape information into the denoising network via deep feature interactions (e.g., concatenation, cross-attention) to guide sampling toward complete shapes, often represented by voxel-based distance functions. However, these approaches fail to explicitly model the optimal global transport path, leading to suboptimal completions. Moreover, performing diffusion directly in voxel space imposes resolution constraints, limiting the generation of fine-grained geometric details. To address these challenges, we propose BridgeShape, a novel framework for 3D shape completion via latent diffusion Schr\\\"odinger bridge. The key innovations lie in two aspects: (i) BridgeShape formulates shape completion as an optimal transport problem, explicitly modeling the transition between incomplete and complete shapes to ensure a globally coherent transformation. (ii) We introduce a Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D shapes into a compact latent space, leveraging self-projected multi-view depth information enriched with strong DINOv2 features to enhance geometric structural perception. By operating in a compact yet structurally informative latent space, BridgeShape effectively mitigates resolution constraints and enables more efficient and high-fidelity 3D shape completion. BridgeShape achieves state-of-the-art performance on large-scale 3D shape completion benchmarks, demonstrating superior fidelity at higher resolutions and for unseen object classes.",
        "arxiv_id": "2506.23205",
        "ARXIVID": "2506.23205",
        "COMMENT": "Does not match any specific criteria but involves 3D shape completion, which is tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.22899": {
        "authors": [
            "Ehsan Pajouheshgar",
            "Yitao Xu",
            "Ali Abbasi",
            "Alexander Mordvintsev",
            "Wenzel Jakob",
            "Sabine S\\\"usstrunk"
        ],
        "title": "Neural Cellular Automata: From Cells to Pixels",
        "abstract": "arXiv:2506.22899v1 Announce Type: new  Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical cells self-organize to form complex and coherent patterns by repeatedly applying simple local rules. NCAs display striking emergent behaviors including self-regeneration, generalization and robustness to unseen situations, and spontaneous motion. Despite their success in texture synthesis and morphogenesis, NCAs remain largely confined to low-resolution grids. This limitation stems from (1) training time and memory requirements that grow quadratically with grid size, (2) the strictly local propagation of information which impedes long-range cell communication, and (3) the heavy compute demands of real-time inference at high resolution. In this work, we overcome this limitation by pairing NCA with a tiny, shared implicit decoder, inspired by recent advances in implicit neural representations. Following NCA evolution on a coarse grid, a lightweight decoder renders output images at arbitrary resolution. We also propose novel loss functions for both morphogenesis and texture synthesis tasks, specifically tailored for high-resolution output with minimal memory and computation overhead. Combining our proposed architecture and loss functions brings substantial improvement in quality, efficiency, and performance. NCAs equipped with our implicit decoder can generate full-HD outputs in real time while preserving their self-organizing, emergent properties. Moreover, because each MLP processes cell states independently, inference remains highly parallelizable and efficient. We demonstrate the applicability of our approach across multiple NCA variants (on 2D, 3D grids, and 3D meshes) and multiple tasks, including texture generation and morphogenesis (growing patterns from a seed), showing that with our proposed framework, NCAs seamlessly scale to high-resolution outputs with minimal computational overhead.",
        "arxiv_id": "2506.22899",
        "ARXIVID": "2506.22899",
        "COMMENT": "Does not match any specific criteria. Focuses on Neural Cellular Automata for high-resolution outputs, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23692": {
        "authors": [
            "Boyuan Zheng",
            "Zerui Fang",
            "Zhe Xu",
            "Rui Wang",
            "Yiwen Chen",
            "Cunshi Wang",
            "Mengwei Qu",
            "Lei Lei",
            "Zhen Feng",
            "Yan Liu",
            "Yuyang Li",
            "Mingzhou Tan",
            "Jiaji Wu",
            "Jianwei Shuai",
            "Jia Li",
            "Fangfu Ye"
        ],
        "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models",
        "abstract": "arXiv:2506.23692v1 Announce Type: new  Abstract: While AI for Science (AI4S) serves as an analytical tool in the current research paradigm, it doesn't solve its core inefficiency. We propose \"Agent for Science\" (Agent4S)-the use of LLM-driven agents to automate the entire research workflow-as the true Fifth Scientific Paradigm. This paper introduces a five-level classification for Agent4S, outlining a clear roadmap from simple task automation to fully autonomous, collaborative \"AI Scientists.\" This framework defines the next revolutionary step in scientific discovery.",
        "arxiv_id": "2506.23692",
        "ARXIVID": "2506.23692",
        "COMMENT": "Does not closely match any specific criteria but discusses the use of large language models in scientific research automation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23077": {
        "authors": [
            "Suofei Zhang",
            "Xinxin Wang",
            "Xiaofu Wu",
            "Quan Zhou",
            "Haifeng Hu"
        ],
        "title": "Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization",
        "abstract": "arXiv:2506.23077v1 Announce Type: new  Abstract: Existing deep learning-based cross-view geo-localization methods primarily focus on improving the accuracy of cross-domain image matching, rather than enabling models to comprehensively capture contextual information around the target and minimize the cost of localization errors. To support systematic research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem, we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs multi-view imagery with precise distance annotations across three spatial resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical retrieval problem across different domains. Our study further reveals that, due to the inherent complexity of spatial relationships among buildings, this problem can only be addressed via a contrastive learning paradigm, rather than conventional metric learning. To tackle this challenge, we propose Dynamic Contrastive Learning (DyCL), a novel framework that progressively aligns feature representations according to hierarchical spatial margins. Extensive experiments demonstrate that DyCL is highly complementary to existing multi-scale metric learning methods and yields substantial improvements in both hierarchical retrieval performance and overall cross-view geo-localization accuracy. Our code and benchmark are publicly available at https://github.com/anocodetest1/DyCL.",
        "arxiv_id": "2506.23077",
        "ARXIVID": "2506.23077",
        "COMMENT": "Does not closely match any specific criteria but is related to spatial intelligence and embodied agents in the context of geo-localization.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.23141": {
        "authors": [
            "Siyuan Li",
            "Ruitong Liu",
            "Yan Wen",
            "Te Sun"
        ],
        "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing",
        "abstract": "arXiv:2506.23141v1 Announce Type: new  Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge Graph Completion (KGC), providing vital cues for prediction. However, traditional node-based message passing mechanisms, when applied to knowledge graphs, often introduce noise and suffer from information dilution or over-smoothing by indiscriminately aggregating information from all neighboring edges. To address this challenge, we propose a semantic-aware relational message passing. A core innovation of this framework is the introduction of a \\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this strategy first evaluates the semantic relevance between a central node and its incident edges within a shared latent space, selecting only the Top-K most pertinent ones. Subsequently, information from these selected edges is effectively fused with the central node's own representation using a \\textbf{multi-head attention aggregator} to generate a semantically focused node message. In this manner, our model not only leverages the structure and features of edges within the knowledge graph but also more accurately captures and propagates the contextual information most relevant to the specific link prediction task, thereby effectively mitigating interference from irrelevant information. Extensive experiments demonstrate that our method achieves superior performance compared to existing approaches on several established benchmarks.",
        "arxiv_id": "2506.23141",
        "ARXIVID": "2506.23141",
        "COMMENT": "Does not match any specific criteria but discusses knowledge graph completion with semantic-aware message passing, which is tangentially related to AI but not directly relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22806": {
        "authors": [
            "Byung Hyun Lee",
            "Sungjin Lim",
            "Seunggyu Lee",
            "Dong Un Kang",
            "Se Young Chun"
        ],
        "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate",
        "abstract": "arXiv:2506.22806v1 Announce Type: new  Abstract: Remarkable progress in text-to-image diffusion models has brought a major concern about potentially generating images on inappropriate or trademarked concepts. Concept erasing has been investigated with the goals of deleting target concepts in diffusion models while preserving other concepts with minimal distortion. To achieve these goals, recent concept erasing methods usually fine-tune the cross-attention layers of diffusion models. In this work, we first show that merely updating the cross-attention layers in diffusion models, which is mathematically equivalent to adding \\emph{linear} modules to weights, may not be able to preserve diverse remaining concepts. Then, we propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding \\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or cut) target concepts while safeguarding remaining concepts from broad distributions by employing an attention anchoring loss to prevent the forgetting. Moreover, we adversarially train CPE with ResAG and learnable text embeddings in an iterative manner to maximize erasing performance and enhance robustness against adversarial attacks. Extensive experiments on the erasure of celebrities, artistic styles, and explicit contents demonstrated that the proposed CPE outperforms prior arts by keeping diverse remaining concepts while deleting the target concepts with robustness against attack prompts. Code is available at https://github.com/Hyun1A/CPE",
        "arxiv_id": "2506.22806",
        "ARXIVID": "2506.22806",
        "COMMENT": "Does not match any specific criteria but discusses concept erasure in text-to-image diffusion models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23254": {
        "authors": [
            "Aradhana Mishra",
            "Bumshik Lee"
        ],
        "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution",
        "abstract": "arXiv:2506.23254v1 Announce Type: new  Abstract: Diffusion-model-based image super-resolution techniques often face a trade-off between realistic image generation and computational efficiency. This issue is exacerbated when inference times by decreasing sampling steps, resulting in less realistic and hazy images. To overcome this challenge, we introduce a novel diffusion model named PixelBoost that underscores the significance of embracing the stochastic nature of Brownian motion in advancing image super-resolution, resulting in a high degree of realism, particularly focusing on texture and edge definitions. By integrating controlled stochasticity into the training regimen, our proposed model avoids convergence to local optima, effectively capturing and reproducing the inherent uncertainty of image textures and patterns. Our proposed model demonstrates superior objective results in terms of learned perceptual image patch similarity (LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR), structural similarity index measure (SSIM), as well as visual quality. To determine the edge enhancement, we evaluated the gradient magnitude and pixel value, and our proposed model exhibited a better edge reconstruction capability. Additionally, our model demonstrates adaptive learning capabilities by effectively adjusting to Brownian noise patterns and introduces a sigmoidal noise sequencing method that simplifies training, resulting in faster inference speeds.",
        "arxiv_id": "2506.23254",
        "ARXIVID": "2506.23254",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision due to its focus on image super-resolution using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23074": {
        "authors": [
            "Yu Zheng",
            "Boyang Gong",
            "Fanye Kong",
            "Yueqi Duan",
            "Bingyao Yu",
            "Wenzhao Zheng",
            "Lei Chen",
            "Jiwen Lu",
            "Jie Zhou"
        ],
        "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution",
        "abstract": "arXiv:2506.23074v1 Announce Type: new  Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning (CDAL) method for open-world model attribution. Existing methods rely on handcrafted design of region partitioning or feature space, which could be confounded by the spurious statistical correlations and struggle with novel attacks in open-world scenarios. To address this, CDAL explicitly models the causal relationships between the attentional visual traces and source model attribution, and counterfactually decouples the discriminative model-specific artifacts from confounding source biases for comparison. In this way, the resulting causal effect provides a quantification on the quality of learned attention maps, thus encouraging the network to capture essential generation patterns that generalize to unseen source models by maximizing the effect. Extensive experiments on existing open-world model attribution benchmarks show that with minimal computational overhead, our method consistently improves state-of-the-art models by large margins, particularly for unseen novel attacks. Source code: https://github.com/yzheng97/CDAL.",
        "arxiv_id": "2506.23074",
        "ARXIVID": "2506.23074",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning due to its focus on open-world model attribution.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23881": {
        "authors": [
            "Reihaneh Zohrabi",
            "Hosein Hasani",
            "Mahdieh Soleymani Baghshah",
            "Anna Rohrbach",
            "Marcus Rohrbach",
            "Mohammad Hossein Rohban"
        ],
        "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection",
        "abstract": "arXiv:2506.23881v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications, where they frequently face data distributions unseen during training. Despite progress, existing methods are often vulnerable to spurious correlations that mislead models and compromise robustness. To address this, we propose SPROD, a novel prototype-based OOD detection approach that explicitly addresses the challenge posed by unknown spurious correlations. Our post-hoc method refines class prototypes to mitigate bias from spurious features without additional data or hyperparameter tuning, and is broadly applicable across diverse backbones and OOD detection settings. We conduct a comprehensive spurious correlation OOD detection benchmarking, comparing our method against existing approaches and demonstrating its superior performance across challenging OOD datasets, such as CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3% over the second best.",
        "arxiv_id": "2506.23881",
        "ARXIVID": "2506.23881",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning due to its focus on out-of-distribution detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22836": {
        "authors": [
            "Hongyan An",
            "Kuan Zhu",
            "Xin He",
            "Haiyun Guo",
            "Chaoyang Zhao",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition",
        "abstract": "arXiv:2506.22836v1 Announce Type: new  Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in intelligent transportation and security. To tackle this fine-grained task, most existing methods focus on extracting regional features to enrich attribute information. However, a regional feature is typically used to predict a fixed set of pre-defined attributes in these methods, which limits the performance and practicality in two aspects: 1) Regional features may compromise fine-grained patterns unique to certain attributes in favor of capturing common characteristics shared across attributes. 2) Regional features cannot generalize to predict unseen attributes in the test time. In this paper, we propose the \\textbf{F}ine-grained \\textbf{O}ptimization with semanti\\textbf{C} g\\textbf{U}ided under\\textbf{S}tanding (FOCUS) approach for PAR, which adaptively extracts fine-grained attribute-level features for each attribute individually, regardless of whether the attributes are seen or not during training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to capture latent features at varying levels of visual granularity, thereby enriching the diversity of the extracted information. Next, we introduce the Attribute-guided Visual Feature Extraction (AVFE) module, which leverages textual attributes as queries to retrieve their corresponding visual attribute features from the Mix Tokens using a cross-attention mechanism. To ensure that textual attributes focus on the appropriate Mix Tokens, we further incorporate a Region-Aware Contrastive Learning (RACL) method, encouraging attributes within the same region to share consistent attention maps. Extensive experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness and strong generalization ability of our method.",
        "arxiv_id": "2506.22836",
        "ARXIVID": "2506.22836",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on fine-grained pedestrian attribute recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23227": {
        "authors": [
            "Lunhao Duan",
            "Shanshan Zhao",
            "Xingxing Weng",
            "Jing Zhang",
            "Gui-Song Xia"
        ],
        "title": "High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation",
        "abstract": "arXiv:2506.23227v1 Announce Type: new  Abstract: This paper investigates indoor point cloud semantic segmentation under scene-level annotation, which is less explored compared to methods relying on sparse point-level labels. In the absence of precise point-level labels, current methods first generate point-level pseudo-labels, which are then used to train segmentation models. However, generating accurate pseudo-labels for each point solely based on scene-level annotations poses a considerable challenge, substantially affecting segmentation performance. Consequently, to enhance accuracy, this paper proposes a high-quality pseudo-label generation framework by exploring contemporary multi-modal information and region-point semantic consistency. Specifically, with a cross-modal feature guidance module, our method utilizes 2D-3D correspondences to align point cloud features with corresponding 2D image pixels, thereby assisting point cloud feature learning. To further alleviate the challenge presented by the scene-level annotation, we introduce a region-point semantic consistency module. It produces regional semantics through a region-voting strategy derived from point-level semantics, which are subsequently employed to guide the point-level semantic predictions. Leveraging the aforementioned modules, our method can rectify inaccurate point-level semantic predictions during training and obtain high-quality pseudo-labels. Significant improvements over previous works on ScanNet v2 and S3DIS datasets under scene-level annotation can demonstrate the effectiveness. Additionally, comprehensive ablation studies validate the contributions of our approach's individual components. The code is available at https://github.com/LHDuan/WSegPC .",
        "arxiv_id": "2506.23227",
        "ARXIVID": "2506.23227",
        "COMMENT": "Does not match any specific criterion but focuses on point cloud segmentation with scene-level annotation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23482": {
        "authors": [
            "Jun Huang",
            "Ting Liu",
            "Yihang Wu",
            "Xiaochao Qu",
            "Luoqi Liu",
            "Xiaolin Hu"
        ],
        "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting",
        "abstract": "arXiv:2506.23482v1 Announce Type: new  Abstract: Advancements in generative models have enabled image inpainting models to generate content within specific regions of an image based on provided prompts and masks. However, existing inpainting methods often suffer from problems such as semantic misalignment, structural distortion, and style inconsistency. In this work, we present MTADiffusion, a Mask-Text Alignment diffusion model designed for object inpainting. To enhance the semantic capabilities of the inpainting model, we introduce MTAPipeline, an automatic solution for annotating masks with detailed descriptions. Based on the MTAPipeline, we construct a new MTADataset comprising 5 million images and 25 million mask-text pairs. Furthermore, we propose a multi-task training strategy that integrates both inpainting and edge prediction tasks to improve structural stability. To promote style consistency, we present a novel inpainting style-consistency loss using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations on BrushBench and EditBench demonstrate that MTADiffusion achieves state-of-the-art performance compared to other methods.",
        "arxiv_id": "2506.23482",
        "ARXIVID": "2506.23482",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23897": {
        "authors": [
            "Longliang Liu",
            "Miaojie Feng",
            "Junda Cheng",
            "Jijun Xiang",
            "Xuan Zhu",
            "Xin Yang"
        ],
        "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View",
        "abstract": "arXiv:2506.23897v1 Announce Type: new  Abstract: Panoramic optical flow enables a comprehensive understanding of temporal dynamics across wide fields of view. However, severe distortions caused by sphere-to-plane projections, such as the equirectangular projection (ERP), significantly degrade the performance of conventional perspective-based optical flow methods, especially in polar regions. To address this challenge, we propose PriOr-Flow, a novel dual-branch framework that leverages the low-distortion nature of the orthogonal view to enhance optical flow estimation in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup (DCCL) operator, which jointly retrieves correlation information from both the primitive and orthogonal cost volumes, effectively mitigating distortion noise during cost volume construction. Furthermore, our Ortho-Driven Distortion Compensation (ODDC) module iteratively refines motion features from both branches, further suppressing polar distortions. Extensive experiments demonstrate that PriOr-Flow is compatible with various perspective-based iterative optical flow methods and consistently achieves state-of-the-art performance on publicly available panoramic optical flow datasets, setting a new benchmark for wide-field motion estimation. The code is publicly available at: https://github.com/longliangLiu/PriOr-Flow.",
        "arxiv_id": "2506.23897",
        "ARXIVID": "2506.23897",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23292": {
        "authors": [
            "Changtao Miao",
            "Yi Zhang",
            "Weize Gao",
            "Man Luo",
            "Weiwei Feng",
            "Zhiya Tan",
            "Jianshu Li",
            "Ajian Liu",
            "Yunfeng Diao",
            "Qi Chu",
            "Tao Gong",
            "Zhe Li",
            "Weibin Yao",
            "Joey Tianyi Zhou"
        ],
        "title": "DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios",
        "abstract": "arXiv:2506.23292v1 Announce Type: new  Abstract: Recent advances in AIGC have exacerbated the misuse of malicious deepfake content, making the development of reliable deepfake detection methods an essential means to address this challenge. Although existing deepfake detection models demonstrate outstanding performance in detection metrics, most methods only provide simple binary classification results, lacking interpretability. In critical domains such as law, interpretability is crucial for enhancing the credibility and authority of decisions. Recent studies attempt to improve the interpretability of classification results by providing spatial manipulation masks or temporal forgery segments. However, the practical effectiveness of these methods remains suboptimal due to limitations of the forgery data. Most current deepfake datasets predominantly offer binary labels, only a few datasets with localization annotations. However, they suffer from restricted forgery scenarios, limited diversity in deepfake types, and insufficient data scale, making them inadequate for complex real-world scenarios. To address this predicament, we construct a novel large-scale deepfake detection and localization ($\\textbf{DDL}$) dataset containing over $\\textbf{1.8M}$ forged samples and encompassing up to $\\textbf{75}$ distinct deepfake methods. The DDL design incorporates four key innovations: (1) $\\textbf{Diverse Forgery Scenarios}$, (2) $\\textbf{Comprehensive Deepfake Methods}$, (3) $\\textbf{Varied Manipulation Modes}$, and (4) $\\textbf{Fine-grained Forgery Annotations}$. Through these improvements, our DDL not only provides a more challenging benchmark for complex real-world forgeries, but also offers crucial support for building next-generation deepfake detection, localization, and interpretability methods. The DDL dataset project page is on https://deepfake-workshop-ijcai2025.github.io/main/index.html.",
        "arxiv_id": "2506.23292",
        "ARXIVID": "2506.23292",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23703": {
        "authors": [
            "Lars Ullrich",
            "Walter Zimmer",
            "Ross Greer",
            "Knut Graichen",
            "Alois C. Knoll",
            "Mohan Trivedi"
        ],
        "title": "A New Perspective On AI Safety Through Control Theory Methodologies",
        "abstract": "arXiv:2506.23703v1 Announce Type: new  Abstract: While artificial intelligence (AI) is advancing rapidly and mastering increasingly complex problems with astonishing performance, the safety assurance of such systems is a major concern. Particularly in the context of safety-critical, real-world cyber-physical systems, AI promises to achieve a new level of autonomy but is hampered by a lack of safety assurance. While data-driven control takes up recent developments in AI to improve control systems, control theory in general could be leveraged to improve AI safety. Therefore, this article outlines a new perspective on AI safety based on an interdisciplinary interpretation of the underlying data-generation process and the respective abstraction by AI systems in a system theory-inspired and system analysis-driven manner. In this context, the new perspective, also referred to as data control, aims to stimulate AI engineering to take advantage of existing safety analysis and assurance in an interdisciplinary way to drive the paradigm of data control. Following a top-down approach, a generic foundation for safety analysis and assurance is outlined at an abstract level that can be refined for specific AI systems and applications and is prepared for future innovation.",
        "arxiv_id": "2506.23703",
        "ARXIVID": "2506.23703",
        "COMMENT": "Does not match any specific criteria but discusses AI safety in a general context, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23675": {
        "authors": [
            "Patrick Glandorf",
            "Bodo Rosenhahn"
        ],
        "title": "Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation",
        "abstract": "arXiv:2506.23675v1 Announce Type: new  Abstract: Vision Transformer have set new benchmarks in several tasks, but these models come with the lack of high computational costs which makes them impractical for resource limited hardware. Network pruning reduces the computational complexity by removing less important operations while maintaining performance. However, pruning a model on an unseen data domain, leads to a misevaluation of weight significance, resulting in suboptimal resource assignment. In this work, we find that task-sensitive layers initially fail to improve the feature representation on downstream tasks, leading to performance loss for early pruning decisions. To address this problem, we introduce Pruning by Block Benefit (P3B), a pruning method that utilizes the relative contribution on block level to globally assign parameter resources. P3B identifies low-impact components to reduce parameter allocation while preserving critical ones. Classical pruning mask optimization struggles to reactivate zero-mask-elements. In contrast, P3B sets a layerwise keep ratio based on global performance metrics, ensuring the reactivation of late-converging blocks. We show in extensive experiments that P3B is a state of the art pruning method with most noticeable gains in transfer learning tasks. Notably, P3B is able to conserve high performance, even in high sparsity regimes of 70% parameter reduction while only losing 0.64% in accuracy.",
        "arxiv_id": "2506.23675",
        "ARXIVID": "2506.23675",
        "COMMENT": "Does not match any specific criterion but is relevant to vision transformers and domain adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22736": {
        "authors": [
            "Dayong Su",
            "Yafei Zhang",
            "Huafeng Li",
            "Jinxing Li",
            "Yu Liu"
        ],
        "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments",
        "abstract": "arXiv:2506.22736v1 Announce Type: new  Abstract: Current multimodal medical image fusion typically assumes that source images are of high quality and perfectly aligned at the pixel level. Its effectiveness heavily relies on these conditions and often deteriorates when handling misaligned or degraded medical images. To address this, we propose UniFuse, a general fusion framework. By embedding a degradation-aware prompt learning module, UniFuse seamlessly integrates multi-directional information from input images and correlates cross-modal alignment with restoration, enabling joint optimization of both tasks within a unified framework. Additionally, we design an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to encode multi-directional features and mitigate modality differences in feature alignment. To enable simultaneous restoration and fusion within an All-in-One configuration, we propose a Universal Feature Restoration & Fusion module, incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA principles. By leveraging ALSN's adaptive feature representation along with degradation-type guidance, we enable joint restoration and fusion within a single-stage framework. Compared to staged approaches, UniFuse unifies alignment, restoration, and fusion within a single framework. Experimental results across multiple datasets demonstrate the method's effectiveness and significant advantages over existing approaches.",
        "arxiv_id": "2506.22736",
        "ARXIVID": "2506.22736",
        "COMMENT": "Does not match any specific criterion but is relevant to multimodal learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23529": {
        "authors": [
            "Jisu Han",
            "Jihee Park",
            "Dongyoon Han",
            "Wonjun Hwang"
        ],
        "title": "When Test-Time Adaptation Meets Self-Supervised Models",
        "abstract": "arXiv:2506.23529v1 Announce Type: new  Abstract: Training on test-time data enables deep learning models to adapt to dynamic environmental changes, enhancing their practical applicability. Online adaptation from source to target domains is promising but it remains highly reliant on the performance of source pretrained model. In this paper, we investigate whether test-time adaptation (TTA) methods can continuously improve models trained via self-supervised learning (SSL) without relying on source pretraining. We introduce a self-supervised TTA protocol after observing that existing TTA approaches struggle when directly applied to self-supervised models with low accuracy on the source domain. Furthermore, we propose a collaborative learning framework that integrates SSL and TTA models, leveraging contrastive learning and knowledge distillation for stepwise representation refinement. We validate our method on diverse self-supervised models, including DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the effectiveness of our approach in SSL, showing that it achieves competitive performance even without source pretraining.",
        "arxiv_id": "2506.23529",
        "ARXIVID": "2506.23529",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning and adaptation techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22920": {
        "authors": [
            "Pinzheng Wang",
            "Juntao Li",
            "Zecheng Tang",
            "Haijia Gui",
            "Min zhang"
        ],
        "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game",
        "abstract": "arXiv:2506.22920v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.",
        "arxiv_id": "2506.22920",
        "ARXIVID": "2506.22920",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reasoning in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23271": {
        "authors": [
            "Jinxing Zhou",
            "Zhihui Li",
            "Yongqiang Yu",
            "Yanghao Zhou",
            "Ruohao Guo",
            "Guangyao Li",
            "Yuxin Mao",
            "Mingfei Han",
            "Xiaojun Chang",
            "Meng Wang"
        ],
        "title": "Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation",
        "abstract": "arXiv:2506.23271v1 Announce Type: new  Abstract: We present \\textbf{Met}a-\\textbf{T}oken \\textbf{Le}arning (Mettle), a simple and memory-efficient method for adapting large-scale pretrained transformer models to downstream audio-visual tasks. Instead of sequentially modifying the output feature distribution of the transformer backbone, Mettle utilizes a lightweight \\textit{Layer-Centric Distillation (LCD)} module to distill in parallel the intact audio or visual features embedded by each transformer layer into compact meta-tokens. This distillation process considers both pretrained knowledge preservation and task-specific adaptation. The obtained meta-tokens can be directly applied to classification tasks, such as audio-visual event localization and audio-visual video parsing. To further support fine-grained segmentation tasks, such as audio-visual segmentation, we introduce a \\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual meta-tokens distilled from the top transformer layer to guide feature adaptation in earlier layers. Extensive experiments on multiple audiovisual benchmarks demonstrate that our method significantly reduces memory usage and training time while maintaining parameter efficiency and competitive accuracy.",
        "arxiv_id": "2506.23271",
        "ARXIVID": "2506.23271",
        "COMMENT": "Does not match any specific criteria. Focuses on memory-efficient adaptation for audio-visual tasks, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.23674": {
        "authors": [
            "Dongyue Wu",
            "Zilin Guo",
            "Jialong Zuo",
            "Nong Sang",
            "Changxin Gao"
        ],
        "title": "Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration",
        "abstract": "arXiv:2506.23674v1 Announce Type: new  Abstract: The ever-growing size of training datasets enhances the generalization capability of modern machine learning models but also incurs exorbitant computational costs. Existing data pruning approaches aim to accelerate training by removing those less important samples. However, they often rely on gradients or proxy models, leading to prohibitive additional costs of gradient back-propagation and proxy model training. In this paper, we propose Partial Forward Blocking (PFB), a novel framework for lossless training acceleration. The efficiency of PFB stems from its unique adaptive pruning pipeline: sample importance is assessed based on features extracted from the shallow layers of the target model. Less important samples are then pruned, allowing only the retained ones to proceed with the subsequent forward pass and loss back-propagation. This mechanism significantly reduces the computational overhead of deep-layer forward passes and back-propagation for pruned samples, while also eliminating the need for auxiliary backward computations and proxy model training. Moreover, PFB introduces probability density as an indicator of sample importance. Combined with an adaptive distribution estimation module, our method dynamically prioritizes relatively rare samples, aligning with the constantly evolving training state. Extensive experiments demonstrate the significant superiority of PFB in performance and speed. On ImageNet, PFB achieves a 0.5% accuracy improvement and 33% training time reduction with 40% data pruned.",
        "arxiv_id": "2506.23674",
        "ARXIVID": "2506.23674",
        "COMMENT": "Does not match any specific criteria. Focuses on data pruning for training acceleration, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22726": {
        "authors": [
            "Yu Zhang",
            "Xi Zhang",
            "Hualin zhou",
            "Xinyuan Chen",
            "Shang Gao",
            "Hong Jia",
            "Jianfei Yang",
            "Yuankai Qi",
            "Tao Gu"
        ],
        "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge",
        "abstract": "arXiv:2506.22726v1 Announce Type: new  Abstract: Deep learning for human sensing on edge systems offers significant opportunities for smart applications. However, its training and development are hindered by the limited availability of sensor data and resource constraints of edge systems. Current methods that rely on transferring pre-trained models often encounter issues such as modality shift and high resource demands, resulting in substantial accuracy loss, resource overhead, and poor adaptability across different sensing applications. In this paper, we propose XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic model transfer. XTransfer freely leverages single or multiple pre-trained models and transfers knowledge across different modalities by (i) model repairing that safely repairs modality shift in pre-trained model layers with only few sensor data, and (ii) layer recombining that efficiently searches and recombines layers of interest from source models in a layer-wise manner to create compact models. We benchmark various baselines across diverse human sensing datasets spanning different modalities. Comprehensive results demonstrate that XTransfer achieves state-of-the-art performance on human sensing tasks while significantly reducing the costs of sensor data collection, model training, and edge deployment.",
        "arxiv_id": "2506.22726",
        "ARXIVID": "2506.22726",
        "COMMENT": "Does not closely match any specific criteria but is related to cross-modality model transfer for human sensing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.24123": {
        "authors": [
            "Yue Ma",
            "Qingyan Bai",
            "Hao Ouyang",
            "Ka Leong Cheng",
            "Qiuyu Wang",
            "Hongyu Liu",
            "Zichen Liu",
            "Haofan Wang",
            "Jingye Chen",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "title": "Calligrapher: Freestyle Text Image Customization",
        "abstract": "arXiv:2506.24123v1 Announce Type: new  Abstract: We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.",
        "arxiv_id": "2506.24123",
        "ARXIVID": "2506.24123",
        "COMMENT": "Does not closely match any specific criteria but is related to generative modeling in typography and design.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.22774": {
        "authors": [
            "Michael Papademas",
            "Xenia Ziouvelou",
            "Antonis Troumpoukis",
            "Vangelis Karkaletsis"
        ],
        "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems",
        "abstract": "arXiv:2506.22774v1 Announce Type: new  Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges posed by human-made artifacts, particularly those widely integrated into society and exert significant influence, highlighting potential benefits and their negative consequences. While other technologies may also pose substantial risks, AI's pervasive reach makes its societal effects especially profound. The complexity of AI systems, coupled with their remarkable capabilities, can lead to a reliance on technologies that operate beyond direct human oversight or understanding. To mitigate the risks that arise, several theoretical tools and guidelines have been developed, alongside efforts to create technological tools aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view of the issue but fail to provide techniques for quantifying trustworthiness. Conversely, while technological tools are better at achieving such quantification, they lack a holistic perspective, focusing instead on specific aspects of Trustworthy AI. This paper aims to introduce an assessment method that combines the ethical components of Trustworthy AI with the algorithmic processes of PageRank and TrustRank. The goal is to establish an assessment framework that minimizes the subjectivity inherent in the self-assessment techniques prevalent in the field by introducing algorithmic criteria. The application of our approach indicates that a holistic assessment of an AI system's trustworthiness can be achieved by providing quantitative insights while considering the theoretical content of relevant guidelines.",
        "arxiv_id": "2506.22774",
        "ARXIVID": "2506.22774",
        "COMMENT": "Does not match any specific criteria but discusses trustworthiness in AI systems, which is tangentially related to your friend's general interest in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.22513": {
        "authors": [
            "Aditya Sharma"
        ],
        "title": "Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence",
        "abstract": "arXiv:2506.22513v1 Announce Type: new  Abstract: This investigation attempts to create an automated framework for fault detection and organization for usage in contemporary radiography, as per NDE 4.0. The review's goals are to address the lack of information that is sufficiently explained, learn how to make the most of virtual defect increase, and determine whether the framework is viable by using NDE measurements. As its basic information source, the technique consists of compiling and categorizing 223 CR photographs of airplane welds. Information expansion systems, such as virtual defect increase and standard increase, are used to work on the preparation dataset. A modified U-net model is prepared using the improved data to produce semantic fault division veils. To assess the effectiveness of the model, NDE boundaries such as Case, estimating exactness, and misleading call rate are used. Tiny a90/95 characteristics, which provide strong differentiating evidence of flaws, reveal that the suggested approach achieves exceptional awareness in defect detection. Considering a 90/95, size error, and fake call rate in the weld area, the consolidated expansion approach clearly wins. Due to the framework's fast derivation speed, large images can be broken down efficiently and quickly. Professional controllers evaluate the transmitted system in the field and believe that it has a guarantee as a support device in the testing cycle, irrespective of particular equipment cut-off points and programming resemblance.",
        "arxiv_id": "2506.22513",
        "ARXIVID": "2506.22513",
        "COMMENT": "Does not match any specific criteria but is tangentially related to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23106": {
        "authors": [
            "Ryo Ishiyama",
            "Shinnosuke Matsuo",
            "Seiichi Uchida"
        ],
        "title": "Computer-Aided Multi-Stroke Character Simplification by Stroke Removal",
        "abstract": "arXiv:2506.23106v1 Announce Type: new  Abstract: Multi-stroke characters in scripts such as Chinese and Japanese can be highly complex, posing significant challenges for both native speakers and, especially, non-native learners. If these characters can be simplified without degrading their legibility, it could reduce learning barriers for non-native speakers, facilitate simpler and legible font designs, and contribute to efficient character-based communication systems. In this paper, we propose a framework to systematically simplify multi-stroke characters by selectively removing strokes while preserving their overall legibility. More specifically, we use a highly accurate character recognition model to assess legibility and remove those strokes that minimally impact it. Experimental results on 1,256 character classes with 5, 10, 15, and 20 strokes reveal several key findings, including the observation that even after removing multiple strokes, many characters remain distinguishable. These findings suggest the potential for more formalized simplification strategies.",
        "arxiv_id": "2506.23106",
        "ARXIVID": "2506.23106",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and character simplification.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23209": {
        "authors": [
            "Chia-Wen Huang",
            "Haw Hwai",
            "Chien-Chang Lee",
            "Pei-Yuan Wu"
        ],
        "title": "A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans",
        "abstract": "arXiv:2506.23209v1 Announce Type: new  Abstract: Timely and accurate diagnosis of appendicitis is critical in clinical settings to prevent serious complications. While CT imaging remains the standard diagnostic tool, the growing number of cases can overwhelm radiologists, potentially causing delays. In this paper, we propose a deep learning model that leverages 3D CT scans for appendicitis classification, incorporating Slice Attention mechanisms guided by external 2D datasets to enhance small lesion detection. Additionally, we introduce a hierarchical classification framework using pre-trained 2D models to differentiate between simple and complicated appendicitis. Our approach improves AUC by 3% for appendicitis and 5.9% for complicated appendicitis, offering a more efficient and reliable diagnostic solution compared to previous work.",
        "arxiv_id": "2506.23209",
        "ARXIVID": "2506.23209",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23470": {
        "authors": [
            "Ngoc-Do Tran",
            "Minh-Tuan Huynh",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "Interactive Interface For Semantic Segmentation Dataset Synthesis",
        "abstract": "arXiv:2506.23470v1 Announce Type: new  Abstract: The rapid advancement of AI and computer vision has significantly increased the demand for high-quality annotated datasets, particularly for semantic segmentation. However, creating such datasets is resource-intensive, requiring substantial time, labor, and financial investment, and often raises privacy concerns due to the use of real-world data. To mitigate these challenges, we present SynthLab, consisting of a modular platform for visual data synthesis and a user-friendly interface. The modular architecture of SynthLab enables easy maintenance, scalability with centralized updates, and seamless integration of new features. Each module handles distinct aspects of computer vision tasks, enhancing flexibility and adaptability. Meanwhile, its interactive, user-friendly interface allows users to quickly customize their data pipelines through drag-and-drop actions. Extensive user studies involving a diverse range of users across different ages, professions, and expertise levels, have demonstrated flexible usage, and high accessibility of SynthLab, enabling users without deep technical expertise to harness AI for real-world applications.",
        "arxiv_id": "2506.23470",
        "ARXIVID": "2506.23470",
        "COMMENT": "Does not match any specific criterion but is generally relevant to dataset synthesis for computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.23247": {
        "authors": [
            "James Hinns",
            "David Martens"
        ],
        "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification",
        "abstract": "arXiv:2506.23247v1 Announce Type: new  Abstract: Deep learning dominates image classification tasks, yet understanding how models arrive at predictions remains a challenge. Much research focuses on local explanations of individual predictions, such as saliency maps, which visualise the influence of specific pixels on a model's prediction. However, reviewing many of these explanations to identify recurring patterns is infeasible, while global methods often oversimplify and miss important local behaviours. To address this, we propose Segment Attribution Tables (SATs), a method for summarising local saliency explanations into (semi-)global insights. SATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency maps to quantify their influence. These segments highlight concepts the model relies on across instances and reveal spurious correlations, such as reliance on backgrounds or watermarks, even when out-of-distribution test performance sees little change. SATs can explain any classifier for which a form of saliency map can be produced, using segmentation maps that provide named segments. SATs bridge the gap between oversimplified global summaries and overly detailed local explanations, offering a practical tool for analysing and debugging image classifiers.",
        "arxiv_id": "2506.23247",
        "ARXIVID": "2506.23247",
        "COMMENT": "Does not match any specific criterion but is generally relevant to explainability in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}