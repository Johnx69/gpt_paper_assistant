{
    "2506.18472": {
        "authors": [
            "Gengyuan Zhang",
            "Tanveer Hannan",
            "Hermine Kleiner",
            "Beste Aydemir",
            "Xinyu Xie",
            "Jian Lan",
            "Thomas Seidl",
            "Volker Tresp",
            "Jindong Gu"
        ],
        "title": "AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction",
        "abstract": "arXiv:2506.18472v1 Announce Type: new  Abstract: An ideal vision-language agent serves as a bridge between the human users and their surrounding physical world in real-world applications like autonomous driving and embodied agents, and proactively provides accurate and timely responses given user intents. An intriguing challenge arises when agents interact with the world as a dynamic data stream and ad-hoc queries from users: supporting knowledge for queries, namely evidence, usually appears asynchronously with the arrival time of queries, and agents need to ground their responses in historical data, present observations, and even future streams. We frame this challenge as Query-Evidence Asynchrony, where user queries and their supporting evidence typically arrive asynchronously in the streaming setting. This setting requires not only strong reasoning capabilities but also the ability to retain past observations and respond to queries with temporal awareness. In this paper, we introduce a diagnostic benchmark that evaluates Multimodal Large Language Models (MLLMs) on their ability to handle interaction with streaming data. Further, we present AViLA, Asynchronous Video-Language Agent for streaming data interaction that can handle ad-hoc queries and give time-aware responses. For this purpose, AViLA consists of three key modules: comprehensive memory retention, evidence identification, and evidence-grounded trigger, that are designed to maintain a general-purpose memory and respond readily and timely to queries. Our experiments show that existing models often fail to respond at appropriate times, while AViLA significantly improves both accuracy and temporal awareness. Our code and dataset will be publicly available.",
        "arxiv_id": "2506.18472",
        "ARXIVID": "2506.18472",
        "COMMENT": "Matches criteria 2 and 3 closely. The paper introduces a novel Multimodal Large Language Model (MLLM) for streaming data interaction and proposes a new benchmark for evaluating temporal awareness in such models.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.18204": {
        "authors": [
            "Youjie Zhou",
            "Guofeng Mei",
            "Yiming Wang",
            "Yi Wan",
            "Fabio Poiesi"
        ],
        "title": "Multimodal Fusion SLAM with Fourier Attention",
        "abstract": "arXiv:2506.18204v1 Announce Type: new  Abstract: Visual SLAM is particularly challenging in environments affected by noise, varying lighting conditions, and darkness. Learning-based optical flow algorithms can leverage multiple modalities to address these challenges, but traditional optical flow-based visual SLAM approaches often require significant computational resources.To overcome this limitation, we propose FMF-SLAM, an efficient multimodal fusion SLAM method that utilizes fast Fourier transform (FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel Fourier-based self-attention and cross-attention mechanism to extract features from RGB and depth signals. We further enhance the interaction of multimodal features by incorporating multi-scale knowledge distillation across modalities. We also demonstrate the practical feasibility of FMF-SLAM in real-world scenarios with real time performance by integrating it with a security robot by fusing with a global positioning module GNSS-RTK and global Bundle Adjustment. Our approach is validated using video sequences from TUM, TartanAir, and our real-world datasets, showcasing state-of-the-art performance under noisy, varying lighting, and dark conditions.Our code and datasets are available at https://github.com/youjie-zhou/FMF-SLAM.git.",
        "arxiv_id": "2506.18204",
        "ARXIVID": "2506.18204",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel multimodal SLAM method with Fourier attention and validates it in real-world scenarios.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.17705": {
        "authors": [
            "Bo Pan",
            "Yang Chen",
            "Yingwei Pan",
            "Ting Yao",
            "Wei Chen",
            "Tao Mei"
        ],
        "title": "DreamJourney: Perpetual View Generation with Video Diffusion Models",
        "abstract": "arXiv:2506.17705v1 Announce Type: new  Abstract: Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: https://dream-journey.vercel.app.",
        "arxiv_id": "2506.17705",
        "ARXIVID": "2506.17705",
        "COMMENT": "Matches criterion 6 as it focuses on perpetual view generation using video diffusion models, which is a novel video understanding task.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.18899": {
        "authors": [
            "Kaiyi Huang",
            "Yukun Huang",
            "Xintao Wang",
            "Zinan Lin",
            "Xuefei Ning",
            "Pengfei Wan",
            "Di Zhang",
            "Yu Wang",
            "Xihui Liu"
        ],
        "title": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation",
        "abstract": "arXiv:2506.18899v1 Announce Type: new  Abstract: AI-driven content creation has shown potential in film production. However, existing film generation systems struggle to implement cinematic principles and thus fail to generate professional-quality films, particularly lacking diverse camera language and cinematic rhythm. This results in templated visuals and unengaging narratives. To address this, we introduce FilMaster, an end-to-end AI system that integrates real-world cinematic principles for professional-grade film generation, yielding editable, industry-standard outputs. FilMaster is built on two key principles: (1) learning cinematography from extensive real-world film data and (2) emulating professional, audience-centric post-production workflows. Inspired by these principles, FilMaster incorporates two stages: a Reference-Guided Generation Stage which transforms user input to video clips, and a Generative Post-Production Stage which transforms raw footage into audiovisual outputs by orchestrating visual and auditory elements for cinematic rhythm. Our generation stage highlights a Multi-shot Synergized RAG Camera Language Design module to guide the AI in generating professional camera language by retrieving reference clips from a vast corpus of 440,000 film clips. Our post-production stage emulates professional workflows by designing an Audience-Centric Cinematic Rhythm Control module, including Rough Cut and Fine Cut processes informed by simulated audience feedback, for effective integration of audiovisual elements to achieve engaging content. The system is empowered by generative AI models like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a comprehensive benchmark for evaluating AI-generated films. Extensive experiments show FilMaster's superior performance in camera language design and cinematic rhythm control, advancing generative AI in professional filmmaking.",
        "arxiv_id": "2506.18899",
        "ARXIVID": "2506.18899",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) and criterion 6 (Video Understanding) due to its focus on generative AI for film production and integration of audiovisual elements.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.18246": {
        "authors": [
            "Xiangzhao Hao",
            "Kuan Zhu",
            "Hongyu Guo",
            "Haiyun Guo",
            "Ming Tang",
            "JinQiao Wang"
        ],
        "title": "Referring Expression Instance Retrieval and A Strong End-to-End Baseline",
        "abstract": "arXiv:2506.18246v1 Announce Type: new  Abstract: Natural language querying of visual content underpins many vision-language tasks, typically categorized by text granularity and visual search scope. Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions, while Referring Expression Comprehension (REC) localizes objects using fine-grained expressions within a single image. However, real-world scenarios often require both instance-level retrieval and localization across large galleries -- tasks where TIR lacks precision and REC lacks scalability. To address this gap, we propose a new task: Referring Expression Instance Retrieval (REIR), which jointly supports instance-level retrieval and localization. We introduce REIRCOCO, a large-scale benchmark constructed by prompting vision-language models to generate fine-grained expressions for MSCOCO and RefCOCO instances. We also present a baseline method, CLARE, featuring a dual-stream architecture with a Mix of Relation Experts (MORE) module for capturing inter-instance relationships. CLARE integrates object detection and REC pretraining with Contrastive Language-Instance Alignment (CLIA) for end-to-end optimization. Experiments show that CLARE achieves state-of-the-art performance on REIR and generalizes well to TIR and REC, highlighting its effectiveness and versatility.",
        "arxiv_id": "2506.18246",
        "ARXIVID": "2506.18246",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a new task combining vision and language with a novel architecture.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.18898": {
        "authors": [
            "Jiaming Han",
            "Hao Chen",
            "Yang Zhao",
            "Hanyu Wang",
            "Qi Zhao",
            "Ziyan Yang",
            "Hao He",
            "Xiangyu Yue",
            "Lu Jiang"
        ],
        "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
        "abstract": "arXiv:2506.18898v1 Announce Type: new  Abstract: This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com",
        "arxiv_id": "2506.18898",
        "ARXIVID": "2506.18898",
        "COMMENT": "Matches criterion 2 and 5 as it explores a multimodal framework unifying visual understanding and generation via text-aligned representations, which is directly relevant to vision-language integration.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.18084": {
        "authors": [
            "Wenzhuo Liu",
            "Yicheng Qiao",
            "Zhen Wang",
            "Qiannan Guo",
            "Zilong Chen",
            "Meihua Zhou",
            "Xinran Li",
            "Letian Wang",
            "Zhiwei Li",
            "Huaping Liu",
            "Wenshuo Wang"
        ],
        "title": "TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving",
        "abstract": "arXiv:2506.18084v1 Announce Type: new  Abstract: Multi-task learning (MTL) can advance assistive driving by exploring inter-task correlations through shared representations. However, existing methods face two critical limitations: single-modality constraints limiting comprehensive scene understanding and inefficient architectures impeding real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient Multimodal Multi-task Learning), a novel framework that jointly optimizes driver emotion recognition, driver behavior recognition, traffic context recognition, and vehicle behavior recognition through a two-stage architecture. The first component, the mamba-based multi-view temporal-spatial feature extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal scanning mechanism and global-local spatial attention to efficiently extract low-cost temporal-spatial features from multi-view sequential images. The second component, the MTL-based gated multimodal feature integrator (MGMI), employs task-specific multi-gating modules to adaptively highlight the most relevant modality features for each task, effectively alleviating the negative transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model achieves state-of-the-art accuracy across all four tasks, maintaining a lightweight architecture with fewer than 6 million parameters and delivering an impressive 142.32 FPS inference speed. Rigorous ablation studies further validate the effectiveness of the proposed framework and the independent contributions of each module. The code is available on https://github.com/Wenzhuo-Liu/TEM3-Learning.",
        "arxiv_id": "2506.18084",
        "ARXIVID": "2506.18084",
        "COMMENT": "Matches criteria 3 closely. The paper introduces a novel multimodal multi-task learning framework for assistive driving, addressing challenges in real-time deployment and task-specific feature integration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.18839": {
        "authors": [
            "Chaoyang Wang",
            "Ashkan Mirzaei",
            "Vidit Goel",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Avalon Vinella",
            "Michael Vasilkovsky",
            "Ivan Skorokhodov",
            "Vladislav Shakhrai",
            "Sergey Korolev",
            "Sergey Tulyakov",
            "Peter Wonka"
        ],
        "title": "4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation",
        "abstract": "arXiv:2506.18839v1 Announce Type: new  Abstract: We propose the first framework capable of computing a 4D spatio-temporal grid of video frames and 3D Gaussian particles for each time step using a feed-forward architecture. Our architecture has two main components, a 4D video model and a 4D reconstruction model. In the first part, we analyze current 4D video diffusion architectures that perform spatial and temporal attention either sequentially or in parallel within a two-stream design. We highlight the limitations of existing approaches and introduce a novel fused architecture that performs spatial and temporal attention within a single layer. The key to our method is a sparse attention pattern, where tokens attend to others in the same frame, at the same timestamp, or from the same viewpoint. In the second part, we extend existing 3D reconstruction algorithms by introducing a Gaussian head, a camera token replacement algorithm, and additional dynamic layers and training. Overall, we establish a new state of the art for 4D generation, improving both visual quality and reconstruction capability.",
        "arxiv_id": "2506.18839",
        "ARXIVID": "2506.18839",
        "COMMENT": "Matches criterion 6 as it focuses on 4D video understanding and generation, which aligns with video-based tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2506.18385": {
        "authors": [
            "Nianchen Deng",
            "Lixin Gu",
            "Shenglong Ye",
            "Yinan He",
            "Zhe Chen",
            "Songze Li",
            "Haomin Wang",
            "Xingguang Wei",
            "Tianshuo Yang",
            "Min Dou",
            "Tong He",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Yi Wang",
            "Botian Shi",
            "Yanting Zhang",
            "Jifeng Dai",
            "Yu Qiao",
            "Hongjie Zhang",
            "Wenhai Wang"
        ],
        "title": "InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models",
        "abstract": "arXiv:2506.18385v1 Announce Type: new  Abstract: Recent benchmarks and datasets have been proposed to improve spatial reasoning in vision-language models (VLMs), yet existing open resources remain limited in scale, visual diversity, and instruction expressiveness. In this work, we introduce InternSpatial, the largest open-source dataset for spatial reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation benchmark designed to assess spatial understanding under diverse instruction formats. InternSpatial comprises 12 million QA pairs spanning both single-view and multi-view settings, drawn from diverse visual environments and supporting 19 instruction formats that reflect varied query styles. For evaluation, we propose InternSpatial-Bench for single-view tasks and expand multi-view reasoning by introducing a novel rotation angle prediction task that has not been explored in prior work. Experimental results show that models trained on InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on VSI-Bench, while maintaining strong performance on general-purpose benchmarks. We hope these resources will support the development of spatially capable VLMs in practical applications such as robotics and embodied AI.",
        "arxiv_id": "2506.18385",
        "ARXIVID": "2506.18385",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a new dataset and benchmark for spatial reasoning in vision-language models, which is relevant for embodied agents and spatial intelligence.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.17545": {
        "authors": [
            "Zhihao Yuan",
            "Shuyi Jiang",
            "Chun-Mei Feng",
            "Yaolun Zhang",
            "Shuguang Cui",
            "Zhen Li",
            "Na Zhao"
        ],
        "title": "Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations",
        "abstract": "arXiv:2506.17545v1 Announce Type: new  Abstract: Currently, utilizing large language models to understand the 3D world is becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output bounding boxes or textual answers without revealing how those decisions are made, and they still rely on pre-trained 3D detectors to supply object proposals. We introduce Scene-R1, a video-grounded framework that learns to reason about 3D scenes without any point-wise 3D instance supervision by pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline. In the temporal grounding stage, we explicitly reason about the video and select the video snippets most relevant to an open-ended query. In the subsequent image grounding stage, we analyze the image and predict the 2D bounding box. After that, we track the object using SAM2 to produce pixel-accurate masks in RGB frames, and project them back into 3D, thereby eliminating the need for 3D detector-based proposals while capturing fine geometry and material cues. Scene-R1 can also adapt to the 3D visual question answering task to answer free-form questions directly from video. Our training pipeline only needs task-level 2D boxes or textual labels without dense 3D point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on multiple datasets, while delivering transparent, step-by-step rationales. These results show that reinforcement-learning-based reasoning combined with RGB-D video alone offers a practical, annotation-efficient route to trustworthy 3D scene understanding.",
        "arxiv_id": "2506.17545",
        "ARXIVID": "2506.17545",
        "COMMENT": "Matches criterion 2 and 5 as it discusses video-grounded large language models for 3D scene reasoning, integrating video understanding and language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.17457": {
        "authors": [
            "Dong Xiao",
            "Guangyao Chen",
            "Peixi Peng",
            "Yangru Huang",
            "Yifan Zhao",
            "Yongxing Dai",
            "Yonghong Tian"
        ],
        "title": "When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network",
        "abstract": "arXiv:2506.17457v1 Announce Type: new  Abstract: Anomaly detection is essential for the safety and reliability of autonomous driving systems. Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios. In this paper, we introduce real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy. We propose a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images. This combination effectively captures both the temporal dynamics and spatial details of the driving environment, enabling swift and precise anomaly detection. Extensive experiments on benchmark datasets show that our approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance.",
        "arxiv_id": "2506.17457",
        "ARXIVID": "2506.17457",
        "COMMENT": "Matches criteria 3 closely. The paper proposes a novel multimodal asynchronous hybrid network for real-time anomaly detection in autonomous driving, which is relevant to embodied/robotic AI methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18901": {
        "authors": [
            "Wenqiang Sun",
            "Fangyun Wei",
            "Jinjing Zhao",
            "Xi Chen",
            "Zilong Chen",
            "Hongyang Zhang",
            "Jun Zhang",
            "Yan Lu"
        ],
        "title": "From Virtual Games to Real-World Play",
        "abstract": "arXiv:2506.18901v1 Announce Type: new  Abstract: We introduce RealPlay, a neural network-based real-world game engine that enables interactive video generation from user control signals. Unlike prior works focused on game-style visuals, RealPlay aims to produce photorealistic, temporally consistent video sequences that resemble real-world footage. It operates in an interactive loop: users observe a generated scene, issue a control command, and receive a short video chunk in response. To enable such realistic and responsive generation, we address key challenges including iterative chunk-wise prediction for low-latency feedback, temporal consistency across iterations, and accurate control response. RealPlay is trained on a combination of labeled game data and unlabeled real-world videos, without requiring real-world action annotations. Notably, we observe two forms of generalization: (1) control transfer-RealPlay effectively maps control signals from virtual to real-world scenarios; and (2) entity transfer-although training labels originate solely from a car racing game, RealPlay generalizes to control diverse real-world entities, including bicycles and pedestrians, beyond vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/",
        "arxiv_id": "2506.18901",
        "ARXIVID": "2506.18901",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a neural network-based engine for interactive video generation with control signals.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18322": {
        "authors": [
            "Yiwei Yang",
            "Chung Peng Lee",
            "Shangbin Feng",
            "Dora Zhao",
            "Bingbing Wen",
            "Anthony Z. Liu",
            "Yulia Tsvetkov",
            "Bill Howe"
        ],
        "title": "Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?",
        "abstract": "arXiv:2506.18322v1 Announce Type: new  Abstract: Finetuning can cause spurious correlations to arise between non-essential features and the target labels, but benchmarks to study these effects involve contrived settings and narrow tasks. In contrast, we consider spurious correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on extensive and diverse datasets without explicit task supervision. We develop a benchmark by sourcing GPT-4o errors on real-world visual-question-answering (VQA) benchmarks, then curating a subset through LVLM-human annotation and synthetic counterfactual evaluation to identify errors caused by spurious correlations. This process yields SpuriVerse, a novel benchmark comprised of 124 distinct types of spurious correlations extracted from real-world datasets, each containing 1 realistic and 10 synthetic VQA samples for a total of 1364 multiple choice questions. We evaluate 15 open and closed-source LVLMs on SpuriVerse, finding that even state-of-the-art closed-source models struggle significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic examples that emphasize the spurious correlation improves performance to 78.40%, suggesting that training on diverse spurious patterns generalizes to unseen situations: models appear to learn to avoid \"shortcuts\" and attend to the overall image context.",
        "arxiv_id": "2506.18322",
        "ARXIVID": "2506.18322",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores spurious correlations in multi-modal LVLMs and introduces a novel benchmark for evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18493": {
        "authors": [
            "Trong-Vu Hoang",
            "Quang-Binh Nguyen",
            "Thanh-Toan Do",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation",
        "abstract": "arXiv:2506.18493v1 Announce Type: new  Abstract: Customizing image generation remains a core challenge in controllable image synthesis. For single-concept generation, maintaining both identity preservation and prompt alignment is challenging. In multi-concept scenarios, relying solely on a prompt without additional conditions like layout boxes or semantic masks, often leads to identity loss and concept omission. In this paper, we introduce ShowFlow, a comprehensive framework designed to tackle these challenges. We propose ShowFlow-S for single-concept image generation, and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a KronA-WED adapter, which integrates a Kronecker adapter with weight and embedding decomposition, and employs a disentangled learning approach with a novel attention regularization objective to enhance single-concept generation. Building on this foundation, ShowFlow-M directly reuses the learned models from ShowFlow-S to support multi-concept generation without extra conditions, incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout consistency strategy as the plug-and-play module. Extensive experiments and user studies validate ShowFlow's effectiveness, highlighting its potential in real-world applications like advertising and virtual dressing.",
        "arxiv_id": "2506.18493",
        "ARXIVID": "2506.18493",
        "COMMENT": "Matches criterion 5 as it focuses on multi-concept image generation and integrates vision and language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18904": {
        "authors": [
            "Yang Liu",
            "Chuanchen Luo",
            "Zimo Tang",
            "Yingyan Li",
            "Yuran Yang",
            "Yuanyong Ning",
            "Lue Fan",
            "Junran Peng",
            "Zhaoxiang Zhang"
        ],
        "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos",
        "abstract": "arXiv:2506.18904v1 Announce Type: new  Abstract: Editing illumination in long videos with complex dynamics has significant value in various downstream tasks, including visual content creation and manipulation, as well as data scaling up for embodied AI through sim2real and real2real transfer. Nevertheless, existing video relighting techniques are predominantly limited to portrait videos or fall into the bottleneck of temporal consistency and computation efficiency. In this paper, we propose TC-Light, a novel paradigm characterized by the proposed two-stage post optimization mechanism. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible relighting results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.",
        "arxiv_id": "2506.18904",
        "ARXIVID": "2506.18904",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for dynamic video relighting and provides a novel method for temporal consistency in video editing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18158": {
        "authors": [
            "Xinzge Gao",
            "Chuanrui Hu",
            "Bin Chen",
            "Teng Li"
        ],
        "title": "Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation",
        "abstract": "arXiv:2506.18158v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are attracting growing attention in the development of Graphical User Interface (GUI) agents. Existing approaches often rely on historical screenshots or actions to implicitly represent the task state. This reliance poses challenges for GUI agents in accurately understanding task states and underscores the absence of effective mechanisms to store critical information in complex and lengthy cross-app tasks. To address these challenges, we propose Chain-of-Memory (CoM), a novel approach for explicitly modeling short-term and long-term memory in GUI agents. CoM achieves this by capturing action descriptions, integrating task-relevant screen information, and maintaining a dedicated memory module to store and manage this information. By leveraging explicit memory representations, CoM enables GUI agents to better understand task states and retain critical historical information persistently. To equip GUI agents with memory management capabilities and evaluate the effectiveness of CoM, we developed the GUI Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with Chain-of-Memory. Experimental results demonstrate that CoM significantly improves GUI agents' performance in cross-application tasks. Additionally, GUI Odyssey-CoM enables 7B models to achieve memory management capabilities comparable to 72B models. The dataset and code will be open-sourced.",
        "arxiv_id": "2506.18158",
        "ARXIVID": "2506.18158",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) due to its focus on multimodal large language models for GUI agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.17784": {
        "authors": [
            "Song Wang",
            "Zhen Tan",
            "Zihan Chen",
            "Shuang Zhou",
            "Tianlong Chen",
            "Jundong Li"
        ],
        "title": "AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction",
        "abstract": "arXiv:2506.17784v1 Announce Type: new  Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration highlights the power of structured communication in enabling collective intelligence. However, existing methods largely rely on static or graph-based inter-agent topologies, lacking the potential adaptability and flexibility in communication. In this work, we propose a new framework that rethinks multi-agent coordination through a sequential structure rather than a graph structure, offering a significantly larger topology space for multi-agent communication. Our method focuses on two key directions: (1) Next-Agent Prediction, which selects the most suitable agent role at each step, and (2) Next-Context Selection (NCS), which enables each agent to selectively access relevant information from any previous step. Together, these components construct task-adaptive communication pipelines that support both role flexibility and global information flow. Extensive evaluations across multiple benchmarks demonstrate that our approach achieves superior performance while substantially reducing communication overhead.",
        "arxiv_id": "2506.17784",
        "ARXIVID": "2506.17784",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on multi-agent collaboration and task-adaptive communication pipelines.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.17912": {
        "authors": [
            "Chuhao Jin",
            "Haosen Li",
            "Bingzi Zhang",
            "Che Liu",
            "Xiting Wang",
            "Ruihua Song",
            "Wenbing Huang",
            "Ying Qin",
            "Fuzheng Zhang",
            "Di Zhang"
        ],
        "title": "PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis",
        "abstract": "arXiv:2506.17912v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in many multimodal generation tasks, but a significant performance gap still exists in text-to-motion generation, where LLM-based methods lag far behind non-LLM methods. We identify the granularity of motion tokenization as a critical bottleneck: fine-grained tokenization induces local dependency issues, where LLMs overemphasize short-term coherence at the expense of global semantic alignment, while coarse-grained tokenization sacrifices motion details. To resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating progressive planning and flow-enhanced fine-grained motion tokenization. First, our progressive planning mechanism leverages LLMs' autoregressive capabilities to hierarchically generate motion tokens by starting from sparse global plans and iteratively refining them into full sequences. Second, our flow-enhanced tokenizer doubles the downsampling resolution and expands the codebook size by eight times, minimizing detail loss during discretization, while a flow-enhanced decoder recovers motion nuances. Extensive experiments on text-to-motion benchmarks demonstrate that it achieves state-of-the-art performance, improving FID scores by 63.8% (from 0.380 to 0.141) on long-sequence generation while enhancing motion diversity by 49.9% compared to existing methods. The proposed framework successfully resolves the diversity-quality trade-off that plagues current non-LLM approaches, establishing new standards for text-to-motion generation.",
        "arxiv_id": "2506.17912",
        "ARXIVID": "2506.17912",
        "COMMENT": "Matches criterion 5. Proposes a novel framework for text-to-motion synthesis, integrating image/video understanding with LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.18890": {
        "authors": [
            "Ziqiao Ma",
            "Xuweiyi Chen",
            "Shoubin Yu",
            "Sai Bi",
            "Kai Zhang",
            "Chen Ziwen",
            "Sihan Xu",
            "Jianing Yang",
            "Zexiang Xu",
            "Kalyan Sunkavalli",
            "Mohit Bansal",
            "Joyce Chai",
            "Hao Tan"
        ],
        "title": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time",
        "abstract": "arXiv:2506.18890v1 Announce Type: new  Abstract: Can we scale 4D pretraining to learn general space-time representations that reconstruct an object from a few views at some times to any view at any time? We provide an affirmative answer with 4D-LRM, the first large-scale 4D reconstruction model that takes input from unconstrained views and timestamps and renders arbitrary novel view-time combinations. Unlike prior 4D approaches, e.g., optimization-based, geometry-based, or generative, that struggle with efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time representation and directly predicts per-pixel 4D Gaussian primitives from posed image tokens across time, enabling fast, high-quality rendering at, in principle, infinite frame rate. Our results demonstrate that scaling spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We show that 4D-LRM generalizes to novel objects, interpolates across time, and handles diverse camera setups. It reconstructs 24-frame sequences in one forward pass with less than 1.5 seconds on a single A100 GPU.",
        "arxiv_id": "2506.18890",
        "ARXIVID": "2506.18890",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding through a novel 4D reconstruction model for space-time representations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.17788": {
        "authors": [
            "Shahab Rahimirad",
            "Guven Gergerli",
            "Lucia Romero",
            "Angela Qian",
            "Matthew Lyle Olson",
            "Simon Stepputtis",
            "Joseph Campbell"
        ],
        "title": "Bayesian Social Deduction with Graph-Informed Language Models",
        "abstract": "arXiv:2506.17788v1 Announce Type: new  Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial observations of other agents - remains a challenging task for large language models (LLMs). We evaluate the limits of current reasoning language models in the social deduction game Avalon and find that while the largest models demonstrate strong performance, they require extensive test-time inference and degrade sharply when distilled to smaller, real-time-capable variants. To address this, we introduce a hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction. Our approach achieves competitive performance with much larger models in Agent-Agent play and, notably, is the first language agent to defeat human players in a controlled study - achieving a 67% win rate and receiving higher qualitative ratings than both reasoning baselines and human teammates. We release code, models, and a dataset to support future work on social reasoning in LLM agents, which can be found at https://camp-lab-purdue.github.io/bayesian-social-deduction/",
        "arxiv_id": "2506.17788",
        "ARXIVID": "2506.17788",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores social reasoning in LLMs with a hybrid reasoning framework.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.18701": {
        "authors": [
            "Yifan Zhang",
            "Chunli Peng",
            "Boyang Wang",
            "Puyi Wang",
            "Qingcheng Zhu",
            "Fei Kang",
            "Biao Jiang",
            "Zedong Gao",
            "Eric Li",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "title": "Matrix-Game: Interactive World Foundation Model",
        "abstract": "arXiv:2506.18701v1 Announce Type: new  Abstract: We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
        "arxiv_id": "2506.18701",
        "ARXIVID": "2506.18701",
        "COMMENT": "Matches criterion 3. Introduces an interactive world foundation model for game world generation, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.17707": {
        "authors": [
            "Jihyun Kim",
            "Junho Park",
            "Kyeongbo Kong",
            "Suk-Ju Kang"
        ],
        "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models",
        "abstract": "arXiv:2506.17707v1 Announce Type: new  Abstract: We present Programmable-Room, a framework which interactively generates and edits a 3D room mesh, given natural language instructions. For precise control of a room's each attribute, we decompose the challenging task into simpler steps such as creating plausible 3D coordinates for room meshes, generating panorama images for the texture, constructing 3D meshes by integrating the coordinates and panorama texture images, and arranging furniture. To support the various decomposed tasks with a unified framework, we incorporate visual programming (VP). VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules for the various tasks given in natural language. We develop most of the modules. Especially, for the texture generating module, we utilize a pretrained large-scale diffusion model to generate panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously. Specifically, we enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in generating and editing 3D room meshes, and prove our framework's superiority to an existing model quantitatively and qualitatively. Project page is available in https://jihyun0510.github.io/Programmable_Room_Page/.",
        "arxiv_id": "2506.17707",
        "ARXIVID": "2506.17707",
        "COMMENT": "Matches criterion 1. Proposes a framework for generating 3D room meshes using LLMs, which involves spatial reasoning for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.18564": {
        "authors": [
            "Xuanyu Zhang",
            "Weiqi Li",
            "Shijie Zhao",
            "Junlin Li",
            "Li Zhang",
            "Jian Zhang"
        ],
        "title": "VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning",
        "abstract": "arXiv:2506.18564v1 Announce Type: new  Abstract: Recent advances in AI-generated content (AIGC) have led to the emergence of powerful text-to-video generation models. Despite these successes, evaluating the quality of AIGC-generated videos remains challenging due to limited generalization, lack of temporal awareness, heavy reliance on large-scale annotated datasets, and the lack of effective interaction with generation models. Most current approaches rely on supervised finetuning of vision-language models (VLMs), which often require large-scale annotated datasets and tend to decouple understanding and generation. To address these shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for AIGC video quality assessment. Our approach features: (1) a progressive video quality learning scheme that combines image quality warm-up, general task-specific temporal learning, and joint optimization with the video generation model; (2) the design of multi-dimension scoring rewards, preference comparison rewards, and temporal modeling rewards to enhance both generalization and specialization in video quality evaluation. Extensive experiments demonstrate that VQ-Insight consistently outperforms state-of-the-art baselines in preference comparison, multi-dimension scoring, and natural video scoring, bringing significant improvements for video generation tasks.",
        "arxiv_id": "2506.18564",
        "ARXIVID": "2506.18564",
        "COMMENT": "Matches criterion 6 as it focuses on video quality understanding and evaluation using a novel reasoning-style VLM framework.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.17939": {
        "authors": [
            "Bo Liu",
            "Xiangyu Zhao",
            "Along He",
            "Yidi Chen",
            "Huazhu Fu",
            "Xiao-Ming Wu"
        ],
        "title": "GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning",
        "abstract": "arXiv:2506.17939v1 Announce Type: new  Abstract: Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model-generated answers. To address this, this work first proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer generation is decomposed into intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the model's reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.",
        "arxiv_id": "2506.17939",
        "ARXIVID": "2506.17939",
        "COMMENT": "Matches criterion 5 as it integrates visual grounding and reasoning in medical visual question answering, combining image understanding and language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.17587": {
        "authors": [
            "Le Yu",
            "Kaishen Wang",
            "Jianlong Xiong",
            "Yue Cao",
            "Tao He"
        ],
        "title": "HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models",
        "abstract": "arXiv:2506.17587v1 Announce Type: new  Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable performance across various tasks, they are still prone to hallucinations-generating outputs that are textually plausible but visually ungrounded. While prior approaches generally address this issue through data-centric fine-tuning or innovative decoding strategies, these methods often require substantial resources or task-specific configurations. In this work, we introduce an architecture-level solution, HalluRNN, which enhances model stability through recurrent cross-layer reasoning. Specifically, we propose a novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across layers and recurrently refines hidden states. This allows for the adaptive propagation of information throughout the model, enforces consistency across layers, and mitigates hallucinations caused by representational drift. By fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust performance across multiple benchmarks.",
        "arxiv_id": "2506.17587",
        "ARXIVID": "2506.17587",
        "COMMENT": "Matches criterion 2 as it explores a novel architecture-level solution for mitigating hallucinations in Large Vision-Language Models (LVLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.17873": {
        "authors": [
            "Guankun Wang",
            "Wenjin Mo",
            "Junyi Wang",
            "Long Bai",
            "Kun Yuan",
            "Ming Hu",
            "Jinlin Wu",
            "Junjun He",
            "Yiming Huang",
            "Nicolas Padoy",
            "Zhen Lei",
            "Hongbin Liu",
            "Nassir Navab",
            "Hongliang Ren"
        ],
        "title": "SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model",
        "abstract": "arXiv:2506.17873v1 Announce Type: new  Abstract: Recent advances in Multimodal Large Language Models have demonstrated great potential in the medical domain, facilitating users to understand surgical scenes and procedures. Beyond image-based methods, the exploration of Video Large Language Models (Vid-LLMs) has emerged as a promising avenue for capturing the complex sequences of information involved in surgery. However, there is still a lack of Vid-LLMs specialized for fine-grained surgical video understanding tasks, which is crucial for analyzing specific processes or details within a surgical procedure. To bridge this gap, we propose SurgVidLM, the first video language model designed to address both full and fine-grained surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K dataset which consists of over 31K video-instruction pairs, enabling both holistic understanding and detailed analysis of surgical procedures. Furthermore, we introduce the StageFocus mechanism which is a two-stage framework performing the multi-grained, progressive understanding of surgical videos. We also develop the Multi-frequency Fusion Attention to effectively integrate low and high-frequency visual tokens, ensuring the retention of critical information. Experimental results demonstrate that SurgVidLM significantly outperforms state-of-the-art Vid-LLMs in both full and fine-grained video understanding tasks, showcasing its superior capability in capturing complex procedural contexts.",
        "arxiv_id": "2506.17873",
        "ARXIVID": "2506.17873",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) and criterion 6 (Video Understanding) as it introduces SurgVidLM, a Video LLM for fine-grained surgical video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.18792": {
        "authors": [
            "Michal Nazarczuk",
            "Sibi Catley-Chandar",
            "Thomas Tanay",
            "Zhensong Zhang",
            "Gregory Slabaugh",
            "Eduardo P\\'erez-Pellitero"
        ],
        "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
        "abstract": "arXiv:2506.18792v1 Announce Type: new  Abstract: Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io",
        "arxiv_id": "2506.18792",
        "ARXIVID": "2506.18792",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks, specifically 4D reconstruction from monocular inputs, which is a novel approach to video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.18862": {
        "authors": [
            "Zhongbin Guo",
            "Yuhao Wang",
            "Ping Jian",
            "Xinyue Chen",
            "Wei Peng",
            "Ertai E"
        ],
        "title": "TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting",
        "abstract": "arXiv:2506.18862v1 Announce Type: new  Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal reasoning, which remains a challenge for existing multimodal large language models (MLLMs). In this work, we study the capabilities of MLLMs on a novel task that jointly targets temporal change understanding and future scene generation, aiming to assess their potential for modeling complex multimodal dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for satellite image change understanding and forecasting, which enhances frozen MLLMs with lightweight temporal modules for structured sequence encoding and contextual prompting. To guide future image generation, TAMMs introduces a Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines high-level semantic reasoning and structural priors within an enhanced ControlNet. This dual-path conditioning enables temporally consistent and semantically grounded image synthesis. Experiments demonstrate that TAMMs outperforms strong MLLM baselines in both temporal change understanding and future image forecasting tasks, highlighting how carefully designed temporal reasoning and semantic fusion can unlock the full potential of MLLMs for spatio-temporal understanding.",
        "arxiv_id": "2506.18862",
        "ARXIVID": "2506.18862",
        "COMMENT": "Matches criterion 2 and 6 as it introduces a temporal-aware multimodal model for satellite image change understanding and forecasting, focusing on spatio-temporal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.17612": {
        "authors": [
            "Yunlong Lin",
            "Zixu Lin",
            "Kunjie Lin",
            "Jinbin Bai",
            "Panwang Pan",
            "Chenxin Li",
            "Haoyu Chen",
            "Zhongdao Wang",
            "Xinghao Ding",
            "Wenbo Li",
            "Shuicheng Yan"
        ],
        "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent",
        "abstract": "arXiv:2506.17612v1 Announce Type: new  Abstract: Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: https://jarvisart.vercel.app/.",
        "arxiv_id": "2506.17612",
        "ARXIVID": "2506.17612",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces a multi-modal large language model-driven agent for photo retouching.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.18651": {
        "authors": [
            "Shuocun Yang",
            "Huawen Hu",
            "Enze Shi",
            "Shu Zhang"
        ],
        "title": "Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems",
        "abstract": "arXiv:2506.18651v1 Announce Type: new  Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents an emerging and promising research area. Prior work has largely centered on intra-group behavioral consistency in multi-agent systems, with limited attention given to behavioral consistency in multi-agent grouping scenarios. In this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL control method designed to explicitly regulate agent behaviors at both intra-group and inter-group levels. DLBC partitions agents into distinct groups and dynamically modulates behavioral diversity both within and between these groups. By dynamically modulating behavioral diversity within and between these groups, DLBC achieves enhanced division of labor through inter-group consistency, which constrains behavioral strategies across different groups. Simultaneously, intra-group consistency, achieved by aligning behavioral strategies within each group, fosters stronger intra-group cooperation. Crucially, DLBC's direct constraint of agent policy functions ensures its broad applicability across various algorithmic frameworks. Experimental results in various grouping cooperation scenarios demonstrate that DLBC significantly enhances both intra-group cooperative performance and inter-group task specialization, yielding substantial performance improvements. DLBC provides new ideas for behavioral consistency control of multi-intelligent body systems, and its potential for application in more complex tasks and dynamic environments can be further explored in the future.",
        "arxiv_id": "2506.18651",
        "ARXIVID": "2506.18651",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for behavioral consistency in multi-agent systems, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.18476": {
        "authors": [
            "Yaokun Zhong",
            "Siyu Jiang",
            "Jian Zhu",
            "Jian-Fang Hu"
        ],
        "title": "Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding",
        "abstract": "arXiv:2506.18476v1 Announce Type: new  Abstract: Semi-Supervised Video Paragraph Grounding (SSVPG) aims to localize multiple sentences in a paragraph from an untrimmed video with limited temporal annotations. Existing methods focus on teacher-student consistency learning and video-level contrastive loss, but they overlook the importance of perturbing query contexts to generate strong supervisory signals. In this work, we propose a novel Context Consistency Learning (CCL) framework that unifies the paradigms of consistency regularization and pseudo-labeling to enhance semi-supervised learning. Specifically, we first conduct teacher-student learning where the student model takes as inputs strongly-augmented samples with sentences removed and is enforced to learn from the adequately strong supervisory signals from the teacher model. Afterward, we conduct model retraining based on the generated pseudo labels, where the mutual agreement between the original and augmented views' predictions is utilized as the label confidence. Extensive experiments show that CCL outperforms existing methods by a large margin.",
        "arxiv_id": "2506.18476",
        "ARXIVID": "2506.18476",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding tasks, specifically semi-supervised video paragraph grounding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.17449": {
        "authors": [
            "Manasa Bharadwaj",
            "Nikhil Verma",
            "Kevin Ferreira"
        ],
        "title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections",
        "abstract": "arXiv:2506.17449v1 Announce Type: new  Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.",
        "arxiv_id": "2506.17449",
        "ARXIVID": "2506.17449",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for improving LLM agents in dynamic environments, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.17558": {
        "authors": [
            "Jake Levi",
            "Mark van der Wilk"
        ],
        "title": "SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference",
        "abstract": "arXiv:2506.17558v1 Announce Type: new  Abstract: Learning to infer object representations, and in particular part-whole hierarchies, has been the focus of extensive research in computer vision, in pursuit of improving data efficiency, systematic generalisation, and robustness. Models which are \\emph{designed} to infer part-whole hierarchies, often referred to as capsule networks, are typically trained end-to-end on supervised tasks such as object classification, in which case it is difficult to evaluate whether such a model \\emph{actually} learns to infer part-whole hierarchies, as claimed. To address this difficulty, we present a SYNthetic DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and establish its utility by (1) demonstrating the precise bottleneck in a prominent existing capsule model, and (2) demonstrating that permutation-equivariant self-attention is highly effective for parts-to-wholes inference, which motivates future directions for designing effective inductive biases for computer vision.",
        "arxiv_id": "2506.17558",
        "ARXIVID": "2506.17558",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it introduces a synthetic dataset for evaluating part-whole hierarchical inference, which is relevant to understanding progress in this area.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.18268": {
        "authors": [
            "Yu Liu",
            "Yangtao Meng",
            "Xianfei Pan",
            "Jie Jiang",
            "Changhao Chen"
        ],
        "title": "ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments",
        "abstract": "arXiv:2506.18268v1 Announce Type: new  Abstract: Thermal cameras capture environmental data through heat emission, a fundamentally different mechanism compared to visible light cameras, which rely on pinhole imaging. As a result, traditional visual relocalization methods designed for visible light images are not directly applicable to thermal images. Despite significant advancements in deep learning for camera relocalization, approaches specifically tailored for thermal camera-based relocalization remain underexplored. To address this gap, we introduce ThermalLoc, a novel end-to-end deep learning method for thermal image relocalization. ThermalLoc effectively extracts both local and global features from thermal images by integrating EfficientNet with Transformers, and performs absolute pose regression using two MLP networks. We evaluated ThermalLoc on both the publicly available thermal-odometry dataset and our own dataset. The results demonstrate that ThermalLoc outperforms existing representative methods employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet, and RobustLoc, achieving superior accuracy and robustness.",
        "arxiv_id": "2506.18268",
        "ARXIVID": "2506.18268",
        "COMMENT": "Matches criterion 3. Proposes a novel method for thermal camera relocalization, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.17561": {
        "authors": [
            "Chongkai Gao",
            "Zixuan Liu",
            "Zhenghao Chi",
            "Junshan Huang",
            "Xin Fei",
            "Yiwen Hou",
            "Yuxuan Zhang",
            "Yudi Lin",
            "Zhirui Fang",
            "Zeyu Jiang",
            "Lin Shao"
        ],
        "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models",
        "abstract": "arXiv:2506.17561v1 Announce Type: new  Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.",
        "arxiv_id": "2506.17561",
        "ARXIVID": "2506.17561",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it systematically evaluates planning paradigms in Vision-Language-Action models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.18856": {
        "authors": [
            "Kuanning Wang",
            "Yuqian Fu",
            "Tianyu Wang",
            "Yanwei Fu",
            "Longfei Liang",
            "Yu-Gang Jiang",
            "Xiangyang Xue"
        ],
        "title": "RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base",
        "abstract": "arXiv:2506.18856v1 Announce Type: new  Abstract: Accurate 6D pose estimation is key for robotic manipulation, enabling precise object localization for tasks like grasping. We present RAG-6DPose, a retrieval-augmented approach that leverages 3D CAD models as a knowledge base by integrating both visual and geometric cues. Our RAG-6DPose roughly contains three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D visual features from multi-view CAD rendered images and also attaching 3D points; 2) Retrieving relevant CAD features from the knowledge base based on the current query image via our ReSPC module; and 3) Incorporating retrieved CAD information to refine pose predictions via retrieval-augmented decoding. Experimental results on standard benchmarks and real-world robotic tasks demonstrate the effectiveness and robustness of our approach, particularly in handling occlusions and novel viewpoints. Supplementary material is available on our project website: https://sressers.github.io/RAG-6DPose .",
        "arxiv_id": "2506.18856",
        "ARXIVID": "2506.18856",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces RAG-6DPose, a retrieval-augmented 6D pose estimation method for robotic manipulation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.18655": {
        "authors": [
            "Wenxu Qian",
            "Chaoyue Wang",
            "Hou Peng",
            "Zhiyu Tan",
            "Hao Li",
            "Anxiang Zeng"
        ],
        "title": "RDPO: Real Data Preference Optimization for Physics Consistency Video Generation",
        "abstract": "arXiv:2506.18655v1 Announce Type: new  Abstract: Video generation techniques have achieved remarkable advancements in visual quality, yet faithfully reproducing real-world physics remains elusive. Preference-based model post-training may improve physical consistency, but requires costly human-annotated datasets or reward models that are not yet feasible. To address these challenges, we present Real Data Preference Optimisation (RDPO), an annotation-free framework that distills physical priors directly from real-world videos. Specifically, the proposed RDPO reverse-samples real video sequences with a pre-trained generator to automatically build preference pairs that are statistically distinguishable in terms of physical correctness. A multi-stage iterative training schedule then guides the generator to obey physical laws increasingly well. Benefiting from the dynamic information explored from real videos, our proposed RDPO significantly improves the action coherence and physical realism of the generated videos. Evaluations on multiple benchmarks and human evaluations have demonstrated that RDPO achieves improvements across multiple dimensions. The source code and demonstration of this paper are available at: https://wwenxu.github.io/RDPO/",
        "arxiv_id": "2506.18655",
        "ARXIVID": "2506.18655",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on improving video generation with physical consistency, which is a novel aspect of video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.17667": {
        "authors": [
            "Lintao Wang",
            "Encheng Su",
            "Jiaqi Liu",
            "Pengze Li",
            "Peng Xia",
            "Jiabei Xiao",
            "Wenlong Zhang",
            "Xinnan Dai",
            "Xi Chen",
            "Yuan Meng",
            "Mingyu Ding",
            "Lei Bai",
            "Wanli Ouyang",
            "Shixiang Tang",
            "Aoran Wang",
            "Xinzhu Ma"
        ],
        "title": "PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models",
        "abstract": "arXiv:2506.17667v1 Announce Type: new  Abstract: Physics problem-solving is a challenging domain for large AI models, requiring integration of conceptual understanding, mathematical reasoning, and interpretation of physical diagrams. Current evaluation methodologies show notable limitations in capturing the breadth and complexity of undergraduate-level physics, underscoring the need for more rigorous assessments. To this end, we present PhysUniBench, a large-scale multimodal benchmark designed to evaluate and improve the reasoning capabilities of multimodal large language models (MLLMs) specifically on undergraduate-level physics problems. PhysUniBench consists of 3,304 physics questions spanning 8 major sub-disciplines of physics, each accompanied by one visual diagrams. The benchmark includes both open-ended and multiple-choice questions, systematically curated and difficulty-rated through an iterative model-in-the-loop process. The benchmark's construction involved a rigorous multi-stage process, including multiple roll-outs, expert-level evaluation, automated filtering of easily solved problems, and a nuanced difficulty grading system with five levels. Through extensive experiments, we observe that current state-of-the-art models encounter substantial challenges in physics reasoning. For example, GPT-4o mini achieves only about 34.2\\% accuracy in the proposed PhysUniBench. These results highlight that current MLLMs struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation. By providing a broad and rigorous assessment tool, PhysUniBench aims to drive progress in AI for Science, encouraging the development of models with stronger physical reasoning, problem-solving skills, and multimodal understanding. The benchmark and evaluation scripts are available at https://prismax-team.github.io/PhysUniBenchmark/.",
        "arxiv_id": "2506.17667",
        "ARXIVID": "2506.17667",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for multimodal models in physics reasoning, which could be relevant for embodied AI and reasoning tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.17685": {
        "authors": [
            "Amirshayan Nasirimajd",
            "Chiara Plizzari",
            "Simone Alberto Peirone",
            "Marco Ciccone",
            "Giuseppe Averta",
            "Barbara Caputo"
        ],
        "title": "Domain Generalization using Action Sequences for Egocentric Action Recognition",
        "abstract": "arXiv:2506.17685v1 Announce Type: new  Abstract: Recognizing human activities from visual inputs, particularly through a first-person viewpoint, is essential for enabling robots to replicate human behavior. Egocentric vision, characterized by cameras worn by observers, captures diverse changes in illumination, viewpoint, and environment. This variability leads to a notable drop in the performance of Egocentric Action Recognition models when tested in environments not seen during training. In this paper, we tackle these challenges by proposing a domain generalization approach for Egocentric Action Recognition. Our insight is that action sequences often reflect consistent user intent across visual domains. By leveraging action sequences, we aim to enhance the model's generalization ability across unseen environments. Our proposed method, named SeqDG, introduces a visual-text sequence reconstruction objective (SeqRec) that uses contextual cues from both text and visual inputs to reconstruct the central action of the sequence. Additionally, we enhance the model's robustness by training it on mixed sequences of actions from different domains (SeqMix). We validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement in cross-domain action recognition in unseen environments, and on EGTEA the model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action recognition.",
        "arxiv_id": "2506.17685",
        "ARXIVID": "2506.17685",
        "COMMENT": "Matches criterion 3. Proposes a domain generalization method for egocentric action recognition, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18683": {
        "authors": [
            "Youcef Sklab",
            "Hanane Ariouat",
            "Eric Chenin",
            "Edi Prifti",
            "Jean-Daniel Zucker"
        ],
        "title": "SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification",
        "abstract": "arXiv:2506.18683v1 Announce Type: new  Abstract: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image classification architecture that integrates 3D point cloud representations inferred directly from RGB images. Our key contribution lies in a pixel-to-point transformation that converts 2D object masks into 3D point clouds, enabling the fusion of texture-based and geometric features for enhanced classification performance. SIM-Net is particularly well-suited for the classification of digitized herbarium specimens (a task made challenging by heterogeneous backgrounds), non-plant elements, and occlusions that compromise conventional image-based models. To address these issues, SIM-Net employs a segmentation-based preprocessing step to extract object masks prior to 3D point cloud generation. The architecture comprises a CNN encoder for 2D image features and a PointNet-based encoder for geometric features, which are fused into a unified latent space. Experimental evaluations on herbarium datasets demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several transformer-based state-of-the-art architectures, highlighting the benefits of incorporating 3D structural reasoning into 2D image classification tasks.",
        "arxiv_id": "2506.18683",
        "ARXIVID": "2506.18683",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates 3D point clouds inferred from RGB images for enhanced classification.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.17910": {
        "authors": [
            "Mohamed Benkedadra",
            "Matei Mancas",
            "Sidi Ahmed Mahmoudi"
        ],
        "title": "Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis",
        "abstract": "arXiv:2506.17910v1 Announce Type: new  Abstract: 2D cameras are often used in interactive systems. Other systems like gaming consoles provide more powerful 3D cameras for short range depth sensing. Overall, these cameras are not reliable in large, complex environments. In this work, we propose a 3D stereo vision based pipeline for interactive systems, that is able to handle both ordinary and sensitive applications, through robust scene understanding. We explore the fusion of multiple 3D cameras to do full scene reconstruction, which allows for preforming a wide range of tasks, like event recognition, subject tracking, and notification. Using possible feedback approaches, the system can receive data from the subjects present in the environment, to learn to make better decisions, or to adapt to completely new environments. Throughout the paper, we introduce the pipeline and explain our preliminary experimentation and results. Finally, we draw the roadmap for the next steps that need to be taken, in order to get this pipeline into production",
        "arxiv_id": "2506.17910",
        "ARXIVID": "2506.17910",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a 3D stereo vision pipeline for interactive systems with feedback mechanisms.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18871": {
        "authors": [
            "Chenyuan Wu",
            "Pengfei Zheng",
            "Ruiran Yan",
            "Shitao Xiao",
            "Xin Luo",
            "Yueze Wang",
            "Wanli Li",
            "Xiyan Jiang",
            "Yexin Liu",
            "Junjie Zhou",
            "Ze Liu",
            "Ziyi Xia",
            "Chaofan Li",
            "Haoge Deng",
            "Jiahao Wang",
            "Kun Luo",
            "Bo Zhang",
            "Defu Lian",
            "Xinlong Wang",
            "Zhongyuan Wang",
            "Tiejun Huang",
            "Zheng Liu"
        ],
        "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
        "abstract": "arXiv:2506.18871v1 Announce Type: new  Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
        "arxiv_id": "2506.18871",
        "ARXIVID": "2506.18871",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it introduces OmniGen2, a multimodal generative model for text-to-image and image editing tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18019": {
        "authors": [
            "Yuanchen Bei",
            "Weizhi Zhang",
            "Siwen Wang",
            "Weizhi Chen",
            "Sheng Zhou",
            "Hao Chen",
            "Yong Li",
            "Jiajun Bu",
            "Shirui Pan",
            "Yizhou Yu",
            "Irwin King",
            "Fakhri Karray",
            "Philip S. Yu"
        ],
        "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities",
        "abstract": "arXiv:2506.18019v1 Announce Type: new  Abstract: AI agents have experienced a paradigm shift, from early dominance by reinforcement learning (RL) to the rise of agents powered by large language models (LLMs), and now further advancing towards a synergistic fusion of RL and LLM capabilities. This progression has endowed AI agents with increasingly strong abilities. Despite these advances, to accomplish complex real-world tasks, agents are required to plan and execute effectively, maintain reliable memory, and coordinate smoothly with other agents. Achieving these capabilities involves contending with ever-present intricate information, operations, and interactions. In light of this challenge, data structurization can play a promising role by transforming intricate and disorganized data into well-structured forms that agents can more effectively understand and process. In this context, graphs, with their natural advantage in organizing, managing, and harnessing intricate data relationships, present a powerful data paradigm for structurization to support the capabilities demanded by advanced AI agents. To this end, this survey presents a first systematic review of how graphs can empower AI agents. Specifically, we explore the integration of graph techniques with core agent functionalities, highlight notable applications, and identify prospective avenues for future research. By comprehensively surveying this burgeoning intersection, we hope to inspire the development of next-generation AI agents equipped to tackle increasingly sophisticated challenges with graphs. Related resources are collected and continuously updated for the community in the Github link.",
        "arxiv_id": "2506.18019",
        "ARXIVID": "2506.18019",
        "COMMENT": "Matches criterion 3 as it discusses novel methods for integrating graph techniques with AI agents, which could be relevant for embodied/robotic AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.18233": {
        "authors": [
            "Ruike Zhu",
            "Hanwen Zhang",
            "Tianyu Shi",
            "Chi Wang",
            "Tianyi Zhou",
            "Zengyi Qin"
        ],
        "title": "The 4th Dimension for Scaling Model Size",
        "abstract": "arXiv:2506.18233v1 Announce Type: new  Abstract: Scaling the size of large language models typically involves three dimensions: depth, width, and the number of parameters. In this work, we explore a fourth dimension, virtual logical depth (VLD), which increases the effective algorithmic depth without changing the overall parameter count by reusing parameters within the model. Although parameter reuse is not a new concept, its potential and characteristics in model scaling have not been thoroughly studied. Through carefully designed controlled experiments, we make the following key discoveries regarding VLD scaling:   VLD scaling forces the knowledge capacity of the model to remain almost constant, with only minor variations.   VLD scaling enables a significant improvement in reasoning capability, provided the scaling method is properly implemented.   The number of parameters correlates with knowledge capacity, but not with reasoning capability. Under certain conditions, it is not necessary to increase the parameter count to enhance reasoning.   These findings are consistent across various model configurations and are likely to be generally valid within the scope of our experiments.",
        "arxiv_id": "2506.18233",
        "ARXIVID": "2506.18233",
        "COMMENT": "Does not match any specific criterion but explores a novel scaling dimension for large language models, which is tangentially relevant to general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2506.18438": {
        "authors": [
            "Dinh-Khoi Vo",
            "Thanh-Toan Do",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "title": "CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing",
        "abstract": "arXiv:2506.18438v1 Announce Type: new  Abstract: Editing natural images using textual descriptions in text-to-image diffusion models remains a significant challenge, particularly in achieving consistent generation and handling complex, non-rigid objects. Existing methods often struggle to preserve textures and identity, require extensive fine-tuning, and exhibit limitations in editing specific spatial regions or objects while retaining background details. This paper proposes Context-Preserving Adaptive Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid real image editing. Specifically, we propose a preservation adaptation module that adjusts self-attention mechanisms to preserve and independently control the object and background effectively. This ensures that the objects' shapes, textures, and identities are maintained while keeping the background undistorted during the editing process using the mask guidance technique. Additionally, we develop a localized extraction module to mitigate the interference with the non-desired modified regions during conditioning in cross-attention mechanisms. We also introduce various mask-guidance strategies to facilitate diverse image manipulation tasks in a simple manner. Extensive experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a robust benchmark dataset specifically designed for real image editing, demonstrate that our proposed method is the preferred choice among human raters, outperforming existing state-of-the-art editing techniques.",
        "arxiv_id": "2506.18438",
        "ARXIVID": "2506.18438",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and generative modeling. Focuses on zero-shot real image editing using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.17697": {
        "authors": [
            "Bohan Tang",
            "Dezhao Luo",
            "Jingxuan Chen",
            "Shaogang Gong",
            "Jianye Hao",
            "Jun Wang",
            "Kun Shao"
        ],
        "title": "Beyond Syntax: Action Semantics Learning for App Agents",
        "abstract": "arXiv:2506.17697v1 Announce Type: new  Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents that interpret user intent and operate smartphone Apps through actions such as clicking and scrolling. While prompt-based solutions with closed LLM APIs show promising ability, they incur heavy compute costs and external API dependency. Fine-tuning smaller open-source LLMs solves these limitations. However, current fine-tuning methods use a syntax learning paradigm that forces agents to reproduce exactly the ground truth action strings, leading to out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action Semantics Learning (ASL), a novel learning framework, where the learning objective is capturing the semantics of the ground truth actions. Specifically, inspired by the programming language theory, we define the action semantics for App agents as the state transition induced by the action in the user interface. With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a semantic reward to train the App agents in generating actions aligned with the semantics of ground truth actions, even when the syntactic forms differ. To support the effectiveness of ASL, we theoretically demonstrate the superior robustness of ASL for the OOD problem compared with the existing syntax learning paradigm. Extensive experiments on offline and online smartphone App operation benchmarks show that ASL significantly improves the accuracy and generalisation of App agents over existing methods.",
        "arxiv_id": "2506.17697",
        "ARXIVID": "2506.17697",
        "COMMENT": "Does not match any specific criteria. Focuses on action semantics learning for app agents, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.17585": {
        "authors": [
            "Yukun Huang",
            "Sanxing Chen",
            "Jian Pei",
            "Manzil Zaheer",
            "Bhuwan Dhingra"
        ],
        "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models",
        "abstract": "arXiv:2506.17585v1 Announce Type: new  Abstract: Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during (continual) pretraining--without test-time retrieval--by revising the training process. To evaluate this, we release CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and probes both short-form (single fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We find that simple Passive Indexing, which appends an identifier to each document, helps memorize verbatim text but fails on paraphrased or compositional facts. Instead, we propose Active Indexing, which continually pretrains on synthetic QA pairs that (1) restate each fact in diverse compositional forms, and (2) require bidirectional source-to-fact and fact-to-source generation, jointly teaching the model to generate content from a cited source and to attribute its own answers. Experiments with Qwen2.5-7B and 3B show that Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16 times the original token count.",
        "arxiv_id": "2506.17585",
        "ARXIVID": "2506.17585",
        "COMMENT": "Does not match any specific criterion but is related to language models and knowledge attribution, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18135": {
        "authors": [
            "Zijun Chen",
            "Zhanpeng Zhou",
            "Bo Zhang",
            "Weinan Zhang",
            "Xi Sun",
            "Junchi Yan"
        ],
        "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging",
        "abstract": "arXiv:2506.18135v1 Announce Type: new  Abstract: Model merging has gained increasing attention due to its intriguing property: interpolating the parameters of different task-specific fine-tuned models leads to multi-task abilities. However, despite its empirical success, the underlying mechanisms of model merging remain poorly understood. In this work, we delve into the mechanism behind model merging from a representation perspective. Our analysis reveals that model merging achieves multi-task abilities through two key capabilities: i) distinguishing samples from different tasks, and ii) adapting to the corresponding expert model for each sample. These two capabilities allow the merged model to retain task-specific expertise, enabling efficient multi-task adaptation. Building on these insights, we propose \\texttt{SE-Merging}, a self-enhanced model merging framework that leverages these two characteristics to dynamically identify the corresponding task for each sample and then adaptively rescales the merging coefficients to further enhance task-specific expertise in the merged model. Notably, \\texttt{SE-Merging} achieves dynamic model merging without additional training. Extensive experiments demonstrate that \\texttt{SE-Merging} achieves significant performance improvements while remaining compatible with existing model merging techniques.",
        "arxiv_id": "2506.18135",
        "ARXIVID": "2506.18135",
        "COMMENT": "Does not match any specific criterion but explores model merging, which is tangentially relevant to general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18096": {
        "authors": [
            "Yuxuan Huang",
            "Yihang Chen",
            "Haozheng Zhang",
            "Kang Li",
            "Meng Fang",
            "Linyi Yang",
            "Xiaoguang Li",
            "Lifeng Shang",
            "Songcen Xu",
            "Jianye Hao",
            "Kun Shao",
            "Jun Wang"
        ],
        "title": "Deep Research Agents: A Systematic Examination And Roadmap",
        "abstract": "arXiv:2506.18096v1 Announce Type: new  Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.",
        "arxiv_id": "2506.18096",
        "ARXIVID": "2506.18096",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to the development of autonomous AI systems and multi-modal input processing.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.18777": {
        "authors": [
            "Jonathan Cook",
            "Silvia Sapora",
            "Arash Ahmadian",
            "Akbir Khan",
            "Tim Rocktaschel",
            "Jakob Foerster",
            "Laura Ruis"
        ],
        "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training",
        "abstract": "arXiv:2506.18777v1 Announce Type: new  Abstract: Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.",
        "arxiv_id": "2506.18777",
        "ARXIVID": "2506.18777",
        "COMMENT": "Does not match any specific criteria. Focuses on understanding LLMs' reasoning abilities through code training, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.17931": {
        "authors": [
            "Ravi Kant Gupta",
            "Shounak Das",
            "Amit Sethi"
        ],
        "title": "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset",
        "abstract": "arXiv:2506.17931v1 Announce Type: new  Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for natural images. A commonly-used objective for UDA schemes is to enhance domain alignment in representation space even if there is a domain shift in the input space. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions associated with classification problems. Our approach has two main features. Firstly, its neural architecture uses the deep structure of ResNet and the effective separation of scales of feature pyramidal network (FPN) to work with both content and style features. Secondly, it uses a combination of a novel loss function and judiciously selected existing loss functions to train the network architecture. This tailored combination is designed to address challenges inherent to natural images, such as scale, noise, and style shifts, that occur on top of a multi-modal (multi-class) distribution. The combined loss function not only enhances model accuracy and robustness on the target domain but also speeds up training convergence. Our proposed UDA scheme generalizes better than state-of-the-art for CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets and comaparable for DomainNet dataset.",
        "arxiv_id": "2506.17931",
        "ARXIVID": "2506.17931",
        "COMMENT": "Does not match any specific criteria. Focuses on domain adaptation for natural images, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18682": {
        "authors": [
            "Imad Ali Shah",
            "Jiarong Li",
            "Tim Brophy",
            "Martin Glavin",
            "Edward Jones",
            "Enda Ward",
            "Brian Deegan"
        ],
        "title": "Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios",
        "abstract": "arXiv:2506.18682v1 Announce Type: new  Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly in challenging weather and lighting conditions. However, efficiently processing its high-dimensional spectral data remains a significant challenge. This paper introduces a Multi-scale Spectral Attention Module (MSAM) that enhances spectral feature extraction through three parallel 1D convolutions with varying kernel sizes between 1 to 11, coupled with an adaptive feature aggregation mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our proposed UNet-MSAM achieves significant improvements in semantic segmentation performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and Hyperspectral City v2. Our comprehensive experiments demonstrate that with minimal computational overhead (on average 0.02% in parameters and 0.82% GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets. Through extensive ablation studies, we have established that multi-scale kernel combinations perform better than single-scale configurations. These findings demonstrate the potential of HSI processing for AD and provide valuable insights into designing robust, multi-scale spectral feature extractors for real-world applications.",
        "arxiv_id": "2506.18682",
        "ARXIVID": "2506.18682",
        "COMMENT": "Does not match any specific criterion but focuses on hyperspectral segmentation for autonomous driving, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18323": {
        "authors": [
            "Muhammad Azeem Aslam",
            "Hassan Khalid",
            "Nisar Ahmed"
        ],
        "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement",
        "abstract": "arXiv:2506.18323v1 Announce Type: new  Abstract: Low-light image enhancement remains a challenging task, particularly in the absence of paired training data. In this study, we present LucentVisionNet, a novel zero-shot learning framework that addresses the limitations of traditional and deep learning-based enhancement methods. The proposed approach integrates multi-scale spatial attention with a deep curve estimation network, enabling fine-grained enhancement while preserving semantic and perceptual fidelity. To further improve generalization, we adopt a recurrent enhancement strategy and optimize the model using a composite loss function comprising six tailored components, including a novel no-reference image quality loss inspired by human visual perception. Extensive experiments on both paired and unpaired benchmark datasets demonstrate that LucentVisionNet consistently outperforms state-of-the-art supervised, unsupervised, and zero-shot methods across multiple full-reference and no-reference image quality metrics. Our framework achieves high visual quality, structural consistency, and computational efficiency, making it well-suited for deployment in real-world applications such as mobile photography, surveillance, and autonomous navigation.",
        "arxiv_id": "2506.18323",
        "ARXIVID": "2506.18323",
        "COMMENT": "Does not match any specific criterion but focuses on low-light image enhancement, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17290": {
        "authors": [
            "Yuqi Li",
            "Junhao Dong",
            "Zeyu Dong",
            "Chuanguang Yang",
            "Zhulin An",
            "Yongjun Xu"
        ],
        "title": "SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation",
        "abstract": "arXiv:2506.17290v1 Announce Type: new  Abstract: 3D point cloud segmentation faces practical challenges due to the computational complexity and deployment limitations of large-scale transformer-based models. To address this, we propose a novel Structure- and Relation-aware Knowledge Distillation framework, named SRKD, that transfers rich geometric and semantic knowledge from a large frozen teacher model (>100M) to a lightweight student model (<15M). Specifically, we propose an affinity matrix-based relation alignment module, which distills structural dependencies from the teacher to the student through point-wise similarity matching, enhancing the student's capability to learn contextual interactions. Meanwhile, we introduce a cross-sample mini-batch construction strategy that enables the student to perceive stable and generalized geometric structure. This aligns across diverse point cloud instances of the teacher, rather than within a single sample. Additionally, KL divergence is applied to align semantic distributions, and ground-truth supervision further reinforces accurate segmentation. Our method achieves state of the art performance with significantly reduced model complexity, demonstrating its effectiveness and efficiency in real-world deployment scenarios. Our Code is available at https://github.com/itsnotacie/SRKD.",
        "arxiv_id": "2506.17290",
        "ARXIVID": "2506.17290",
        "COMMENT": "Does not match any specific criterion but focuses on knowledge distillation for 3D point cloud segmentation, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18677": {
        "authors": [
            "Adam Yang",
            "Nadula Kadawedduwa",
            "Tianfu Wang",
            "Maria Molina",
            "Christopher Metzler"
        ],
        "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting",
        "abstract": "arXiv:2506.18677v1 Announce Type: new  Abstract: Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.",
        "arxiv_id": "2506.18677",
        "ARXIVID": "2506.18677",
        "COMMENT": "Does not match any specific criterion but is an application of 3D reconstruction techniques to tornado visualization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18060": {
        "authors": [
            "Olivia Zumsteg",
            "Nico Graf",
            "Aaron Haeusler",
            "Norbert Kirchgessner",
            "Nicola Storni",
            "Lukas Roth",
            "Andreas Hund"
        ],
        "title": "Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes",
        "abstract": "arXiv:2506.18060v1 Announce Type: new  Abstract: Estimating three-dimensional morphological traits from two-dimensional RGB images presents inherent challenges due to the loss of depth information, projection distortions, and occlusions under field conditions. In this work, we explore multiple approaches for non-destructive volume estimation of wheat spikes, using RGB image sequences and structured-light 3D scans as ground truth references. Due to the complex geometry of the spikes, we propose a neural network approach for volume estimation in 2D images, employing a transfer learning pipeline that combines DINOv2, a self-supervised Vision Transformer, with a unidirectional Long Short-Term Memory (LSTM) network. By using deep supervision, the model is able to learn more robust intermediate representations, which enhances its generalisation ability across varying evaluation sequences. We benchmark our model against two conventional baselines: a 2D area-based projection and a geometric reconstruction using axis-aligned cross-sections. Our deep supervised model achieves a mean absolute percentage error (MAPE) of 6.46% on six-view indoor images, outperforming the area (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on field-based single-image data enables domain adaptation, yielding a MAPE of 10.82%. We demonstrate that object shape significantly impacts volume prediction accuracy, with irregular geometries such as wheat spikes posing greater challenges for geometric methods compared to our deep learning approach.",
        "arxiv_id": "2506.18060",
        "ARXIVID": "2506.18060",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision applications in agriculture.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17596": {
        "authors": [
            "Wei Huang",
            "Yinxuan Xu",
            "Yintao Zhou",
            "Zhengyu Li",
            "Jing Huang",
            "Meng Pang"
        ],
        "title": "A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data",
        "abstract": "arXiv:2506.17596v1 Announce Type: new  Abstract: Parkinson's disease (PD), characterized by its incurable nature, rapid progression, and severe disability, poses significant challenges to the lives of patients and their families. Given the aging population, the need for early detection of PD is increasing. In vitro diagnosis has garnered attention due to its non-invasive nature and low cost. However, existing methods present several challenges: 1) limited training data for facial expression diagnosis; 2) specialized equipment and acquisition environments required for gait diagnosis, resulting in poor generalizability; 3) the risk of misdiagnosis or missed diagnosis when relying on a single modality. To address these issues, we propose a novel multimodal in vitro diagnostic method for PD, leveraging facial expressions and behavioral gait. Our method employs a lightweight deep learning model for feature extraction and fusion, aimed at improving diagnostic accuracy and facilitating deployment on mobile devices. Furthermore, we have established the largest multimodal PD dataset in collaboration with a hospital and conducted extensive experiments to validate the effectiveness of our proposed method.",
        "arxiv_id": "2506.17596",
        "ARXIVID": "2506.17596",
        "COMMENT": "Does not match any specific criterion but is an application of multimodal learning in a medical context.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17694": {
        "authors": [
            "Gnana Praveen Rajasekhar",
            "Jahangir Alam"
        ],
        "title": "SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification",
        "abstract": "arXiv:2506.17694v1 Announce Type: new  Abstract: Conventional audio-visual methods for speaker verification rely on large amounts of labeled data and separate modality-specific architectures, which is computationally expensive, limiting their scalability. To address these problems, we propose a self-supervised learning framework based on contrastive learning with asymmetric masking and masked data modeling to obtain robust audiovisual feature representations. In particular, we employ a unified framework for self-supervised audiovisual speaker verification using a single shared backbone for audio and visual inputs, leveraging the versatility of vision transformers. The proposed unified framework can handle audio, visual, or audiovisual inputs using a single shared vision transformer backbone during training and testing while being computationally efficient and robust to missing modalities. Extensive experiments demonstrate that our method achieves competitive performance without labeled data while reducing computational costs compared to traditional approaches.",
        "arxiv_id": "2506.17694",
        "ARXIVID": "2506.17694",
        "COMMENT": "Does not match any specific criteria. Focuses on self-supervised audio-visual speaker verification, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18882": {
        "authors": [
            "Hong Li",
            "Houyuan Chen",
            "Chongjie Ye",
            "Zhaoxi Chen",
            "Bohan Li",
            "Shaocong Xu",
            "Xianda Guo",
            "Xuhui Liu",
            "Yikai Wang",
            "Baochang Zhang",
            "Satoshi Ikehata",
            "Boxin Shi",
            "Anyi Rao",
            "Hao Zhao"
        ],
        "title": "Light of Normals: Unified Feature Representation for Universal Photometric Stereo",
        "abstract": "arXiv:2506.18882v1 Announce Type: new  Abstract: Universal photometric stereo (PS) aims to recover high-quality surface normals from objects under arbitrary lighting conditions without relying on specific illumination models. Despite recent advances such as SDM-UniPS and Uni MS-PS, two fundamental challenges persist: 1) the deep coupling between varying illumination and surface normal features, where ambiguity in observed intensity makes it difficult to determine whether brightness variations stem from lighting changes or surface orientation; and 2) the preservation of high-frequency geometric details in complex surfaces, where intricate geometries create self-shadowing, inter-reflections, and subtle normal variations that conventional feature processing operations struggle to capture accurately.",
        "arxiv_id": "2506.18882",
        "ARXIVID": "2506.18882",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and photometric stereo.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18183": {
        "authors": [
            "Zhiting Mei",
            "Christina Zhang",
            "Tenny Yin",
            "Justin Lidard",
            "Ola Shorinwa",
            "Anirudha Majumdar"
        ],
        "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?",
        "abstract": "arXiv:2506.18183v1 Announce Type: new  Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.",
        "arxiv_id": "2506.18183",
        "ARXIVID": "2506.18183",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reasoning and uncertainty quantification in language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18658": {
        "authors": [
            "Ling Zhang",
            "Boxiang Yun",
            "Qingli Li",
            "Yan Wang"
        ],
        "title": "Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation",
        "abstract": "arXiv:2506.18658v1 Announce Type: new  Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces two key challenges: (1) lack of semantic content in visual features and (2) inherent information redundancy in WSIs. To address these issues, we propose a novel Historical Report Guided \\textbf{Bi}-modal Concurrent Learning Framework for Pathology Report \\textbf{Gen}eration (BiGen) emulating pathologists' diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to provide rich semantic content, which retrieves WSI-relevant knowledge from pre-built medical knowledge bank by matching high-attention patches and (2) A bi-modal concurrent learning strategy instantiated via a learnable visual token and a learnable textual token to dynamically extract key visual features and retrieved knowledge, where weight-shared layers enable cross-modal alignment between visual features and knowledge features. Our multi-modal decoder integrates both modals for comprehensive diagnostic reports generation. Experiments on the PathText (BRCA) dataset demonstrate our framework's superiority, achieving state-of-the-art performance with 7.4\\% relative improvement in NLP metrics and 19.1\\% enhancement in classification metrics for Her-2 prediction versus existing methods. Ablation studies validate the necessity of our proposed modules, highlighting our method's ability to provide WSI-relevant rich semantic content and suppress information redundancy in WSIs. Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.",
        "arxiv_id": "2506.18658",
        "ARXIVID": "2506.18658",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multi-modal learning and generative modeling in a specific domain (pathology).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18437": {
        "authors": [
            "Sijin He",
            "Guangfeng Lin",
            "Tao Li",
            "Yajun Chen"
        ],
        "title": "Frequency-Domain Fusion Transformer for Image Inpainting",
        "abstract": "arXiv:2506.18437v1 Announce Type: new  Abstract: Image inpainting plays a vital role in restoring missing image regions and supporting high-level vision tasks, but traditional methods struggle with complex textures and large occlusions. Although Transformer-based approaches have demonstrated strong global modeling capabilities, they often fail to preserve high-frequency details due to the low-pass nature of self-attention and suffer from high computational costs. To address these challenges, this paper proposes a Transformer-based image inpainting method incorporating frequency-domain fusion. Specifically, an attention mechanism combining wavelet transform and Gabor filtering is introduced to enhance multi-scale structural modeling and detail preservation. Additionally, a learnable frequency-domain filter based on the fast Fourier transform is designed to replace the feedforward network, enabling adaptive noise suppression and detail retention. The model adopts a four-level encoder-decoder structure and is guided by a novel loss strategy to balance global semantics and fine details. Experimental results demonstrate that the proposed method effectively improves the quality of image inpainting by preserving more high-frequency information.",
        "arxiv_id": "2506.18437",
        "ARXIVID": "2506.18437",
        "COMMENT": "Does not match any specific criteria. Focuses on image inpainting with frequency-domain techniques, which is not directly related to the listed topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17500": {
        "authors": [
            "Julio Silva-Rodr\\'iguez",
            "Fereshteh Shakeri",
            "Houda Bahig",
            "Jose Dolz",
            "Ismail Ben Ayed"
        ],
        "title": "Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation",
        "abstract": "arXiv:2506.17500v1 Announce Type: new  Abstract: Vision-language models (VLMs) are gaining attention in medical image analysis. These are pre-trained on large, heterogeneous data sources, yielding rich and transferable representations. Notably, the combination of modality-specialized VLMs with few-shot adaptation has provided fruitful results, enabling the efficient deployment of high-performing solutions. However, previous works on this topic make strong assumptions about the distribution of adaptation data, which are unrealistic in the medical domain. First, prior art assumes access to a balanced support set, a condition that breaks the natural imbalance in disease prevalence found in real-world scenarios. Second, these works typically assume the presence of an additional validation set to fix critical hyper-parameters, which is highly data-inefficient. This work challenges these favorable deployment scenarios and introduces a realistic, imbalanced, validation-free adaptation setting. Our extensive benchmark across various modalities and downstream tasks demonstrates that current methods systematically compromise their performance when operating under realistic conditions, occasionally even performing worse than zero-shot inference. Also, we introduce a training-free linear probe that adaptively blends visual and textual supervision. Detailed studies demonstrate that the proposed solver is a strong, efficient baseline, enabling robust adaptation in challenging scenarios.",
        "arxiv_id": "2506.17500",
        "ARXIVID": "2506.17500",
        "COMMENT": "Does not match any specific criterion but is related to vision-language models in medical imaging, which is tangentially relevant to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18042": {
        "authors": [
            "Dongdong Meng",
            "Sheng Li",
            "Hao Wu",
            "Suqing Tian",
            "Wenjun Ma",
            "Guoping Wang",
            "Xueqing Yan"
        ],
        "title": "CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images",
        "abstract": "arXiv:2506.18042v1 Announce Type: new  Abstract: Accurate automatic medical image segmentation relies on high-quality, dense annotations, which are costly and time-consuming. Weakly supervised learning provides a more efficient alternative by leveraging sparse and coarse annotations instead of dense, precise ones. However, segmentation performance degradation and overfitting caused by sparse annotations remain key challenges. To address these issues, we propose CmFNet, a novel 3D weakly supervised cross-modal medical image segmentation approach. CmFNet consists of three main components: a modality-specific feature learning network, a cross-modal feature learning network, and a hybrid-supervised learning strategy. Specifically, the modality-specific feature learning network and the cross-modal feature learning network effectively integrate complementary information from multi-modal images, enhancing shared features across modalities to improve segmentation performance. Additionally, the hybrid-supervised learning strategy guides segmentation through scribble supervision, intra-modal regularization, and inter-modal consistency, modeling spatial and contextual relationships while promoting feature alignment. Our approach effectively mitigates overfitting, delivering robust segmentation results. It excels in segmenting both challenging small tumor regions and common anatomical structures. Extensive experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset (including CT and MR imaging) and the publicly available CT Whole Abdominal Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly supervised methods. In addition, our approach also outperforms fully supervised methods when full annotation is used. Our approach can facilitate clinical therapy and benefit various specialists, including physicists, radiologists, pathologists, and oncologists.",
        "arxiv_id": "2506.18042",
        "ARXIVID": "2506.18042",
        "COMMENT": "Does not match any specific criterion but is related to weakly-supervised segmentation in medical imaging, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.18669": {
        "authors": [
            "Hao Shao",
            "Qibin Hou"
        ],
        "title": "MedSeg-R: Medical Image Segmentation with Clinical Reasoning",
        "abstract": "arXiv:2506.18669v1 Announce Type: new  Abstract: Medical image segmentation is challenging due to overlapping anatomies with ambiguous boundaries and a severe imbalance between the foreground and background classes, which particularly affects the delineation of small lesions. Existing methods, including encoder-decoder networks and prompt-driven variants of the Segment Anything Model (SAM), rely heavily on local cues or user prompts and lack integrated semantic priors, thus failing to generalize well to low-contrast or overlapping targets. To address these issues, we propose MedSeg-R, a lightweight, dual-stage framework inspired by inspired by clinical reasoning. Its cognitive stage interprets medical report into structured semantic priors (location, texture, shape), which are fused via transformer block. In the perceptual stage, these priors modulate the SAM backbone: spatial attention highlights likely lesion regions, dynamic convolution adapts feature filters to expected textures, and deformable sampling refines spatial support. By embedding this fine-grained guidance early, MedSeg-R disentangles inter-class confusion and amplifies minority-class cues, greatly improving sensitivity to small lesions. In challenging benchmarks, MedSeg-R produces large Dice improvements in overlapping and ambiguous structures, demonstrating plug-and-play compatibility with SAM-based systems.",
        "arxiv_id": "2506.18669",
        "ARXIVID": "2506.18669",
        "COMMENT": "Does not match any specific criterion but focuses on medical image segmentation, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.17969": {
        "authors": [
            "Chenyue Song",
            "Chen Hui",
            "Wei Zhang",
            "Haiqi Zhu",
            "Shaohui Liu",
            "Hong Huang",
            "Feng Jiang"
        ],
        "title": "BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP",
        "abstract": "arXiv:2506.17969v1 Announce Type: new  Abstract: Image Quality Assessment (IQA) aims to evaluate the perceptual quality of images based on human subjective perception. Existing methods generally combine multiscale features to achieve high performance, but most rely on straightforward linear fusion of these features, which may not adequately capture the impact of distortions on semantic content. To address this, we propose a bottom-up image quality assessment approach based on the Contrastive Language-Image Pre-training (CLIP, a recently proposed model that aligns images and text in a shared feature space), named BPCLIP, which progressively extracts the impact of low-level distortions on high-level semantics. Specifically, we utilize an encoder to extract multiscale features from the input image and introduce a bottom-up multiscale cross attention module designed to capture the relationships between shallow and deep features. In addition, by incorporating 40 image quality adjectives across six distinct dimensions, we enable the pre-trained CLIP text encoder to generate representations of the intrinsic quality of the image, thereby strengthening the connection between image quality perception and human language. Our method achieves superior results on most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while demonstrating greater robustness.",
        "arxiv_id": "2506.17969",
        "ARXIVID": "2506.17969",
        "COMMENT": "Does not match any specific criterion but is generally related to computer vision and image quality assessment.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}