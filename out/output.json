{
    "2507.17680": {
        "authors": [
            "Yongchao Zeng",
            "Calum Brown",
            "Ioannis Kyriakou",
            "Ronja Hotz",
            "Mark Rounsevell"
        ],
        "title": "Simulating multiple human perspectives in socio-ecological systems using large language models",
        "abstract": "arXiv:2507.17680v1 Announce Type: new  Abstract: Understanding socio-ecological systems requires insights from diverse stakeholder perspectives, which are often hard to access. To enable alternative, simulation-based exploration of different stakeholder perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting) modelling framework. HoPeS employs agents powered by large language models (LLMs) to represent various stakeholders; users can step into the agent roles to experience perspectival differences. A simulation protocol serves as a \"scaffold\" to streamline multiple perspective-taking simulations, supporting users in reflecting on, transitioning between, and integrating across perspectives. A prototype system is developed to demonstrate HoPeS in the context of institutional dynamics and land use change, enabling both narrative-driven and numerical experiments. In an illustrative experiment, a user successively adopts the perspectives of a system observer and a researcher - a role that analyses data from the embedded land use model to inform evidence-based decision-making for other LLM agents representing various institutions. Despite the user's effort to recommend technically sound policies, discrepancies persist between the policy recommendation and implementation due to stakeholders' competing advocacies, mirroring real-world misalignment between researcher and policymaker perspectives. The user's reflection highlights the subjective feelings of frustration and disappointment as a researcher, especially due to the challenge of maintaining political neutrality while attempting to gain political influence. Despite this, the user exhibits high motivation to experiment with alternative narrative framing strategies, suggesting the system's potential in exploring different perspectives. Further system and protocol refinement are likely to enable new forms of interdisciplinary collaboration in socio-ecological simulations.",
        "arxiv_id": "2507.17680",
        "ARXIVID": "2507.17680",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) and 5 (Integration of Image/Video and Large Language Models) due to its use of LLMs for simulating human perspectives and integrating narrative-driven and numerical experiments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.17462": {
        "authors": [
            "Chang Nie",
            "Guangming Wang",
            "Zhe Lie",
            "Hesheng Wang"
        ],
        "title": "ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents",
        "abstract": "arXiv:2507.17462v1 Announce Type: new  Abstract: Robot imitation learning relies on 4D multi-view sequential images. However, the high cost of data collection and the scarcity of high-quality data severely constrain the generalization and application of embodied intelligence policies like Vision-Language-Action (VLA) models. Data augmentation is a powerful strategy to overcome data scarcity, but methods for editing 4D multi-view sequential images for manipulation tasks are currently lacking. Thus, we propose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation framework that efficiently edits an entire multi-view sequence based on single-frame editing and robot state conditions. This task presents three core challenges: (1) maintaining geometric and appearance consistency across dynamic views and long time horizons; (2) expanding the working window with low computational costs; and (3) ensuring the semantic integrity of critical objects like the robot arm. ERMV addresses these challenges through a series of innovations. First, to ensure spatio-temporal consistency in motion blur, we introduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that learns pixel shift caused by movement before applying geometric constraints. Second, to maximize the editing working window, ERMV pioneers a Sparse Spatio-Temporal (STT) module, which decouples the temporal and spatial views and remodels a single-frame multi-view problem through sparse sampling of the views to reduce computational demands. Third, to alleviate error accumulation, we incorporate a feedback intervention Mechanism, which uses a Multimodal Large Language Model (MLLM) to check editing inconsistencies and request targeted expert guidance only when necessary. Extensive experiments demonstrate that ERMV-augmented data significantly boosts the robustness and generalization of VLA models in both simulated and real-world environments.",
        "arxiv_id": "2507.17462",
        "ARXIVID": "2507.17462",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel data augmentation framework for embodied agents and addresses spatial reasoning challenges in 4D multi-view sequential images.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.16863": {
        "authors": [
            "Hongcheng Gao",
            "Zihao Huang",
            "Lin Xu",
            "Jingyi Tang",
            "Xinhao Li",
            "Yue Liu",
            "Haoyang Li",
            "Taihang Hu",
            "Minhua Lin",
            "Xinlong Yang",
            "Ge Wu",
            "Balong Bi",
            "Hongyu Chen",
            "Wentao Zhang"
        ],
        "title": "Pixels, Patterns, but No Poetry: To See The World like Humans",
        "abstract": "arXiv:2507.16863v1 Announce Type: new  Abstract: Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge in artificial intelligence. While recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do? This paper shifts focus from reasoning to perception. Rather than constructing benchmarks specifically for reasoning, we introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively. Our findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks trivial for humans. Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting that our benchmark poses challenges for vision tower generalization rather than for the knowledge and reasoning capabilities of the language backbone-a key gap between current MLLMs and human perception. We release a representative subset of TET tasks in this version, and will introduce more diverse tasks and methods to enhance visual generalization in future work.",
        "arxiv_id": "2507.16863",
        "ARXIVID": "2507.16863",
        "COMMENT": "Matches criteria 2 as it evaluates perception in multimodal large language models and introduces a new benchmark.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.17664": {
        "authors": [
            "Lingdong Kong",
            "Dongyue Lu",
            "Ao Liang",
            "Rong Li",
            "Yuhao Dong",
            "Tianshuai Hu",
            "Lai Xing Ng",
            "Wei Tsang Ooi",
            "Benoit R. Cottereau"
        ],
        "title": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras",
        "abstract": "arXiv:2507.17664v1 Announce Type: new  Abstract: Event cameras offer microsecond-level latency and robustness to motion blur, making them ideal for understanding dynamic environments. Yet, connecting these asynchronous streams to human language remains an open challenge. We introduce Talk2Event, the first large-scale benchmark for language-driven object grounding in event-based perception. Built from real-world driving data, we provide over 30,000 validated referring expressions, each enriched with four grounding attributes -- appearance, status, relation to viewer, and relation to other objects -- bridging spatial, temporal, and relational reasoning. To fully exploit these cues, we propose EventRefer, an attribute-aware grounding framework that dynamically fuses multi-attribute representations through a Mixture of Event-Attribute Experts (MoEE). Our method adapts to different modalities and scene dynamics, achieving consistent gains over state-of-the-art baselines in event-only, frame-only, and event-frame fusion settings. We hope our dataset and approach will establish a foundation for advancing multimodal, temporally-aware, and language-driven perception in real-world robotics and autonomy.",
        "arxiv_id": "2507.17664",
        "ARXIVID": "2507.17664",
        "COMMENT": "Matches criteria 1 (Spatial Intelligence and Embodied Agents) and 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on event-based perception and language-driven object grounding in dynamic scenes.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17342": {
        "authors": [
            "Bozhou Zhang",
            "Nan Song",
            "Xiatian Zhu",
            "Li Zhang"
        ],
        "title": "DeMo++: Motion Decoupling for Autonomous Driving",
        "abstract": "arXiv:2507.17342v1 Announce Type: new  Abstract: Motion forecasting and planning are tasked with estimating the trajectories of traffic agents and the ego vehicle, respectively, to ensure the safety and efficiency of autonomous driving systems in dynamically changing environments. State-of-the-art methods typically adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-mode trajectories. While this paradigm can produce diverse motion intentions, it often falls short in modeling the intricate spatiotemporal evolution of trajectories, which can lead to collisions or suboptimal outcomes. To overcome this limitation, we propose DeMo++, a framework that decouples motion estimation into two distinct components: holistic motion intentions to capture the diverse potential directions of movement, and fine spatiotemporal states to track the agent's dynamic progress within the scene and enable a self-refinement capability. Further, we introduce a cross-scene trajectory interaction mechanism to explore the relationships between motions in adjacent scenes. This allows DeMo++ to comprehensively model both the diversity of motion intentions and the spatiotemporal evolution of each trajectory. To effectively implement this framework, we developed a hybrid model combining Attention and Mamba. This architecture leverages the strengths of both mechanisms for efficient scene information aggregation and precise trajectory state sequence modeling. Extensive experiments demonstrate that DeMo++ achieves state-of-the-art performance across various benchmarks, including motion forecasting (Argoverse 2 and nuScenes), motion planning (nuPlan), and end-to-end planning (NAVSIM).",
        "arxiv_id": "2507.17342",
        "ARXIVID": "2507.17342",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (DeMo++) for motion forecasting and planning in autonomous driving, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.16940": {
        "authors": [
            "Nima Fathi",
            "Amar Kumar",
            "Tal Arbel"
        ],
        "title": "AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation",
        "abstract": "arXiv:2507.16940v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm shift from static prediction systems to agentic AI agents capable of reasoning, interacting with tools, and adapting to complex tasks. While LLM-based agentic systems have shown promise across many domains, their application to medical imaging remains in its infancy. In this work, we introduce AURA, the first visual linguistic explainability agent designed specifically for comprehensive analysis, explanation, and evaluation of medical images. By enabling dynamic interactions, contextual explanations, and hypothesis testing, AURA represents a significant advancement toward more transparent, adaptable, and clinically aligned AI systems. We highlight the promise of agentic AI in transforming medical image analysis from static predictions to interactive decision support. Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular toolbox comprising: (i) a segmentation suite with phase grounding, pathology segmentation, and anatomy segmentation to localize clinically meaningful regions; (ii) a counterfactual image-generation module that supports reasoning through image-level explanations; and (iii) a set of evaluation tools including pixel-wise difference-map analysis, classification, and advanced state-of-the-art components to assess diagnostic relevance and visual interpretability.",
        "arxiv_id": "2507.16940",
        "ARXIVID": "2507.16940",
        "COMMENT": "Matches criterion 2 as it explores a multimodal large language model (AURA) for medical imaging, integrating vision and language.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17613": {
        "authors": [
            "Xiaoxue Chen",
            "Bhargav Chandaka",
            "Chih-Hao Lin",
            "Ya-Qin Zhang",
            "David Forsyth",
            "Hao Zhao",
            "Shenlong Wang"
        ],
        "title": "InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling",
        "abstract": "arXiv:2507.17613v1 Announce Type: new  Abstract: We present InvRGB+L, a novel inverse rendering model that reconstructs large, relightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional inverse graphics methods rely primarily on RGB observations and use LiDAR mainly for geometric information, often resulting in suboptimal material estimates due to visible light interference. We find that LiDAR's intensity values-captured with active illumination in a different spectral range-offer complementary cues for robust material estimation under variable lighting. Inspired by this, InvRGB+L leverages LiDAR intensity cues to overcome challenges inherent in RGB-centric inverse graphics through two key innovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR material consistency losses. The model produces novel-view RGB and LiDAR renderings of urban and indoor scenes and supports relighting, night simulations, and dynamic object insertions, achieving results that surpass current state-of-the-art methods in both scene-level urban inverse rendering and LiDAR simulation.",
        "arxiv_id": "2507.17613",
        "ARXIVID": "2507.17613",
        "COMMENT": "Matches criteria 4 as it focuses on inverse rendering and LiDAR reflectance modeling, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17722": {
        "authors": [
            "Malsha Ashani Mahawatta Dona",
            "Beatriz Cabrero-Daniel",
            "Yinan Yu",
            "Christian Berger"
        ],
        "title": "BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems",
        "abstract": "arXiv:2507.17722v1 Announce Type: new  Abstract: Large language models (LLMs) are growingly extended to process multimodal data such as text and video simultaneously. Their remarkable performance in understanding what is shown in images is surpassing specialized neural networks (NNs) such as Yolo that is supporting only a well-formed but very limited vocabulary, ie., objects that they are able to detect. When being non-restricted, LLMs and in particular state-of-the-art vision language models (VLMs) show impressive performance to describe even complex traffic situations. This is making them potentially suitable components for automotive perception systems to support the understanding of complex traffic situations or edge case situation. However, LLMs and VLMs are prone to hallucination, which mean to either potentially not seeing traffic agents such as vulnerable road users who are present in a situation, or to seeing traffic agents who are not there in reality. While the latter is unwanted making an ADAS or autonomous driving systems (ADS) to unnecessarily slow down, the former could lead to disastrous decisions from an ADS. In our work, we are systematically assessing the performance of 3 state-of-the-art VLMs on a diverse subset of traffic situations sampled from the Waymo Open Dataset to support safety guardrails for capturing such hallucinations in VLM-supported perception systems. We observe that both, proprietary and open VLMs exhibit remarkable image understanding capabilities even paying thorough attention to fine details sometimes difficult to spot for us humans. However, they are also still prone to making up elements in their descriptions to date requiring hallucination detection strategies such as BetterCheck that we propose in our work.",
        "arxiv_id": "2507.17722",
        "ARXIVID": "2507.17722",
        "COMMENT": "Matches criteria 2 as it evaluates vision-language models (VLMs) in the context of automotive perception systems and proposes a hallucination detection strategy.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17659": {
        "authors": [
            "Junjie Wang",
            "Yunhan Tang",
            "Yijie Wang",
            "Zhihao Yuan",
            "Huan Wang",
            "Yangfan He",
            "Bin Li"
        ],
        "title": "See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering",
        "abstract": "arXiv:2507.17659v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have pushed the frontiers of Knowledge-Based Visual Question Answering (KBVQA), yet their reasoning is fundamentally bottlenecked by a reliance on uni-dimensional evidence. This \"seeing only the trees, but not the forest\" approach prevents robust, multi-faceted understanding. Inspired by the principle of seeing both the forest and trees, we propose Synergos-VQA, a novel synergistic reasoning framework. At its core, Synergos-VQA concurrently generates and fuses three complementary evidence streams at inference time: (1) Holistic Evidence to perceive the entire scene (the \"forest\"), (2) Structural Evidence from a prototype-driven module to identify key objects (the \"trees\"), and (3) Causal Evidence from a counterfactual probe to ensure the reasoning is robustly grounded. By synergistically fusing this multi-faceted evidence, our framework achieves a more comprehensive and reliable reasoning process. Extensive experiments show that Synergos-VQA decisively establishes a new state-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA. Furthermore, our approach demonstrates strong plug-and-play capabilities, significantly boosting various open-source MLLMs and proving that superior methodological design can outperform sheer model scale.",
        "arxiv_id": "2507.17659",
        "ARXIVID": "2507.17659",
        "COMMENT": "Matches criterion 2 as it proposes a novel reasoning framework for Multimodal Large Language Models (MLLMs) in visual question answering.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17239": {
        "authors": [
            "Lei Zhu",
            "Jun Zhou",
            "Rick Siow Mong Goh",
            "Yong Liu"
        ],
        "title": "MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training",
        "abstract": "arXiv:2507.17239v1 Announce Type: new  Abstract: Foundation models have recently gained tremendous popularity in medical image analysis. State-of-the-art methods leverage either paired image-text data via vision-language pre-training or unpaired image data via self-supervised pre-training to learn foundation models with generalizable image features to boost downstream task performance. However, learning foundation models exclusively on either paired or unpaired image data limits their ability to learn richer and more comprehensive image features. In this paper, we investigate a novel task termed semi-supervised vision-language pre-training, aiming to fully harness the potential of both paired and unpaired image data for foundation model learning. To this end, we propose MaskedCLIP, a synergistic masked image modeling and contrastive language-image pre-training framework for semi-supervised vision-language pre-training. The key challenge in combining paired and unpaired image data for learning a foundation model lies in the incompatible feature spaces derived from these two types of data. To address this issue, we propose to connect the masked feature space with the CLIP feature space with a bridge transformer. In this way, the more semantic specific CLIP features can benefit from the more general masked features for semantic feature extraction. We further propose a masked knowledge distillation loss to distill semantic knowledge of original image features in CLIP feature space back to the predicted masked image features in masked feature space. With this mutually interactive design, our framework effectively leverages both paired and unpaired image data to learn more generalizable image features for downstream tasks. Extensive experiments on retinal image analysis demonstrate the effectiveness and data efficiency of our method.",
        "arxiv_id": "2507.17239",
        "ARXIVID": "2507.17239",
        "COMMENT": "Matches criteria 5 as it combines masked image modeling and contrastive language-image pre-training for medical vision-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17456": {
        "authors": [
            "Francesco Tonini",
            "Lorenzo Vaquero",
            "Alessandro Conti",
            "Cigdem Beyan",
            "Elisa Ricci"
        ],
        "title": "Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection",
        "abstract": "arXiv:2507.17456v1 Announce Type: new  Abstract: Human-Object Interaction (HOI) detection aims to identify humans and objects within images and interpret their interactions. Existing HOI methods rely heavily on large datasets with manual annotations to learn interactions from visual cues. These annotations are labor-intensive to create, prone to inconsistency, and limit scalability to new domains and rare interactions. We argue that recent advances in Vision-Language Models (VLMs) offer untapped potential, particularly in enhancing interaction representation. While prior work has injected such potential and even proposed training-free methods, there remain key gaps. Consequently, we propose a novel training-free HOI detection framework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively utilizes textual and visual interaction representations within a multimodal registry, enabling robust and nuanced interaction understanding. This registry incorporates a small set of visual cues and uses innovative interaction signatures to improve the semantic alignment of verbs, facilitating effective generalization to rare interactions. Additionally, we propose a unique multi-head attention mechanism that adaptively weights the contributions of the visual and textual features. Experimental results demonstrate that our DYSCO surpasses training-free state-of-the-art models and is competitive with training-based approaches, particularly excelling in rare interactions. Code is available at https://github.com/francescotonini/dysco.",
        "arxiv_id": "2507.17456",
        "ARXIVID": "2507.17456",
        "COMMENT": "Matches criteria 5 as it proposes a training-free framework for human-object interaction detection using vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17585": {
        "authors": [
            "Anna-Maria Halacheva",
            "Jan-Nico Zaech",
            "Sombit Dey",
            "Luc Van Gool",
            "Danda Pani Paudel"
        ],
        "title": "From Scan to Action: Leveraging Realistic Scans for Embodied Scene Understanding",
        "abstract": "arXiv:2507.17585v1 Announce Type: new  Abstract: Real-world 3D scene-level scans offer realism and can enable better real-world generalizability for downstream applications. However, challenges such as data volume, diverse annotation formats, and tool compatibility limit their use. This paper demonstrates a methodology to effectively leverage these scans and their annotations. We propose a unified annotation integration using USD, with application-specific USD flavors. We identify challenges in utilizing holistic real-world scan datasets and present mitigation strategies. The efficacy of our approach is demonstrated through two downstream applications: LLM-based scene editing, enabling effective LLM understanding and adaptation of the data (80% success), and robotic simulation, achieving an 87% success rate in policy learning.",
        "arxiv_id": "2507.17585",
        "ARXIVID": "2507.17585",
        "COMMENT": "Matches criteria 1 and 3 as it focuses on embodied scene understanding and robotic simulation using realistic 3D scans.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.17377": {
        "authors": [
            "Peng Wu",
            "Qiuxia Lai",
            "Hao Fang",
            "Guo-Sen Xie",
            "Yilong Yin",
            "Xiankai Lu",
            "Wenguan Wang"
        ],
        "title": "A Conditional Probability Framework for Compositional Zero-shot Learning",
        "abstract": "arXiv:2507.17377v1 Announce Type: new  Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations of known objects and attributes by leveraging knowledge from previously seen compositions. Traditional approaches primarily focus on disentangling attributes and objects, treating them as independent entities during learning. However, this assumption overlooks the semantic constraints and contextual dependencies inside a composition. For example, certain attributes naturally pair with specific objects (e.g., \"striped\" applies to \"zebra\" or \"shirts\" but not \"sky\" or \"water\"), while the same attribute can manifest differently depending on context (e.g., \"young\" in \"young tree\" vs. \"young dog\"). Thus, capturing attribute-object interdependence remains a fundamental yet long-ignored challenge in CZSL. In this paper, we adopt a Conditional Probability Framework (CPF) to explicitly model attribute-object dependencies. We decompose the probability of a composition into two components: the likelihood of an object and the conditional likelihood of its attribute. To enhance object feature learning, we incorporate textual descriptors to highlight semantically relevant image regions. These enhanced object features then guide attribute learning through a cross-attention mechanism, ensuring better contextual alignment. By jointly optimizing object likelihood and conditional attribute likelihood, our method effectively captures compositional dependencies and generalizes well to unseen compositions. Extensive experiments on multiple CZSL benchmarks demonstrate the superiority of our approach. Code is available at here.",
        "arxiv_id": "2507.17377",
        "ARXIVID": "2507.17377",
        "COMMENT": "Matches criteria 2 (Visual and Multimodal Large Language Models) as it explores compositional zero-shot learning with a focus on contextual dependencies, which aligns with vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.17012": {
        "authors": [
            "Zhihan Zhang",
            "Alexander Metzger",
            "Yuxuan Mei",
            "Felix H\\\"ahnlein",
            "Zachary Englhardt",
            "Tingyu Cheng",
            "Gregory D. Abowd",
            "Shwetak Patel",
            "Adriana Schulz",
            "Vikram Iyer"
        ],
        "title": "Towards Autonomous Sustainability Assessment via Multimodal AI Agents",
        "abstract": "arXiv:2507.17012v1 Announce Type: new  Abstract: Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.",
        "arxiv_id": "2507.17012",
        "ARXIVID": "2507.17012",
        "COMMENT": "Matches criterion 2 as it explores multimodal AI agents for sustainability assessment, integrating vision and language.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.17343": {
        "authors": [
            "Xiaohao Liu",
            "Xiaobo Xia",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "title": "Principled Multimodal Representation Learning",
        "abstract": "arXiv:2507.17343v1 Announce Type: new  Abstract: Multimodal representation learning seeks to create a unified representation space by integrating diverse data modalities to improve multimodal understanding. Traditional methods often depend on pairwise contrastive learning, which relies on a predefined anchor modality, restricting alignment across all modalities. Recent advances have investigated the simultaneous alignment of multiple modalities, yet several challenges remain, such as limitations imposed by fixed anchor points and instability arising from optimizing the product of singular values. To address the challenges, in this paper, we propose Principled Multimodal Representation Learning (PMRL), a novel framework that achieves simultaneous alignment of multiple modalities without anchor dependency in a more stable manner. Specifically, grounded in the theoretical insight that full alignment corresponds to a rank-1 Gram matrix, PMRL optimizes the dominant singular value of the representation matrix to align modalities along a shared leading direction. We propose a softmax-based loss function that treats singular values as logits to prioritize the largest singular value. Besides, instance-wise contrastive regularization on the leading eigenvectors maintains inter-instance separability and prevents representation collapse. Extensive experiments across diverse tasks demonstrate PMRL's superiority compared to baseline methods. The source code will be publicly available.",
        "arxiv_id": "2507.17343",
        "ARXIVID": "2507.17343",
        "COMMENT": "Matches criteria 2 as it proposes a novel framework for multimodal representation learning, which aligns with vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.17220": {
        "authors": [
            "Jiansong Wan",
            "Chengming Zhou",
            "Jinkua Liu",
            "Xiangge Huang",
            "Xiaoyu Chen",
            "Xiaohan Yi",
            "Qisen Yang",
            "Baiting Zhu",
            "Xin-Qiang Cai",
            "Lixing Liu",
            "Rushuai Yang",
            "Chuheng Zhang",
            "Sherif Abdelfattah",
            "Hayong Shin",
            "Pushi Zhang",
            "Li Zhao",
            "Jiang Bian"
        ],
        "title": "PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models",
        "abstract": "arXiv:2507.17220v1 Announce Type: new  Abstract: Recent studies have explored pretrained (foundation) models for vision-based robotic navigation, aiming to achieve generalizable navigation and positive transfer across diverse environments while enhancing zero-shot performance in unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal Navigation), a new approach that further investigates pretraining strategies for vision-based navigation models and contributes in two key areas. Model-wise, we identify two critical design choices that consistently improve the performance of pretrained navigation models: (1) integrating an early-fusion network structure to combine visual observations and goal images via appropriately pretrained Vision Transformer (ViT) image encoder, and (2) introducing suitable auxiliary tasks to enhance global navigation representation learning, thus further improving navigation performance. Dataset-wise, we propose a novel data preprocessing pipeline for efficiently labeling large-scale game video datasets for navigation model training. We demonstrate that augmenting existing open navigation datasets with diverse gameplay videos improves model performance. Our model achieves an average improvement of 22.6% in zero-shot settings and a 37.5% improvement in fine-tuning settings over existing visual navigation foundation models in two complex simulated environments and one real-world environment. These results advance the state-of-the-art in pretrained image-goal navigation models. Notably, our model maintains competitive performance while requiring significantly less fine-tuning data, highlighting its potential for real-world deployment with minimal labeled supervision.",
        "arxiv_id": "2507.17220",
        "ARXIVID": "2507.17220",
        "COMMENT": "Matches criterion 1 as it explores pretrained models for vision-based robotic navigation, focusing on spatial intelligence for embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.17436": {
        "authors": [
            "Yehao Lu",
            "Minghe Weng",
            "Zekang Xiao",
            "Rui Jiang",
            "Wei Su",
            "Guangcong Zheng",
            "Ping Lu",
            "Xi Li"
        ],
        "title": "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection",
        "abstract": "arXiv:2507.17436v1 Announce Type: new  Abstract: The Mixture of Experts (MoE) architecture has excelled in Large Vision-Language Models (LVLMs), yet its potential in real-time open-vocabulary object detectors, which also leverage large-scale vision-language datasets but smaller models, remains unexplored. This work investigates this domain, revealing intriguing insights. In the shallow layers, experts tend to cooperate with diverse peers to expand the search space. While in the deeper layers, fixed collaborative structures emerge, where each expert maintains 2-3 fixed partners and distinct expert combinations are specialized in processing specific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding DINO 1.5 Edge from a dense model to a dynamic inference framework via an efficient MoE-Tuning strategy. Additionally, we design a granularity decomposition mechanism to decompose the Feed-Forward Network (FFN) of base model into multiple smaller expert networks, expanding the subnet search space. To prevent performance degradation at the start of fine-tuning, we further propose a pre-trained weight allocation strategy for the experts, coupled with a specific router initialization. During inference, only the input-relevant experts are activated to form a compact subnet. Experiments show that, pretrained with merely 1.56M open-source data, Dynamic-DINO outperforms Grounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.",
        "arxiv_id": "2507.17436",
        "ARXIVID": "2507.17436",
        "COMMENT": "Matches criterion 4 as it focuses on a novel fine-grained mixture of experts tuning for vision foundation models in object detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.17596": {
        "authors": [
            "Maciej K. Wozniak",
            "Lianhang Liu",
            "Yixi Cai",
            "Patric Jensfelt"
        ],
        "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
        "abstract": "arXiv:2507.17596v1 Announce Type: new  Abstract: While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at https://maxiuw.github.io/prix.",
        "arxiv_id": "2507.17596",
        "ARXIVID": "2507.17596",
        "COMMENT": "Matches criteria 3 as it introduces a novel end-to-end autonomous driving architecture using raw pixel inputs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.17402": {
        "authors": [
            "Li Jun",
            "Wang Jinpeng",
            "Tan Chaolei",
            "Lian Niu",
            "Chen Long",
            "Zhang Min",
            "Wang Yaowei",
            "Xia Shu-Tao",
            "Chen Bin"
        ],
        "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning",
        "abstract": "arXiv:2507.17402v1 Announce Type: new  Abstract: Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of matching untrimmed videos with text queries describing only partial content. Existing methods suffer from geometric distortion in Euclidean space that sometimes misrepresents the intrinsic hierarchical structure of videos and overlooks certain hierarchical semantics, ultimately leading to suboptimal temporal modeling. To address this issue, we propose the first hyperbolic modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space learning to compensate for the suboptimal hierarchical modeling capabilities of Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block and Euclidean Attention Block to encode video embeddings in hybrid spaces, using the Mean-Guided Adaptive Interaction Module to dynamically fuse features. Additionally, we introduce a Partial Order Preservation Loss to enforce \"text < video\" hierarchy through Lorentzian cone constraints. This approach further enhances cross-modal matching by reinforcing partial relevance between video content and text queries. Extensive experiments show that HLFormer outperforms state-of-the-art methods. Code is released at https://github.com/lijun2005/ICCV25-HLFormer.",
        "arxiv_id": "2507.17402",
        "ARXIVID": "2507.17402",
        "COMMENT": "Matches criteria 6 as it introduces a novel methodology for video retrieval with hierarchical modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.17617": {
        "authors": [
            "Yang Li",
            "Zongzheng Zhang",
            "Xuchong Qiu",
            "Xinrun Li",
            "Ziming Liu",
            "Leichen Wang",
            "Ruikai Li",
            "Zhenxin Zhu",
            "Huan-ang Gao",
            "Xiaojian Lin",
            "Zhiyong Cui",
            "Hang Zhao",
            "Hao Zhao"
        ],
        "title": "Reusing Attention for One-stage Lane Topology Understanding",
        "abstract": "arXiv:2507.17617v1 Announce Type: new  Abstract: Understanding lane toplogy relationships accurately is critical for safe autonomous driving. However, existing two-stage methods suffer from inefficiencies due to error propagations and increased computational overheads. To address these challenges, we propose a one-stage architecture that simultaneously predicts traffic elements, lane centerlines and topology relationship, improving both the accuracy and inference speed of lane topology understanding for autonomous driving. Our key innovation lies in reusing intermediate attention resources within distinct transformer decoders. This approach effectively leverages the inherent relational knowledge within the element detection module to enable the modeling of topology relationships among traffic elements and lanes without requiring additional computationally expensive graph networks. Furthermore, we are the first to demonstrate that knowledge can be distilled from models that utilize standard definition (SD) maps to those operates without using SD maps, enabling superior performance even in the absence of SD maps. Extensive experiments on the OpenLane-V2 dataset show that our approach outperforms baseline methods in both accuracy and efficiency, achieving superior results in lane detection, traffic element identification, and topology reasoning. Our code is available at https://github.com/Yang-Li-2000/one-stage.git.",
        "arxiv_id": "2507.17617",
        "ARXIVID": "2507.17617",
        "COMMENT": "Matches criterion 3 as it introduces a novel one-stage architecture for lane topology understanding, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.17594": {
        "authors": [
            "Yuqing Lan",
            "Chenyang Zhu",
            "Shuaifeng Zhi",
            "Jiazhao Zhang",
            "Zhoufeng Wang",
            "Renjiao Yi",
            "Yijie Wang",
            "Kai Xu"
        ],
        "title": "RemixFusion: Residual-based Mixed Representation for Large-scale Online RGB-D Reconstruction",
        "abstract": "arXiv:2507.17594v1 Announce Type: new  Abstract: The introduction of the neural implicit representation has notably propelled the advancement of online dense reconstruction techniques. Compared to traditional explicit representations, such as TSDF, it improves the mapping completeness and memory efficiency. However, the lack of reconstruction details and the time-consuming learning of neural representations hinder the widespread application of neural-based methods to large-scale online reconstruction. We introduce RemixFusion, a novel residual-based mixed representation for scene reconstruction and camera pose estimation dedicated to high-quality and large-scale online RGB-D reconstruction. In particular, we propose a residual-based map representation comprised of an explicit coarse TSDF grid and an implicit neural module that produces residuals representing fine-grained details to be added to the coarse grid. Such mixed representation allows for detail-rich reconstruction with bounded time and memory budget, contrasting with the overly-smoothed results by the purely implicit representations, thus paving the way for high-quality camera tracking. Furthermore, we extend the residual-based representation to handle multi-frame joint pose optimization via bundle adjustment (BA). In contrast to the existing methods, which optimize poses directly, we opt to optimize pose changes. Combined with a novel technique for adaptive gradient amplification, our method attains better optimization convergence and global optimality. Furthermore, we adopt a local moving volume to factorize the mixed scene representation with a divide-and-conquer design to facilitate efficient online learning in our residual-based framework. Extensive experiments demonstrate that our method surpasses all state-of-the-art ones, including those based either on explicit or implicit representations, in terms of the accuracy of both mapping and tracking on large-scale scenes.",
        "arxiv_id": "2507.17594",
        "ARXIVID": "2507.17594",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for large-scale online RGB-D reconstruction, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.17079": {
        "authors": [
            "Md Meftahul Ferdaus",
            "Kendall N. Niles",
            "Joe Tom",
            "Mahdi Abdelguerfi",
            "Elias Ioup"
        ],
        "title": "Few-Shot Learning in Video and 3D Object Detection: A Survey",
        "abstract": "arXiv:2507.17079v1 Announce Type: new  Abstract: Few-shot learning (FSL) enables object detection models to recognize novel classes given only a few annotated examples, thereby reducing expensive manual data labeling. This survey examines recent FSL advances for video and 3D object detection. For video, FSL is especially valuable since annotating objects across frames is more laborious than for static images. By propagating information across frames, techniques like tube proposals and temporal matching networks can detect new classes from a couple examples, efficiently leveraging spatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces challenges like sparsity and lack of texture. Solutions integrate FSL with specialized point cloud networks and losses tailored for class imbalance. Few-shot 3D detection enables practical autonomous driving deployment by minimizing costly 3D annotation needs. Core issues in both domains include balancing generalization and overfitting, integrating prototype matching, and handling data modality properties. In summary, FSL shows promise for reducing annotation requirements and enabling real-world video, 3D, and other applications by efficiently leveraging information across feature, temporal, and data modalities. By comprehensively surveying recent advancements, this paper illuminates FSL's potential to minimize supervision needs and enable deployment across video, 3D, and other real-world applications.",
        "arxiv_id": "2507.17079",
        "ARXIVID": "2507.17079",
        "COMMENT": "Matches criterion 7 as it is a survey paper on few-shot learning in video and 3D object detection, synthesizing the state of the art.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2507.17522": {
        "authors": [
            "Tian Guo",
            "Hui Yuan",
            "Xiaolong Mao",
            "Shiqi Jiang",
            "Raouf Hamzaoui",
            "Sam Kwong"
        ],
        "title": "STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed Dynamic Point Clouds",
        "abstract": "arXiv:2507.17522v1 Announce Type: new  Abstract: Very few studies have addressed quality enhancement for compressed dynamic point clouds. In particular, the effective exploitation of spatial-temporal correlations between point cloud frames remains largely unexplored. Addressing this gap, we propose a spatial-temporal attribute quality enhancement (STQE) network that exploits both spatial and temporal correlations to improve the visual quality of G-PCC compressed dynamic point clouds. Our contributions include a recoloring-based motion compensation module that remaps reference attribute information to the current frame geometry to achieve precise inter-frame geometric alignment, a channel-aware temporal attention module that dynamically highlights relevant regions across bidirectional reference frames, a Gaussian-guided neighborhood feature aggregation module that efficiently captures spatial dependencies between geometry and color attributes, and a joint loss function based on the Pearson correlation coefficient, designed to alleviate over-smoothing effects typical of point-wise mean squared error optimization. When applied to the latest G-PCC test model, STQE achieved improvements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, with Bj{\\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5% for the Luma, Cb, and Cr components, respectively.",
        "arxiv_id": "2507.17522",
        "ARXIVID": "2507.17522",
        "COMMENT": "Matches criterion 1 as it focuses on spatial-temporal quality enhancement for dynamic point clouds, which involves spatial reasoning.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2507.17192": {
        "authors": [
            "Haiyu Wu",
            "Jaskirat Singh",
            "Sicong Tian",
            "Liang Zheng",
            "Kevin W. Bowyer"
        ],
        "title": "Vec2Face+ for Face Dataset Generation",
        "abstract": "arXiv:2507.17192v1 Announce Type: new  Abstract: When synthesizing identities as face recognition training data, it is generally believed that large inter-class separability and intra-class attribute variation are essential for synthesizing a quality dataset. % This belief is generally correct, and this is what we aim for. However, when increasing intra-class variation, existing methods overlook the necessity of maintaining intra-class identity consistency. % To address this and generate high-quality face training data, we propose Vec2Face+, a generative model that creates images directly from image features and allows for continuous and easy control of face identities and attributes. Using Vec2Face+, we obtain datasets with proper inter-class separability and intra-class variation and identity consistency using three strategies: 1) we sample vectors sufficiently different from others to generate well-separated identities; 2) we propose an AttrOP algorithm for increasing general attribute variations; 3) we propose LoRA-based pose control for generating images with profile head poses, which is more efficient and identity-preserving than AttrOP. % Our system generates VFace10K, a synthetic face dataset with 10K identities, which allows an FR model to achieve state-of-the-art accuracy on seven real-world test sets. Scaling the size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets yield higher accuracy than the real-world training dataset, CASIA-WebFace, on five real-world test sets. This is the first time a synthetic dataset beats the CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11 synthetic datasets outperforms random guessing (\\emph{i.e., 50\\%}) in twin verification and that models trained with synthetic identities are more biased than those trained with real identities. Both are important aspects for future investigation.",
        "arxiv_id": "2507.17192",
        "ARXIVID": "2507.17192",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and synthetic data, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.16886": {
        "authors": [
            "Yaoyu Fang",
            "Jiahe Qian",
            "Xinkun Wang",
            "Lee A. Cooper",
            "Bo Zhou"
        ],
        "title": "Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning",
        "abstract": "arXiv:2507.16886v1 Announce Type: new  Abstract: Spatial transcriptomics (ST) has revolutionized biomedical research by enabling high resolution gene expression profiling within tissues. However, the high cost and scarcity of high resolution ST data remain significant challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel framework for accurate ST imputation that requires only a single and low-cost sparsely sampled ST dataset alongside widely available natural images for co-training. Our approach integrates three key innovations: (1) a sparser-to-sparse self-supervised learning strategy that leverages intrinsic spatial patterns in ST data, (2) cross-domain co-learning with natural images to enhance feature representation, and (3) a Cascaded Data Consistent Imputation Network (CDCIN) that iteratively refines predictions while preserving sampled gene data fidelity. Extensive experiments on diverse tissue types, including breast cancer, liver, and lymphoid tissue, demonstrate that our method outperforms state-of-the-art approaches in imputation accuracy. By enabling robust ST reconstruction from sparse inputs, our framework significantly reduces reliance on costly high resolution data, facilitating potential broader adoption in biomedical research and clinical applications.",
        "arxiv_id": "2507.16886",
        "ARXIVID": "2507.16886",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and spatial transcriptomics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.17332": {
        "authors": [
            "Hyeongjin Nam",
            "Donghwan Kim",
            "Gyeongsik Moon",
            "Kyoung Mu Lee"
        ],
        "title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image",
        "abstract": "arXiv:2507.17332v1 Announce Type: new  Abstract: The misaligned human texture across different human parts is one of the main limitations of existing 3D human reconstruction methods. Each human part, such as a jacket or pants, should maintain a distinct texture without blending into others. The structural coherence of human parts serves as a crucial cue to infer human textures in the invisible regions of a single image. However, most existing 3D human reconstruction methods do not explicitly exploit such part segmentation priors, leading to misaligned textures in their reconstructions. In this regard, we present PARTE, which utilizes 3D human part information as a key guide to reconstruct 3D human textures. Our framework comprises two core components. First, to infer 3D human part information from a single image, we propose a 3D part segmentation module (PartSegmenter) that initially reconstructs a textureless human surface and predicts human part labels based on the textureless surface. Second, to incorporate part information into texture reconstruction, we introduce a part-guided texturing module (PartTexturer), which acquires prior knowledge from a pre-trained image generation network on texture alignment of human parts. Extensive experiments demonstrate that our framework achieves state-of-the-art quality in 3D human reconstruction. The project page is available at https://hygenie1228.github.io/PARTE/.",
        "arxiv_id": "2507.17332",
        "ARXIVID": "2507.17332",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and generative modeling in 3D human reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.17168": {
        "authors": [
            "Qifan Zhang",
            "Nuo Chen",
            "Zehua Li",
            "Miao Peng",
            "Jing Tang",
            "Jia Li"
        ],
        "title": "Improving LLMs' Generalized Reasoning Abilities by Graph Problems",
        "abstract": "arXiv:2507.17168v1 Announce Type: new  Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks, yet their performance often falters on novel and complex problems. Domain-specific continued pretraining (CPT) methods, such as those tailored for mathematical reasoning, have shown promise but lack transferability to broader reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning (GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks, spanning pathfinding, network analysis, numerical computation, and topological reasoning, require sophisticated logical and relational reasoning, making them ideal for teaching diverse reasoning patterns. To achieve this, we introduce GraphPile, the first large-scale corpus specifically designed for CPT using GPR data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes chain-of-thought, program-of-thought, trace of execution, and real-world graph data. Using GraphPile, we train GraphMind on popular base models Llama 3 and 3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in mathematical reasoning and up to 21.2 percent improvement in non-mathematical reasoning tasks such as logical and commonsense reasoning. By being the first to harness GPR for enhancing reasoning patterns and introducing the first dataset of its kind, our work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs.",
        "arxiv_id": "2507.17168",
        "ARXIVID": "2507.17168",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in improving reasoning abilities in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.17657": {
        "authors": [
            "Yotam Erel",
            "Olaf D\\\"unkel",
            "Rishabh Dabral",
            "Vladislav Golyanik",
            "Christian Theobalt",
            "Amit H. Bermano"
        ],
        "title": "Attention (as Discrete-Time Markov) Chains",
        "abstract": "arXiv:2507.17657v1 Announce Type: new  Abstract: We introduce a new interpretation of the attention matrix as a discrete-time Markov chain. Our interpretation sheds light on common operations involving attention scores such as selection, summation, and averaging in a unified framework. It further extends them by considering indirect attention, propagated through the Markov chain, as opposed to previous studies that only model immediate effects. Our main observation is that tokens corresponding to semantically similar regions form a set of metastable states, where the attention clusters, while noisy attention scores tend to disperse. Metastable states and their prevalence can be easily computed through simple matrix multiplication and eigenanalysis, respectively. Using these lightweight tools, we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define TokenRank -- the steady state vector of the Markov chain, which measures global token importance. We demonstrate that using it brings improvements in unconditional image generation. We believe our framework offers a fresh view of how tokens are being attended in modern visual transformers.",
        "arxiv_id": "2507.17657",
        "ARXIVID": "2507.17657",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in computer vision and machine learning due to its novel interpretation of attention matrices.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.17511": {
        "authors": [
            "Jiajun Luo",
            "Yicheng Xiao",
            "Jianru Xu",
            "Yangxiu You",
            "Rongwei Lu",
            "Chen Tang",
            "Jingyan Jiang",
            "Zhi Wang"
        ],
        "title": "Accelerating Parallel Diffusion Model Serving with Residual Compression",
        "abstract": "arXiv:2507.17511v1 Announce Type: new  Abstract: Diffusion models produce realistic images and videos but require substantial computational resources, necessitating multi-accelerator parallelism for real-time deployment. However, parallel inference introduces significant communication overhead from exchanging large activations between devices, limiting efficiency and scalability. We present CompactFusion, a compression framework that significantly reduces communication while preserving generation quality. Our key observation is that diffusion activations exhibit strong temporal redundancy-adjacent steps produce highly similar activations, saturating bandwidth with near-duplicate data carrying little new information. To address this inefficiency, we seek a more compact representation that encodes only the essential information. CompactFusion achieves this via Residual Compression that transmits only compressed residuals (step-wise activation differences). Based on empirical analysis and theoretical justification, we show that it effectively removes redundant data, enabling substantial data reduction while maintaining high fidelity. We also integrate lightweight error feedback to prevent error accumulation. CompactFusion establishes a new paradigm for parallel diffusion inference, delivering lower latency and significantly higher generation quality than prior methods. On 4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also uniquely supports communication-heavy strategies like sequence parallelism on slow networks, achieving 6.7x speedup over prior overlap-based method. CompactFusion applies broadly across diffusion models and parallel settings, and integrates easily without requiring pipeline rework. Portable implementation demonstrated on xDiT is publicly available at https://github.com/Cobalt-27/CompactFusion",
        "arxiv_id": "2507.17511",
        "ARXIVID": "2507.17511",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and efficient diffusion model serving.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.17486": {
        "authors": [
            "Hugues Roy",
            "Reuben Dorent",
            "Ninon Burgos"
        ],
        "title": "Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease",
        "abstract": "arXiv:2507.17486v1 Announce Type: new  Abstract: Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for identifying deviations from healthy subject data and thus facilitating the diagnosis of neurological disorders. In this work, we focus on Bayesian flow networks (BFNs), a novel class of generative models, which have not yet been applied to medical imaging or anomaly detection. BFNs combine the strength of diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension of BFNs for UAD, designed to: i) perform conditional image generation under high levels of spatially correlated noise, and ii) preserve subject specificity by incorporating a recursive feedback from the input image throughout the generative process. We evaluate AnoBFN on the challenging task of Alzheimer's disease-related anomaly detection in FDG PET images. Our approach outperforms other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and diffusion models (AnoDDPM), demonstrating its effectiveness at detecting anomalies while reducing false positive rates.",
        "arxiv_id": "2507.17486",
        "ARXIVID": "2507.17486",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and anomaly detection in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.17089": {
        "authors": [
            "Shanshan Zhang",
            "Siyue Wang",
            "Tianshui Wen",
            "Qi Zhang",
            "Ziheng Zhou",
            "Lingxiang Zheng",
            "Yu Yang"
        ],
        "title": "IONext: Unlocking the Next Era of Inertial Odometry",
        "abstract": "arXiv:2507.17089v1 Announce Type: new  Abstract: Researchers have increasingly adopted Transformer-based models for inertial odometry. While Transformers excel at modeling long-range dependencies, their limited sensitivity to local, fine-grained motion variations and lack of inherent inductive biases often hinder localization accuracy and generalization. Recent studies have shown that incorporating large-kernel convolutions and Transformer-inspired architectural designs into CNN can effectively expand the receptive field, thereby improving global motion perception. Motivated by these insights, we propose a novel CNN-based module called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures both global motion patterns and local, fine-grained motion features from dynamic inputs. This module dynamically generates selective weights based on the input, enabling efficient multi-scale feature aggregation. To further improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU), which selectively extracts representative and task-relevant motion features in the temporal domain. This unit addresses the limitations of temporal modeling observed in existing CNN approaches. Built upon DADM and STGU, we present a new CNN-based inertial odometry backbone, named Next Era of Inertial Odometry (IONext). Extensive experiments on six public datasets demonstrate that IONext consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based methods. For instance, on the RNIN dataset, IONext reduces the average ATE by 10% and the average RTE by 12% compared to the representative model iMOT.",
        "arxiv_id": "2507.17089",
        "ARXIVID": "2507.17089",
        "COMMENT": "Does not match any specific criteria but is relevant to embodied AI through inertial odometry advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.16880": {
        "authors": [
            "Antoni Kowalczuk",
            "Dominik Hintersdorf",
            "Lukas Struppek",
            "Kristian Kersting",
            "Adam Dziedzic",
            "Franziska Boenisch"
        ],
        "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed",
        "abstract": "arXiv:2507.16880v1 Announce Type: new  Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.",
        "arxiv_id": "2507.16880",
        "ARXIVID": "2507.16880",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and understanding memorization in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.17157": {
        "authors": [
            "Ruodai Cui",
            "Lei Zhang"
        ],
        "title": "UNICE: Training A Universal Image Contrast Enhancer",
        "abstract": "arXiv:2507.17157v1 Announce Type: new  Abstract: Existing image contrast enhancement methods are typically designed for specific tasks such as under-/over-exposure correction, low-light and backlit image enhancement, etc. The learned models, however, exhibit poor generalization performance across different tasks, even across different datasets of a specific task. It is important to explore whether we can learn a universal and generalized model for various contrast enhancement tasks. In this work, we observe that the common key factor of these tasks lies in the need of exposure and contrast adjustment, which can be well-addressed if high-dynamic range (HDR) inputs are available. We hence collect 46,928 HDR raw images from public sources, and render 328,496 sRGB images to build multi-exposure sequences (MES) and the corresponding pseudo sRGB ground-truths via multi-exposure fusion. Consequently, we train a network to generate an MES from a single sRGB image, followed by training another network to fuse the generated MES into an enhanced image. Our proposed method, namely UNiversal Image Contrast Enhancer (UNICE), is free of costly human labeling. However, it demonstrates significantly stronger generalization performance than existing image contrast enhancement methods across and within different tasks, even outperforming manually created ground-truths in multiple no-reference image quality metrics. The dataset, code and model are available at https://github.com/BeyondHeaven/UNICE.",
        "arxiv_id": "2507.17157",
        "ARXIVID": "2507.17157",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and image enhancement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17420": {
        "authors": [
            "Sneha George Gnanakalavathy",
            "Hairil Abdul Razak",
            "Robert Meertens",
            "Jonathan E. Fieldsend",
            "Xujiong Ye",
            "Mohammed M. Abdelsamea"
        ],
        "title": "CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality Optimization in Computed Tomography",
        "abstract": "arXiv:2507.17420v1 Announce Type: new  Abstract: In computed tomography (CT), achieving high image quality while minimizing radiation exposure remains a key clinical challenge. This paper presents CAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and Predictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT integrates image data with acquisition metadata (such as tube voltage, tube current, and contrast agent types) to model the underlying causal relationships that influence image quality. An ensemble of Variational Autoencoders (VAEs) is employed to extract meaningful features and generate causal representations from observational data, including CT images and associated imaging parameters. These input features are fused to predict the Signal-to-Noise Ratio (SNR) and support counterfactual inference, enabling what-if simulations, such as changes in contrast agents (types and concentrations) or scan parameters. CAPRI-CT is trained and validated using an ensemble learning approach, achieving strong predictive performance. By facilitating both prediction and interpretability, CAPRI-CT provides actionable insights that could help radiologists and technicians design more efficient CT protocols without repeated physical scans. The source code and dataset are publicly available at https://github.com/SnehaGeorge22/capri-ct.",
        "arxiv_id": "2507.17420",
        "ARXIVID": "2507.17420",
        "COMMENT": "Does not closely match any specific criterion but is relevant to medical imaging and causal analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17240": {
        "authors": [
            "Krishna Srikar Durbha",
            "Asvin Kumar Venkataramanan",
            "Rajesh Sureddi",
            "Alan C. Bovik"
        ],
        "title": "Perceptual Classifiers: Detecting Generative Images using Perceptual Features",
        "abstract": "arXiv:2507.17240v1 Announce Type: new  Abstract: Image Quality Assessment (IQA) models are employed in many practical image and video processing pipelines to reduce storage, minimize transmission costs, and improve the Quality of Experience (QoE) of millions of viewers. These models are sensitive to a diverse range of image distortions and can accurately predict image quality as judged by human viewers. Recent advancements in generative models have resulted in a significant influx of \"GenAI\" content on the internet. Existing methods for detecting GenAI content have progressed significantly with improved generalization performance on images from unseen generative models. Here, we leverage the capabilities of existing IQA models, which effectively capture the manifold of real images within a bandpass statistical space, to distinguish between real and AI-generated images. We investigate the generalization ability of these perceptual classifiers to the task of GenAI image detection and evaluate their robustness against various image degradations. Our results show that a two-layer network trained on the feature space of IQA models demonstrates state-of-the-art performance in detecting fake images across generative models, while maintaining significant robustness against image degradations.",
        "arxiv_id": "2507.17240",
        "ARXIVID": "2507.17240",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and generative image detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17640": {
        "authors": [
            "Thomas M. Metz",
            "Matthew Q. Hill",
            "Alice J. O'Toole"
        ],
        "title": "The Early Bird Identifies the Worm: You Can't Beat a Head Start in Long-Term Body Re-ID (ECHO-BID)",
        "abstract": "arXiv:2507.17640v1 Announce Type: new  Abstract: Person identification in unconstrained viewing environments presents significant challenges due to variations in distance, viewpoint, imaging conditions, and clothing. We introduce $\\textbf{E}$va $\\textbf{C}$lothes-Change from $\\textbf{H}$idden $\\textbf{O}$bjects - $\\textbf{B}$ody $\\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built on object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 other models that vary systematically in backbone architecture, model size, scale of object classification pretraining, and transfer learning protocol. Models were evaluated on benchmark datasets across constrained, unconstrained, and occluded settings. ECHO-BID, with transfer learning on the most challenging clothes-change data, achieved state-of-the-art results on long-term re-id -- substantially outperforming other methods. ECHO-BID also surpassed other methods by a wide margin in occluded viewing scenarios. A combination of increased model size and Masked Image Modeling during pretraining underlie ECHO-BID's strong performance on long-term re-id. Notably, a smaller, but more challenging transfer learning dataset, generalized better across datasets than a larger, less challenging one. However, the larger dataset with an additional fine-tuning step proved best on the most difficult data. Selecting the correct pretrained backbone architecture and transfer learning protocols can drive substantial gains in long-term re-id performance.",
        "arxiv_id": "2507.17640",
        "ARXIVID": "2507.17640",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in person re-identification and long-term body identification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17312": {
        "authors": [
            "Peiqi Chen",
            "Lei Yu",
            "Yi Wan",
            "Yingying Pei",
            "Xinyi Liu",
            "Yongxiang Yao",
            "Yingying Zhang",
            "Lixiang Ru",
            "Liheng Zhong",
            "Jingdong Chen",
            "Ming Yang",
            "Yongjun Zhang"
        ],
        "title": "CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance",
        "abstract": "arXiv:2507.17312v1 Announce Type: new  Abstract: Semi-dense feature matching methods have shown strong performance in challenging scenarios. However, the existing pipeline relies on a global search across the entire feature map to establish coarse matches, limiting further improvements in accuracy and efficiency. Motivated by this limitation, we propose a novel pipeline, CasP, which leverages cascaded correspondence priors for guidance. Specifically, the matching stage is decomposed into two progressive phases, bridged by a region-based selective cross-attention mechanism designed to enhance feature discriminability. In the second phase, one-to-one matches are determined by restricting the search range to the one-to-many prior areas identified in the first phase. Additionally, this pipeline benefits from incorporating high-level features, which helps reduce the computational costs of low-level feature extraction. The acceleration gains of CasP increase with higher resolution, and our lite model achieves a speedup of $\\sim2.2\\times$ at a resolution of 1152 compared to the most efficient method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority in geometric estimation, particularly with impressive cross-domain generalization. These advantages highlight its potential for latency-sensitive and high-robustness applications, such as SLAM and UAV systems. Code is available at https://github.com/pq-chen/CasP.",
        "arxiv_id": "2507.17312",
        "ARXIVID": "2507.17312",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in feature matching and geometric estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17554": {
        "authors": [
            "Xide Xu",
            "Sandesh Kamath",
            "Muhammad Atif Butt",
            "Bogdan Raducanu"
        ],
        "title": "An h-space Based Adversarial Attack for Protection Against Few-shot Personalization",
        "abstract": "arXiv:2507.17554v1 Announce Type: new  Abstract: The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (`h-space'), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness.",
        "arxiv_id": "2507.17554",
        "ARXIVID": "2507.17554",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in adversarial attacks and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17351": {
        "authors": [
            "Yuzhe Zhu",
            "Lile Cai",
            "Kangkang Lu",
            "Fayao Liu",
            "Xulei Yang"
        ],
        "title": "Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field",
        "abstract": "arXiv:2507.17351v1 Announce Type: new  Abstract: Neural Radiance Field (NeRF) models are implicit neural scene representation methods that offer unprecedented capabilities in novel view synthesis. Semantically-aware NeRFs not only capture the shape and radiance of a scene, but also encode semantic information of the scene. The training of semantically-aware NeRFs typically requires pixel-level class labels, which can be prohibitively expensive to collect. In this work, we explore active learning as a potential solution to alleviate the annotation burden. We investigate various design choices for active learning of semantically-aware NeRF, including selection granularity and selection strategies. We further propose a novel active learning strategy that takes into account 3D geometric constraints in sample selection. Our experiments demonstrate that active learning can effectively reduce the annotation cost of training semantically-aware NeRF, achieving more than 2X reduction in annotation cost compared to random sampling.",
        "arxiv_id": "2507.17351",
        "ARXIVID": "2507.17351",
        "COMMENT": "Does not match any specific criteria but is relevant to general interest in computer vision and machine learning due to its exploration of active learning for semantic NeRFs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17334": {
        "authors": [
            "Weihua Gao",
            "Chunxu Ren",
            "Wenlong Niu",
            "Xiaodong Peng"
        ],
        "title": "Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection",
        "abstract": "arXiv:2507.17334v1 Announce Type: new  Abstract: In low-altitude surveillance and early warning systems, detecting weak moving targets remains a significant challenge due to low signal energy, small spatial extent, and complex background clutter. Existing methods struggle with extracting robust features and suffer from the lack of reliable annotations. To address these limitations, we propose a novel Temporal Point-Supervised (TPS) framework that enables high-performance detection of weak targets without any manual annotations.Instead of conventional frame-based detection, our framework reformulates the task as a pixel-wise temporal signal modeling problem, where weak targets manifest as short-duration pulse-like responses. A Temporal Signal Reconstruction Network (TSRNet) is developed under the TPS paradigm to reconstruct these transient signals.TSRNet adopts an encoder-decoder architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention) module to enhance its sensitivity to diverse temporal patterns. Additionally, a graph-based trajectory mining strategy is employed to suppress false alarms and ensure temporal consistency.Extensive experiments on a purpose-built low-SNR dataset demonstrate that our framework outperforms state-of-the-art methods while requiring no human annotations. It achieves strong detection performance and operates at over 1000 FPS, underscoring its potential for real-time deployment in practical scenarios.",
        "arxiv_id": "2507.17334",
        "ARXIVID": "2507.17334",
        "COMMENT": "Does not match any specific criterion but is related to signal reconstruction and detection tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17508": {
        "authors": [
            "Jorgen Cani",
            "Christos Diou",
            "Spyridon Evangelatos",
            "Vasileios Argyriou",
            "Panagiotis Radoglou-Grammatikis",
            "Panagiotis Sarigiannidis",
            "Iraklis Varlamis",
            "Georgios Th. Papadopoulos"
        ],
        "title": "Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation",
        "abstract": "arXiv:2507.17508v1 Announce Type: new  Abstract: Automated X-ray inspection is crucial for efficient and unobtrusive security screening in various public settings. However, challenges such as object occlusion, variations in the physical properties of items, diversity in X-ray scanning devices, and limited training data hinder accurate and reliable detection of illicit items. Despite the large body of research in the field, reported experimental evaluations are often incomplete, with frequently conflicting outcomes. To shed light on the research landscape and facilitate further research, a systematic, detailed, and thorough comparative evaluation of recent Deep Learning (DL)-based methods for X-ray object detection is conducted. For this, a comprehensive evaluation framework is developed, composed of: a) Six recent, large-scale, and widely used public datasets for X-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and PIDray), b) Ten different state-of-the-art object detection schemes covering all main categories in the literature, including generic Convolutional Neural Network (CNN), custom CNN, generic transformer, and hybrid CNN-transformer architectures, and c) Various detection (mAP50 and mAP50:95) and time/computational-complexity (inference time (ms), parameter size (M), and computational load (GFLOPS)) metrics. A thorough analysis of the results leads to critical observations and insights, emphasizing key aspects such as: a) Overall behavior of the object detection schemes, b) Object-level detection performance, c) Dataset-specific observations, and d) Time efficiency and computational complexity analysis. To support reproducibility of the reported experimental results, the evaluation code and model weights are made publicly available at https://github.com/jgenc/xray-comparative-evaluation.",
        "arxiv_id": "2507.17508",
        "ARXIVID": "2507.17508",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and X-ray object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.17252": {
        "authors": [
            "Ruodai Cui",
            "Li Niu",
            "Guosheng Hu"
        ],
        "title": "Unsupervised Exposure Correction",
        "abstract": "arXiv:2507.17252v1 Announce Type: new  Abstract: Current exposure correction methods have three challenges, labor-intensive paired data annotation, limited generalizability, and performance degradation in low-level computer vision tasks. In this work, we introduce an innovative Unsupervised Exposure Correction (UEC) method that eliminates the need for manual annotations, offers improved generalizability, and enhances performance in low-level downstream tasks. Our model is trained using freely available paired data from an emulated Image Signal Processing (ISP) pipeline. This approach does not need expensive manual annotations, thereby minimizing individual style biases from the annotation and consequently improving its generalizability. Furthermore, we present a large-scale Radiometry Correction Dataset, specifically designed to emphasize exposure variations, to facilitate unsupervised learning. In addition, we develop a transformation function that preserves image details and outperforms state-of-the-art supervised methods [12], while utilizing only 0.01% of their parameters. Our work further investigates the broader impact of exposure correction on downstream tasks, including edge detection, demonstrating its effectiveness in mitigating the adverse effects of poor exposure on low-level features. The source code and dataset are publicly available at https://github.com/BeyondHeaven/uec_code.",
        "arxiv_id": "2507.17252",
        "ARXIVID": "2507.17252",
        "COMMENT": "Does not match any specific criterion but is related to low-level computer vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.17729": {
        "authors": [
            "Kagan Ozturk",
            "Louisa Conwill",
            "Jacob Gutierrez",
            "Kevin Bowyer",
            "Walter J. Scheirer"
        ],
        "title": "A Comprehensive Evaluation Framework for the Study of the Effects of Facial Filters on Face Recognition Accuracy",
        "abstract": "arXiv:2507.17729v1 Announce Type: new  Abstract: Facial filters are now commonplace for social media users around the world. Previous work has demonstrated that facial filters can negatively impact automated face recognition performance. However, these studies focus on small numbers of hand-picked filters in particular styles. In order to more effectively incorporate the wide ranges of filters present on various social media applications, we introduce a framework that allows for larger-scale study of the impact of facial filters on automated recognition. This framework includes a controlled dataset of face images, a principled filter selection process that selects a representative range of filters for experimentation, and a set of experiments to evaluate the filters' impact on recognition. We demonstrate our framework with a case study of filters from the American applications Instagram and Snapchat and the Chinese applications Meitu and Pitu to uncover cross-cultural differences. Finally, we show how the filtering effect in a face embedding space can easily be detected and restored to improve face recognition performance.",
        "arxiv_id": "2507.17729",
        "ARXIVID": "2507.17729",
        "COMMENT": "Does not match any specific criterion but is tangentially related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}