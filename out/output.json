{
    "2507.18678": {
        "authors": [
            "Xingyu Miao",
            "Haoran Duan",
            "Quanhao Qian",
            "Jiuniu Wang",
            "Yang Long",
            "Ling Shao",
            "Deli Zhao",
            "Ran Xu",
            "Gongjie Zhang"
        ],
        "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting",
        "abstract": "arXiv:2507.18678v1 Announce Type: new  Abstract: Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation. In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations - including point clouds, camera poses, depth maps, and pseudo-RGBD - via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence. We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments.",
        "arxiv_id": "2507.18678",
        "ARXIVID": "2507.18678",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) due to its scalable pipeline for 2D-to-3D data lifting and its contribution to spatial scene understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.19451": {
        "authors": [
            "Baijun Ye",
            "Minghui Qin",
            "Saining Zhang",
            "Moonjun Gong",
            "Shaoting Zhu",
            "Zebang Shen",
            "Luan Zhang",
            "Lu Zhang",
            "Hao Zhao",
            "Hang Zhao"
        ],
        "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting",
        "abstract": "arXiv:2507.19451v1 Announce Type: new  Abstract: Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for autonomous driving perception. Project Page: https://gs-occ3d.github.io/",
        "arxiv_id": "2507.19451",
        "ARXIVID": "2507.19451",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) and criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) due to its focus on vision-only occupancy reconstruction for autonomous driving and its novel Octree-based Gaussian Surfel formulation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.19280": {
        "authors": [
            "Liang Yao",
            "Fan Liu",
            "Hongbo Lu",
            "Chuanyi Zhang",
            "Rui Min",
            "Shengxiang Xu",
            "Shimin Di",
            "Pai Peng"
        ],
        "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow",
        "abstract": "arXiv:2507.19280v1 Announce Type: new  Abstract: Remote sensing imagery presents vast, inherently unstructured spatial data, demanding sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should be somewhat autonomous, where predefined ground-truth reasoning paths do not constrain the learning process. Furthermore, its architecture ought to be unified yet flexible, enabling the model to perform diverse reasoning tasks with distinct output formats through a single forward pass. Existing remote sensing approaches fail to address these requirements, as they rely on supervised fine-tuning paradigms that constrain the autonomy of reasoning. To this end, we propose RemoteReasoner, a flexible and robust workflow for remote sensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task adaptation strategies that enable multi-granularity output generation. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient autonomy for precise reasoning. At the inference stage, our adaptation strategies enable diverse output formats at inference time without requiring task-specific decoders or further fine-tuning. Preliminary experiments demonstrated that RemoteReasoner achieves remarkable performance across multi-granularity reasoning tasks, including region-level and pixel-level. Additionally, our framework enables novel capabilities such as the contour extraction task beyond the reach of existing reasoning pipelines.",
        "arxiv_id": "2507.19280",
        "ARXIVID": "2507.19280",
        "COMMENT": "Matches criteria 1 and 2 as it presents a novel geospatial reasoning workflow using a multi-modal large language model (MLLM) for spatial intelligence and reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2507.19468": {
        "authors": [
            "Federico Baldassarre",
            "Marc Szafraniec",
            "Basile Terver",
            "Vasil Khalidov",
            "Francisco Massa",
            "Yann LeCun",
            "Patrick Labatut",
            "Maximilian Seitzer",
            "Piotr Bojanowski"
        ],
        "title": "Back to the Features: DINO as a Foundation for Video World Models",
        "abstract": "arXiv:2507.19468v1 Announce Type: new  Abstract: We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms previous models on a variety of video prediction benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong understanding of intuitive physics. Furthermore, we show that it is possible to fine-tune the predictor on observation-action trajectories. The resulting action-conditioned world model can be used for planning by simulating candidate trajectories in latent space.",
        "arxiv_id": "2507.19468",
        "ARXIVID": "2507.19468",
        "COMMENT": "Matches criterion 6. The paper focuses on video understanding through a video world model, which aligns with video-based tasks and understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2507.19360": {
        "authors": [
            "Chen Zhu",
            "Wangbo Zhao",
            "Huiwen Zhang",
            "Samir Khaki",
            "Yuhao Zhou",
            "Weidong Tang",
            "Shuo Wang",
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Xiaojiang Peng",
            "Kai Wang",
            "Dawei Yang"
        ],
        "title": "EA-ViT: Efficient Adaptation for Elastic Vision Transformer",
        "abstract": "arXiv:2507.19360v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have emerged as a foundational model in computer vision, excelling in generalization and adaptation to downstream tasks. However, deploying ViTs to support diverse resource constraints typically requires retraining multiple, size-specific ViTs, which is both time-consuming and energy-intensive. To address this issue, we propose an efficient ViT adaptation framework that enables a single adaptation process to generate multiple models of varying sizes for deployment on platforms with various resource constraints. Our approach comprises two stages. In the first stage, we enhance a pre-trained ViT with a nested elastic architecture that enables structural flexibility across MLP expansion ratio, number of attention heads, embedding dimension, and network depth. To preserve pre-trained knowledge and ensure stable adaptation, we adopt a curriculum-based training strategy that progressively increases elasticity. In the second stage, we design a lightweight router to select submodels according to computational budgets and downstream task demands. Initialized with Pareto-optimal configurations derived via a customized NSGA-II algorithm, the router is then jointly optimized with the backbone. Extensive experiments on multiple benchmarks demonstrate the effectiveness and versatility of EA-ViT. The code is available at https://github.com/zcxcf/EA-ViT.",
        "arxiv_id": "2507.19360",
        "ARXIVID": "2507.19360",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it focuses on efficient adaptation of Vision Transformers for diverse resource constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19478": {
        "authors": [
            "Xuehui Wang",
            "Zhenyu Wu",
            "JingJing Xie",
            "Zichen Ding",
            "Bowen Yang",
            "Zehao Li",
            "Zhaoyang Liu",
            "Qingyun Li",
            "Xuan Dong",
            "Zhe Chen",
            "Weiyun Wang",
            "Xiangyu Zhao",
            "Jixuan Chen",
            "Haodong Duan",
            "Tianbao Xie",
            "Chenyu Yang",
            "Shiqian Su",
            "Yue Yu",
            "Yuan Huang",
            "Yiqian Liu",
            "Xiao Zhang",
            "Yanting Zhang",
            "Xiangyu Yue",
            "Weijie Su",
            "Xizhou Zhu",
            "Wei Shen",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents",
        "abstract": "arXiv:2507.19478v1 Announce Type: new  Abstract: We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web platforms. It comprises four levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA) metric to assess GUI agent execution efficiency in online automation scenarios. Through MMBench-GUI, we identify accurate visual grounding as a critical determinant of overall task success, emphasizing the substantial benefits of modular frameworks that integrate specialized grounding modules. Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and cross-platform generalization abilities, with long-context memory, a broad action space, and long-term reasoning playing a critical role. More important, task efficiency remains a critically underexplored dimension, and all models suffer from substantial inefficiencies, with excessive redundant steps even when tasks are ultimately completed. The integration of precise localization, effective planning, and early stopping strategies is indispensable to enable truly efficient and scalable GUI automation. Our benchmark code, evaluation data, and running environment will be publicly available at https://github.com/open-compass/MMBench-GUI.",
        "arxiv_id": "2507.19478",
        "ARXIVID": "2507.19478",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for GUI automation agents across multiple platforms.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18921": {
        "authors": [
            "Elham Soltani Kazemi",
            "Imad Eddine Toubal",
            "Gani Rahmon",
            "Jaired Collins",
            "K. Palaniappan"
        ],
        "title": "HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback",
        "abstract": "arXiv:2507.18921v1 Announce Type: new  Abstract: Video Object Segmentation (VOS) is foundational to numerous computer vision applications, including surveillance, autonomous driving, robotics and generative video editing. However, existing VOS models often struggle with precise mask delineation, deformable objects, topologically transforming objects, tracking drift and long video sequences. In this paper, we introduce HQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a novel method that enhances the performance of VOS base models by addressing these limitations. Our approach incorporates three key innovations: (i) leveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based candidate-selection to refine coarse segmentation masks, resulting in improved object boundaries; (ii) implementing a dynamic smart memory mechanism that selectively stores relevant key frames while discarding redundant ones, thereby optimizing memory usage and processing efficiency for long-term videos; and (iii) dynamically updating the appearance model to effectively handle complex topological object variations and reduce drift throughout the video. These contributions mitigate several limitations of existing VOS models including, coarse segmentations that mix-in background pixels, fixed memory update schedules, brittleness to drift and occlusions, and prompt ambiguity issues associated with SAM. Extensive experiments conducted on multiple public datasets and state-of-the-art base trackers demonstrate that our method consistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover, HQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its effectiveness in challenging scenarios characterized by complex multi-object dynamics over extended temporal durations.",
        "arxiv_id": "2507.18921",
        "ARXIVID": "2507.18921",
        "COMMENT": "Matches criterion 6 (Video Understanding) due to its focus on video object segmentation and tracking with novel memory-efficient methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.20342": {
        "authors": [
            "Zhipeng Tang",
            "Sha Zhang",
            "Jiajun Deng",
            "Chenjie Wang",
            "Guoliang You",
            "Yuting Huang",
            "Xinrui Lin",
            "Yanyong Zhang"
        ],
        "title": "VLMPlanner: Integrating Visual Language Models with Motion Planning",
        "abstract": "arXiv:2507.20342v1 Announce Type: new  Abstract: Integrating large language models (LLMs) into autonomous driving motion planning has recently emerged as a promising direction, offering enhanced interpretability, better controllability, and improved generalization in rare and long-tail scenarios. However, existing methods often rely on abstracted perception or map-based inputs, missing crucial visual context, such as fine-grained road cues, accident aftermath, or unexpected obstacles, which are essential for robust decision-making in complex driving environments. To bridge this gap, we propose VLMPlanner, a hybrid framework that combines a learning-based real-time planner with a vision-language model (VLM) capable of reasoning over raw images. The VLM processes multi-view images to capture rich, detailed visual information and leverages its common-sense reasoning capabilities to guide the real-time planner in generating robust and safe trajectories. Furthermore, we develop the Context-Adaptive Inference Gate (CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by dynamically adjusting its inference frequency based on scene complexity, thereby achieving an optimal balance between planning performance and computational efficiency. We evaluate our approach on the large-scale, challenging nuPlan benchmark, with comprehensive experimental results demonstrating superior planning performance in scenarios with intricate road conditions and dynamic elements. Code will be available.",
        "arxiv_id": "2507.20342",
        "ARXIVID": "2507.20342",
        "COMMENT": "Matches criterion 3. The paper introduces a novel framework integrating vision-language models with motion planning, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19272": {
        "authors": [
            "Marcel Simon",
            "Tae-Ho Kim",
            "Seul-Ki Yeom"
        ],
        "title": "Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception",
        "abstract": "arXiv:2507.19272v1 Announce Type: new  Abstract: Self-supervised image encoders such as DINO have recently gained significant interest for learning robust visual features without labels. However, most SSL methods train on static images and miss the temporal cues inherent in videos. We introduce a video-distilled single-image encoder trained to predict the next-frame representation from the current frame. This simple objective injects 3D spatial and temporal priors without optical flow or tracking. When pre-training on a single 2-hour video, our approach raises the mean Intersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while remaining a drop-in replacement for image-only pipelines. Our results highlight video self-distillation as a lightweight route to geometry-aware perception an essential ingredient for physically plausible world models and Physical AI.",
        "arxiv_id": "2507.19272",
        "ARXIVID": "2507.19272",
        "COMMENT": "Matches criteria 6 as it introduces a video self-distillation method for single-image encoders, relevant to video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19292": {
        "authors": [
            "Sakuya Ota",
            "Qing Yu",
            "Kent Fujiwara",
            "Satoshi Ikehata",
            "Ikuro Sato"
        ],
        "title": "PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups",
        "abstract": "arXiv:2507.19292v1 Announce Type: new  Abstract: Generating realistic group interactions involving multiple characters remains challenging due to increasing complexity as group size expands. While existing conditional diffusion models incrementally generate motions by conditioning on previously generated characters, they rely on single shared prompts, limiting nuanced control and leading to overly simplified interactions. In this paper, we introduce Person-Interaction Noise Optimization (PINO), a novel, training-free framework designed for generating realistic and customizable interactions among groups of arbitrary size. PINO decomposes complex group interactions into semantically relevant pairwise interactions, and leverages pretrained two-person interaction diffusion models to incrementally compose group interactions. To ensure physical plausibility and avoid common artifacts such as overlapping or penetration between characters, PINO employs physics-based penalties during noise optimization. This approach allows precise user control over character orientation, speed, and spatial relationships without additional training. Comprehensive evaluations demonstrate that PINO generates visually realistic, physically coherent, and adaptable multi-person interactions suitable for diverse animation, gaming, and robotics applications.",
        "arxiv_id": "2507.19292",
        "ARXIVID": "2507.19292",
        "COMMENT": "Matches criteria 3 as it introduces a novel framework for generating realistic group interactions, relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19058": {
        "authors": [
            "Chong Xia",
            "Shengjun Zhang",
            "Fangfu Liu",
            "Chang Liu",
            "Khodchaphun Hirunyaratsameewong",
            "Yueqi Duan"
        ],
        "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment",
        "abstract": "arXiv:2507.19058v1 Announce Type: new  Abstract: Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences. Project Page: https://xiac20.github.io/ScenePainter/.",
        "arxiv_id": "2507.19058",
        "ARXIVID": "2507.19058",
        "COMMENT": "Matches criteria 6 as it focuses on perpetual 3D scene generation, which is a video understanding task.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19253": {
        "authors": [
            "An Xiang",
            "Zixuan Huang",
            "Xitong Gao",
            "Kejiang Ye",
            "Cheng-zhong Xu"
        ],
        "title": "BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection",
        "abstract": "arXiv:2507.19253v1 Announce Type: new  Abstract: Industrial anomaly detection for 2D objects has gained significant attention and achieved progress in anomaly detection (AD) methods. However, identifying 3D depth anomalies using only 2D information is insufficient. Despite explicitly fusing depth information into RGB images or using point cloud backbone networks to extract depth features, both approaches struggle to adequately represent 3D information in multimodal scenarios due to the disparities among different modal information. Additionally, due to the scarcity of abnormal samples in industrial data, especially in multimodal scenarios, it is necessary to perform anomaly generation to simulate real-world abnormal samples. Therefore, we propose a novel unified multimodal anomaly detection framework to address these issues. Our contributions consist of 3 key aspects. (1) We extract visible depth information from 3D point cloud data simply and use 2D RGB images to represent appearance, which disentangles depth and appearance to support unified anomaly generation. (2) Benefiting from the flexible input representation, the proposed Multi-Scale Gaussian Anomaly Generator and Unified Texture Anomaly Generator can generate richer anomalies in RGB and depth. (3) All modules share parameters for both RGB and depth data, effectively bridging 2D and 3D anomaly detection. Subsequent modules can directly leverage features from both modalities without complex fusion. Experiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD and Eyecandies datasets. Code available at: https://github.com/Xantastic/BridgeNet",
        "arxiv_id": "2507.19253",
        "ARXIVID": "2507.19253",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a unified multimodal framework for anomaly detection, integrating 2D and 3D modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19110": {
        "authors": [
            "Zhihui Guo",
            "Xin Man",
            "Hui Xu",
            "Jie Shao"
        ],
        "title": "LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models",
        "abstract": "arXiv:2507.19110v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks such as image captioning but remain prone to object hallucinations, where they describe objects that do not appear in the image. To mitigate this, we propose \\textbf{LISA}, a \\textbf{L}ayer-wise \\textbf{I}ntegration and \\textbf{S}uppression \\textbf{A}pproach that enhances generation consistency through hierarchical modulation and multi-layer fusion. LISA leverages the functional hierarchy within MLLMs, where shallow layers provide visual grounding, middle layers encode semantics, and deep layers tend to amplify spurious signals. First, zone-specific spectral modulation stabilizes attention by suppressing over-amplified activations in deeper layers while preserving alignment cues in earlier layers. Second, token-level logits from selected layers are fused via anchor-based routing, with token-wise anchor selection and soft logit fusion enabling adaptive integration during decoding. LISA is fully \\textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs, including Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces hallucinations by up to 53.6\\% in $\\mathrm{CHAIR}_I$ and improves POPE F1 by 4.5\\%, demonstrating strong generalization across models and tasks.",
        "arxiv_id": "2507.19110",
        "ARXIVID": "2507.19110",
        "COMMENT": "Matches criteria 2 as it proposes a novel method to mitigate hallucinations in Multimodal Large Language Models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19474": {
        "authors": [
            "Ziren Gong",
            "Xiaohan Li",
            "Fabio Tosi",
            "Youmin Zhang",
            "Stefano Mattoccia",
            "Jun Wu",
            "Matteo Poggi"
        ],
        "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations",
        "abstract": "arXiv:2507.19474v1 Announce Type: new  Abstract: This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.",
        "arxiv_id": "2507.19474",
        "ARXIVID": "2507.19474",
        "COMMENT": "Matches criteria 1 and 3 as it presents a novel SLAM system integrating spatial intelligence and embodied agent methodologies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19420": {
        "authors": [
            "Yiming Zhang",
            "Chengzhang Yu",
            "Zhuokai Zhao",
            "Kun Wang",
            "Qiankun Li",
            "Zihan Chen",
            "Yang Liu",
            "Zenghui Ding",
            "Yining Sun"
        ],
        "title": "CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing",
        "abstract": "arXiv:2507.19420v1 Announce Type: new  Abstract: The processing mechanisms underlying language and image understanding in large vision-language models (LVLMs) have been extensively studied. However, the internal reasoning mechanisms of LVLMs for spatiotemporal understanding remain poorly understood. In this work, we introduce a systematic, circuit-based framework designed to investigate how spatiotemporal visual semantics are represented and processed within these LVLMs. Specifically, our framework comprises three circuits: visual auditing circuit, semantic tracing circuit, and attention flow circuit. Through the lens of these circuits, we discover that visual semantics are highly localized to specific object tokens--removing these tokens can degrade model performance by up to 92.6%. Furthermore, we identify that interpretable concepts of objects and actions emerge and become progressively refined in the middle-to-late layers of LVLMs. In contrary to the current works that solely focus on objects in one image, we reveal that the middle-to-late layers of LVLMs exhibit specialized functional localization for spatiotemporal semantics. Our findings offer significant mechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a foundation for designing more robust and interpretable models.",
        "arxiv_id": "2507.19420",
        "ARXIVID": "2507.19420",
        "COMMENT": "Matches criterion 6 as it investigates spatiotemporal visual semantics in large vision-language models, contributing to video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.18881": {
        "authors": [
            "Bolei Chen",
            "Jiaxu Kang",
            "Haonan Yang",
            "Ping Zhong",
            "Jianxin Wang"
        ],
        "title": "Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?",
        "abstract": "arXiv:2507.18881v1 Announce Type: new  Abstract: Since a building's floorplans are easily accessible, consistent over time, and inherently robust to changes in visual appearance, self-localization within the floorplan has attracted researchers' interest. However, since floorplans are minimalist representations of a building's structure, modal and geometric differences between visual perceptions and floorplans pose challenges to this task. While existing methods cleverly utilize 2D geometric features and pose filters to achieve promising performance, they fail to address the localization errors caused by frequent visual changes and view occlusions due to variously shaped 3D objects. To tackle these issues, this paper views the 2D Floorplan Localization (FLoc) problem from a higher dimension by injecting 3D geometric priors into the visual FLoc algorithm. For the 3D geometric prior modeling, we first model geometrically aware view invariance using multi-view constraints, i.e., leveraging imaging geometric principles to provide matching constraints between multiple images that see the same points. Then, we further model the view-scene aligned geometric priors, enhancing the cross-modal geometry-color correspondences by associating the scene's surface reconstruction with the RGB frames of the sequence. Both 3D priors are modeled through self-supervised contrastive learning, thus no additional geometric or semantic annotations are required. These 3D priors summarized in extensive realistic scenes bridge the modal gap while improving localization success without increasing the computational burden on the FLoc algorithm. Sufficient comparative studies demonstrate that our method significantly outperforms state-of-the-art methods and substantially boosts the FLoc accuracy. All data and code will be released after the anonymous review.",
        "arxiv_id": "2507.18881",
        "ARXIVID": "2507.18881",
        "COMMENT": "Matches criterion 1 as it presents novel methodological improvements in spatial reasoning for embodied agents by incorporating 3D geometric priors into floorplan localization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2507.19002": {
        "authors": [
            "Ying Ba",
            "Tianyu Zhang",
            "Yalong Bai",
            "Wenyi Mo",
            "Tao Liang",
            "Bing Su",
            "Ji-Rong Wen"
        ],
        "title": "Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment",
        "abstract": "arXiv:2507.19002v1 Announce Type: new  Abstract: Contemporary image generation systems have achieved high fidelity and superior aesthetic quality beyond basic text-image alignment. However, existing evaluation frameworks have failed to evolve in parallel. This study reveals that human preference reward models fine-tuned based on CLIP and BLIP architectures have inherent flaws: they inappropriately assign low scores to images with rich details and high aesthetic value, creating a significant discrepancy with actual human aesthetic preferences. To address this issue, we design a novel evaluation score, ICT (Image-Contained-Text) score, that achieves and surpasses the objectives of text-image alignment by assessing the degree to which images represent textual content. Building upon this foundation, we further train an HP (High-Preference) score model using solely the image modality to enhance image aesthetics and detail quality while maintaining text-image alignment. Experiments demonstrate that the proposed evaluation model improves scoring accuracy by over 10\\% compared to existing methods, and achieves significant results in optimizing state-of-the-art text-to-image models. This research provides theoretical and empirical support for evolving image generation technology toward higher-order human aesthetic preferences. Code is available at https://github.com/BarretBa/ICTHP.",
        "arxiv_id": "2507.19002",
        "ARXIVID": "2507.19002",
        "COMMENT": "Matches criteria 5 as it focuses on enhancing reward models for image generation, integrating image understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.19459": {
        "authors": [
            "Pol Francesch Huc",
            "Emily Bates",
            "Simone D'Amico"
        ],
        "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization",
        "abstract": "arXiv:2507.19459v1 Announce Type: new  Abstract: The advent of novel view synthesis techniques such as NeRF and 3D Gaussian Splatting (3DGS) has enabled learning precise 3D models only from posed monocular images. Although these methods are attractive, they hold two major limitations that prevent their use in space applications: they require poses during training, and have high computational cost at training and inference. To address these limitations, this work contributes: (1) a Convolutional Neural Network (CNN) based primitive initializer for 3DGS using monocular images; (2) a pipeline capable of training with noisy or implicit pose estimates; and (3) and analysis of initialization variants that reduce the training cost of precise 3D models. A CNN takes a single image as input and outputs a coarse 3D model represented as an assembly of primitives, along with the target's pose relative to the camera. This assembly of primitives is then used to initialize 3DGS, significantly reducing the number of training iterations and input images needed -- by at least an order of magnitude. For additional flexibility, the CNN component has multiple variants with different pose estimation techniques. This work performs a comparison between these variants, evaluating their effectiveness for downstream 3DGS training under noisy or implicit pose estimates. The results demonstrate that even with imperfect pose supervision, the pipeline is able to learn high-fidelity 3D representations, opening the door for the use of novel view synthesis in space applications.",
        "arxiv_id": "2507.19459",
        "ARXIVID": "2507.19459",
        "COMMENT": "Matches criteria 3 as it introduces a novel method for learning 3D models of spacecraft, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2507.18863": {
        "authors": [
            "Matthew Kit Khinn Teng",
            "Haibo Zhang",
            "Takeshi Saitoh"
        ],
        "title": "Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction",
        "abstract": "arXiv:2507.18863v1 Announce Type: new  Abstract: Visual Automatic Speech Recognition (V-ASR) is a challenging task that involves interpreting spoken language solely from visual information, such as lip movements and facial expressions. This task is notably challenging due to the absence of auditory cues and the visual ambiguity of phonemes that exhibit similar visemes-distinct sounds that appear identical in lip motions. Existing methods often aim to predict words or characters directly from visual cues, but they commonly suffer from high error rates due to viseme ambiguity and require large amounts of pre-training data. We propose a novel phoneme-based two-stage framework that fuses visual and landmark motion features, followed by an LLM model for word reconstruction to address these challenges. Stage 1 consists of V-ASR, which outputs the predicted phonemes, thereby reducing training complexity. Meanwhile, the facial landmark features address speaker-specific facial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB, that reconstructs the output phonemes back to words. Besides using a large visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates superior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the LRS3 dataset.",
        "arxiv_id": "2507.18863",
        "ARXIVID": "2507.18863",
        "COMMENT": "Matches criteria 5 as it integrates visual speech recognition with a language model for phoneme-level tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2507.18743": {
        "authors": [
            "Xinjun Cheng",
            "Yiguo He",
            "Junjie Zhu",
            "Chunping Qiu",
            "Jun Wang",
            "Qiangjuan Huang",
            "Ke Yang"
        ],
        "title": "SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning",
        "abstract": "arXiv:2507.18743v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have achieved remarkable breakthroughs in the field of remote sensing in recent years. Synthetic Aperture Radar (SAR) imagery, with its all-weather capability, is essential in remote sensing, yet the lack of large-scale, high-quality SAR image-text datasets hinders its semantic understanding. In this paper, we construct SAR-Text, a large-scale and high-quality dataset consisting of over 130,000 SAR image-text pairs. To construct the SAR-Text dataset, we design the SAR-Narrator framework, which generates textual descriptions for SAR images through a multi-stage progressive transfer learning strategy. To verify the effectiveness of the SAR-TEXT dataset, we conduct experiments on three typical vision-language tasks: image-text retrieval, image captioning, and visual question answering (VQA). Specifically, we construct three representative models on SAR-TEXT: SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable improvements in retrieval performance, boosting average recall by 16.43% and 10.54% on the OSdataset-512 and HRSID test sets, respectively. In the captioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding those of the original CoCa model by more than 8x, 4x, and 10x, respectively. In the VQA task, SAR-GPT outperforms baseline and single-stage models on multiple SAR-VQA datasets, demonstrating stronger semantic understanding and reasoning ability, as further confirmed by qualitative results. It is worth noting that, as a flexible captioning tool, SAR-Narrator can be readily adopted by the community to construct larger-scale SAR image-text datasets.",
        "arxiv_id": "2507.18743",
        "ARXIVID": "2507.18743",
        "COMMENT": "Matches criterion 2. The paper explores Vision Language Models (VLMs) and their application to SAR imagery, which aligns with the interest in multimodal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.19481": {
        "authors": [
            "Byungjun Kim",
            "Shunsuke Saito",
            "Giljoo Nam",
            "Tomas Simon",
            "Jason Saragih",
            "Hanbyul Joo",
            "Junxuan Li"
        ],
        "title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars",
        "abstract": "arXiv:2507.19481v1 Announce Type: new  Abstract: We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.",
        "arxiv_id": "2507.19481",
        "ARXIVID": "2507.19481",
        "COMMENT": "Matches criteria 4 as it focuses on a foundation model for 3D head avatars with explicit compositionality.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.18763": {
        "authors": [
            "Keshav Gupta",
            "Tejas S. Stanley",
            "Pranjal Paul",
            "Arun K. Singh",
            "K. Madhava Krishna"
        ],
        "title": "Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving",
        "abstract": "arXiv:2507.18763v1 Announce Type: new  Abstract: Drivable Free-space prediction is a fundamental and crucial problem in autonomous driving. Recent works have addressed the problem by representing the entire non-obstacle road regions as the free-space. In contrast our aim is to estimate the driving corridors that are a navigable subset of the entire road region. Unfortunately, existing corridor estimation methods directly assume a BEV-centric representation, which is hard to obtain. In contrast, we frame drivable free-space corridor prediction as a pure image perception task, using only monocular camera input. However such a formulation poses several challenges as one doesn't have the corresponding data for such free-space corridor segments in the image. Consequently, we develop a novel self-supervised approach for free-space sample generation by leveraging future ego trajectories and front-view camera images, making the process of visual corridor estimation dependent on the ego trajectory. We then employ a diffusion process to model the distribution of such segments in the image. However, the existing binary mask-based representation for a segment poses many limitations. Therefore, we introduce ContourDiff, a specialized diffusion-based architecture that denoises over contour points rather than relying on binary mask representations, enabling structured and interpretable free-space predictions. We evaluate our approach qualitatively and quantitatively on both nuScenes and CARLA, demonstrating its effectiveness in accurately predicting safe multimodal navigable corridors in the image.",
        "arxiv_id": "2507.18763",
        "ARXIVID": "2507.18763",
        "COMMENT": "Matches criteria 3 as it introduces a novel self-supervised method for free-space prediction in autonomous driving, relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.18923": {
        "authors": [
            "Zhentao Huang",
            "Di Wu",
            "Zhenbang He",
            "Minglun Gong"
        ],
        "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization",
        "abstract": "arXiv:2507.18923v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its flexible representation, yet fails to accurately reconstruct scene geometry. While modern variants like PGSR introduce additional losses to ensure proper depth and normal maps through Gaussian fusion, they still neglect individual placement optimization. This results in unevenly distributed Gaussians that deviate from the latent surface, complicating both reconstruction refinement and scene editing. Motivated by pioneering work on Point Set Surfaces, we propose Gaussian Set Surface Reconstruction (GSSR), a method designed to distribute Gaussians evenly along the latent surface while aligning their dominant normals with the surface normal. GSSR enforces fine-grained geometric alignment through a combination of pixel-level and Gaussian-level single-view normal consistency and multi-view photometric consistency, optimizing both local and global perspectives. To further refine the representation, we introduce an opacity regularization loss to eliminate redundant Gaussians and apply periodic depth- and normal-guided Gaussian reinitialization for a cleaner, more uniform spatial distribution. Our reconstruction results demonstrate significantly improved geometric precision in Gaussian placement, enabling intuitive scene editing and efficient generation of novel Gaussian-based 3D environments. Extensive experiments validate GSSR's effectiveness, showing enhanced geometric accuracy while preserving high-quality rendering performance.",
        "arxiv_id": "2507.18923",
        "ARXIVID": "2507.18923",
        "COMMENT": "Relevant to criteria 3 as it introduces a novel method for 3D Gaussian Splatting with improved geometric precision, which could be useful for embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.18929": {
        "authors": [
            "Jian Chen",
            "Yuxuan Hu",
            "Haifeng Lu",
            "Wei Wang",
            "Min Yang",
            "Chengming Li",
            "Xiping Hu"
        ],
        "title": "MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition",
        "abstract": "arXiv:2507.18929v1 Announce Type: new  Abstract: Although pre-trained visual models with text have demonstrated strong capabilities in visual feature extraction, sticker emotion understanding remains challenging due to its reliance on multi-view information, such as background knowledge and stylistic cues. To address this, we propose a novel multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view sticker interpreter based on Multimodal Large Language Models. Specifically, inspired by the human ability to interpret sticker emotions from multiple views, we first use Multimodal Large Language Models to interpret stickers by providing rich textual context via multi-view descriptions. Then, we design a hierarchical fusion strategy to fuse the textual context into visual understanding, which builds upon a pyramid visual transformer to extract both global and local sticker features at multiple stages. Through contrastive learning and attention mechanisms, textual features are injected at different stages of the visual backbone, enhancing the fusion of global- and local-granularity visual semantics with textual guidance. Finally, we introduce a text-guided fusion attention mechanism to effectively integrate the overall multimodal features, enhancing semantic understanding. Extensive experiments on 2 public sticker emotion datasets demonstrate that MGHFT significantly outperforms existing sticker emotion recognition approaches, achieving higher accuracy and more fine-grained emotion recognition. Compared to the best pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4% on F1 and 4.0% on accuracy. The code is released at https://github.com/cccccj-03/MGHFT_ACMMM2025.",
        "arxiv_id": "2507.18929",
        "ARXIVID": "2507.18929",
        "COMMENT": "Matches criterion 2 as it explores a multimodal large language model (MLLM) for sticker emotion recognition, integrating vision and language.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2507.18870": {
        "authors": [
            "Keke Tang",
            "Yuze Gao",
            "Weilong Peng",
            "Xiaofei Wang",
            "Meie Fang",
            "Peican Zhu"
        ],
        "title": "Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform",
        "abstract": "arXiv:2507.18870v1 Announce Type: new  Abstract: Studying adversarial attacks on point clouds is essential for evaluating and improving the robustness of 3D deep learning models. However, most existing attack methods are developed under ideal white-box settings and often suffer from limited transferability to unseen models and insufficient robustness against common defense mechanisms. In this paper, we propose MAT-Adv, a novel adversarial attack framework that enhances both transferability and undefendability by explicitly perturbing the medial axis transform (MAT) representations, in order to induce inherent adversarialness in the resulting point clouds. Specifically, we employ an autoencoder to project input point clouds into compact MAT representations that capture the intrinsic geometric structure of point clouds. By perturbing these intrinsic representations, MAT-Adv introduces structural-level adversarial characteristics that remain effective across diverse models and defense strategies. To mitigate overfitting and prevent perturbation collapse, we incorporate a dropout strategy into the optimization of MAT perturbations, further improving transferability and undefendability. Extensive experiments demonstrate that MAT-Adv significantly outperforms existing state-of-the-art methods in both transferability and undefendability. Codes will be made public upon paper acceptance.",
        "arxiv_id": "2507.18870",
        "ARXIVID": "2507.18870",
        "COMMENT": "Does not match any specific criteria but is relevant to robustness in 3D deep learning, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.20758": {
        "authors": [
            "Hao Yang",
            "Qinghua Zhao",
            "Lei Li"
        ],
        "title": "How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation",
        "abstract": "arXiv:2507.20758v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet its internal mechanisms remain poorly understood. We analyze CoT's operational principles by reversely tracing information flow across decoding, projection, and activation phases. Our quantitative analysis suggests that CoT may serve as a decoding space pruner, leveraging answer templates to guide output generation, with higher template adherence strongly correlating with improved performance. Furthermore, we surprisingly find that CoT modulates neuron engagement in a task-dependent manner: reducing neuron activation in open-domain tasks, yet increasing it in closed-domain scenarios. These findings offer a novel mechanistic interpretability framework and critical insights for enabling targeted CoT interventions to design more efficient and robust prompts. We released our code and data at https://anonymous.4open.science/r/cot-D247.",
        "arxiv_id": "2507.20758",
        "ARXIVID": "2507.20758",
        "COMMENT": "Does not match any specific criteria but provides insights into chain-of-thought reasoning in LLMs, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.20541": {
        "authors": [
            "Zishang Qiu",
            "Xinan Chen",
            "Long Chen",
            "Ruibin Bai"
        ],
        "title": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design",
        "abstract": "arXiv:2507.20541v1 Announce Type: new  Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of \"prompt evolution\" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA's architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.",
        "arxiv_id": "2507.20541",
        "ARXIVID": "2507.20541",
        "COMMENT": "Does not match any specific criteria but explores heuristic design using LLMs, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.18713": {
        "authors": [
            "Yun Chen",
            "Matthew Haines",
            "Jingkang Wang",
            "Krzysztof Baron-Lis",
            "Sivabalan Manivasagam",
            "Ze Yang",
            "Raquel Urtasun"
        ],
        "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time",
        "abstract": "arXiv:2507.18713v1 Announce Type: new  Abstract: High-fidelity sensor simulation of light-based sensors such as cameras and LiDARs is critical for safe and accurate autonomy testing. Neural radiance field (NeRF)-based methods that reconstruct sensor observations via ray-casting of implicit representations have demonstrated accurate simulation of driving scenes, but are slow to train and render, hampering scale. 3D Gaussian Splatting (3DGS) has demonstrated faster training and rendering times through rasterization, but is primarily restricted to pinhole camera sensors, preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both NeRF and 3DGS couple the representation with the rendering procedure (implicit networks for ray-based evaluation, particles for rasterization), preventing interoperability, which is key for general usage. In this work, we present Sparse Local Fields (SaLF), a novel volumetric representation that supports rasterization and raytracing. SaLF represents volumes as a sparse set of 3D voxel primitives, where each voxel is a local implicit field. SaLF has fast training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS LiDAR), has adaptive pruning and densification to easily handle large scenes, and can support non-pinhole cameras and spinning LiDARs. We demonstrate that SaLF has similar realism as existing self-driving sensor simulation methods while improving efficiency and enhancing capabilities, enabling more scalable simulation. https://waabi.ai/salf/",
        "arxiv_id": "2507.18713",
        "ARXIVID": "2507.18713",
        "COMMENT": "Does not match any specific criteria but is relevant to simulation and rendering for autonomous systems, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.21046": {
        "authors": [
            "Huan-ang Gao",
            "Jiayi Geng",
            "Wenyue Hua",
            "Mengkang Hu",
            "Xinzhe Juan",
            "Hongzhang Liu",
            "Shilong Liu",
            "Jiahao Qiu",
            "Xuan Qi",
            "Yiran Wu",
            "Hongru Wang",
            "Han Xiao",
            "Yuhang Zhou",
            "Shaokun Zhang",
            "Jiayi Zhang",
            "Jinyu Xiang",
            "Yixiong Fang",
            "Qiwen Zhao",
            "Dongrui Liu",
            "Qihan Ren",
            "Cheng Qian",
            "Zhenghailong Wang",
            "Minda Hu",
            "Huazheng Wang",
            "Qingyun Wu",
            "Heng Ji",
            "Mengdi Wang"
        ],
        "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence",
        "abstract": "arXiv:2507.21046v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.",
        "arxiv_id": "2507.21046",
        "ARXIVID": "2507.21046",
        "COMMENT": "Does not match any specific criteria but is a survey paper on self-evolving agents, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19608": {
        "authors": [
            "Jiawen Qi",
            "Chang Gao",
            "Zhaochun Ren",
            "Qinyu Chen"
        ],
        "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference",
        "abstract": "arXiv:2507.19608v1 Announce Type: new  Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.",
        "arxiv_id": "2507.19608",
        "ARXIVID": "2507.19608",
        "COMMENT": "Does not match any specific criteria but is relevant to efficient LLM inference, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19098": {
        "authors": [
            "Francisco Caetano",
            "Lemar Abdi",
            "Christiaan Viviers",
            "Amaan Valiuddin",
            "Fons van der Sommen"
        ],
        "title": "MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching",
        "abstract": "arXiv:2507.19098v1 Announce Type: new  Abstract: Reliable medical image classification requires accurate predictions and well-calibrated uncertainty estimates, especially in high-stakes clinical settings. This work presents MedSymmFlow, a generative-discriminative hybrid model built on Symmetrical Flow Matching, designed to unify classification, generation, and uncertainty quantification in medical imaging. MedSymmFlow leverages a latent-space formulation that scales to high-resolution inputs and introduces a semantic mask conditioning mechanism to enhance diagnostic relevance. Unlike standard discriminative models, it naturally estimates uncertainty through its generative sampling process. The model is evaluated on four MedMNIST datasets, covering a range of modalities and pathologies. The results show that MedSymmFlow matches or exceeds the performance of established baselines in classification accuracy and AUC, while also delivering reliable uncertainty estimates validated by performance improvements under selective prediction.",
        "arxiv_id": "2507.19098",
        "ARXIVID": "2507.19098",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and classification in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.20280": {
        "authors": [
            "Keyan Ding",
            "Jing Yu",
            "Junjie Huang",
            "Yuchen Yang",
            "Qiang Zhang",
            "Huajun Chen"
        ],
        "title": "SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration",
        "abstract": "arXiv:2507.20280v1 Announce Type: new  Abstract: Scientific research increasingly relies on specialized computational tools, yet effectively utilizing these tools demands substantial domain expertise. While Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials science. At its core, SciToolAgent leverages a scientific tool knowledge graph that enables intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage. Extensive evaluations on a curated benchmark demonstrate that SciToolAgent significantly outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening further demonstrate SciToolAgent's capability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts.",
        "arxiv_id": "2507.20280",
        "ARXIVID": "2507.20280",
        "COMMENT": "Does not match any specific criteria but is related to multi-tool integration and LLMs, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19059": {
        "authors": [
            "Xiaocheng Fang",
            "Jieyi Cai",
            "Huanyu Liu",
            "Wenxiu Cai",
            "Yishu Liu",
            "Bingzhi Chen"
        ],
        "title": "Revisiting DETR for Small Object Detection via Noise-Resilient Query Optimization",
        "abstract": "arXiv:2507.19059v1 Announce Type: new  Abstract: Despite advancements in Transformer-based detectors for small object detection (SOD), recent studies show that these detectors still face challenges due to inherent noise sensitivity in feature pyramid networks (FPN) and diminished query quality in existing label assignment strategies. In this paper, we propose a novel Noise-Resilient Query Optimization (NRQO) paradigm, which innovatively incorporates the Noise-Tolerance Feature Pyramid Network (NT-FPN) and the Pairwise-Similarity Region Proposal Network (PS-RPN). Specifically, NT-FPN mitigates noise during feature fusion in FPN by preserving spatial and semantic information integrity. Unlike existing label assignment strategies, PS-RPN generates a sufficient number of high-quality positive queries by enhancing anchor-ground truth matching through position and shape similarities, without the need for additional hyperparameters. Extensive experiments on multiple benchmarks consistently demonstrate the superiority of NRQO over state-of-the-art baselines.",
        "arxiv_id": "2507.19059",
        "ARXIVID": "2507.19059",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision advancements in small object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.20199": {
        "authors": [
            "Shijie Shang",
            "Ruosi Wan",
            "Yue Peng",
            "Yutong Wu",
            "Xiong-hui Chen",
            "Jie Yan",
            "Xiangyu Zhang"
        ],
        "title": "StepFun-Prover Preview: Let's Think and Verify Step by Step",
        "abstract": "arXiv:2507.20199v1 Announce Type: new  Abstract: We present StepFun-Prover Preview, a large language model designed for formal theorem proving through tool-integrated reasoning. Using a reinforcement learning pipeline that incorporates tool-based interactions, StepFun-Prover can achieve strong performance in generating Lean 4 proofs with minimal sampling. Our approach enables the model to emulate human-like problem-solving strategies by iteratively refining proofs based on real-time environment feedback. On the miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of $70.0\\%$. Beyond advancing benchmark performance, we introduce an end-to-end training framework for developing tool-integrated reasoning models, offering a promising direction for automated theorem proving and Math AI assistant.",
        "arxiv_id": "2507.20199",
        "ARXIVID": "2507.20199",
        "COMMENT": "Does not match any specific criteria but is related to general advancements in AI and theorem proving.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.19209": {
        "authors": [
            "Xiaoyu Zhang",
            "Zhifeng Bao",
            "Hai Dong",
            "Ziwei Wang",
            "Jiajun Liu"
        ],
        "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet",
        "abstract": "arXiv:2507.19209v1 Announce Type: new  Abstract: Autonomous vehicles generate massive volumes of point cloud data, yet only a subset is relevant for specific tasks such as collision detection, traffic analysis, or congestion monitoring. Effectively querying this data is essential to enable targeted analytics. In this work, we formalize point cloud querying by defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each aligned with distinct analytical scenarios. All these queries rely heavily on accurate object counts to produce meaningful results, making precise object counting a critical component of query execution. Prior work has focused on indexing techniques for 2D video data, assuming detection models provide accurate counting information. However, when applied to 3D point cloud data, state-of-the-art detection models often fail to generate reliable object counts, leading to substantial errors in query results. To address this limitation, we propose CounterNet, a heatmap-based network designed for accurate object counting in large-scale point cloud data. Rather than focusing on accurate object localization, CounterNet detects object presence by finding object centers to improve counting accuracy. We further enhance its performance with a feature map partitioning strategy using overlapping regions, enabling better handling of both small and large objects in complex traffic scenes. To adapt to varying frame characteristics, we introduce a per-frame dynamic model selection strategy that selects the most effective configuration for each input. Evaluations on three real-world autonomous vehicle datasets show that CounterNet improves counting accuracy by 5% to 20% across object categories, resulting in more reliable query outcomes across all supported query types.",
        "arxiv_id": "2507.19209",
        "ARXIVID": "2507.19209",
        "COMMENT": "Does not match any specific criteria but is related to point cloud data and object counting, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}