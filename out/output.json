{
    "2508.04369": {
        "authors": [
            "Canhui Tang",
            "Zifan Han",
            "Hongbo Sun",
            "Sanping Zhou",
            "Xuchong Zhang",
            "Xin Wei",
            "Ye Yuan",
            "Jinglin Xu",
            "Hao Sun"
        ],
        "title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding",
        "abstract": "arXiv:2508.04369v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant progress in vision-language tasks, yet they still face challenges when processing long-duration video inputs. The limitation arises from MLLMs' context limit and training costs, necessitating sparse frame sampling before feeding videos into MLLMs. Existing video MLLMs adopt training-free uniform sampling or keyframe search, which may miss critical events or be constrained by the pre-trained models' event understanding capabilities. Meanwhile, building a training-based method remains challenging due to the unsupervised and non-differentiable nature of sparse frame sampling. To address these problems, we propose Temporal Sampling Policy Optimization (TSPO), advancing MLLMs' long-form video-language understanding via reinforcement learning. Specifically, we first propose a trainable event-aware temporal agent, which captures event-query correlation for performing probabilistic keyframe selection. Then, we propose the TSPO reinforcement learning paradigm, which models keyframe selection and language generation as a joint decision-making process, enabling end-to-end group relative optimization with efficient rule-based rewards. Furthermore, for the TSPO's training, we propose a long video training data construction pipeline with comprehensive temporal data and video Needle-in-a-Haystack data. Finally, we incorporate rule-based answering accuracy and temporal locating reward mechanisms to optimize the temporal sampling policy. Comprehensive experiments show that our TSPO achieves state-of-the-art performance across multiple long video understanding benchmarks, and shows transferable ability across different cutting-edge Video-MLLMs.",
        "arxiv_id": "2508.04369",
        "ARXIVID": "2508.04369",
        "COMMENT": "This paper matches Criterion 2 and Criterion 6 as it explores Multimodal Large Language Models (MLLMs) for long-form video understanding, addressing challenges in video-based tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.04361": {
        "authors": [
            "Fuqing Bie",
            "Shiyu Huang",
            "Xijia Tao",
            "Zhiqin Fang",
            "Leyi Pan",
            "Junzhe Chen",
            "Min Ren",
            "Liuyu Xiang",
            "Zhaofeng He"
        ],
        "title": "OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing",
        "abstract": "arXiv:2508.04361v1 Announce Type: new  Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate impressive multi-modal competence, existing evaluations fail to test their intelligence in dynamic, interactive worlds. Static benchmarks lack agency, while interactive benchmarks suffer from a severe modal bottleneck, typically ignoring crucial auditory and temporal cues. To bridge this evaluation chasm, we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate, but to probe the fusion and reasoning capabilities of agentic models across the full sensory spectrum. Built on a core philosophy of modality interdependence, OmniPlay comprises a suite of five game environments that systematically create scenarios of both synergy and conflict, forcing agents to perform genuine cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal models reveals a critical dichotomy: they exhibit superhuman performance on high-fidelity memory tasks but suffer from systemic failures in challenges requiring robust reasoning and strategic planning. We demonstrate that this fragility stems from brittle fusion mechanisms, which lead to catastrophic performance degradation under modality conflict and uncover a counter-intuitive \"less is more\" paradox, where removing sensory information can paradoxically improve performance. Our findings suggest that the path toward robust AGI requires a research focus beyond scaling to explicitly address synergistic fusion. Our platform is available for anonymous review at https://github.com/fuqingbie/omni-game-benchmark.",
        "arxiv_id": "2508.04361",
        "ARXIVID": "2508.04361",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark for omni-modal game playing, focusing on multi-modal reasoning and interaction.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2508.04424": {
        "authors": [
            "Tong Wang",
            "Guanyu Yang",
            "Nian Liu",
            "Zongyan Han",
            "Jinxing Zhou",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "title": "Composed Object Retrieval: Object-level Retrieval via Composed Expressions",
        "abstract": "arXiv:2508.04424v1 Announce Type: new  Abstract: Retrieving fine-grained visual content based on user intent remains a challenge in multi-modal systems. Although current Composed Image Retrieval (CIR) methods combine reference images with retrieval texts, they are constrained to image-level matching and cannot localize specific objects. To this end, we propose Composed Object Retrieval (COR), a brand-new task that goes beyond image-level retrieval to achieve object-level precision, allowing the retrieval and segmentation of target objects based on composed expressions combining reference objects and retrieval texts. COR presents significant challenges in retrieval flexibility, which requires systems to identify arbitrary objects satisfying composed expressions while avoiding semantically similar but irrelevant negative objects within the same scene. We construct COR127K, the first large-scale COR benchmark that contains 127,166 retrieval triplets with various semantic transformations in 408 categories. We also present CORE, a unified end-to-end model that integrates reference region encoding, adaptive visual-textual interaction, and region-level contrastive learning. Extensive experiments demonstrate that CORE significantly outperforms existing models in both base and novel categories, establishing a simple and effective baseline for this challenging task while opening new directions for fine-grained multi-modal retrieval research.",
        "arxiv_id": "2508.04424",
        "ARXIVID": "2508.04424",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it introduces a task and model for object-level retrieval using composed expressions, integrating vision and language.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.04546": {
        "authors": [
            "Minghang Zheng",
            "Yuxin Peng",
            "Benyuan Sun",
            "Yi Yang",
            "Yang Liu"
        ],
        "title": "Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding",
        "abstract": "arXiv:2508.04546v1 Announce Type: new  Abstract: In this paper, we tackle the task of online video temporal grounding (OnVTG), which requires the model to locate events related to a given text query within a video stream. Unlike regular video temporal grounding, OnVTG requires the model to make predictions without observing future frames. As online videos are streaming inputs and can go on indefinitely, it is impractical and inefficient to store all historical inputs. The existing OnVTG models employ memory to store recent historical video frame features and predict scores indicating whether the current frame corresponds to the start or end time of the target event. However, these methods lack effective event modeling and cannot retain long-term historical information, leading to low performance. To tackle these challenges, we propose a hierarchical event memory for OnVTG. We propose an event-based OnVTG framework that makes predictions based on event proposals that model event-level information with various durations. To preserve historically valuable event information, we introduce a hierarchical event memory that retains historical events, allowing the model to access both recent and long-term information. To enable the real-time prediction, we further propose a future prediction branch that predicts whether the target event will occur shortly and further regresses the start time of the event. We achieve state-of-the-art performance on the TACoS, ActivityNet Captions, and MAD datasets. Code is available at https://github.com/minghangz/OnVTG.",
        "arxiv_id": "2508.04546",
        "ARXIVID": "2508.04546",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it proposes a hierarchical event memory for online video temporal grounding, addressing challenges in real-time video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.04611": {
        "authors": [
            "Tongfan Guan",
            "Jiaxin Guo",
            "Chen Wang",
            "Yun-Hui Liu"
        ],
        "title": "OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment",
        "abstract": "arXiv:2508.04611v1 Announce Type: new  Abstract: Monocular and stereo depth estimation offer complementary strengths: monocular methods capture rich contextual priors but lack geometric precision, while stereo approaches leverage epipolar geometry yet struggle with ambiguities such as reflective or textureless surfaces. Despite post-hoc synergies, these paradigms remain largely disjoint in practice. We introduce OmniDepth, a unified framework that bridges both through iterative bidirectional alignment of their latent representations. At its core, a novel cross-attentive alignment mechanism dynamically synchronizes monocular contextual cues with stereo hypothesis representations during stereo reasoning. This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by injecting monocular structure priors while refining monocular depth with stereo geometry within a single network. Extensive experiments demonstrate state-of-the-art results: \\textbf{OmniDepth reduces zero-shot generalization error by $\\!>\\!40\\%$ on Middlebury and ETH3D}, while addressing longstanding failures on transparent and reflective surfaces. By harmonizing multi-view geometry with monocular context, OmniDepth enables robust 3D perception that transcends modality-specific limitations. Codes available at https://github.com/aeolusguan/OmniDepth.",
        "arxiv_id": "2508.04611",
        "ARXIVID": "2508.04611",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a novel framework for depth estimation that bridges monocular and stereo reasoning, improving spatial reasoning for embodied agents.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2508.04682": {
        "authors": [
            "Zewei Zhou",
            "Seth Z. Zhao",
            "Tianhui Cai",
            "Zhiyu Huang",
            "Bolei Zhou",
            "Jiaqi Ma"
        ],
        "title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction",
        "abstract": "arXiv:2508.04682v1 Announce Type: new  Abstract: End-to-end training of multi-agent systems offers significant advantages in improving multi-task performance. However, training such models remains challenging and requires extensive manual design and monitoring. In this work, we introduce TurboTrain, a novel and efficient training framework for multi-agent perception and prediction. TurboTrain comprises two key components: a multi-agent spatiotemporal pretraining scheme based on masked reconstruction learning and a balanced multi-task learning strategy based on gradient conflict suppression. By streamlining the training process, our framework eliminates the need for manually designing and tuning complex multi-stage training pipelines, substantially reducing training time and improving performance. We evaluate TurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and demonstrate that it further improves the performance of state-of-the-art multi-agent perception and prediction models. Our results highlight that pretraining effectively captures spatiotemporal multi-agent features and significantly benefits downstream tasks. Moreover, the proposed balanced multi-task learning strategy enhances detection and prediction.",
        "arxiv_id": "2508.04682",
        "ARXIVID": "2508.04682",
        "COMMENT": "This paper aligns with Criterion 3 as it introduces a novel training framework for multi-agent perception and prediction, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04197": {
        "authors": [
            "Yan Zhang",
            "Gangyan Zeng",
            "Daiqing Wu",
            "Huawen Shen",
            "Binbin Li",
            "Yu Zhou",
            "Can Ma",
            "Xiaojun Bi"
        ],
        "title": "Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective",
        "abstract": "arXiv:2508.04197v1 Announce Type: new  Abstract: Video text-based visual question answering (Video TextVQA) aims to answer questions by explicitly reading and reasoning about the text involved in a video. Most works in this field follow a frame-level framework which suffers from redundant text entities and implicit relation modeling, resulting in limitations in both accuracy and efficiency. In this paper, we rethink the Video TextVQA task from an instance-oriented perspective and propose a novel model termed GAT (Gather and Trace). First, to obtain accurate reading result for each video text instance, a context-aggregated instance gathering module is designed to integrate the visual appearance, layout characteristics, and textual contents of the related entities into a unified textual representation. Then, to capture dynamic evolution of text in the video flow, an instance-focused trajectory tracing module is utilized to establish spatio-temporal relationships between instances and infer the final answer. Extensive experiments on several public Video TextVQA datasets validate the effectiveness and generalization of our framework. GAT outperforms existing Video TextVQA methods, video-language pretraining methods, and video large language models in both accuracy and inference speed. Notably, GAT surpasses the previous state-of-the-art Video TextVQA methods by 3.86\\% in accuracy and achieves ten times of faster inference speed than video large language models. The source code is available at https://github.com/zhangyan-ucas/GAT.",
        "arxiv_id": "2508.04197",
        "ARXIVID": "2508.04197",
        "COMMENT": "This paper matches Criterion 6 as it focuses on video-based tasks, specifically Video TextVQA, and introduces novel methodologies for video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04043": {
        "authors": [
            "Yuheng Ji",
            "Yipu Wang",
            "Yuyang Liu",
            "Xiaoshuai Hao",
            "Yue Liu",
            "Yuting Zhao",
            "Huaihai Lyu",
            "Xiaolong Zheng"
        ],
        "title": "VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning",
        "abstract": "arXiv:2508.04043v1 Announce Type: new  Abstract: Visual transformation reasoning (VTR) is a vital cognitive capability that empowers intelligent agents to understand dynamic scenes, model causal relationships, and predict future states, and thereby guiding actions and laying the foundation for advanced intelligent systems. However, existing benchmarks suffer from a sim-to-real gap, limited task complexity, and incomplete reasoning coverage, limiting their practical use in real-world scenarios. To address these limitations, we introduce VisualTrans, the first comprehensive benchmark specifically designed for VTR in real-world human-object interaction scenarios. VisualTrans encompasses 12 semantically diverse manipulation tasks and systematically evaluates three essential reasoning dimensions - spatial, procedural, and quantitative - through 6 well-defined subtask types. The benchmark features 472 high-quality question-answer pairs in various formats, including multiple-choice, open-ended counting, and target enumeration. We introduce a scalable data construction pipeline built upon first-person manipulation videos, which integrates task selection, image pair extraction, automated metadata annotation with large multimodal models, and structured question generation. Human verification ensures the final benchmark is both high-quality and interpretable. Evaluations of various state-of-the-art vision-language models show strong performance in static spatial tasks. However, they reveal notable shortcomings in dynamic, multi-step reasoning scenarios, particularly in areas like intermediate state recognition and transformation sequence planning. These findings highlight fundamental weaknesses in temporal modeling and causal reasoning, providing clear directions for future research aimed at developing more capable and generalizable VTR systems. The dataset and code are available at https://github.com/WangYipu2002/VisualTrans.",
        "arxiv_id": "2508.04043",
        "ARXIVID": "2508.04043",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a benchmark for visual transformation reasoning in real-world scenarios, focusing on dynamic scene understanding and causal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04260": {
        "authors": [
            "Xiao Wang",
            "Ziwen Wang",
            "Wentao Wu",
            "Anjie Wang",
            "Jiashu Wu",
            "Yantao Pan",
            "Chenglong Li"
        ],
        "title": "Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark",
        "abstract": "arXiv:2508.04260v1 Announce Type: new  Abstract: With the rapid advancement of autonomous driving, vehicle perception, particularly detection and segmentation, has placed increasingly higher demands on algorithmic performance. Pre-trained large segmentation models, especially Segment Anything Model (SAM), have sparked significant interest and inspired new research directions in artificial intelligence. However, SAM cannot be directly applied to the fine-grained task of vehicle part segmentation, as its text-prompted segmentation functionality is not publicly accessible, and the mask regions generated by its default mode lack semantic labels, limiting its utility in structured, category-specific segmentation tasks. To address these limitations, we propose SAV, a novel framework comprising three core components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a context sample retrieval encoding module. The knowledge graph explicitly models the spatial and geometric relationships among vehicle parts through a structured ontology, effectively encoding prior structural knowledge. Meanwhile, the context retrieval module enhances segmentation by identifying and leveraging visually similar vehicle instances from training data, providing rich contextual priors for improved generalization. Furthermore, we introduce a new large-scale benchmark dataset for vehicle part segmentation, named VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations across diverse scenes and viewpoints. We conduct comprehensive experiments on this dataset and two other datasets, benchmarking multiple representative baselines to establish a solid foundation for future research and comparison. % Both the dataset and source code of this paper will be released upon acceptance. Both the dataset and source code of this paper will be released on https://github.com/Event-AHU/SAV",
        "arxiv_id": "2508.04260",
        "ARXIVID": "2508.04260",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it extends the Segment Anything Model (SAM) for fine-grained vehicle part segmentation and introduces a new benchmark dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04567": {
        "authors": [
            "Yifan Li",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Lei Fang",
            "Ji-Rong Wen"
        ],
        "title": "Analyzing and Mitigating Object Hallucination: A Training Bias Perspective",
        "abstract": "arXiv:2508.04567v1 Announce Type: new  Abstract: As scaling up training data has significantly improved the general multimodal capabilities of Large Vision-Language Models (LVLMs), they still suffer from the hallucination issue, generating text that is inconsistent with the visual input. This phenomenon motivates us to systematically investigate the role of training data in hallucination. We introduce a new benchmark, POPEv2, which consists of counterfactual images collected from the training data of LVLMs with certain objects masked. Through comprehensive evaluation on POPEv2, we find that current LVLMs suffer from training bias: they fail to fully leverage their training data and hallucinate more frequently on images seen during training. Specifically, they perform poorly on counterfactual images, often incorrectly answering ``Yes'' to questions about masked objects. To understand this issue, we conduct probing experiments on the models' internal components, revealing that this training bias is primarily located in the language modeling (LM) head. Based on these findings, we propose Obliviate, an efficient and lightweight unlearning method designed to mitigate object hallucination via training bias unlearning. Obliviate identifies the discrepancy between ground-truth labels and model outputs on the training data as a proxy for bias and adopts a parameter- and data-efficient fine-tuning strategy that only updates the LM head. Extensive experiments demonstrate the effectiveness of our approach. While only reusing the training data and updating approximately 2\\% of the parameters, Obliviate significantly reduces hallucination across both discriminative and generative tasks. Furthermore, it demonstrates strong scalability with respect to both model size (2B to 72B) and training data volume, and exhibits promising generalization to hallucination types beyond object-level hallucination. Our code and data will be publicly released.",
        "arxiv_id": "2508.04567",
        "ARXIVID": "2508.04567",
        "COMMENT": "Matches criterion 2 as it explores hallucination in Large Vision-Language Models (LVLMs) and proposes a novel unlearning method to mitigate it.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04120": {
        "authors": [
            "Likai Wang",
            "Ruize Han",
            "Xiangqun Zhang",
            "Wei Feng"
        ],
        "title": "CLIPVehicle: A Unified Framework for Vision-based Vehicle Search",
        "abstract": "arXiv:2508.04120v1 Announce Type: new  Abstract: Vehicles, as one of the most common and significant objects in the real world, the researches on which using computer vision technologies have made remarkable progress, such as vehicle detection, vehicle re-identification, etc. To search an interested vehicle from the surveillance videos, existing methods first pre-detect and store all vehicle patches, and then apply vehicle re-identification models, which is resource-intensive and not very practical. In this work, we aim to achieve the joint detection and re-identification for vehicle search. However, the conflicting objectives between detection that focuses on shared vehicle commonness and re-identification that focuses on individual vehicle uniqueness make it challenging for a model to learn in an end-to-end system. For this problem, we propose a new unified framework, namely CLIPVehicle, which contains a dual-granularity semantic-region alignment module to leverage the VLMs (Vision-Language Models) for vehicle discrimination modeling, and a multi-level vehicle identification learning strategy to learn the identity representation from global, instance and feature levels. We also construct a new benchmark, including a real-world dataset CityFlowVS, and two synthetic datasets SynVS-Day and SynVS-All, for vehicle search. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods of both vehicle Re-ID and person search tasks.",
        "arxiv_id": "2508.04120",
        "ARXIVID": "2508.04120",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it proposes a unified framework leveraging vision-language models for vehicle search.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03722": {
        "authors": [
            "Zhepeng Wang",
            "Yingjian Zhu",
            "Guanghao Dong",
            "Hongzhu Yi",
            "Feng Chen",
            "Xinming Wang",
            "Jun Xie"
        ],
        "title": "Multimodal Video Emotion Recognition with Reliable Reasoning Priors",
        "abstract": "arXiv:2508.03722v1 Announce Type: new  Abstract: This study investigates the integration of trustworthy prior reasoning knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to generate fine-grained, modality-separable reasoning traces, which are injected as priors during the fusion stage to enrich cross-modal interactions. To mitigate the pronounced class-imbalance in multimodal emotion recognition, we introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly balances inter-class and intra-class distributions. Applied to the MER2024 benchmark, our prior-enhanced framework yields substantial performance gains, demonstrating that the reliability of MLLM-derived reasoning can be synergistically combined with the domain adaptability of lightweight fusion networks for robust, scalable emotion recognition.",
        "arxiv_id": "2508.03722",
        "ARXIVID": "2508.03722",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it integrates reasoning from MLLMs into multimodal emotion recognition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.04566": {
        "authors": [
            "Jinxing Zhou",
            "Ziheng Zhou",
            "Yanghao Zhou",
            "Yuxin Mao",
            "Zhangling Duan",
            "Dan Guo"
        ],
        "title": "CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization",
        "abstract": "arXiv:2508.04566v1 Announce Type: new  Abstract: The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally localize events in untrimmed videos that occur simultaneously in both the audio and visual modalities. This paper explores DAVEL under a new and more challenging weakly-supervised setting (W-DAVEL task), where only video-level event labels are provided and the temporal boundaries of each event are unknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors}, which are defined as reliable timestamps that are well predicted under weak supervision and exhibit highly consistent event semantics across audio and visual modalities. Specifically, we propose a \\textit{Mutual Event Agreement Evaluation} module, which generates an agreement score by measuring the discrepancy between the predicted audio and visual event classes. Then, the agreement score is utilized in a \\textit{Cross-modal Salient Anchor Identification} module, which identifies the audio and visual anchor features through global-video and local temporal window identification mechanisms. The anchor features after multimodal integration are fed into an \\textit{Anchor-based Temporal Propagation} module to enhance event semantic encoding in the original temporal audio and visual features, facilitating better temporal localization under weak supervision. We establish benchmarks for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive experiments demonstrate that our method achieves state-of-the-art performance.",
        "arxiv_id": "2508.04566",
        "ARXIVID": "2508.04566",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on dense audio-visual event localization in videos, including novel methodologies and benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2508.03986": {
        "authors": [
            "Yuan Xun",
            "Xiaojun Jia",
            "Xinwei Liu",
            "Hua Zhang"
        ],
        "title": "The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?",
        "abstract": "arXiv:2508.03986v1 Announce Type: new  Abstract: We observe that MLRMs oriented toward human-centric service are highly susceptible to user emotional cues during the deep-thinking stage, often overriding safety protocols or built-in safety checks under high emotional intensity. Inspired by this key insight, we propose EmoAgent, an autonomous adversarial emotion-agent framework that orchestrates exaggerated affective prompts to hijack reasoning pathways. Even when visual risks are correctly identified, models can still produce harmful completions through emotional misalignment. We further identify persistent high-risk failure modes in transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning masked behind seemingly safe responses. These failures expose misalignments between internal inference and surface-level behavior, eluding existing content-based safeguards. To quantify these risks, we introduce three metrics: (1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for evaluating refusal unstability under prompt variants. Extensive experiments on advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper emotional cognitive misalignments in model safety behavior.",
        "arxiv_id": "2508.03986",
        "ARXIVID": "2508.03986",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores emotional misalignment in multimodal large reasoning models, revealing safety vulnerabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.04299": {
        "authors": [
            "Yifan Wang",
            "Ziyi Liu",
            "Xiaolong Sun",
            "Jiawei Wang",
            "Hongmin Liu"
        ],
        "title": "Length Matters: Length-Aware Transformer for Temporal Sentence Grounding",
        "abstract": "arXiv:2508.04299v1 Announce Type: new  Abstract: Temporal sentence grounding (TSG) is a highly challenging task aiming to localize the temporal segment within an untrimmed video corresponding to a given natural language description. Benefiting from the design of learnable queries, the DETR-based models have achieved substantial advancements in the TSG task. However, the absence of explicit supervision often causes the learned queries to overlap in roles, leading to redundant predictions. Therefore, we propose to improve TSG by making each query fulfill its designated role, leveraging the length priors of the video-description pairs. In this paper, we introduce the Length-Aware Transformer (LATR) for TSG, which assigns different queries to handle predictions based on varying temporal lengths. Specifically, we divide all queries into three groups, responsible for segments with short, middle, and long temporal durations, respectively. During training, an additional length classification task is introduced. Predictions from queries with mismatched lengths are suppressed, guiding each query to specialize in its designated function. Extensive experiments demonstrate the effectiveness of our LATR, achieving state-of-the-art performance on three public benchmarks. Furthermore, the ablation studies validate the contribution of each component of our method and the critical role of incorporating length priors into the TSG task.",
        "arxiv_id": "2508.04299",
        "ARXIVID": "2508.04299",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it introduces a Length-Aware Transformer for temporal sentence grounding in videos, improving video-based task performance.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.04597": {
        "authors": [
            "Linqing Zhao",
            "Xiuwei Xu",
            "Yirui Wang",
            "Hao Wang",
            "Wenzhao Zheng",
            "Yansong Tang",
            "Haibin Yan",
            "Jiwen Lu"
        ],
        "title": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline",
        "abstract": "arXiv:2508.04597v1 Announce Type: new  Abstract: Incrementally recovering real-sized 3D geometry from a pose-free RGB stream is a challenging task in 3D reconstruction, requiring minimal assumptions on input data. Existing methods can be broadly categorized into end-to-end and visual SLAM-based approaches, both of which either struggle with long sequences or depend on slow test-time optimization and depth sensors. To address this, we first integrate a depth estimator into an RGB-D SLAM system, but this approach is hindered by inaccurate geometric details in predicted depth. Through further investigation, we find that 3D Gaussian mapping can effectively solve this problem. Building on this, we propose an online 3D reconstruction method using 3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction module to directly infer camera pose from optical flow. This approach replaces slow test-time optimization with fast network inference, significantly improving tracking speed. Additionally, we introduce a local graph rendering technique to enhance robustness in feed-forward pose prediction. Experimental results on the Replica and TUM-RGBD datasets, along with a real-world deployment demonstration, show that our method achieves performance on par with the state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%.",
        "arxiv_id": "2508.04597",
        "ARXIVID": "2508.04597",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it proposes a novel online 3D reconstruction method using 3D Gaussian-based SLAM, addressing challenges in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2508.04681": {
        "authors": [
            "Liang Xu",
            "Chengqun Yang",
            "Zili Lin",
            "Fei Xu",
            "Yifan Liu",
            "Congsheng Xu",
            "Yiyi Zhang",
            "Jie Qin",
            "Xingdong Sheng",
            "Yunhui Liu",
            "Xin Jin",
            "Yichao Yan",
            "Wenjun Zeng",
            "Xiaokang Yang"
        ],
        "title": "Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions",
        "abstract": "arXiv:2508.04681v1 Announce Type: new  Abstract: Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.",
        "arxiv_id": "2508.04681",
        "ARXIVID": "2508.04681",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and tasks for embodied AI in egocentric human-object-human interactions.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2508.04335": {
        "authors": [
            "Yanyan Li",
            "Ze Yang",
            "Keisuke Tateno",
            "Federico Tombari Liang Zhao",
            "Gim Hee Lee"
        ],
        "title": "RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization",
        "abstract": "arXiv:2508.04335v1 Announce Type: new  Abstract: Minimal parametrization of 3D lines plays a critical role in camera localization and structural mapping. Existing representations in robotics and computer vision predominantly handle independent lines, overlooking structural regularities such as sets of parallel lines that are pervasive in man-made environments. This paper introduces \\textbf{RiemanLine}, a unified minimal representation for 3D lines formulated on Riemannian manifolds that jointly accommodates both individual lines and parallel-line groups. Our key idea is to decouple each line landmark into global and local components: a shared vanishing direction optimized on the unit sphere $\\mathcal{S}^2$, and scaled normal vectors constrained on orthogonal subspaces, enabling compact encoding of structural regularities. For $n$ parallel lines, the proposed representation reduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally embedding parallelism without explicit constraints. We further integrate this parameterization into a factor graph framework, allowing global direction alignment and local reprojection optimization within a unified manifold-based bundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic benchmarks demonstrate that our method achieves significantly more accurate pose estimation and line reconstruction, while reducing parameter dimensionality and improving convergence stability.",
        "arxiv_id": "2508.04335",
        "ARXIVID": "2508.04335",
        "COMMENT": "This paper matches Criterion 1 as it focuses on spatial intelligence through a novel representation of 3D lines for structural mapping and camera localization.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04050": {
        "authors": [
            "Jitong Liao",
            "Yulu Gao",
            "Shaofei Huang",
            "Jialin Gao",
            "Jie Lei",
            "Ronghua Liang",
            "Si Liu"
        ],
        "title": "DOMR: Establishing Cross-View Segmentation via Dense Object Matching",
        "abstract": "arXiv:2508.04050v1 Announce Type: new  Abstract: Cross-view object correspondence involves matching objects between egocentric (first-person) and exocentric (third-person) views. It is a critical yet challenging task for visual understanding. In this work, we propose the Dense Object Matching and Refinement (DOMR) framework to establish dense object correspondences across views. The framework centers around the Dense Object Matcher (DOM) module, which jointly models multiple objects. Unlike methods that directly match individual object masks to image features, DOM leverages both positional and semantic relationships among objects to find correspondences. DOM integrates a proposal generation module with a dense matching module that jointly encodes visual, spatial, and semantic cues, explicitly constructing inter-object relationships to achieve dense matching among objects. Furthermore, we combine DOM with a mask refinement head designed to improve the completeness and accuracy of the predicted masks, forming the complete DOMR framework. Extensive evaluations on the Ego-Exo4D benchmark demonstrate that our approach achieves state-of-the-art performance with a mean IoU of 49.7% on Ego$\\to$Exo and 55.2% on Exo$\\to$Ego. These results outperform those of previous methods by 5.8% and 4.3%, respectively, validating the effectiveness of our integrated approach for cross-view understanding.",
        "arxiv_id": "2508.04050",
        "ARXIVID": "2508.04050",
        "COMMENT": "This paper aligns with Criterion 1 as it focuses on spatial reasoning and object correspondence across views, which is relevant to spatial intelligence for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03736": {
        "authors": [
            "Rafayel Mkrtchyan",
            "Armen Manukyan",
            "Hrant Khachatrian",
            "Theofanis P. Raptis"
        ],
        "title": "Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities",
        "abstract": "arXiv:2508.03736v1 Announce Type: new  Abstract: Environment mapping is an important computing task for a wide range of smart city applications, including autonomous navigation, wireless network operations and extended reality environments. Conventional smart city mapping techniques, such as satellite imagery, LiDAR scans, and manual annotations, often suffer from limitations related to cost, accessibility and accuracy. Open-source mapping platforms have been widely utilized in artificial intelligence applications for environment mapping, serving as a source of ground truth. However, human errors and the evolving nature of real-world environments introduce biases that can negatively impact the performance of neural networks trained on such data. In this paper, we present a deep learning-based approach that integrates the DINOv2 architecture to improve building mapping by combining maps from open-source platforms with radio frequency (RF) data collected from multiple wireless user equipments and base stations. Our approach leverages a vision transformer-based architecture to jointly process both RF and map modalities within a unified framework, effectively capturing spatial dependencies and structural priors for enhanced mapping accuracy. For the evaluation purposes, we employ a synthetic dataset co-produced by Huawei. We develop and train a model that leverages only aggregated path loss information to tackle the mapping problem. We measure the results according to three performance metrics which capture different qualities: (i) The Jaccard index, also known as intersection over union (IoU), (ii) the Hausdorff distance, and (iii) the Chamfer distance. Our design achieves a macro IoU of 65.3%, significantly surpassing (i) the erroneous maps baseline, which yields 40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and (iii) a non-AI fusion baseline that we designed which yields 42.2%.",
        "arxiv_id": "2508.03736",
        "ARXIVID": "2508.03736",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it integrates RF data with spatial images using vision transformers for enhanced mapping in smart cities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04700": {
        "authors": [
            "Zeyi Sun",
            "Ziyu Liu",
            "Yuhang Zang",
            "Yuhang Cao",
            "Xiaoyi Dong",
            "Tong Wu",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience",
        "abstract": "arXiv:2508.04700v1 Announce Type: new  Abstract: Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.",
        "arxiv_id": "2508.04700",
        "ARXIVID": "2508.04700",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces SEAgent, a self-evolving agent framework for autonomous learning in novel software environments, which involves spatial reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.03967": {
        "authors": [
            "Mamadou Keita",
            "Wassim Hamidouche",
            "Hessen Bougueffa Eutamene",
            "Abdelmalik Taleb-Ahmed",
            "Abdenour Hadid"
        ],
        "title": "RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification",
        "abstract": "arXiv:2508.03967v1 Announce Type: new  Abstract: In this paper, we introduce RAVID, the first framework for AI-generated image detection that leverages visual retrieval-augmented generation (RAG). While RAG methods have shown promise in mitigating factual inaccuracies in foundation models, they have primarily focused on text, leaving visual knowledge underexplored. Meanwhile, existing detection methods, which struggle with generalization and robustness, often rely on low-level artifacts and model-specific features, limiting their adaptability. To address this, RAVID dynamically retrieves relevant images to enhance detection. Our approach utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with category-related prompts to improve representation learning. We further integrate a vision-language model (VLM) to fuse retrieved images with the query, enriching the input and improving accuracy. Given a query image, RAVID generates an embedding using RAVID CLIP, retrieves the most relevant images from a database, and combines these with the query image to form an enriched input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the UniversalFakeDetect benchmark, which covers 19 generative models, show that RAVID achieves state-of-the-art performance with an average accuracy of 93.85%. RAVID also outperforms traditional methods in terms of robustness, maintaining high accuracy even under image degradations such as Gaussian blur and JPEG compression. Specifically, RAVID achieves an average accuracy of 80.27% under degradation conditions, compared to 63.44% for the state-of-the-art model C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG compression scenarios. The code will be publicly available upon acceptance.",
        "arxiv_id": "2508.03967",
        "ARXIVID": "2508.03967",
        "COMMENT": "Matches criterion 5 as it integrates image understanding tasks with vision-language models for AI-generated image detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2508.04016": {
        "authors": [
            "Weilun Feng",
            "Haotong Qin",
            "Chuanguang Yang",
            "Xiangqi Li",
            "Han Yang",
            "Yuqi Li",
            "Zhulin An",
            "Libo Huang",
            "Michele Magno",
            "Yongjun Xu"
        ],
        "title": "$\\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation",
        "abstract": "arXiv:2508.04016v1 Announce Type: new  Abstract: Diffusion transformers have emerged as the mainstream paradigm for video generation models. However, the use of up to billions of parameters incurs significant computational costs. Quantization offers a promising solution by reducing memory usage and accelerating inference. Nonetheless, we observe that the joint modeling of spatial and temporal information in video diffusion models (V-DMs) leads to extremely long token sequences, which introduces high calibration variance and learning challenges. To address these issues, we propose \\textbf{$\\text{S}^2$Q-VDiT}, a post-training quantization framework for V-DMs that leverages \\textbf{S}alient data and \\textbf{S}parse token distillation. During the calibration phase, we identify that quantization performance is highly sensitive to the choice of calibration data. To mitigate this, we introduce \\textit{Hessian-aware Salient Data Selection}, which constructs high-quality calibration datasets by considering both diffusion and quantization characteristics unique to V-DMs. To tackle the learning challenges, we further analyze the sparse attention patterns inherent in V-DMs. Based on this observation, we propose \\textit{Attention-guided Sparse Token Distillation}, which exploits token-wise attention distributions to emphasize tokens that are more influential to the model's output. Under W4A6 quantization, $\\text{S}^2$Q-VDiT achieves lossless performance while delivering $3.9\\times$ model compression and $1.3\\times$ inference acceleration. Code will be available at \\href{https://github.com/wlfeng0509/s2q-vdit}{https://github.com/wlfeng0509/s2q-vdit}.",
        "arxiv_id": "2508.04016",
        "ARXIVID": "2508.04016",
        "COMMENT": "Matches criterion 6 as it focuses on video understanding tasks and proposes a novel quantization framework for video diffusion models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.04136": {
        "authors": [
            "Hongyu Guo",
            "Kuan Zhu",
            "Xiangzhao Hao",
            "Haiyun Guo",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval",
        "abstract": "arXiv:2508.04136v1 Announce Type: new  Abstract: Few-shot fine-grained visual classification (FGVC) aims to leverage limited data to enable models to discriminate subtly distinct categories. Recent works mostly finetuned the pre-trained visual language models to achieve performance gain, yet suffering from overfitting and weak generalization. To deal with this, we introduce UniFGVC, a universal training-free framework that reformulates few-shot FGVC as multimodal retrieval. First, we propose the Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the open-world knowledge of multimodal large language models (MLLMs) to generate a structured text description that captures the fine-grained attribute features distinguishing closely related classes. CDV-Captioner uses chain-of-thought prompting and visually similar reference images to reduce hallucination and enhance discrimination of generated captions. Using it we can convert each image into an image-description pair, enabling more comprehensive feature representation, and construct the multimodal category templates using few-shot samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and text encoders embed query and template pairs, and FGVC is accomplished by retrieving the nearest template in the joint space. UniFGVC ensures broad compatibility with diverse MLLMs and encoders, offering reliable generalization and adaptability across few-shot FGVC scenarios. Extensive experiments on 12 FGVC benchmarks demonstrate its consistent superiority over prior few-shot CLIP-based methods and even several fully-supervised MLLMs-based approaches.",
        "arxiv_id": "2508.04136",
        "ARXIVID": "2508.04136",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (MLLMs) for fine-grained vision classification.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.04677": {
        "authors": [
            "Yansheng Gao",
            "Yufei Zheng",
            "Jinghan Qu",
            "Zixi Zhu",
            "Yukuan Zhang",
            "Shengsheng Wang"
        ],
        "title": "ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models",
        "abstract": "arXiv:2508.04677v1 Announce Type: new  Abstract: Prompt tuning has emerged as an efficient and effective technique for adapting vision-language models (VLMs) with low computational overhead. However, existing methods often overlook the vulnerability of prompt-tuned VLMs to weak semantic perturbations-such as subtle image or text noise-that degrade their generalization to unseen classes. To address this limitation, we propose ANPrompt, a novel prompt tuning framework designed to enhance robustness under such perturbations. ANPrompt first constructs weak noise text features by fusing original and noise-perturbed text embeddings, which are then clustered to form noise prompts. These noise prompts are integrated with learnable prompt tokens to generate anti-noise prompts, which are injected into the deeper layers of both image and text encoders. To further capture the noise-aware visual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype (NRVPP) by averaging the output prompt tokens from the vision encoder. Finally, ANPrompt introduces alignment, robustness, and anti-noise objectives by computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard cross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that ANPrompt consistently outperforms existing prompt tuning approaches, achieving superior robustness to semantic noise and improved generalization to novel categories.",
        "arxiv_id": "2508.04677",
        "ARXIVID": "2508.04677",
        "COMMENT": "Matches criterion 2 as it explores robustness improvements in vision-language models (VLMs) through novel prompt tuning techniques.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2508.04572": {
        "authors": [
            "Jun Li",
            "Che Liu",
            "Wenjia Bai",
            "Mingxuan Liu",
            "Rossella Arcucci",
            "Cosmin I. Bercea",
            "Julia A. Schnabel"
        ],
        "title": "Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding",
        "abstract": "arXiv:2508.04572v1 Announce Type: new  Abstract: In this work, we address the problem of grounding abnormalities in medical images, where the goal is to localize clinical findings based on textual descriptions. While generalist Vision-Language Models (VLMs) excel in natural grounding tasks, they often struggle in the medical domain due to rare, compositional, and domain-specific terms that are poorly aligned with visual patterns. Specialized medical VLMs address this challenge via large-scale domain pretraining, but at the cost of substantial annotation and computational resources. To overcome these limitations, we propose \\textbf{Knowledge to Sight (K2Sight)}, a framework that introduces structured semantic supervision by decomposing clinical concepts into interpretable visual attributes, such as shape, density, and anatomical location. These attributes are distilled from domain ontologies and encoded into concise instruction-style prompts, which guide region-text alignment during training. Unlike conventional report-level supervision, our approach explicitly bridges domain knowledge and spatial structure, enabling data-efficient training of compact models. We train compact models with 0.23B and 2B parameters using only 1.5\\% of the data required by state-of-the-art medical VLMs. Despite their small size and limited training data, these models achieve performance on par with or better than 7B+ medical VLMs, with up to 9.82\\% improvement in $mAP_{50}$. Code and models: \\href{https://lijunrio.github.io/K2Sight/}{\\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.",
        "arxiv_id": "2508.04572",
        "ARXIVID": "2508.04572",
        "COMMENT": "Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores reasoning over visual attributes with domain-specific knowledge.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.04453": {
        "authors": [
            "Qingguo Hu",
            "Ante Wang",
            "Jia Song",
            "Delai Qiu",
            "Qingsong Liu",
            "Jinsong Su"
        ],
        "title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion",
        "abstract": "arXiv:2508.04453v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) have experienced significant advancements in recent years. However, their performance still falls short in tasks requiring deep visual perception, such as identifying subtle differences between images. A potential cause is the scarcity of visual knowledge in popular instruction-tuning corpora, resulting in inadequate visual perception and reasoning capabilities. To address this challenge, we introduce a self-improvement framework grounded in a novel visual knowledge-intensive task, \\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion (CVC). This task requires LVLMs to infer the masked object in an image based on its \\textit{causal} relationships with the other visible information. We first obtain rich examples cheaply through our automated instance construction pipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or human assistance. Then, LVLMs effectively self-improve through trial and error learning using these created instances. Our experiments demonstrate substantial gains across four challenging specialized tasks and four widely-used comprehensive benchmarks. Especially on specialized tasks, our method achieves an average improvement of 5.4\\% and 4.0\\% compared to the corresponding baselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code is available at https://github.com/XMUDeepLIT/CVC.",
        "arxiv_id": "2508.04453",
        "ARXIVID": "2508.04453",
        "COMMENT": "Matches criterion 2 as it focuses on improving Large Vision-Language Models (LVLMs) through a novel visual knowledge-intensive task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04101": {
        "authors": [
            "Zelin Peng",
            "Yichen Zhao",
            "Yu Huang",
            "Piao Yang",
            "Feilong Tang",
            "Zhengqin Xu",
            "Xiaokang Yang",
            "Wei Shen"
        ],
        "title": "NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding",
        "abstract": "arXiv:2508.04101v1 Announce Type: new  Abstract: Computer-aided medical image analysis is crucial for disease diagnosis and treatment planning, yet limited annotated datasets restrict medical-specific model development. While vision-language models (VLMs) like CLIP offer strong generalization capabilities, their direct application to medical imaging analysis is impeded by a significant domain gap. Existing approaches to bridge this gap, including prompt learning and one-way modality interaction techniques, typically focus on introducing domain knowledge to a single modality. Although this may offer performance gains, it often causes modality misalignment, thereby failing to unlock the full potential of VLMs. In this paper, we propose \\textbf{NEARL-CLIP} (i\\underline{N}teracted qu\\underline{E}ry \\underline{A}daptation with o\\underline{R}thogona\\underline{L} Regularization), a novel cross-modality interaction VLM-based framework that contains two contributions: (1) Unified Synergy Embedding Transformer (USEformer), which dynamically generates cross-modality queries to promote interaction between modalities, thus fostering the mutual enrichment and enhancement of multi-modal medical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA introduces an orthogonality technique to decouple the new knowledge from USEformer into two distinct components: the truly novel information and the incremental knowledge. By isolating the learning process from the interference of incremental knowledge, OCA enables a more focused acquisition of new information, thereby further facilitating modality interaction and unleashing the capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in a parameter-efficient style, which only introduces \\textbf{1.46M} learnable parameters.",
        "arxiv_id": "2508.04101",
        "ARXIVID": "2508.04101",
        "COMMENT": "Matches criterion 5 as it focuses on integrating vision-language models with medical image understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04418": {
        "authors": [
            "Jinxing Zhou",
            "Yanghao Zhou",
            "Mingfei Han",
            "Tong Wang",
            "Xiaojun Chang",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer"
        ],
        "title": "Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation",
        "abstract": "arXiv:2508.04418v1 Announce Type: new  Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects in audible videos based on given reference expressions. Prior works typically rely on learning latent embeddings via multimodal fusion to prompt a tunable SAM/SAM2 decoder for segmentation, which requires strong pixel-level supervision and lacks interpretability. From a novel perspective of explicit reference understanding, we propose TGS-Agent, which decomposes the task into a Think-Ground-Segment process, mimicking the human reasoning procedure by first identifying the referred object through multimodal analysis, followed by coarse-grained grounding and precise segmentation. To this end, we first propose Ref-Thinker, a multimodal language model capable of reasoning over textual, visual, and auditory cues. We construct an instruction-tuning dataset with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The object description inferred by Ref-Thinker is used as an explicit prompt for Grounding-DINO and SAM2, which perform grounding and segmentation without relying on pixel-level supervision. Additionally, we introduce R\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and reasoning-intensive references for better evaluating model generalization. Our approach achieves state-of-the-art results on both standard Ref-AVSBench and proposed R\\textsuperscript{2}-AVSBench. Code will be available at https://github.com/jasongief/TGS-Agent.",
        "arxiv_id": "2508.04418",
        "ARXIVID": "2508.04418",
        "COMMENT": "Matches criterion 5 as it integrates image understanding and language models for audio-visual segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04472": {
        "authors": [
            "Hongxu Chen",
            "Zhen Wang",
            "Taoran Mei",
            "Lin Li",
            "Bowei Zhu",
            "Runshi Li",
            "Long Chen"
        ],
        "title": "Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model",
        "abstract": "arXiv:2508.04472v1 Announce Type: new  Abstract: Concept Erasure, which aims to prevent pretrained text-to-image models from generating content associated with semantic-harmful concepts (i.e., target concepts), is getting increased attention. State-of-the-art methods formulate this task as an optimization problem: they align all target concepts with semantic-harmless anchor concepts, and apply closed-form solutions to update the model accordingly. While these closed-form methods are efficient, we argue that existing methods have two overlooked limitations: 1) They often result in incomplete erasure due to \"non-zero alignment residual\", especially when text prompts are relatively complex. 2) They may suffer from generation quality degradation as they always concentrate parameter updates in a few deep layers. To address these issues, we propose a novel closed-form method ErasePro: it is designed for more complete concept erasure and better preserving overall generative quality. Specifically, ErasePro first introduces a strict zero-residual constraint into the optimization objective, ensuring perfect alignment between target and anchor concept features and enabling more complete erasure. Secondly, it employs a progressive, layer-wise update strategy that gradually transfers target concept features to those of the anchor concept from shallow to deep layers. As the depth increases, the required parameter changes diminish, thereby reducing deviations in sensitive deep layers and preserving generative quality. Empirical results across different concept erasure tasks (including instance, art style, and nudity erasure) have demonstrated the effectiveness of our ErasePro.",
        "arxiv_id": "2508.04472",
        "ARXIVID": "2508.04472",
        "COMMENT": "Matches criterion 5 as it discusses techniques combining image understanding tasks with large language models in the context of text-to-image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04441": {
        "authors": [
            "Jonas Ammeling",
            "Jonathan Ganz",
            "Emely Rosbach",
            "Ludwig Lausser",
            "Christof A. Bertram",
            "Katharina Breininger",
            "Marc Aubreville"
        ],
        "title": "Benchmarking Foundation Models for Mitotic Figure Classification",
        "abstract": "arXiv:2508.04441v1 Announce Type: new  Abstract: The performance of deep learning models is known to scale with data quantity and diversity. In pathology, as in many other medical imaging domains, the availability of labeled images for a specific task is often limited. Self-supervised learning techniques have enabled the use of vast amounts of unlabeled data to train large-scale neural networks, i.e., foundation models, that can address the limited data problem by providing semantically rich feature vectors that can generalize well to new tasks with minimal training effort increasing model performance and robustness. In this work, we investigate the use of foundation models for mitotic figure classification. The mitotic count, which can be derived from this classification task, is an independent prognostic marker for specific tumors and part of certain tumor grading systems. In particular, we investigate the data scaling laws on multiple current foundation models and evaluate their robustness to unseen tumor domains. Next to the commonly used linear probing paradigm, we also adapt the models using low-rank adaptation (LoRA) of their attention mechanisms. We compare all models against end-to-end-trained baselines, both CNNs and Vision Transformers. Our results demonstrate that LoRA-adapted foundation models provide superior performance to those adapted with standard linear probing, reaching performance levels close to 100% data availability with only 10% of training data. Furthermore, LoRA-adaptation of the most recent foundation models almost closes the out-of-domain performance gap when evaluated on unseen tumor domains. However, full fine-tuning of traditional architectures still yields competitive performance.",
        "arxiv_id": "2508.04441",
        "ARXIVID": "2508.04441",
        "COMMENT": "Matches criterion 4 as it focuses on foundation models in computer vision and their applications, specifically in medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04229": {
        "authors": [
            "Yu Liu",
            "Zhijie Liu",
            "Xiao Ren",
            "You-Fu Li",
            "He Kong"
        ],
        "title": "Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction",
        "abstract": "arXiv:2508.04229v1 Announce Type: new  Abstract: Predicting pedestrian motion trajectories is critical for path planning and motion control of autonomous vehicles. However, accurately forecasting crowd trajectories remains a challenging task due to the inherently multimodal and uncertain nature of human motion. Recent diffusion-based models have shown promising results in capturing the stochasticity of pedestrian behavior for trajectory prediction. However, few diffusion-based approaches explicitly incorporate the underlying motion intentions of pedestrians, which can limit the interpretability and precision of prediction models. In this work, we propose a diffusion-based multimodal trajectory prediction model that incorporates pedestrians' motion intentions into the prediction framework. The motion intentions are decomposed into lateral and longitudinal components, and a pedestrian intention recognition module is introduced to enable the model to effectively capture these intentions. Furthermore, we adopt an efficient guidance mechanism that facilitates the generation of interpretable trajectories. The proposed framework is evaluated on two widely used human trajectory prediction benchmarks, ETH and UCY, on which it is compared against state-of-the-art methods. The experimental results demonstrate that our method achieves competitive performance.",
        "arxiv_id": "2508.04229",
        "ARXIVID": "2508.04229",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it focuses on pedestrian trajectory prediction for autonomous vehicles.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04267": {
        "authors": [
            "Duzhen Zhang",
            "Yong Ren",
            "Wei Cong",
            "Junhao Zheng",
            "Qiaoyi Su",
            "Shuncheng Jia",
            "Zhong-Zhi Li",
            "Xuanle Zhao",
            "Ye Bai",
            "Feilong Chen",
            "Qi Tian",
            "Tielin Zhang"
        ],
        "title": "Revisiting Continual Semantic Segmentation with Pre-trained Vision Models",
        "abstract": "arXiv:2508.04267v1 Announce Type: new  Abstract: Continual Semantic Segmentation (CSS) seeks to incrementally learn to segment novel classes while preserving knowledge of previously encountered ones. Recent advancements in CSS have been largely driven by the adoption of Pre-trained Vision Models (PVMs) as backbones. Among existing strategies, Direct Fine-Tuning (DFT), which sequentially fine-tunes the model across classes, remains the most straightforward approach. Prior work often regards DFT as a performance lower bound due to its presumed vulnerability to severe catastrophic forgetting, leading to the development of numerous complex mitigation techniques. However, we contend that this prevailing assumption is flawed. In this paper, we systematically revisit forgetting in DFT across two standard benchmarks, Pascal VOC 2012 and ADE20K, under eight CSS settings using two representative PVM backbones: ResNet101 and Swin-B. Through a detailed probing analysis, our findings reveal that existing methods significantly underestimate the inherent anti-forgetting capabilities of PVMs. Even under DFT, PVMs retain previously learned knowledge with minimal forgetting. Further investigation of the feature space indicates that the observed forgetting primarily arises from the classifier's drift away from the PVM, rather than from degradation of the backbone representations. Based on this insight, we propose DFT*, a simple yet effective enhancement to DFT that incorporates strategies such as freezing the PVM backbone and previously learned classifiers, as well as pre-allocating future classifiers. Extensive experiments show that DFT* consistently achieves competitive or superior performance compared to sixteen state-of-the-art CSS methods, while requiring substantially fewer trainable parameters and less training time.",
        "arxiv_id": "2508.04267",
        "ARXIVID": "2508.04267",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it revisits continual semantic segmentation with pre-trained vision models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04273": {
        "authors": [
            "Junan Lin",
            "Daizong Liu",
            "Xianke Chen",
            "Xiaoye Qu",
            "Xun Yang",
            "Jixiang Zhu",
            "Sanyuan Zhang",
            "Jianfeng Dong"
        ],
        "title": "Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval",
        "abstract": "arXiv:2508.04273v1 Announce Type: new  Abstract: Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically related to the given query. To tackle this task, most existing VMR methods solely focus on the visual and textual modalities while neglecting the complementary but important audio modality. Although a few recent works try to tackle the joint audio-vision-text reasoning, they treat all modalities equally and simply embed them without fine-grained interaction for moment retrieval. These designs are counter-practical as: Not all audios are helpful for video moment retrieval, and the audio of some videos may be complete noise or background sound that is meaningless to the moment determination. To this end, we propose a novel Importance-aware Multi-Granularity fusion model (IMG), which learns to dynamically and selectively aggregate the audio-vision-text contexts for VMR. Specifically, after integrating the textual guidance with vision and audio separately, we first design a pseudo-label-supervised audio importance predictor that predicts the importance score of the audio, and accordingly assigns weights to mitigate the interference caused by noisy audio. Then, we design a multi-granularity audio fusion module that adaptively fuses audio and visual modalities at local-, event-, and global-level, fully capturing their complementary contexts. We further propose a cross-modal knowledge distillation strategy to address the challenge of missing audio modality during inference. To evaluate our method, we further construct a new VMR dataset, i.e., Charades-AudioMatter, where audio-related samples are manually selected and re-organized from the original Charades-STA to validate the model's capability in utilizing audio modality. Extensive experiments validate the effectiveness of our method, achieving state-of-the-art with audio-video fusion in VMR methods. Our code is available at https://github.com/HuiGuanLab/IMG.",
        "arxiv_id": "2508.04273",
        "ARXIVID": "2508.04273",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it focuses on video moment retrieval with novel methodologies and datasets.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.04652": {
        "authors": [
            "Shuo Liu",
            "Zeyu Liang",
            "Xueguang Lyu",
            "Christopher Amato"
        ],
        "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2508.04652v1 Announce Type: new  Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.",
        "arxiv_id": "2508.04652",
        "ARXIVID": "2508.04652",
        "COMMENT": "Does not closely match any specific criterion but explores multi-agent reinforcement learning with LLMs, which is tangentially related to embodied agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2508.04559": {
        "authors": [
            "Jinxi Liu",
            "Zijian He",
            "Guangrun Wang",
            "Guanbin Li",
            "Liang Lin"
        ],
        "title": "One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose",
        "abstract": "arXiv:2508.04559v1 Announce Type: new  Abstract: Recent diffusion-based approaches have made significant advances in image-based virtual try-on, enabling more realistic and end-to-end garment synthesis. However, most existing methods remain constrained by their reliance on exhibition garments and segmentation masks, as well as their limited ability to handle flexible pose variations. These limitations reduce their practicality in real-world scenarios-for instance, users cannot easily transfer garments worn by one person onto another, and the generated try-on results are typically restricted to the same pose as the reference image. In this paper, we introduce \\textbf{OMFA} (\\emph{One Model For All}), a unified diffusion framework for both virtual try-on and try-off that operates without the need for exhibition garments and supports arbitrary poses. For example, OMFA enables removing garments from a source person (try-off) and transferring them onto a target person (try-on), while also allowing the generated target to appear in novel poses-even without access to multi-pose images of that person. OMFA is built upon a novel \\emph{partial diffusion} strategy that selectively applies noise and denoising to individual components of the joint input-such as the garment, the person image, or the face-enabling dynamic subtask control and efficient bidirectional garment-person transformation. The framework is entirely mask-free and requires only a single portrait and a target pose as input, making it well-suited for real-world applications. Additionally, by leveraging SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose try-on from just one image. Extensive experiments demonstrate that OMFA achieves state-of-the-art results on both try-on and try-off tasks, providing a practical and generalizable solution for virtual garment synthesis. The project page is here: https://onemodelforall.github.io/.",
        "arxiv_id": "2508.04559",
        "ARXIVID": "2508.04559",
        "COMMENT": "Does not match any specific criteria. Focuses on virtual try-on and try-off using diffusion models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.03789": {
        "authors": [
            "Yuhang Ma",
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Hongsheng Li"
        ],
        "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
        "abstract": "arXiv:2508.03789v1 Announce Type: new  Abstract: Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.",
        "arxiv_id": "2508.03789",
        "ARXIVID": "2508.03789",
        "COMMENT": "Does not match any specific criteria. Focuses on human preference scoring for text-to-image generation, which is tangential to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04227": {
        "authors": [
            "Yuyang Liu",
            "Qiuhe Hong",
            "Linlan Huang",
            "Alexandra Gomez-Villa",
            "Dipam Goswami",
            "Xialei Liu",
            "Joost van de Weijer",
            "Yonghong Tian"
        ],
        "title": "Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting",
        "abstract": "arXiv:2508.04227v1 Announce Type: new  Abstract: Vision-language models (VLMs) have achieved impressive performance across diverse multimodal tasks by leveraging large-scale pre-training. However, enabling them to learn continually from non-stationary data remains a major challenge, as their cross-modal alignment and generalization capabilities are particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal continual learning (CL), VLMs face unique challenges such as cross-modal feature drift, parameter interference due to shared architectures, and zero-shot capability erosion. This survey offers the first focused and systematic review of continual learning for VLMs (VLM-CL). We begin by identifying the three core failure modes that degrade performance in VLM-CL. Based on these, we propose a challenge-driven taxonomy that maps solutions to their target problems: (1) \\textit{Multi-Modal Replay Strategies} address cross-modal drift through explicit or implicit memory mechanisms; (2) \\textit{Cross-Modal Regularization} preserves modality alignment during updates; and (3) \\textit{Parameter-Efficient Adaptation} mitigates parameter interference with modular or low-rank updates. We further analyze current evaluation protocols, datasets, and metrics, highlighting the need for better benchmarks that capture VLM-specific forgetting and compositional generalization. Finally, we outline open problems and future directions, including continual pre-training and compositional zero-shot learning. This survey aims to serve as a comprehensive and diagnostic reference for researchers developing lifelong vision-language systems. All resources are available at: https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.",
        "arxiv_id": "2508.04227",
        "ARXIVID": "2508.04227",
        "COMMENT": "Matches criterion 2 as it is a survey on continual learning for Vision-Language Models (VLMs), which aligns with the exploration of VLLMs and MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2508.04482": {
        "authors": [
            "Xueyu Hu",
            "Tao Xiong",
            "Biao Yi",
            "Zishu Wei",
            "Ruixuan Xiao",
            "Yurun Chen",
            "Jiasheng Ye",
            "Meiling Tao",
            "Xiangxin Zhou",
            "Ziyu Zhao",
            "Yuhuai Li",
            "Shengze Xu",
            "Shenzhi Wang",
            "Xinchen Xu",
            "Shuofei Qiao",
            "Zhaokai Wang",
            "Kun Kuang",
            "Tieyong Zeng",
            "Liang Wang",
            "Jiwei Li",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou",
            "Guoyin Wang",
            "Keting Yin",
            "Zhou Zhao",
            "Hongxia Yang",
            "Fan Wu",
            "Shengyu Zhang",
            "Fei Wu"
        ],
        "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use",
        "abstract": "arXiv:2508.04482v1 Announce Type: new  Abstract: The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.",
        "arxiv_id": "2508.04482",
        "ARXIVID": "2508.04482",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it provides a comprehensive survey on MLLM-based agents.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2508.04406": {
        "authors": [
            "Yinan Yu",
            "Alex Gonzalez-Caceres",
            "Samuel Scheidegger",
            "Sanjay Somanath",
            "Alexander Hollberg"
        ],
        "title": "Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models",
        "abstract": "arXiv:2508.04406v1 Announce Type: new  Abstract: Renovating existing buildings is essential for climate impact. Early-phase renovation planning requires simulations based on thermal 3D models at Level of Detail (LoD) 3, which include features like windows. However, scalable and accurate identification of such features remains a challenge. This paper presents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that generates LoD3 thermal models by extracting geometries from images using both computer vision and deep learning. Unlike existing methods relying on segmentation and projection, SI3FP directly models geometric primitives in the orthographic image plane, providing a unified interface while reducing perspective distortions. SI3FP supports both sparse (e.g., Google Street View) and dense (e.g., hand-held camera) data sources. Tested on typical Swedish residential buildings, SI3FP achieved approximately 5% error in window-to-wall ratio estimates, demonstrating sufficient accuracy for early-stage renovation analysis. The pipeline facilitates large-scale energy renovation planning and has broader applications in urban development and planning.",
        "arxiv_id": "2508.04406",
        "ARXIVID": "2508.04406",
        "COMMENT": "Does not closely match any specific criterion but is related to computer vision applications in urban planning and energy modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04551": {
        "authors": [
            "Angang Zhang",
            "Fang Deng",
            "Hao Chen",
            "Zhongjian Chen",
            "Junyan Li"
        ],
        "title": "Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis",
        "abstract": "arXiv:2508.04551v1 Announce Type: new  Abstract: While recent advances in virtual try-on (VTON) have achieved realistic garment transfer to human subjects, its inverse task, virtual try-off (VTOFF), which aims to reconstruct canonical garment templates from dressed humans, remains critically underexplored and lacks systematic investigation. Existing works predominantly treat them as isolated tasks: VTON focuses on garment dressing while VTOFF addresses garment extraction, thereby neglecting their complementary symmetry. To bridge this fundamental gap, we propose the Two-Way Garment Transfer Model (TWGTM), to the best of our knowledge, the first unified framework for joint clothing-centric image synthesis that simultaneously resolves both mask-guided VTON and mask-free VTOFF through bidirectional feature disentanglement. Specifically, our framework employs dual-conditioned guidance from both latent and pixel spaces of reference images to seamlessly bridge the dual tasks. On the other hand, to resolve the inherent mask dependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a phased training paradigm that progressively bridges this modality gap. Extensive qualitative and quantitative experiments conducted across the DressCode and VITON-HD datasets validate the efficacy and competitive edge of our proposed approach.",
        "arxiv_id": "2508.04551",
        "ARXIVID": "2508.04551",
        "COMMENT": "Does not closely match any specific criterion but is related to generative modeling and image synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04025": {
        "authors": [
            "Chao Hao",
            "Shuai Wang",
            "Kaiwen Zhou"
        ],
        "title": "Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement",
        "abstract": "arXiv:2508.04025v1 Announce Type: new  Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile tasks but still struggle with input redundancy and decision ambiguity. In this paper, we present \\textbf{RecAgent}, an uncertainty-aware agent that addresses these issues through adaptive perception. We distinguish two types of uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input redundancy and noise from comprehensive screen information, and (2) decision uncertainty, arising from ambiguous tasks and complex reasoning. To reduce perceptual uncertainty, RecAgent employs a component recommendation mechanism that identifies and focuses on the most relevant UI elements. For decision uncertainty, it uses an interactive module to request user feedback in ambiguous situations, enabling intent-aware decisions. These components are integrated into a unified framework that proactively reduces input complexity and reacts to high-uncertainty cases via human-in-the-loop refinement. Additionally, we propose a dataset called \\textbf{ComplexAction} to evaluate the success rate of GUI agents in executing specified single-step actions within complex scenarios. Extensive experiments validate the effectiveness of our approach. The dataset and code will be available at https://github.com/Fanye12/RecAgent.",
        "arxiv_id": "2508.04025",
        "ARXIVID": "2508.04025",
        "COMMENT": "Does not closely match any specific criterion but is tangentially related to embodied agents and adaptive perception.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.04153": {
        "authors": [
            "Yihua Shao",
            "Xiaofeng Lin",
            "Xinwei Long",
            "Siyu Chen",
            "Minxi Yan",
            "Yang Liu",
            "Ziyang Yan",
            "Ao Ma",
            "Hao Tang",
            "Jingcai Guo"
        ],
        "title": "ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation",
        "abstract": "arXiv:2508.04153v1 Announce Type: new  Abstract: Enabling multi-task adaptation in pre-trained Low-Rank Adaptation (LoRA) models is crucial for enhancing their generalization capabilities. Most existing pre-trained LoRA fusion methods decompose weight matrices, sharing similar parameters while merging divergent ones. However, this paradigm inevitably induces inter-weight conflicts and leads to catastrophic domain forgetting. While incremental learning enables adaptation to multiple tasks, it struggles to achieve generalization in few-shot scenarios. Consequently, when the weight data follows a long-tailed distribution, it can lead to forgetting in the fused weights. To address this issue, we propose In-Context Meta LoRA Fusion (ICM-Fusion), a novel framework that synergizes meta-learning with in-context adaptation. The key innovation lies in our task vector arithmetic, which dynamically balances conflicting optimization directions across domains through learned manifold projections. ICM-Fusion obtains the optimal task vector orientation for the fused model in the latent space by adjusting the orientation of the task vectors. Subsequently, the fused LoRA is reconstructed by a self-designed Fusion VAE (F-VAE) to realize multi-task LoRA generation. We have conducted extensive experiments on visual and linguistic tasks, and the experimental results demonstrate that ICM-Fusion can be adapted to a wide range of architectural models and applied to various tasks. Compared to the current pre-trained LoRA fusion method, ICM-Fusion fused LoRA can significantly reduce the multi-tasking loss and can even achieve task enhancement in few-shot scenarios.",
        "arxiv_id": "2508.04153",
        "ARXIVID": "2508.04153",
        "COMMENT": "This paper does not directly match any specific criteria but is generally related to multi-task adaptation in vision and language tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04663": {
        "authors": [
            "Young D. Kwon",
            "Rui Li",
            "Sijia Li",
            "Da Li",
            "Sourav Bhattacharya",
            "Stylianos I. Venieris"
        ],
        "title": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models",
        "abstract": "arXiv:2508.04663v1 Announce Type: new  Abstract: State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, when combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Last but not least, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works.",
        "arxiv_id": "2508.04663",
        "ARXIVID": "2508.04663",
        "COMMENT": "Does not match any specific criteria. Focuses on compression of diffusion models, which is tangential to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04161": {
        "authors": [
            "Yuqin Cao",
            "Yixuan Gao",
            "Wei Sun",
            "Xiaohong Liu",
            "Yulun Zhang",
            "Xiongkuo Min"
        ],
        "title": "Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning",
        "abstract": "arXiv:2508.04161v1 Announce Type: new  Abstract: Face videos accompanied by audio have become integral to our daily lives, while they often suffer from complex degradations. Most face video restoration methods neglect the intrinsic correlations between the visual and audio features, especially in mouth regions. A few audio-aided face video restoration methods have been proposed, but they only focus on compression artifact removal. In this paper, we propose a General Audio-assisted face Video restoration Network (GAVN) to address various types of streaming video distortions via identity and temporal complementary learning. Specifically, GAVN first captures inter-frame temporal features in the low-resolution space to restore frames coarsely and save computational cost. Then, GAVN extracts intra-frame identity features in the high-resolution space with the assistance of audio signals and face landmarks to restore more facial details. Finally, the reconstruction module integrates temporal features and identity features to generate high-quality face videos. Experimental results demonstrate that GAVN outperforms the existing state-of-the-art methods on face video compression artifact removal, deblurring, and super-resolution. Codes will be released upon publication.",
        "arxiv_id": "2508.04161",
        "ARXIVID": "2508.04161",
        "COMMENT": "Does not match any specific criteria. Focuses on audio-assisted face video restoration, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03955": {
        "authors": [
            "Lin Zhang",
            "Zefan Cai",
            "Yufan Zhou",
            "Shentong Mo",
            "Jinhong Lin",
            "Cheng-En Wu",
            "Yibing Wei",
            "Yijing Zhang",
            "Ruiyi Zhang",
            "Wen Xiao",
            "Tong Sun",
            "Junjie Hu",
            "Pedro Morgado"
        ],
        "title": "Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm",
        "abstract": "arXiv:2508.03955v1 Announce Type: new  Abstract: Recent advances in audio-synchronized visual animation enable control of video content using audios from specific classes. However, existing methods rely heavily on expensive manual curation of high-quality, class-specific training videos, posing challenges to scaling up to diverse audio-video classes in the open world. In this work, we propose an efficient two-stage training paradigm to scale up audio-synchronized visual animation using abundant but noisy videos. In stage one, we automatically curate large-scale videos for pretraining, allowing the model to learn diverse but imperfect audio-video alignments. In stage two, we finetune the model on manually curated high-quality examples, but only at a small scale, significantly reducing the required human effort. We further enhance synchronization by allowing each frame to access rich audio context via multi-feature conditioning and window attention. To efficiently train the model, we leverage pretrained text-to-video generator and audio encoders, introducing only 1.9\\% additional trainable parameters to learn audio-conditioning capability without compromising the generator's prior knowledge. For evaluation, we introduce AVSync48, a benchmark with videos from 48 classes, which is 3$\\times$ more diverse than previous benchmarks. Extensive experiments show that our method significantly reduces reliance on manual curation by over 10$\\times$, while generalizing to many open classes.",
        "arxiv_id": "2508.03955",
        "ARXIVID": "2508.03955",
        "COMMENT": "Does not match any specific criteria. Focuses on audio-synchronized visual animation, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04236": {
        "authors": [
            "Muhua Zhu",
            "Xinhao Jin",
            "Chengbo Wang",
            "Yongcong Zhang",
            "Yifei Xue",
            "Tie Ji",
            "Yizhen Lao"
        ],
        "title": "PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction",
        "abstract": "arXiv:2508.04236v1 Announce Type: new  Abstract: Image stitching aim to align two images taken from different viewpoints into one seamless, wider image. However, when the 3D scene contains depth variations and the camera baseline is significant, noticeable parallax occurs-meaning the relative positions of scene elements differ substantially between views. Most existing stitching methods struggle to handle such images with large parallax effectively. To address this challenge, in this paper, we propose an image stitching solution called PIS3R that is robust to very large parallax based on the novel concept of deep 3D reconstruction. First, we apply visual geometry grounded transformer to two input images with very large parallax to obtain both intrinsic and extrinsic parameters, as well as the dense 3D scene reconstruction. Subsequently, we reproject reconstructed dense point cloud onto a designated reference view using the recovered camera parameters, achieving pixel-wise alignment and generating an initial stitched image. Finally, to further address potential artifacts such as holes or noise in the initial stitching, we propose a point-conditioned image diffusion module to obtain the refined result.Compared with existing methods, our solution is very large parallax tolerant and also provides results that fully preserve the geometric integrity of all pixels in the 3D photogrammetric context, enabling direct applicability to downstream 3D vision tasks such as SfM. Experimental results demonstrate that the proposed algorithm provides accurate stitching results for images with very large parallax, and outperforms the existing methods qualitatively and quantitatively.",
        "arxiv_id": "2508.04236",
        "ARXIVID": "2508.04236",
        "COMMENT": "Does not match any specific criteria but is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04625": {
        "authors": [
            "Zichen Tang",
            "Haihong E",
            "Jiacheng Liu",
            "Zhongjun Yang",
            "Rongjin Li",
            "Zihua Rong",
            "Haoyang He",
            "Zhuodi Hao",
            "Xinyang Hu",
            "Kun Ji",
            "Ziyan Ma",
            "Mengyuan Ji",
            "Jun Zhang",
            "Chenghao Ma",
            "Qianhe Zheng",
            "Yang Liu",
            "Yiling Huang",
            "Xinyi Hu",
            "Qing Huang",
            "Zijian Xie",
            "Shiyao Peng"
        ],
        "title": "FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging",
        "abstract": "arXiv:2508.04625v1 Announce Type: new  Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.",
        "arxiv_id": "2508.04625",
        "ARXIVID": "2508.04625",
        "COMMENT": "Does not match any specific criteria but is tangentially related to multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04205": {
        "authors": [
            "Jianxun Yu",
            "Ruiquan Ge",
            "Zhipeng Wang",
            "Cheng Yang",
            "Chenyu Lin",
            "Xianjun Fu",
            "Jikui Liu",
            "Ahmed Elazab",
            "Changmiao Wang"
        ],
        "title": "Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification",
        "abstract": "arXiv:2508.04205v1 Announce Type: new  Abstract: The diagnosis of medical diseases faces challenges such as the misdiagnosis of small lesions. Deep learning, particularly multimodal approaches, has shown great potential in the field of medical disease diagnosis. However, the differences in dimensionality between medical imaging and electronic health record data present challenges for effective alignment and fusion. To address these issues, we propose the Multimodal Multiscale Cross-Attention Fusion Network (MMCAF-Net). This model employs a feature pyramid structure combined with an efficient 3D multi-scale convolutional attention module to extract lesion-specific features from 3D medical images. To further enhance multimodal data integration, MMCAF-Net incorporates a multi-scale cross-attention module, which resolves dimensional inconsistencies, enabling more effective feature fusion. We evaluated MMCAF-Net on the Lung-PET-CT-Dx dataset, and the results showed a significant improvement in diagnostic accuracy, surpassing current state-of-the-art methods. The code is available at https://github.com/yjx1234/MMCAF-Net",
        "arxiv_id": "2508.04205",
        "ARXIVID": "2508.04205",
        "COMMENT": "Does not closely match any specific criterion but is relevant to multimodal learning and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03754": {
        "authors": [
            "Bevin V",
            "Ananthakrishnan P V",
            "Ragesh KR",
            "Sanjay M",
            "Vineeth S",
            "Bibin Wilson"
        ],
        "title": "Generating Synthetic Invoices via Layout-Preserving Content Replacement",
        "abstract": "arXiv:2508.03754v1 Announce Type: new  Abstract: The performance of machine learning models for automated invoice processing is critically dependent on large-scale, diverse datasets. However, the acquisition of such datasets is often constrained by privacy regulations and the high cost of manual annotation. To address this, we present a novel pipeline for generating high-fidelity, synthetic invoice documents and their corresponding structured data. Our method first utilizes Optical Character Recognition (OCR) to extract the text content and precise spatial layout from a source invoice. Select data fields are then replaced with contextually realistic, synthetic content generated by a large language model (LLM). Finally, we employ an inpainting technique to erase the original text from the image and render the new, synthetic text in its place, preserving the exact layout and font characteristics. This process yields a pair of outputs: a visually realistic new invoice image and a perfectly aligned structured data file (JSON) reflecting the synthetic content. Our approach provides a scalable and automated solution to amplify small, private datasets, enabling the creation of large, varied corpora for training more robust and accurate document intelligence models.",
        "arxiv_id": "2508.03754",
        "ARXIVID": "2508.03754",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and generative modeling in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04485": {
        "authors": [
            "Bowen Chai",
            "Zheng Chen",
            "Libo Zhu",
            "Wenbo Li",
            "Yong Guo",
            "Yulun Zhang"
        ],
        "title": "QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution",
        "abstract": "arXiv:2508.04485v1 Announce Type: new  Abstract: Diffusion models have shown superior performance in real-world video super-resolution (VSR). However, the slow processing speeds and heavy resource consumption of diffusion models hinder their practical application and deployment. Quantization offers a potential solution for compressing the VSR model. Nevertheless, quantizing VSR models is challenging due to their temporal characteristics and high fidelity requirements. To address these issues, we propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a spatio-temporal complexity aware (STCA) mechanism, where we first utilize the calibration dataset to measure both spatial and temporal complexities for each layer. Based on these statistics, we allocate layer-specific ranks to the low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine the FP and low-bit branches to achieve simultaneous optimization. In addition, we propose a learnable bias alignment (LBA) module to reduce the biased quantization errors. Extensive experiments on synthetic and real-world datasets demonstrate that our method obtains comparable performance with the FP model and significantly outperforms recent leading low-bit quantization methods. Code is available at: https://github.com/bowenchai/QuantVSR.",
        "arxiv_id": "2508.04485",
        "ARXIVID": "2508.04485",
        "COMMENT": "Does not closely match any specific criterion but is relevant to video understanding and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04366": {
        "authors": [
            "Songyun Yang",
            "Yufei Han",
            "Jilong Zhang",
            "Kongming Liang",
            "Peng Yu",
            "Zhaowei Qu",
            "Heng Guo"
        ],
        "title": "RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light",
        "abstract": "arXiv:2508.04366v1 Announce Type: new  Abstract: Multiview photometric stereo (MVPS) seeks to recover high-fidelity surface shapes and reflectances from images captured under varying views and illuminations. However, existing MVPS methods often require controlled darkroom settings for varying illuminations or overlook the recovery of reflectances and illuminations properties, limiting their applicability in natural illumination scenarios and downstream inverse rendering tasks. In this paper, we propose RotatedMVPS to solve shape and reflectance recovery under rotated natural light, achievable with a practical rotation stage. By ensuring light consistency across different camera and object poses, our method reduces the unknowns associated with complex environment light. Furthermore, we integrate data priors from off-the-shelf learning-based single-view photometric stereo methods into our MVPS framework, significantly enhancing the accuracy of shape and reflectance recovery. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach.",
        "arxiv_id": "2508.04366",
        "ARXIVID": "2508.04366",
        "COMMENT": "Does not closely match any specific criterion but is relevant to computer vision and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04044": {
        "authors": [
            "Qiangguo Jin",
            "Hui Cui",
            "Junbo Wang",
            "Changming Sun",
            "Yimiao He",
            "Ping Xuan",
            "Linlin Wang",
            "Cong Cong",
            "Leyi Wei",
            "Ran Su"
        ],
        "title": "Iterative pseudo-labeling based adaptive copy-paste supervision for semi-supervised tumor segmentation",
        "abstract": "arXiv:2508.04044v1 Announce Type: new  Abstract: Semi-supervised learning (SSL) has attracted considerable attention in medical image processing. The latest SSL methods use a combination of consistency regularization and pseudo-labeling to achieve remarkable success. However, most existing SSL studies focus on segmenting large organs, neglecting the challenging scenarios where there are numerous tumors or tumors of small volume. Furthermore, the extensive capabilities of data augmentation strategies, particularly in the context of both labeled and unlabeled data, have yet to be thoroughly investigated. To tackle these challenges, we introduce a straightforward yet effective approach, termed iterative pseudo-labeling based adaptive copy-paste supervision (IPA-CP), for tumor segmentation in CT scans. IPA-CP incorporates a two-way uncertainty based adaptive augmentation mechanism, aiming to inject tumor uncertainties present in the mean teacher architecture into adaptive augmentation. Additionally, IPA-CP employs an iterative pseudo-label transition strategy to generate more robust and informative pseudo labels for the unlabeled samples. Extensive experiments on both in-house and public datasets show that our framework outperforms state-of-the-art SSL methods in medical image segmentation. Ablation study results demonstrate the effectiveness of our technical contributions.",
        "arxiv_id": "2508.04044",
        "ARXIVID": "2508.04044",
        "COMMENT": "Does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04428": {
        "authors": [
            "Si Chen",
            "Izzy Molnar",
            "Ting Hua",
            "Peiyu Li",
            "Le Huy Khiem",
            "G. Alex Ambrose",
            "Jim Lang",
            "Ronald Metoyer",
            "Nitesh V. Chawla"
        ],
        "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices",
        "abstract": "arXiv:2508.04428v1 Announce Type: new  Abstract: High-quality, multi-turn instructional dialogues between novices and experts are essential for developing AI systems that support teaching, learning, and decision-making. These dialogues often involve scaffolding -- the process by which an expert supports a novice's thinking through questions, feedback, and step-by-step guidance. However, such data are scarce due to privacy concerns in recording and the vulnerability inherent in help-seeking. We present SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding dialogues. Using teaching development coaching as an example domain, SimInstruct simulates novice instructors via LLMs, varying their teaching challenges and LLM's persona traits, while human experts provide multi-turn feedback, reasoning, and instructional support. This design enables the creation of realistic, pedagogically rich dialogues without requiring real novice participants. Our results reveal that persona traits, such as extroversion and introversion, meaningfully influence how experts engage. Compared to real mentoring recordings, SimInstruct dialogues demonstrate comparable pedagogical relevance and cognitive depth. Experts also reported the process as engaging and reflective, improving both data quality and their own professional insight. We further fine-tuned a LLaMA model to be an expert model using the augmented dataset, which outperformed GPT-4o in instructional quality. Our analysis highlights GPT-4o's limitations in weak reflective questioning, overuse of generic praise, a condescending tone, and a tendency to overwhelm novices with excessive suggestions.",
        "arxiv_id": "2508.04428",
        "ARXIVID": "2508.04428",
        "COMMENT": "Does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03725": {
        "authors": [
            "Yida Wang",
            "Taiting Lu",
            "Runze Liu",
            "Lanqing Yang",
            "Yifan Yang",
            "Zhe Chen",
            "Yuehai Wang",
            "Yixin Liu",
            "Kaiyuan Lin",
            "Xiaomeng Chen",
            "Dian Ding",
            "Yijie Li",
            "Yi-Chao Chen",
            "Yincheng Jin",
            "Mahanth Gowda"
        ],
        "title": "A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding",
        "abstract": "arXiv:2508.03725v1 Announce Type: new  Abstract: Printed-Circuit-board (PCB) footprint geometry labeling of integrated circuits (IC) is essential in defining the physical interface between components and the PCB layout, requiring exceptional visual perception proficiency. However, due to the unstructured footprint drawing and abstract diagram annotations, automated parsing and accurate footprint geometry modeling remain highly challenging. Despite its importance, no methods currently exist for automated package geometry labeling directly from IC mechanical drawings. In this paper, we first investigate the visual perception performance of Large Multimodal Models (LMMs) when solving IC footprint geometry understanding. Our findings reveal that current LMMs severely suffer from inaccurate geometric perception, which hinders their performance in solving the footprint geometry labeling problem. To address these limitations, we propose LLM4-IC8K, a novel framework that treats IC mechanical drawings as images and leverages LLMs for structured geometric interpretation. To mimic the step-by-step reasoning approach used by human engineers, LLM4-IC8K addresses three sub-tasks: perceiving the number of pins, computing the center coordinates of each pin, and estimating the dimensions of individual pins. We present a two-stage framework that first trains LMMs on synthetically generated IC footprint diagrams to learn fundamental geometric reasoning and then fine-tunes them on real-world datasheet drawings to enhance robustness and accuracy in practical scenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with 8,608 labeled samples, including 4138 hand-crafted IC footprint samples and 4470 synthetically generated samples. Extensive experiments demonstrate that our model outperforms state-of-the-art LMMs on the proposed benchmark.",
        "arxiv_id": "2508.03725",
        "ARXIVID": "2508.03725",
        "COMMENT": "Does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.03740": {
        "authors": [
            "Jianqiao Chen",
            "Tingting Zhu",
            "Huishi Song",
            "Nan Ma",
            "Xiaodong Xu"
        ],
        "title": "VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission",
        "abstract": "arXiv:2508.03740v1 Announce Type: new  Abstract: Discretization of semantic features enables interoperability between semantic and digital communication systems, showing significant potential for practical applications. The fundamental difficulty in digitizing semantic features stems from the need to preserve continuity and context in inherently analog representations during their compression into discrete symbols while ensuring robustness to channel degradation. In this paper, we propose a vector quantized (VQ)-enabled digital semantic communication system with channel adaptive image transmission, named VQ-DeepISC. Guided by deep joint source-channel coding (DJSCC), we first design a Swin Transformer backbone for hierarchical semantic feature extraction, followed by VQ modules projecting features into discrete latent spaces. Consequently, it enables efficient index-based transmission instead of raw feature transmission. To further optimize this process, we develop an attention mechanism-driven channel adaptation module to dynamically optimize index transmission. Secondly, to counteract codebook collapse during training process, we impose a distributional regularization by minimizing the Kullback-Leibler divergence (KLD) between codeword usage frequencies and a uniform prior. Meanwhile, exponential moving average (EMA) is employed to stabilize training and ensure balanced feature coverage during codebook updates. Finally, digital communication is implemented using quadrature phase shift keying (QPSK) modulation alongside orthogonal frequency division multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental results demonstrate superior reconstruction fidelity of the proposed system over benchmark methods.",
        "arxiv_id": "2508.03740",
        "ARXIVID": "2508.03740",
        "COMMENT": "Does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04539": {
        "authors": [
            "Qi Wang",
            "Jinjia Zhou"
        ],
        "title": "TopKD: Top-scaled Knowledge Distillation",
        "abstract": "arXiv:2508.04539v1 Announce Type: new  Abstract: Recent advances in knowledge distillation (KD) predominantly emphasize feature-level knowledge transfer, frequently overlooking critical information embedded within the teacher's logit distributions. In this paper, we revisit logit-based distillation and reveal an underexplored yet critical element: Top-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge Distillation (TopKD), a simple, efficient, and architecture-agnostic framework that significantly enhances logit-based distillation. TopKD consists of two main components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies the most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers targeted and effective supervision. Notably, TopKD integrates seamlessly into existing KD methods without introducing extra modules or requiring architectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10, and Tiny-ImageNet demonstrate that TopKD consistently surpasses state-of-the-art distillation methods. Moreover, our method demonstrates substantial effectiveness when distilling Vision Transformers, underscoring its versatility across diverse network architectures. These findings highlight the significant potential of logits to advance knowledge distillation.",
        "arxiv_id": "2508.04539",
        "ARXIVID": "2508.04539",
        "COMMENT": "Does not match any specific criterion but is relevant to general machine learning and knowledge distillation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04255": {
        "authors": [
            "Giuseppe Chindemi",
            "Camilla Bellone",
            "Benoit Girard"
        ],
        "title": "From eye to AI: studying rodent social behavior in the era of machine Learning",
        "abstract": "arXiv:2508.04255v1 Announce Type: new  Abstract: The study of rodent social behavior has shifted in the last years from relying on direct human observation to more nuanced approaches integrating computational methods in artificial intelligence (AI) and machine learning. While conventional approaches introduce bias and can fail to capture the complexity of rodent social interactions, modern approaches bridging computer vision, ethology and neuroscience provide more multifaceted insights into behavior which are particularly relevant to social neuroscience. Despite these benefits, the integration of AI into social behavior research also poses several challenges. Here we discuss the main steps involved and the tools available for analyzing rodent social behavior, examining their advantages and limitations. Additionally, we suggest practical solutions to address common hurdles, aiming to guide young researchers in adopting these methods and to stimulate further discussion among experts regarding the evolving requirements of these tools in scientific applications.",
        "arxiv_id": "2508.04255",
        "ARXIVID": "2508.04255",
        "COMMENT": "Does not match any specific criteria but is tangentially related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.03996": {
        "authors": [
            "Michele Andrade",
            "Guilherme A. L. Silva",
            "Val\\'eria Santos",
            "Gladston Moreira",
            "Eduardo Luz"
        ],
        "title": "Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images",
        "abstract": "arXiv:2508.03996v1 Announce Type: new  Abstract: Estimating the nutritional content of food from images is a critical task with significant implications for health and dietary monitoring. This is challenging, especially when relying solely on 2D images, due to the variability in food presentation, lighting, and the inherent difficulty in inferring volume and mass without depth information. Furthermore, reproducibility in this domain is hampered by the reliance of state-of-the-art methods on proprietary datasets for large-scale pre-training. In this paper, we investigate the impact of large-scale pre-training datasets on the performance of deep learning models for nutritional estimation using only 2D images. We fine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large public datasets, ImageNet and COYO, comparing their performance against baseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method pre-trained on the proprietary JFT-300M dataset. We conduct extensive experiments on the Nutrition5k dataset, a large-scale collection of real-world food plates with high-precision nutritional annotations. Our evaluation using Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals that models pre-trained on JFT-300M significantly outperform those pre-trained on public datasets. Unexpectedly, the model pre-trained on the massive COYO dataset performs worse than the model pre-trained on ImageNet for this specific regression task, refuting our initial hypothesis. Our analysis provides quantitative evidence highlighting the critical role of pre-training dataset characteristics, including scale, domain relevance, and curation quality, for effective transfer learning in 2D nutritional estimation.",
        "arxiv_id": "2508.03996",
        "ARXIVID": "2508.03996",
        "COMMENT": "Does not match any specific criteria but is tangentially related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2508.03749": {
        "authors": [
            "Riccardo Fiorista",
            "Awad Abdelhalim",
            "Anson F. Stewart",
            "Gabriel L. Pincus",
            "Ian Thistle",
            "Jinhua Zhao"
        ],
        "title": "Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation",
        "abstract": "arXiv:2508.03749v1 Announce Type: new  Abstract: Accurately estimating urban rail platform occupancy can enhance transit agencies' ability to make informed operational decisions, thereby improving safety, operational efficiency, and customer experience, particularly in the context of crowding. However, sensing real-time crowding remains challenging and often depends on indirect proxies such as automatic fare collection data or staff observations. Recently, Closed-Circuit Television (CCTV) footage has emerged as a promising data source with the potential to yield accurate, real-time occupancy estimates. The presented study investigates this potential by comparing three state-of-the-art computer vision approaches for extracting crowd-related features from platform CCTV imagery: (a) object detection and counting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification via a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic segmentation using DeepLabV3. Additionally, we present a novel, highly efficient linear-optimization-based approach to extract counts from the generated segmentation maps while accounting for image object depth and, thus, for passenger dispersion along a platform. Tested on a privacy-preserving dataset created in collaboration with the Washington Metropolitan Area Transit Authority (WMATA) that encompasses more than 600 hours of video material, our results demonstrate that computer vision approaches can provide substantive value for crowd estimation. This work demonstrates that CCTV image data, independent of other data sources available to a transit agency, can enable more precise real-time crowding estimation and, eventually, timely operational responses for platform crowding mitigation.",
        "arxiv_id": "2508.03749",
        "ARXIVID": "2508.03749",
        "COMMENT": "Does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}