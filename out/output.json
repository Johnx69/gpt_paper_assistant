{
    "2511.02779": {
        "authors": [
            "Yiyang Zhou",
            "Haoqin Tu",
            "Zijun Wang",
            "Zeyu Wang",
            "Niklas Muennighoff",
            "Fan Nie",
            "Yejin Choi",
            "James Zou",
            "Chaorui Deng",
            "Shen Yan",
            "Haoqi Fan",
            "Cihang Xie",
            "Huaxiu Yao",
            "Qinghao Ye"
        ],
        "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought",
        "abstract": "arXiv:2511.02779v1 Announce Type: new  Abstract: We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.",
        "arxiv_id": "2511.02779",
        "ARXIVID": "2511.02779",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark (MIRA) for visual chain-of-thought reasoning, emphasizing multimodal reasoning with intermediate visual generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2511.02650": {
        "authors": [
            "Tianfan Peng",
            "Yuntao Du",
            "Pengzhou Ji",
            "Shijie Dong",
            "Kailin Jiang",
            "Mingchuan Ma",
            "Yijun Tian",
            "Jinhe Bi",
            "Qian Li",
            "Wei Du",
            "Feng Xiao",
            "Lizhen Cui"
        ],
        "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models",
        "abstract": "arXiv:2511.02650v1 Announce Type: new  Abstract: Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.",
        "arxiv_id": "2511.02650",
        "ARXIVID": "2511.02650",
        "COMMENT": "Matches criterion 5 as it introduces a benchmark for visual token compression in multimodal LLMs, focusing on image and language integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.00509": {
        "authors": [
            "Yifan Xia",
            "Guorui Chen",
            "Wenqian Yu",
            "Zhijiang Li",
            "Philip Torr",
            "Jindong Gu"
        ],
        "title": "Reimagining Safety Alignment with An Image",
        "abstract": "arXiv:2511.00509v1 Announce Type: new  Abstract: Large language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusal of benign queries due to rigid safety mechanisms. These issues are further complicated by the need to accommodate different value systems and precisely align with given safety preferences. Moreover, traditional methods like SFT and RLHF lack this capability due to their costly parameter tuning requirements and inability to support multiple value systems within a single model. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security while reducing over-refusal. By optimizing image prompts using harmful/benign samples, our method enables a single model to adapt to different value systems and better align with given safety preferences without parameter updates. Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.",
        "arxiv_id": "2511.00509",
        "ARXIVID": "2511.00509",
        "COMMENT": "Matches criterion 2 as it explores safety alignment in multimodal large language models (MLLMs), which is directly related to vision\u2013language integration.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2511.01149": {
        "authors": [
            "Shuaidong Pan",
            "Di Wu"
        ],
        "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models",
        "abstract": "arXiv:2511.01149v1 Announce Type: new  Abstract: This paper addresses the limitations of a single agent in task decomposition and collaboration during complex task execution, and proposes a multi-agent architecture for modular task decomposition and dynamic collaboration based on large language models. The method first converts natural language task descriptions into unified semantic representations through a large language model. On this basis, a modular decomposition mechanism is introduced to break down the overall goal into multiple hierarchical sub-tasks. Then, dynamic scheduling and routing mechanisms enable reasonable division of labor and realtime collaboration among agents, allowing the system to adjust strategies continuously according to environmental feedback, thus maintaining efficiency and stability in complex tasks. Furthermore, a constraint parsing and global consistency mechanism is designed to ensure coherent connections between sub-tasks and balanced workload, preventing performance degradation caused by redundant communication or uneven resource allocation. The experiments validate the architecture across multiple dimensions, including task success rate, decomposition efficiency, sub-task coverage, and collaboration balance. The results show that the proposed method outperforms existing approaches in both overall performance and robustness, achieving a better balance between task complexity and communication overhead. In conclusion, this study demonstrates the effectiveness and feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems, providing a systematic solution for task execution in complex environments.",
        "arxiv_id": "2511.01149",
        "ARXIVID": "2511.01149",
        "COMMENT": "Matches criterion 1 as it focuses on spatial intelligence and task decomposition in multi-agent systems, which is relevant to embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.02384": {
        "authors": [
            "Jiahe Song",
            "Chuang Wang",
            "Bowen Jiang",
            "Yinfan Wang",
            "Hao Zheng",
            "Xingjian Wei",
            "Chengjin Liu",
            "Junyuan Gao",
            "Yubin Wang",
            "Lijun Wu",
            "Jiang Wu",
            "Qian Yu",
            "Conghui He"
        ],
        "title": "RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning",
        "abstract": "arXiv:2511.02384v1 Announce Type: new  Abstract: Large-scale chemical reaction datasets are crucial for AI research in chemistry. However, existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models. In response to this challenge, we propose the RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP). Our framework reformulates the traditional coordinate prediction driven parsing process into an image captioning problem, which Large Vision-Language Models (LVLMs) handle naturally. We introduce a strategy termed \"BBox and Index as Visual Prompt\" (BIVP), which uses our state-of-the-art molecular detector, MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the input image. This turns the downstream parsing into a natural-language description problem. Extensive experiments show that the BIVP strategy significantly improves structural extraction quality while simplifying model design. We further construct the RxnCaption-11k dataset, an order of magnitude larger than prior real-world literature benchmarks, with a balanced test subset across four layout archetypes. Experiments demonstrate that RxnCaption-VL achieves state-of-the-art performance on multiple metrics. We believe our method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry. We will release data, models, and code on GitHub.",
        "arxiv_id": "2511.02384",
        "ARXIVID": "2511.02384",
        "COMMENT": "Matches criterion 5 as it integrates image understanding tasks with large language models for chemical reaction diagram parsing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.02483": {
        "authors": [
            "Xilong Zhou",
            "Jianchun Chen",
            "Pramod Rao",
            "Timo Teufel",
            "Linjie Lyu",
            "Tigran Minasian",
            "Oleksandr Sotnychenko",
            "Xiaoxiao Long",
            "Marc Habermann",
            "Christian Theobalt"
        ],
        "title": "OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control",
        "abstract": "arXiv:2511.02483v1 Announce Type: new  Abstract: We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.",
        "arxiv_id": "2511.02483",
        "ARXIVID": "2511.02483",
        "COMMENT": "Matches criterion 3 as it introduces a new large-scale real-world dataset (OLATverse) for object-centric tasks, which can be used in embodied AI and vision tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2511.01668": {
        "authors": [
            "Yueqing Xi",
            "Yifan Bai",
            "Huasen Luo",
            "Weiliang Wen",
            "Hui Liu",
            "Haoliang Li"
        ],
        "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics",
        "abstract": "arXiv:2511.01668v1 Announce Type: new  Abstract: As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.",
        "arxiv_id": "2511.01668",
        "ARXIVID": "2511.01668",
        "COMMENT": "Matches criterion 5 as it integrates retrieval-augmented generation with LLMs for legal question answering, combining text and structured data.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.01915": {
        "authors": [
            "Edoardo Conti",
            "Riccardo Rosati",
            "Lorenzo Federici",
            "Adriano Mancini",
            "Maria Chiara Fiorentin"
        ],
        "title": "Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound",
        "abstract": "arXiv:2511.01915v1 Announce Type: new  Abstract: Purpose: This study provides the first comprehensive evaluation of foundation models in fetal ultrasound (US) imaging under low inter-class variability conditions. While recent vision foundation models such as DINOv3 have shown remarkable transferability across medical domains, their ability to discriminate anatomically similar structures has not been systematically investigated. We address this gap by focusing on fetal brain standard planes--transthalamic (TT), transventricular (TV), and transcerebellar (TC)--which exhibit highly overlapping anatomical features and pose a critical challenge for reliable biometric assessment.   Methods: To ensure a fair and reproducible evaluation, all publicly available fetal ultrasound datasets were curated and aggregated into a unified multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated images from heterogeneous acquisition settings. DINOv3 was pretrained in a self-supervised manner to learn ultrasound-aware representations. The learned features were then evaluated through standardized adaptation protocols, including linear probing with frozen backbone and full fine-tuning, under two initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization from natural-image DINOv3 weights.   Results: Models pretrained on fetal ultrasound data consistently outperformed those initialized on natural images, with weighted F1-score improvements of up to 20 percent. Domain-adaptive pretraining enabled the network to preserve subtle echogenic and structural cues crucial for distinguishing intermediate planes such as TV.   Conclusion: Results demonstrate that generic foundation models fail to generalize under low inter-class variability, whereas domain-specific pretraining is essential to achieve robust and clinically reliable representations in fetal brain ultrasound imaging.",
        "arxiv_id": "2511.01915",
        "ARXIVID": "2511.01915",
        "COMMENT": "Matches criterion 4 as it evaluates a vision foundation model (DINOv3) in a specific medical imaging application, focusing on its robustness and transferability.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.00267": {
        "authors": [
            "Christian Prothmann",
            "Vijay Gadepally",
            "Jeremy Kepner",
            "Koley Borchard",
            "Luca Carlone",
            "Zachary Folcik",
            "J. Daniel Grith",
            "Michael Houle",
            "Jonathan P. How",
            "Nathan Hughes",
            "Ifueko Igbinedion",
            "Hayden Jananthan",
            "Tejas Jayashankar",
            "Michael Jones",
            "Sertac Karaman",
            "Binoy G. Kurien",
            "Alejandro Lancho",
            "Giovanni Lavezzi",
            "Gary C. F. Lee",
            "Charles E. Leiserson",
            "Richard Linares",
            "Lindsey McEvoy",
            "Peter Michaleas",
            "Chasen Milner",
            "Alex Pentland",
            "Yury Polyanskiy",
            "Jovan Popovich",
            "Jeffrey Price",
            "Tim W. Reid",
            "Stephanie Riley",
            "Siddharth Samsi",
            "Peter Saunders",
            "Olga Simek",
            "Mark S. Veillette",
            "Amir Weiss",
            "Gregory W. Wornell",
            "Daniela Rus",
            "Scott T. Ruppel"
        ],
        "title": "Advancing AI Challenges for the United States Department of the Air Force",
        "abstract": "arXiv:2511.00267v1 Announce Type: new  Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States Department of the Air Force (DAF) and the Massachusetts Institute of Technology (MIT). This program pioneers fundamental advances in artificial intelligence (AI) to expand the competitive advantage of the United States in the defense and civilian sectors. In recent years, AI Accelerator projects have developed and launched public challenge problems aimed at advancing AI research in priority areas. Hallmarks of AI Accelerator challenges include large, publicly available, and AI-ready datasets to stimulate open-source solutions and engage the wider academic and private sector AI ecosystem. This article supplements our previous publication, which introduced AI Accelerator challenges. We provide an update on how ongoing and new challenges have successfully contributed to AI research and applications of AI technologies.",
        "arxiv_id": "2511.00267",
        "ARXIVID": "2511.00267",
        "COMMENT": "Matches criterion 3 as it discusses new benchmarks and challenges for AI research, which could be relevant to embodied or robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.02206": {
        "authors": [
            "Zhengjie Zhang",
            "Xiaoxie Mao",
            "Qihao Guo",
            "Shaoting Zhang",
            "Qi Huang",
            "Mu Zhou",
            "Fang Xie",
            "Mianxin Liu"
        ],
        "title": "Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers",
        "abstract": "arXiv:2511.02206v1 Announce Type: new  Abstract: Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta positron emission tomography (Abeta-PET), which is limited by high cost and limited accessibility. This study explores whether Abeta-PET spatial patterns can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566 participants. A language-enhanced generative model, driven by a large language model (LLM) and multimodal information fusion, was developed to synthesize PET images. Synthesized images were evaluated for image quality, diagnostic consistency, and clinical applicability within a fully automated diagnostic pipeline. Findings: The synthetic PET images closely resemble real PET scans in both structural details (SSIM = 0.920 +/- 0.003) and regional patterns (Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show high agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic PET, we developed a fully automatic AD diagnostic pipeline integrating PET synthesis and classification. The synthetic PET-based model (AUC = 0.78) outperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while combining synthetic PET and BBMs further improved performance (AUC = 0.79). Ablation analysis supports the advantages of LLM integration and prompt engineering. Interpretation: Our language-enhanced generative model synthesizes realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial pattern assessment and improving the diagnostic workflow for Alzheimer's disease.",
        "arxiv_id": "2511.02206",
        "ARXIVID": "2511.02206",
        "COMMENT": "Matches criterion 2 as it explores a language-enhanced generative model integrating multimodal information (MRI, BBMs) for PET synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.02563": {
        "authors": [
            "Akash Sharma",
            "Chinmay Mhatre",
            "Sankalp Gawali",
            "Ruthvik Bokkasam",
            "Brij Kishore",
            "Vishwajeet Pattanaik",
            "Tarun Rambha",
            "Abdul R. Pinjari",
            "Vijay Kovvali",
            "Anirban Chakraborty",
            "Punit Rathore",
            "Raghu Krishnapuram",
            "Yogesh Simmhan"
        ],
        "title": "The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic",
        "abstract": "arXiv:2511.02563v1 Announce Type: new  Abstract: This report describes the UVH-26 dataset, the first public release by AIM@IISc of a large-scale dataset of annotated traffic-camera images from India. The dataset comprises 26,646 high-resolution (1080p) images sampled from 2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently annotated through a crowdsourced hackathon involving 565 college students from across India. In total, 1.8 million bounding boxes were labeled across 14 vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler (Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller, Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k consensus ground truth bounding boxes and labels were derived for distinct objects in the 26k images using Majority Voting and STAPLE algorithms. Further, we train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X, and DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50, mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in mAP50:95 over equivalent baseline models trained on COCO dataset, with RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40 for COCO-trained weights for common classes (Car, Bus, and Truck). This demonstrates the benefits of domain-specific training data for Indian traffic scenarios. The release package provides the 26k images with consensus annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the 6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the heterogeneity of Indian urban mobility directly from operational traffic-camera streams, UVH-26 addresses a critical gap in existing global benchmarks, and offers a foundation for advancing detection, classification, and deployment of intelligent transportation systems in emerging nations with complex traffic conditions.",
        "arxiv_id": "2511.02563",
        "ARXIVID": "2511.02563",
        "COMMENT": "Matches criterion 4 as it focuses on a vision foundation model application for Indian traffic scenarios, leveraging domain-specific datasets.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2511.02826": {
        "authors": [
            "Harshith Padigela",
            "Shima Nofallah",
            "Atchuth Naveen Chilaparasetti",
            "Ryun Han",
            "Andrew Walker",
            "Judy Shen",
            "Chintan Shah",
            "Blake Martin",
            "Aashish Sood",
            "Elliot Miller",
            "Ben Glass",
            "Andy Beck",
            "Harsha Pokkalla",
            "Syed Ashar Javed"
        ],
        "title": "PLUTO-4: Frontier Pathology Foundation Models",
        "abstract": "arXiv:2511.02826v1 Announce Type: new  Abstract: Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4's potential to transform real-world applications as a backbone for translational research and diagnostic use cases.",
        "arxiv_id": "2511.02826",
        "ARXIVID": "2511.02826",
        "COMMENT": "Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a new generation of pathology foundation models with diverse applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.01824": {
        "authors": [
            "Yuetai Li",
            "Huseyin A Inan",
            "Xiang Yue",
            "Wei-Ning Chen",
            "Lukas Wutschitz",
            "Janardhan Kulkarni",
            "Radha Poovendran",
            "Robert Sim",
            "Saravan Rajmohan"
        ],
        "title": "Simulating Environments with Reasoning Models for Agent Training",
        "abstract": "arXiv:2511.01824v1 Announce Type: new  Abstract: LLM agents excel in compact environments requiring deep reasoning but remain brittle when operating in broader, more complex contexts that demand robustness across diverse tools and schemas. Building bespoke environments for training is heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs can simulate realistic environment feedback without access to actual testbed data or APIs. Inspired by this capability, we propose two frameworks: Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables RL training without real environment implementations through LLM-simulated feedback. Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.",
        "arxiv_id": "2511.01824",
        "ARXIVID": "2511.01824",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces frameworks for training agents using simulated environments.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.02489": {
        "authors": [
            "Tao Liu",
            "Kan Ren",
            "Qian Chen"
        ],
        "title": "Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization",
        "abstract": "arXiv:2511.02489v1 Announce Type: new  Abstract: With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems. However, in GNSS-denied areas, satellite-based localization methods are prone to failure. This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching. In typical pipelines, UAV visual localization is formulated as an image-retrieval problem: features are extracted to build a localization map, and the pose of a query image is estimated by matching it to a reference database with known poses. Because publicly available UAV localization datasets are limited, many approaches recast localization as a classification task and rely on scene labels in these datasets to ensure accuracy. Other methods seek to reduce cross-domain differences using polar-coordinate reprojection, perspective transformations, or generative adversarial networks; however, they can suffer from misalignment, content loss, and limited realism. In contrast, we leverage modern object detection to accurately extract salient instances from UAV and satellite images, and integrate a graph neural network to reason about inter-image and intra-image node relationships. Using a fine-grained, graph-based node-similarity metric, our method achieves strong retrieval and localization performance. Extensive experiments on public and real-world datasets show that our approach handles heterogeneous appearance differences effectively and generalizes well, making it applicable to scenarios with larger modality gaps, such as infrared-visible image matching. Our dataset will be publicly available at the following URL: https://github.com/liutao23/ODGNNLoc.git.",
        "arxiv_id": "2511.02489",
        "ARXIVID": "2511.02489",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a cross-view UAV localization framework using object detection and graph neural networks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.02767": {
        "authors": [
            "Tyler Zhu",
            "Tengda Han",
            "Leonidas Guibas",
            "Viorica P\\u{a}tr\\u{a}ucean",
            "Maks Ovsjanikov"
        ],
        "title": "Dynamic Reflections: Probing Video Representations with Text Alignment",
        "abstract": "arXiv:2511.02767v1 Announce Type: new  Abstract: The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/",
        "arxiv_id": "2511.02767",
        "ARXIVID": "2511.02767",
        "COMMENT": "Matches criterion 6 (Video Understanding) as it explores video-text representation alignment and its implications for video understanding tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.00640": {
        "authors": [
            "Zicheng Xu",
            "Guanchu Wang",
            "Yu-Neng Chuang",
            "Guangyao Zheng",
            "Alexander S. Szalay",
            "Zirui Liu",
            "Vladimir Braverman"
        ],
        "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching",
        "abstract": "arXiv:2511.00640v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.",
        "arxiv_id": "2511.00640",
        "ARXIVID": "2511.00640",
        "COMMENT": "Does not match any specific criterion but discusses reasoning optimization in large models, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.02277": {
        "authors": [
            "Giorgos Sfikas",
            "Konstantina Nikolaidou",
            "Foteini Papadopoulou",
            "George Retsinas",
            "Anastasios L. Kesidis"
        ],
        "title": "Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?",
        "abstract": "arXiv:2511.02277v1 Announce Type: new  Abstract: Object pose estimation is a task that is of central importance in 3D Computer Vision. Given a target image and a canonical pose, a single point estimate may very often be sufficient; however, a probabilistic pose output is related to a number of benefits when pose is not unambiguous due to sensor and projection constraints or inherent object symmetries. With this paper, we explore the usefulness of using the well-known Euler angles parameterisation as a basis for a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation, 3D pose has been parameterized in a number of ways, either in or out of the context of parameter estimation. We explore the idea that Euler angles, despite their shortcomings, may lead to useful models in a number of aspects, compared to a model built on a more complex parameterisation.",
        "arxiv_id": "2511.02277",
        "ARXIVID": "2511.02277",
        "COMMENT": "Partially relevant to criterion 1 as it discusses pose estimation, which involves spatial reasoning, but does not focus on embodied agents.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2511.01033": {
        "authors": [
            "Tiberiu Musat",
            "Tiago Pimentel",
            "Lorenzo Noci",
            "Alessandro Stolfo",
            "Mrinmaya Sachan",
            "Thomas Hofmann"
        ],
        "title": "On the Emergence of Induction Heads for In-Context Learning",
        "abstract": "arXiv:2511.01033v1 Announce Type: new  Abstract: Transformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.",
        "arxiv_id": "2511.01033",
        "ARXIVID": "2511.01033",
        "COMMENT": "Does not match any specific criteria but discusses the emergence of induction heads in transformers, which is tangentially related to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.01329": {
        "authors": [
            "Ying Song",
            "Yijing Wang",
            "Hui Yang",
            "Weihan Jin",
            "Jun Xiong",
            "Congyi Zhou",
            "Jialin Zhu",
            "Xiang Gao",
            "Rong Chen",
            "HuaGuang Deng",
            "Ying Dai",
            "Fei Xiao",
            "Haihong Tang",
            "Bo Zheng",
            "KaiFu Zhang"
        ],
        "title": "Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework",
        "abstract": "arXiv:2511.01329v1 Announce Type: new  Abstract: Evaluating platform-level interventions in search-based two-sided marketplaces is fundamentally challenged by systemic effects such as spillovers and network interference. While widely used for causal inference, the PSM (Propensity Score Matching) - DID (Difference-in-Differences) framework remains susceptible to selection bias and cross-unit interference from unaccounted spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel causal framework that integrates propensity score matching with competitive isolation to enable platform-level effect measurement (e.g., order volume, GMV) instead of item-level metrics in search systems.   Our approach provides theoretically guaranteed unbiased estimation under mutual exclusion conditions, with an open dataset released to support reproducible research on marketplace interference (github.com/xxxx). Extensive experiments demonstrate significant reductions in interference effects and estimation variance compared to baseline methods. Successful deployment in a large-scale marketplace confirms the framework's practical utility for platform-level causal inference.",
        "arxiv_id": "2511.01329",
        "ARXIVID": "2511.01329",
        "COMMENT": "Does not match any specific criterion but discusses causal estimation in search systems, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01363": {
        "authors": [
            "Giuseppe Riva",
            "Brenda K. Wiederhold",
            "Fabrizia Mantovani"
        ],
        "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing",
        "abstract": "arXiv:2511.01363v1 Announce Type: new  Abstract: The cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues (for example, the suggestion of a therapist or the prompt of the user) override stable knowledge.   These mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency - the capacity for complex, goal-directed, context-sensitive behavior - without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable AI lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.",
        "arxiv_id": "2511.01363",
        "ARXIVID": "2511.01363",
        "COMMENT": "Does not match any specific criterion but may be of general interest due to its exploration of parallels between cognitive processes and LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01998": {
        "authors": [
            "Benjamin Walder",
            "Daniel Toader",
            "Robert Nuster",
            "G\\\"unther Paltauf",
            "Peter Burgholzer",
            "Gregor Langer",
            "Lukas Krainer",
            "Markus Haltmeier"
        ],
        "title": "Locally-Supervised Global Image Restoration",
        "abstract": "arXiv:2511.01998v1 Announce Type: new  Abstract: We address the problem of image reconstruction from incomplete measurements, encompassing both upsampling and inpainting, within a learning-based framework. Conventional supervised approaches require fully sampled ground truth data, while self-supervised methods allow incomplete ground truth but typically rely on random sampling that, in expectation, covers the entire image. In contrast, we consider fixed, deterministic sampling patterns with inherently incomplete coverage, even in expectation. To overcome this limitation, we exploit multiple invariances of the underlying image distribution, which theoretically allows us to achieve the same reconstruction performance as fully supervised approaches. We validate our method on optical-resolution image upsampling in photoacoustic microscopy (PAM), demonstrating competitive or superior results while requiring substantially less ground truth data.",
        "arxiv_id": "2511.01998",
        "ARXIVID": "2511.01998",
        "COMMENT": "Does not match any specific criterion but is relevant to image restoration and reconstruction tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.00092": {
        "authors": [
            "Shunya Minami",
            "Tatsuya Ishigaki",
            "Ikko Hamamura",
            "Taku Mikuriya",
            "Youmi Ma",
            "Naoaki Okazaki",
            "Hiroya Takamura",
            "Yohichi Suzuki",
            "Tadashi Kadowaki"
        ],
        "title": "QuantumBench: A Benchmark for Quantum Problem Solving",
        "abstract": "arXiv:2511.00092v1 Announce Type: new  Abstract: Large language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.",
        "arxiv_id": "2511.00092",
        "ARXIVID": "2511.00092",
        "COMMENT": "Does not match any specific criterion but is relevant to benchmarking large language models in quantum science.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.02180": {
        "authors": [
            "Mehdi Sefidgar Dilmaghani",
            "Waseem Shariff",
            "Cian Ryan",
            "Joe Lemley",
            "Peter Corcoran"
        ],
        "title": "Autobiasing Event Cameras for Flickering Mitigation",
        "abstract": "arXiv:2511.02180v1 Announce Type: new  Abstract: Understanding and mitigating flicker effects caused by rapid variations in light intensity is critical for enhancing the performance of event cameras in diverse environments. This paper introduces an innovative autonomous mechanism for tuning the biases of event cameras, effectively addressing flicker across a wide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on additional hardware or software for flicker filtering, our approach leverages the event cameras inherent bias settings. Utilizing a simple Convolutional Neural Networks -CNNs, the system identifies instances of flicker in a spatial space and dynamically adjusts specific biases to minimize its impact. The efficacy of this autobiasing system was robustly tested using a face detector framework under both well-lit and low-light conditions, as well as across various frequencies. The results demonstrated significant improvements: enhanced YOLO confidence metrics for face detection, and an increased percentage of frames capturing detected faces. Moreover, the average gradient, which serves as an indicator of flicker presence through edge detection, decreased by 38.2 percent in well-lit conditions and by 53.6 percent in low-light conditions. These findings underscore the potential of our approach to significantly improve the functionality of event cameras in a range of adverse lighting scenarios.",
        "arxiv_id": "2511.02180",
        "ARXIVID": "2511.02180",
        "COMMENT": "Does not match any specific criterion but is relevant to event camera optimization and flicker mitigation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.02086": {
        "authors": [
            "Yue Yang",
            "Fabian Necker",
            "Christoph Leuze",
            "Michelle Chen",
            "Andrey Finegersh",
            "Jake Lee",
            "Vasu Divi",
            "Bruce Daniel",
            "Brian Hargreaves",
            "Jie Ying Wu",
            "Fred M Baik"
        ],
        "title": "Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study",
        "abstract": "arXiv:2511.02086v1 Announce Type: new  Abstract: Purpose: In this paper, we develop and clinically evaluate a depth-only, markerless augmented reality (AR) registration pipeline on a head-mounted display, and assess accuracy across small or low-curvature anatomies in real-life operative settings. Methods: On HoloLens 2, we align Articulated HAnd Tracking (AHAT) depth to Computed Tomography (CT)-derived skin meshes via (i) depth-bias correction, (ii) brief human-in-the-loop initialization, (iii) global and local registration. We validated the surface-tracing error metric by comparing \"skin-to-bone\" relative distances to CT ground truth on leg and foot models, using an AR-tracked tool. We then performed seven intraoperative target trials (feet x2, ear x3, leg x2) during the initial stage of fibula free-flap harvest and mandibular reconstruction surgery, and collected 500+ data per trial. Results: Preclinical validation showed tight agreement between AR-traced and CT distances (leg: median |Delta d| 0.78 mm, RMSE 0.97 mm; feet: 0.80 mm, 1.20 mm). Clinically, per-point error had a median of 3.9 mm. Median errors by anatomy were 3.2 mm (feet), 4.3 mm (ear), and 5.3 mm (lower leg), with 5 mm coverage 92-95%, 84-90%, and 72-86%, respectively. Feet vs. lower leg differed significantly (Delta median ~1.1 mm; p < 0.001). Conclusion: A depth-only, markerless AR pipeline on HMDs achieved ~3-4 mm median error across feet, ear, and lower leg in live surgical settings without fiducials, approaching typical clinical error thresholds for moderate-risk tasks. Human-guided initialization plus global-to-local registration enabled accurate alignment on small or low-curvature targets, improving the clinical readiness of markerless AR guidance.",
        "arxiv_id": "2511.02086",
        "ARXIVID": "2511.02086",
        "COMMENT": "Does not match any specific criterion but is relevant to augmented reality and surgical guidance applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.01320": {
        "authors": [
            "Ziqi Wang",
            "Hailiang Zhao",
            "Yuhao Yang",
            "Daojiang Hu",
            "Cheng Bao",
            "Mingyi Liu",
            "Kai Di",
            "Schahram Dustdar",
            "Zhongjie Wang",
            "Shuiguang Deng"
        ],
        "title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance",
        "abstract": "arXiv:2511.01320v1 Announce Type: new  Abstract: Accurate and timely prediction of tool conditions is critical for intelligent manufacturing systems, where unplanned tool failures can lead to quality degradation and production downtime. In modern industrial environments, predictive maintenance is increasingly implemented as an intelligent service that integrates sensing, analysis, and decision support across production processes. To meet the demand for reliable and service-oriented operation, we present OmniFuser, a multimodal learning framework for predictive maintenance of milling tools that leverages both visual and sensor data. It performs parallel feature extraction from high-resolution tool images and cutting-force signals, capturing complementary spatiotemporal patterns across modalities. To effectively integrate heterogeneous features, OmniFuser employs a contamination-free cross-modal fusion mechanism that disentangles shared and modality-specific components, allowing for efficient cross-modal interaction. Furthermore, a recursive refinement pathway functions as an anchor mechanism, consistently retaining residual information to stabilize fusion dynamics. The learned representations can be encapsulated as reusable maintenance service modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled) and multi-step force signal forecasting. Experiments on real-world milling datasets demonstrate that OmniFuser consistently outperforms state-of-the-art baselines, providing a dependable foundation for building intelligent industrial maintenance services.",
        "arxiv_id": "2511.01320",
        "ARXIVID": "2511.01320",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multimodal learning and intelligent systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.02565": {
        "authors": [
            "Jingyu Lu",
            "Haonan Wang",
            "Qixiang Zhang",
            "Xiaomeng Li"
        ],
        "title": "A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding",
        "abstract": "arXiv:2511.02565v1 Announce Type: new  Abstract: Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.",
        "arxiv_id": "2511.02565",
        "ARXIVID": "2511.02565",
        "COMMENT": "Does not match any specific criteria but focuses on brain visual decoding, which is tangentially related to vision and multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.02193": {
        "authors": [
            "Jiawen Liu",
            "Yuanbo Zeng",
            "Jiaming Liang",
            "Yizhen Yang",
            "Yiheng Zhang",
            "Enhui Cai",
            "Xiaoqi Sheng",
            "Hongmin Cai"
        ],
        "title": "MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation",
        "abstract": "arXiv:2511.02193v1 Announce Type: new  Abstract: Accurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 $\\%$ on DRIVE and 1.25 $\\%$ on STARE, demonstrating its effectiveness and advancement. The project code is public via https://github.com/liujiawen-jpg/MM-UNet.",
        "arxiv_id": "2511.02193",
        "ARXIVID": "2511.02193",
        "COMMENT": "Does not match any specific criteria but focuses on retinal vessel segmentation, which is a specific application of computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.02791": {
        "authors": [
            "Nusrat Tasnim",
            "Kutub Uddin",
            "Khalid Mahmood Malik"
        ],
        "title": "AI-Generated Image Detection: An Empirical Study and Future Research Directions",
        "abstract": "arXiv:2511.02791v1 Announce Type: new  Abstract: The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.",
        "arxiv_id": "2511.02791",
        "ARXIVID": "2511.02791",
        "COMMENT": "Does not match any specific criteria but discusses AI-generated image detection, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}