{
    "2511.08246": {
        "authors": [
            "Ziyu Ma",
            "Chenhui Gou",
            "Yiming Hu",
            "Yong Wang",
            "Xiangxiang Chu",
            "Bohan Zhuang",
            "Jianfei Cai"
        ],
        "title": "Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning",
        "abstract": "arXiv:2511.08246v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have shown promising in-context learning (ICL) capabilities, but scaling to many-shot settings remains difficult due to limited context length and high inference cost. To address these challenges, task-vector-based methods have been explored by inserting compact representations of many-shot in-context demonstrations into model activations. However, existing task-vector-based methods either overlook the importance of where to insert task vectors or struggle to determine suitable values for each location. To this end, we propose a novel Sensitivity-aware Task Vector insertion framework (STV) to figure out where and what to insert. Our key insight is that activation deltas across query-context pairs exhibit consistent structural patterns, providing a reliable cue for insertion. Based on the identified sensitive-aware locations, we construct a pre-clustered activation bank for each location by clustering the activation values, and then apply reinforcement learning to choose the most suitable one to insert. We evaluate STV across a range of multimodal models (e.g., Qwen-VL, Idefics-2) and tasks (e.g., VizWiz, OK-VQA), demonstrating its effectiveness and showing consistent improvements over previous task-vector-based methods with strong generalization.",
        "arxiv_id": "2511.08246",
        "ARXIVID": "2511.08246",
        "COMMENT": "Matches criterion 2 as it explores multimodal large language models (LMMs) and their in-context learning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.09018": {
        "authors": [
            "Liu Yu",
            "Zhonghao Chen",
            "Ping Kuang",
            "Zhikun Feng",
            "Fan Zhou",
            "Lan Wang",
            "Gillian Dobbie"
        ],
        "title": "Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs",
        "abstract": "arXiv:2511.09018v1 Announce Type: new  Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL",
        "arxiv_id": "2511.09018",
        "ARXIVID": "2511.09018",
        "COMMENT": "Matches criterion 2 as it addresses object hallucination in large vision-language models with a novel causally-grounded framework.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2511.09082": {
        "authors": [
            "Zhen Li",
            "Yuwei Wu",
            "Chenchen Jing",
            "Che Sun",
            "Chuanhao Li",
            "Yunde Jia"
        ],
        "title": "Composition-Incremental Learning for Compositional Generalization",
        "abstract": "arXiv:2511.09082v1 Announce Type: new  Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.",
        "arxiv_id": "2511.09082",
        "ARXIVID": "2511.09082",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for compositional generalization in an incremental learning setting, relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.07988": {
        "authors": [
            "Nico Policzer",
            "Cameron Braunstein",
            "Mariya Toneva"
        ],
        "title": "The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends",
        "abstract": "arXiv:2511.07988v1 Announce Type: new  Abstract: Recent studies on audio models show brain-tuning - fine-tuning models to better predict corresponding fMRI activity - improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data - sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.",
        "arxiv_id": "2511.07988",
        "ARXIVID": "2511.07988",
        "COMMENT": "Matches criterion 2 as it explores multimodal models and their alignment with brain activity, which is relevant to vision-language integration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.09139": {
        "authors": [
            "Zijian Chen",
            "Yuze Sun",
            "Yuan Tian",
            "Wenjun Zhang",
            "Guangtao Zhai"
        ],
        "title": "MACEval: A Multi-Agent Continual Evaluation Network for Large Models",
        "abstract": "arXiv:2511.09139v1 Announce Type: new  Abstract: Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \\Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.",
        "arxiv_id": "2511.09139",
        "ARXIVID": "2511.09139",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MACEval) for evaluating large models, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.09502": {
        "authors": [
            "Jerrin Bright",
            "Yuhao Chen",
            "John S. Zelek"
        ],
        "title": "DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation",
        "abstract": "arXiv:2511.09502v1 Announce Type: new  Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.",
        "arxiv_id": "2511.09502",
        "ARXIVID": "2511.09502",
        "COMMENT": "Matches criterion 6 as it focuses on video-based tasks with a novel diffusion-based framework for 3D human pose estimation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.09170": {
        "authors": [
            "Ethan Griffiths",
            "Maryam Haghighat",
            "Simon Denman",
            "Clinton Fookes",
            "Milad Ramezani"
        ],
        "title": "HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests",
        "abstract": "arXiv:2511.09170v1 Announce Type: new  Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\\times$ on average. The code will be available upon acceptance.",
        "arxiv_id": "2511.09170",
        "ARXIVID": "2511.09170",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for embodied AI with a focus on LiDAR place recognition and 6-DoF metric localization.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2511.08609": {
        "authors": [
            "I. Bailo",
            "F. Buonora",
            "G. Ciarfaglia",
            "L. T. Consoli",
            "A. Evangelista",
            "M. Gabusi",
            "M. Ghiani",
            "C. Petracca Ciavarella",
            "F. Picariello",
            "F. Sarcina",
            "F. Tuosto",
            "V. Zullo",
            "L. Airoldi",
            "G. Bruno",
            "D. D. Gobbo",
            "S. Pezzenati",
            "G. A. Tona"
        ],
        "title": "Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants",
        "abstract": "arXiv:2511.08609v1 Announce Type: new  Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\\%.",
        "arxiv_id": "2511.08609",
        "ARXIVID": "2511.08609",
        "COMMENT": "Matches criterion 5 as it combines vision tasks with large language models for digitization of gas plants.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2511.09286": {
        "authors": [
            "Amir M. Mansourian",
            "Amir Mohammad Babaei",
            "Shohreh Kasaei"
        ],
        "title": "Enriching Knowledge Distillation with Cross-Modal Teacher Fusion",
        "abstract": "arXiv:2511.09286v1 Announce Type: new  Abstract: Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.",
        "arxiv_id": "2511.09286",
        "ARXIVID": "2511.09286",
        "COMMENT": "Matches criterion 5 as it explores integration of vision-language models (CLIP) with knowledge distillation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2511.08872": {
        "authors": [
            "Hu Cui",
            "Wenqiang Hua",
            "Renjing Huang",
            "Shurui Jia",
            "Tessai Hayama"
        ],
        "title": "SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation",
        "abstract": "arXiv:2511.08872v1 Announce Type: new  Abstract: Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at https://hucui2022.github.io/sasmamba_proj/.",
        "arxiv_id": "2511.08872",
        "ARXIVID": "2511.08872",
        "COMMENT": "Does not match any specific criterion but is related to 3D human pose estimation, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.09147": {
        "authors": [
            "Jiayue Yuan",
            "Fangting Xie",
            "Guangwen Ouyang",
            "Changhai Ma",
            "Ziyu Wu",
            "Heyu Ding",
            "Quan Wan",
            "Yi Ke",
            "Yuchen Wu",
            "Xiaohui Cai"
        ],
        "title": "PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery",
        "abstract": "arXiv:2511.09147v1 Announce Type: new  Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \\textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \\textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \\& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.",
        "arxiv_id": "2511.09147",
        "ARXIVID": "2511.09147",
        "COMMENT": "Does not match any specific criterion but is related to human motion analysis, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.07973": {
        "authors": [
            "Yue Wang",
            "Yuyang Xu",
            "Renjun Hu",
            "Fanqi Shen",
            "Hanyun Jiang",
            "Jun Wang",
            "Jintai Chen",
            "Danny Z. Chen",
            "Jian Wu",
            "Haochao Ying"
        ],
        "title": "Versatile and Risk-Sensitive Cardiac Diagnosis via Graph-Based ECG Signal Representation",
        "abstract": "arXiv:2511.07973v1 Announce Type: new  Abstract: Despite the rapid advancements of electrocardiogram (ECG) signal diagnosis and analysis methods through deep learning, two major hurdles still limit their clinical adoption: the lack of versatility in processing ECG signals with diverse configurations, and the inadequate detection of risk signals due to sample imbalances. Addressing these challenges, we introduce VersAtile and Risk-Sensitive cardiac diagnosis (VARS), an innovative approach that employs a graph-based representation to uniformly model heterogeneous ECG signals. VARS stands out by transforming ECG signals into versatile graph structures that capture critical diagnostic features, irrespective of signal diversity in the lead count, sampling frequency, and duration. This graph-centric formulation also enhances diagnostic sensitivity, enabling precise localization and identification of abnormal ECG patterns that often elude standard analysis methods. To facilitate representation transformation, our approach integrates denoising reconstruction with contrastive learning to preserve raw ECG information while highlighting pathognomonic patterns. We rigorously evaluate the efficacy of VARS on three distinct ECG datasets, encompassing a range of structural variations. The results demonstrate that VARS not only consistently surpasses existing state-of-the-art models across all these datasets but also exhibits substantial improvement in identifying risk signals. Additionally, VARS offers interpretability by pinpointing the exact waveforms that lead to specific model outputs, thereby assisting clinicians in making informed decisions. These findings suggest that our VARS will likely emerge as an invaluable tool for comprehensive cardiac health assessment.",
        "arxiv_id": "2511.07973",
        "ARXIVID": "2511.07973",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in graph-based methods and cardiac diagnosis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.08833": {
        "authors": [
            "Jiaxun Guo",
            "Manar Amayri",
            "Nizar Bouguila",
            "Xin Liu",
            "Wentao Fan"
        ],
        "title": "Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms",
        "abstract": "arXiv:2511.08833v1 Announce Type: new  Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.",
        "arxiv_id": "2511.08833",
        "ARXIVID": "2511.08833",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in 3D learning and rotation-invariant methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.09388": {
        "authors": [
            "Yang Chen",
            "Miaoge Li",
            "Zhijie Rao",
            "Deze Zeng",
            "Song Guo",
            "Jingcai Guo"
        ],
        "title": "Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition",
        "abstract": "arXiv:2511.09388v1 Announce Type: new  Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an \"align-then-classify\" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\\texttt{$\\textbf{Flora}$}$, which builds upon $\\textbf{F}$lexib$\\textbf{L}$e neighb$\\textbf{O}$r-aware semantic attunement and open-form dist$\\textbf{R}$ibution-aware flow cl$\\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.",
        "arxiv_id": "2511.09388",
        "ARXIVID": "2511.09388",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in machine learning and action recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.07896": {
        "authors": [
            "Dengcan Liu",
            "Jiahao Li",
            "Zheren Fu",
            "Yi Tu",
            "Jiajun Li",
            "Zhendong Mao",
            "Yongdong Zhang"
        ],
        "title": "SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder",
        "abstract": "arXiv:2511.07896v1 Announce Type: new  Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.",
        "arxiv_id": "2511.07896",
        "ARXIVID": "2511.07896",
        "COMMENT": "Does not match any specific criterion but is related to preference modeling in LLMs, which is tangentially relevant to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.07842": {
        "authors": [
            "Sunghyun Wee",
            "Suyoung Kim",
            "Hyeonjin Kim",
            "Kyomin Hwang",
            "Nojun Kwak"
        ],
        "title": "Alignment-Aware Quantization for LLM Safety",
        "abstract": "arXiv:2511.07842v1 Announce Type: new  Abstract: Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. Models can demonstrate low perplexity yet exhibit significant degradation in alignment with the safety policy, highlighting that perplexity alone is an insufficient and often misleading proxy for model safety. To address this, we propose Alignment-Aware Quantization(AAQ), a novel approach that integrates Alignment-Preserving Contrastive(APC) loss into the PTQ pipeline. Compared to simple reconstruction loss, ours explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. Our method achieves this robust safety alignment without resorting to specialized safety-focused calibration datasets, highlighting its practical utility and broad applicability. AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families such as LLaMA, Qwen, and Mistral while maintaining safety where previous methods fail. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.",
        "arxiv_id": "2511.07842",
        "ARXIVID": "2511.07842",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in large language models and quantization techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2511.07863": {
        "authors": [
            "Shinwoo Park",
            "Hyejin Park",
            "Hyeseon Ahn",
            "Yo-Sub Han"
        ],
        "title": "WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking",
        "abstract": "arXiv:2511.07863v1 Announce Type: new  Abstract: Large language models now draft news, legal analyses, and software code with human-level fluency. At the same time, regulations such as the EU AI Act mandate that each synthetic passage carry an imperceptible, machine-verifiable mark for provenance. Conventional logit-based watermarks satisfy this requirement by selecting a pseudorandom green vocabulary at every decoding step and boosting its logits, yet the random split can exclude the highest-probability token and thus erode fluency. WaterMod mitigates this limitation through a probability-aware modular rule. The vocabulary is first sorted in descending model probability; the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent-and therefore semantically similar-tokens across different classes. A fixed bias of small magnitude is applied to one selected class. In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list. Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling. In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d. Biasing the logits of that class embeds exactly one base-k digit per decoding step, thereby enabling fine-grained provenance tracing. The same modular arithmetic therefore supports both binary attribution and rich payloads. Experimental results demonstrate that WaterMod consistently attains strong watermark detection performance while maintaining generation quality in both zero-bit and multi-bit settings. This robustness holds across a range of tasks, including natural language generation, mathematical reasoning, and code synthesis. Our code and data are available at https://github.com/Shinwoo-Park/WaterMod.",
        "arxiv_id": "2511.07863",
        "ARXIVID": "2511.07863",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in large language models and watermarking techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}