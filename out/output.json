{
    "2510.27606": {
        "authors": [
            "Yuhong Liu",
            "Beichen Zhang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Long Xing",
            "Xiaoyi Dong",
            "Haodong Duan",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning",
        "abstract": "arXiv:2510.27606v1 Announce Type: new  Abstract: Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.",
        "arxiv_id": "2510.27606",
        "ARXIVID": "2510.27606",
        "COMMENT": "Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it focuses on improving spatial reasoning in LVLMs using self-supervised reinforcement learning.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2510.27350": {
        "authors": [
            "Weijian Jian",
            "Yajun Zhang",
            "Dawei Liang",
            "Chunyu Xie",
            "Yixiao He",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "title": "RzenEmbed: Towards Comprehensive Multimodal Retrieval",
        "abstract": "arXiv:2510.27350v1 Announce Type: new  Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has extended CLIP-based frameworks to produce powerful, universal embeddings for retrieval tasks. However, existing methods primarily focus on natural images, offering limited support for other crucial visual modalities such as videos and visual documents. To bridge this gap, we introduce RzenEmbed, a unified framework to learn embeddings across a diverse set of modalities, including text, images, videos, and visual documents. We employ a novel two-stage training strategy to learn discriminative representations. The first stage focuses on foundational text and multimodal retrieval. In the second stage, we introduce an improved InfoNCE loss, incorporating two key enhancements. Firstly, a hardness-weighted mechanism guides the model to prioritize challenging samples by assigning them higher weights within each batch. Secondly, we implement an approach to mitigate the impact of false negatives and alleviate data noise. This strategy not only enhances the model's discriminative power but also improves its instruction-following capabilities. We further boost performance with learnable temperature parameter and model souping. RzenEmbed sets a new state-of-the-art on the MMEB benchmark. It not only achieves the best overall score but also outperforms all prior work on the challenging video and visual document retrieval tasks. Our models are available in https://huggingface.co/qihoo360/RzenEmbed.",
        "arxiv_id": "2510.27350",
        "ARXIVID": "2510.27350",
        "COMMENT": "Matches criteria 2 and 5 as it introduces RzenEmbed, a multimodal retrieval framework for text, images, videos, and visual documents, with novel training strategies.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.27680": {
        "authors": [
            "Danyal Maqbool",
            "Changhee Lee",
            "Zachary Huemann",
            "Samuel D. Church",
            "Matthew E. Larson",
            "Scott B. Perlman",
            "Tomas A. Romero",
            "Joshua D. Warner",
            "Meghan Lubner",
            "Xin Tie",
            "Jameson Merkow",
            "Junjie Hu",
            "Steve Y. Cho",
            "Tyler J. Bradshaw"
        ],
        "title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting",
        "abstract": "arXiv:2510.27680v1 Announce Type: new  Abstract: Recent advances in vision-language models (VLMs) have enabled impressive multimodal reasoning, yet most medical applications remain limited to 2D imaging. In this work, we extend VLMs to 3D positron emission tomography and computed tomography (PET/CT), a domain characterized by large volumetric data, small and dispersed lesions, and lengthy radiology reports. We introduce a large-scale dataset comprising over 11,000 lesion-level descriptions paired with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid rule-based and large language model (LLM) pipeline. Building upon this dataset, we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET, CT, and lesion contours for spatially grounded report generation. PETAR bridges global contextual reasoning with fine-grained lesion awareness, producing clinically coherent and localized findings. Comprehensive automated and human evaluations demonstrate that PETAR substantially improves PET/CT report generation quality, advancing 3D medical vision-language understanding.",
        "arxiv_id": "2510.27680",
        "ARXIVID": "2510.27680",
        "COMMENT": "Matches criteria 2 and 5 as it introduces a 3D mask-aware vision-language model for PET/CT report generation, integrating multimodal reasoning and spatial grounding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2510.27391": {
        "authors": [
            "Wu Wei",
            "Xiaomeng Fan",
            "Yuwei Wu",
            "Zhi Gao",
            "Pengxiang Li",
            "Yunde Jia",
            "Mehrtash Harandi"
        ],
        "title": "Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds",
        "abstract": "arXiv:2510.27391v1 Announce Type: new  Abstract: Modality alignment is critical for vision-language models (VLMs) to effectively integrate information across modalities. However, existing methods extract hierarchical features from text while representing each image with a single feature, leading to asymmetric and suboptimal alignment. To address this, we propose Alignment across Trees, a method that constructs and aligns tree-like hierarchical features for both image and text modalities. Specifically, we introduce a semantic-aware visual feature extraction framework that applies a cross-attention mechanism to visual class tokens from intermediate Transformer layers, guided by textual cues to extract visual features with coarse-to-fine semantics. We then embed the feature trees of the two modalities into hyperbolic manifolds with distinct curvatures to effectively model their hierarchical structures. To align across the heterogeneous hyperbolic manifolds with different curvatures, we formulate a KL distance measure between distributions on heterogeneous manifolds, and learn an intermediate manifold for manifold alignment by minimizing the distance. We prove the existence and uniqueness of the optimal intermediate manifold. Experiments on taxonomic open-set classification tasks across multiple image datasets demonstrate that our method consistently outperforms strong baselines under few-shot and cross-domain settings.",
        "arxiv_id": "2510.27391",
        "ARXIVID": "2510.27391",
        "COMMENT": "Matches criterion 5 (Integration of Image/Video and Large Language Models) as it focuses on aligning hierarchical features across modalities using hyperbolic manifolds.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2510.27492": {
        "authors": [
            "Jiawei Gu",
            "Yunzhuo Hao",
            "Huichen Will Wang",
            "Linjie Li",
            "Michael Qizhe Shieh",
            "Yejin Choi",
            "Ranjay Krishna",
            "Yu Cheng"
        ],
        "title": "ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning",
        "abstract": "arXiv:2510.27492v1 Announce Type: new  Abstract: Multimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and image thoughts should function as complementary, rather than isomorphic, modalities that mutually advance reasoning. Guided by this principle, we build ThinkMorph, a unified model fine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with varying visual engagement. ThinkMorph learns to generate progressive text-image reasoning steps that concretely manipulate visual content while maintaining coherent verbal logic. It delivers large gains on vision-centric benchmarks (averaging 34.7% over the base model) and generalizes to out-of-domain tasks, matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits emergent multimodal intelligence, including unseen visual manipulation skills, adaptive switching between reasoning modes, and better test-time scaling through diversified multimodal thoughts.These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning.",
        "arxiv_id": "2510.27492",
        "ARXIVID": "2510.27492",
        "COMMENT": "This paper aligns with criterion 2 as it explores multimodal reasoning and interleaved chain-of-thought processes in vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.27432": {
        "authors": [
            "WonJun Moon",
            "MinSeok Jung",
            "Gilhan Park",
            "Tae-Young Kim",
            "Cheol-Ho Cho",
            "Woojin Jun",
            "Jae-Pil Heo"
        ],
        "title": "Mitigating Semantic Collapse in Partially Relevant Video Retrieval",
        "abstract": "arXiv:2510.27432v1 Announce Type: new  Abstract: Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the content matches a text query. Existing methods treat every annotated text-video pair as a positive and all others as negatives, ignoring the rich semantic variation both within a single video and across different videos. Consequently, embeddings of both queries and their corresponding video-clip segments for distinct events within the same video collapse together, while embeddings of semantically similar queries and segments from different videos are driven apart. This limits retrieval performance when videos contain multiple, diverse events. This paper addresses the aforementioned problems, termed as semantic collapse, in both the text and video embedding spaces. We first introduce Text Correlation Preservation Learning, which preserves the semantic relationships encoded by the foundation model across text queries. To address collapse in video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive alignment method that disentangles hierarchical video representations across temporal scales. Subsequently, we introduce order-preserving token merging and adaptive CBVA to enhance alignment by producing video segments that are internally coherent yet mutually distinctive. Extensive experiments on PRVR benchmarks demonstrate that our framework effectively prevents semantic collapse and substantially improves retrieval accuracy.",
        "arxiv_id": "2510.27432",
        "ARXIVID": "2510.27432",
        "COMMENT": "Matches criteria 6 as it addresses partially relevant video retrieval and introduces methods to mitigate semantic collapse in video embeddings.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.27571": {
        "authors": [
            "Zhuoning Guo",
            "Mingxin Li",
            "Yanzhao Zhang",
            "Dingkun Long",
            "Pengjun Xie",
            "Xiaowen Chu"
        ],
        "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum",
        "abstract": "arXiv:2510.27571v1 Announce Type: new  Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.",
        "arxiv_id": "2510.27571",
        "ARXIVID": "2510.27571",
        "COMMENT": "Matches criteria 6 as it introduces a universal video retrieval benchmark and novel methodologies for video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.27329": {
        "authors": [
            "Kristina Levina",
            "Nikolaos Pappas",
            "Athanasios Karapantelakis",
            "Aneta Vulgarakis Feljan",
            "Jendrik Seipp"
        ],
        "title": "Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines",
        "abstract": "arXiv:2510.27329v1 Announce Type: new  Abstract: Reward machines (RMs) inform reinforcement learning agents about the reward structure of the environment. This is particularly advantageous for complex non-Markovian tasks because agents with access to RMs can learn more efficiently from fewer samples. However, learning with RMs is ill-suited for long-horizon problems in which a set of subtasks can be executed in any order. In such cases, the amount of information to learn increases exponentially with the number of unordered subtasks. In this work, we address this limitation by introducing three generalisations of RMs: (1) Numeric RMs allow users to express complex tasks in a compact form. (2) In Agenda RMs, states are associated with an agenda that tracks the remaining subtasks to complete. (3) Coupled RMs have coupled states associated with each subtask in the agenda. Furthermore, we introduce a new compositional learning algorithm that leverages coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM scales better than state-of-the-art RM algorithms for long-horizon problems with unordered subtasks.",
        "arxiv_id": "2510.27329",
        "ARXIVID": "2510.27329",
        "COMMENT": "Matches criteria 3 as it introduces new generalizations of reward machines and a novel compositional learning algorithm for reinforcement learning in long-horizon tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.27481": {
        "authors": [
            "Wei Xu",
            "Cheng Wang",
            "Dingkang Liang",
            "Zongchuang Zhao",
            "Xingyu Jiang",
            "Peng Zhang",
            "Xiang Bai"
        ],
        "title": "NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding",
        "abstract": "arXiv:2510.27481v1 Announce Type: new  Abstract: Underwater exploration offers critical insights into our planet and attracts increasing attention for its broader applications in resource exploration, national security, etc. We study the underwater scene understanding methods, which aim to achieve automated underwater exploration. The underwater scene understanding task demands multi-task perceptions from multiple granularities. However, the absence of large-scale underwater multi-task instruction-tuning datasets hinders the progress of this research. To bridge this gap, we construct NautData, a dataset containing 1.45 M image-text pairs supporting eight underwater scene understanding tasks. It enables the development and thorough evaluation of the underwater scene understanding models. Underwater image degradation is a widely recognized challenge that interferes with underwater tasks. To improve the robustness of underwater scene understanding, we introduce physical priors derived from underwater imaging models and propose a plug-and-play vision feature enhancement (VFE) module, which explicitly restores clear underwater information. We integrate this module into renowned baselines LLaVA-1.5 and Qwen2.5-VL and build our underwater LMM, NAUTILUS. Experiments conducted on the NautData and public underwater datasets demonstrate the effectiveness of the VFE module, consistently improving the performance of both baselines on the majority of supported tasks, thus ensuring the superiority of NAUTILUS in the underwater scene understanding area. Data and models are available at https://github.com/H-EmbodVis/NAUTILUS.",
        "arxiv_id": "2510.27481",
        "ARXIVID": "2510.27481",
        "COMMENT": "Matches criteria 2 and 4 as it introduces a multimodal large model (NAUTILUS) for underwater scene understanding, leveraging a new dataset and physical priors.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.27164": {
        "authors": [
            "Hankyeol Lee",
            "Gawon Seo",
            "Kyounggyu Lee",
            "Dogun Kim",
            "Kyungwoo Song",
            "Jiyoung Jung"
        ],
        "title": "Generating Accurate and Detailed Captions for High-Resolution Images",
        "abstract": "arXiv:2510.27164v1 Announce Type: new  Abstract: Vision-language models (VLMs) often struggle to generate accurate and detailed captions for high-resolution images since they are typically pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels). Downscaling high-resolution images to these dimensions may result in the loss of visual details and the omission of important objects. To address this limitation, we propose a novel pipeline that integrates vision-language models, large language models (LLMs), and object detection systems to enhance caption quality. Our proposed pipeline refines captions through a novel, multi-stage process. Given a high-resolution image, an initial caption is first generated using a VLM, and key objects in the image are then identified by an LLM. The LLM predicts additional objects likely to co-occur with the identified key objects, and these predictions are verified by object detection systems. Newly detected objects not mentioned in the initial caption undergo focused, region-specific captioning to ensure they are incorporated. This process enriches caption detail while reducing hallucinations by removing references to undetected objects. We evaluate the enhanced captions using pairwise comparison and quantitative scoring from large multimodal models, along with a benchmark for hallucination detection. Experiments on a curated dataset of high-resolution images demonstrate that our pipeline produces more detailed and reliable image captions while effectively minimizing hallucinations.",
        "arxiv_id": "2510.27164",
        "ARXIVID": "2510.27164",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel pipeline integrating vision-language models, large language models, and object detection systems for high-resolution image captioning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2510.27647": {
        "authors": [
            "Congzhang Shao",
            "Quan Yuan",
            "Guiyang Luo",
            "Yue Hu",
            "Danni Wang",
            "Yilin Liu",
            "Rui Pan",
            "Bo Chen",
            "Jinglin Li"
        ],
        "title": "NegoCollab: A Common Representation Negotiation Approach for Heterogeneous Collaborative Perception",
        "abstract": "arXiv:2510.27647v1 Announce Type: new  Abstract: Collaborative perception improves task performance by expanding the perception range through information sharing among agents. . Immutable heterogeneity poses a significant challenge in collaborative perception, as participating agents may employ different and fixed perception models. This leads to domain gaps in the intermediate features shared among agents, consequently degrading collaborative performance. Aligning the features of all agents to a common representation can eliminate domain gaps with low training cost. However, in existing methods, the common representation is designated as the representation of a specific agent, making it difficult for agents with significant domain discrepancies from this specific agent to achieve proper alignment. This paper proposes NegoCollab, a heterogeneous collaboration method based on the negotiated common representation. It introduces a negotiator during training to derive the common representation from the local representations of each modality's agent, effectively reducing the inherent domain gap with the various local representations. In NegoCollab, the mutual transformation of features between the local representation space and the common representation space is achieved by a pair of sender and receiver. To better align local representations to the common representation containing multimodal information, we introduce structural alignment loss and pragmatic alignment loss in addition to the distribution alignment loss to supervise the training. This enables the knowledge in the common representation to be fully distilled into the sender.",
        "arxiv_id": "2510.27647",
        "ARXIVID": "2510.27647",
        "COMMENT": "Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a novel method for collaborative perception in heterogeneous agents.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2510.27219": {
        "authors": [
            "Renjie Ji",
            "Xue Wang",
            "Chao Niu",
            "Wen Zhang",
            "Yong Mei",
            "Kun Tan"
        ],
        "title": "SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping",
        "abstract": "arXiv:2510.27219v1 Announce Type: new  Abstract: Hyperspectral imaging (HSI) is a vital tool for fine-grained land-use and land-cover (LULC) mapping. However, the inherent heterogeneity of HSI data has long posed a major barrier to developing generalized models via joint training. Although HSI foundation models have shown promise for different downstream tasks, the existing approaches typically overlook the critical guiding role of sensor meta-attributes, and struggle with multi-sensor training, limiting their transferability. To address these challenges, we propose SpecAware, which is a novel hyperspectral spectral-content aware foundation model for unifying multi-sensor learning for HSI mapping. We also constructed the Hyper-400K dataset to facilitate this research, which is a new large-scale, high-quality benchmark dataset with over 400k image patches from diverse airborne AVIRIS sensors. The core of SpecAware is a two-step hypernetwork-driven encoding process for HSI data. Firstly, we designed a meta-content aware module to generate a unique conditional input for each HSI patch, tailored to each spectral band of every sample by fusing the sensor meta-attributes and its own image content. Secondly, we designed the HyperEmbedding module, where a sample-conditioned hypernetwork dynamically generates a pair of matrix factors for channel-wise encoding, consisting of adaptive spatial pattern extraction and latent semantic feature re-projection. Thus, SpecAware gains the ability to perceive and interpret spatial-spectral features across diverse scenes and sensors. This, in turn, allows SpecAware to adaptively process a variable number of spectral channels, establishing a unified framework for joint pre-training. Extensive experiments on six datasets demonstrate that SpecAware can learn superior feature representations, excelling in land-cover semantic segmentation classification, change detection, and scene classification.",
        "arxiv_id": "2510.27219",
        "ARXIVID": "2510.27219",
        "COMMENT": "Matches criteria 4 as it focuses on a hyperspectral foundation model (SpecAware) for multi-sensor learning, addressing challenges in hyperspectral imaging.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2510.27335": {
        "authors": [
            "Yijia Wang",
            "Yiqing Shen",
            "Weiming Chen",
            "Zhihai He"
        ],
        "title": "Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing",
        "abstract": "arXiv:2510.27335v1 Announce Type: new  Abstract: Existing image editing methods can handle simple editing instructions very well. To deal with complex editing instructions, they often need to jointly fine-tune the large language models (LLMs) and diffusion models (DMs), which involves very high computational complexity and training cost. To address this issue, we propose a new method, called \\textbf{C}omplex \\textbf{I}mage \\textbf{E}diting via \\textbf{L}LM \\textbf{R}easoning (CIELR), which converts a complex user instruction into a set of simple and explicit editing actions, eliminating the need for jointly fine-tuning the large language models and diffusion models. Specifically, we first construct a structured semantic representation of the input image using foundation models. Then, we introduce an iterative update mechanism that can progressively refine this representation, obtaining a fine-grained visual representation of the image scene. This allows us to perform complex and flexible image editing tasks. Extensive experiments on the SmartEdit Reasoning Scenario Set show that our method surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating its superior preservation of regions that should remain consistent. Due to the limited number of samples of public datasets of complex image editing with reasoning, we construct a benchmark named CIEBench, containing 86 image samples, together with a metric specifically for reasoning-based image editing. CIELR also outperforms previous methods on this benchmark. The code and dataset are available at \\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.",
        "arxiv_id": "2510.27335",
        "ARXIVID": "2510.27335",
        "COMMENT": "This paper aligns with criterion 5 as it integrates image understanding tasks with reasoning using large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.27210": {
        "authors": [
            "Tao Liu",
            "Chongyu Wang",
            "Rongjie Li",
            "Yingchen Yu",
            "Xuming He",
            "Bai Song"
        ],
        "title": "GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation",
        "abstract": "arXiv:2510.27210v1 Announce Type: new  Abstract: While Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our framework's ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.",
        "arxiv_id": "2510.27210",
        "ARXIVID": "2510.27210",
        "COMMENT": "Matches criteria 2 as it focuses on multimodal large language models for GUI navigation with structured reasoning and history summarization.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.27630": {
        "authors": [
            "Dayuan Fu",
            "Yunze Wu",
            "Xiaojie Cai",
            "Lyumanshan Ye",
            "Shijie Xia",
            "Zhen Huang",
            "Weiye Si",
            "Tianze Xu",
            "Jie Sun",
            "Keyu Li",
            "Mohan Jiang",
            "Junfei Wang",
            "Qishuo Hua",
            "Pengrui Lu",
            "Yang Xiao",
            "Pengfei Liu"
        ],
        "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training",
        "abstract": "arXiv:2510.27630v1 Announce Type: new  Abstract: Large Language Model (LLM) agents have recently shown strong potential in domains such as automated coding, deep research, and graphical user interface manipulation. However, training them to succeed on long-horizon, domain-specialized tasks remains challenging. Current methods primarily fall into two categories. The first relies on dense human annotations through behavior cloning, which is prohibitively expensive for long-horizon tasks that can take days or months. The second depends on outcome-driven sampling, which often collapses due to the rarity of valid positive trajectories on domain-specialized tasks. We introduce Apollo, a sampling framework that integrates asynchronous human guidance with action-level data filtering. Instead of requiring annotators to shadow every step, Apollo allows them to intervene only when the agent drifts from a promising trajectory, by providing prior knowledge, strategic advice, etc. This lightweight design makes it possible to sustain interactions for over 30 hours and produces valuable trajectories at a lower cost. Apollo then applies supervision control to filter out sub-optimal actions and prevent error propagation. Together, these components enable reliable and effective data collection in long-horizon environments. To demonstrate the effectiveness of Apollo, we evaluate it using InnovatorBench. Our experiments show that when applied to train the GLM-4.5 model on InnovatorBench, Apollo achieves more than a 50% improvement over the untrained baseline and a 28% improvement over a variant trained without human interaction. These results highlight the critical role of human-in-the-loop sampling and the robustness of Apollo's design in handling long-horizon, domain-specialized tasks.",
        "arxiv_id": "2510.27630",
        "ARXIVID": "2510.27630",
        "COMMENT": "Matches criteria 3 as it introduces a novel framework for training LLM agents on long-horizon tasks with human-agent interaction, relevant to embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2510.27265": {
        "authors": [
            "Raza Imam",
            "Hu Wang",
            "Dwarikanath Mahapatra",
            "Mohammad Yaqub"
        ],
        "title": "T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging Analysis",
        "abstract": "arXiv:2510.27265v1 Announce Type: new  Abstract: In medical imaging, vision-language models face a critical duality: pretrained networks offer broad robustness but lack subtle, modality-specific characteristics, while fine-tuned expert models achieve high in-distribution accuracy yet falter under modality shift. Existing model-merging techniques, designed for natural-image benchmarks, are simple and efficient but fail to deliver consistent gains across diverse medical modalities; their static interpolation limits reliability in varied clinical tasks. To address this, we introduce Test-Time Task adaptive merging (T^3), a backpropagation-free framework that computes per-sample interpolation coefficients via the Jensen-Shannon divergence between the two models' output distributions. T^3 dynamically preserves local precision when models agree and defers to generalist robustness under drift. To overcome the inference costs of sample-wise merging, we further propose a batch-wise extension, T^3_B, that computes a merging coefficient across a batch of samples, dramatically reducing computational bottleneck. Recognizing the lack of a standardized medical-merging benchmark, we present a rigorous cross-evaluation protocol spanning in-domain, base-to-novel, and corruptions across four modalities. Empirically, T^3 sets new state-of-the-art in Top-1 accuracy and error reduction, outperforming strong baselines while maintaining efficiency, paving the way for adaptive MVLM deployment in clinical settings. Our code is available at https://github.com/Razaimam45/TCube.",
        "arxiv_id": "2510.27265",
        "ARXIVID": "2510.27265",
        "COMMENT": "Matches criteria 2 and 5 as it focuses on vision-language models for medical imaging and introduces a novel test-time model merging framework.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.26996": {
        "authors": [
            "Arghavan Rezvani",
            "Xiangyi Yan",
            "Anthony T. Wu",
            "Kun Han",
            "Pooya Khosravi",
            "Xiaohui Xie"
        ],
        "title": "MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation",
        "abstract": "arXiv:2510.26996v1 Announce Type: new  Abstract: In this study, we propose MoME, a Mixture of Visual Language Medical Experts, for Medical Image Segmentation. MoME adapts the successful Mixture of Experts (MoE) paradigm, widely used in Large Language Models (LLMs), for medical vision-language tasks. The architecture enables dynamic expert selection by effectively utilizing multi-scale visual features tailored to the intricacies of medical imagery, enriched with textual embeddings. This work explores a novel integration of vision-language models for this domain. Utilizing an assembly of 10 datasets, encompassing 3,410 CT scans, MoME demonstrates strong performance on a comprehensive medical imaging segmentation benchmark. Our approach explores the integration of foundation models for medical imaging, benefiting from the established efficacy of MoE in boosting model performance by incorporating textual information. Demonstrating competitive precision across multiple datasets, MoME explores a novel architecture for achieving robust results in medical image analysis.",
        "arxiv_id": "2510.26996",
        "ARXIVID": "2510.26996",
        "COMMENT": "Matches criteria 2 and 5 as it explores a novel integration of vision-language models for medical imaging segmentation, leveraging a mixture of experts paradigm.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.26865": {
        "authors": [
            "Fenfen Lin",
            "Yesheng Liu",
            "Haiyu Xu",
            "Chen Yue",
            "Zheqi He",
            "Mingxuan Zhao",
            "Miguel Hu Chen",
            "Jiakang Liu",
            "JG Yao",
            "Xi Yang"
        ],
        "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench",
        "abstract": "arXiv:2510.26865v1 Announce Type: new  Abstract: Reading measurement instruments is effortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we find in preliminary evaluation. In this work, we introduce MeasureBench, a benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates a specified type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. A consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and find encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights a fundamental limitation of current VLMs in fine-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world.",
        "arxiv_id": "2510.26865",
        "ARXIVID": "2510.26865",
        "COMMENT": "Matches criterion 7 (Vision-Focused Survey Papers) as it benchmarks visual measurement reading and highlights limitations in VLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2510.27166": {
        "authors": [
            "Xiaozhi Li",
            "Huijun Di",
            "Jian Li",
            "Feng Liu",
            "Wei Liang"
        ],
        "title": "M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar",
        "abstract": "arXiv:2510.27166v1 Announce Type: new  Abstract: Recent advances in 4D imaging radar have enabled robust perception in adverse weather, while camera sensors provide dense semantic information. Fusing the these complementary modalities has great potential for cost-effective 3D perception. However, most existing camera-radar fusion methods are limited to single-frame inputs, capturing only a partial view of the scene. The incomplete scene information, compounded by image degradation and 4D radar sparsity, hinders overall detection performance. In contrast, multi-frame fusion offers richer spatiotemporal information but faces two challenges: achieving robust and effective object feature fusion across frames and modalities, and mitigating the computational cost of redundant feature extraction. Consequently, we propose M^3Detection, a unified multi-frame 3D object detection framework that performs multi-level feature fusion on multi-modal data from camera and 4D imaging radar. Our framework leverages intermediate features from the baseline detector and employs the tracker to produce reference trajectories, improving computational efficiency and providing richer information for second-stage. In the second stage, we design a global-level inter-object feature aggregation module guided by radar information to align global features across candidate proposals and a local-level inter-grid feature aggregation module that expands local features along the reference trajectories to enhance fine-grained object representation. The aggregated features are then processed by a trajectory-level multi-frame spatiotemporal reasoning module to encode cross-frame interactions and enhance temporal representation. Extensive experiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detection achieves state-of-the-art 3D detection performance, validating its effectiveness in multi-frame detection with camera-4D imaging radar fusion.",
        "arxiv_id": "2510.27166",
        "ARXIVID": "2510.27166",
        "COMMENT": "This paper aligns with criterion 3 as it introduces a new multi-frame 3D object detection framework with multi-modal fusion, which is relevant to embodied/robotic AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2510.27234": {
        "authors": [
            "Jingnan Gao",
            "Zhe Wang",
            "Xianze Fang",
            "Xingyu Ren",
            "Zhuo Chen",
            "Shengqi Liu",
            "Yuhao Cheng",
            "Jiangjing Lyu",
            "Xiaokang Yang",
            "Yichao Yan"
        ],
        "title": "MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts",
        "abstract": "arXiv:2510.27234v1 Announce Type: new  Abstract: Recent advances in language and vision have demonstrated that scaling up model capacity consistently improves performance across diverse tasks. In 3D visual geometry reconstruction, large-scale training has likewise proven effective for learning versatile representations. However, further scaling of 3D models is challenging due to the complexity of geometric supervision and the diversity of 3D data. To overcome these limitations, we propose MoRE, a dense 3D visual foundation model based on a Mixture-of-Experts (MoE) architecture that dynamically routes features to task-specific experts, allowing them to specialize in complementary data aspects and enhance both scalability and adaptability. Aiming to improve robustness under real-world conditions, MoRE incorporates a confidence-based depth refinement module that stabilizes and refines geometric estimation. In addition, it integrates dense semantic features with globally aligned 3D backbone representations for high-fidelity surface normal prediction. MoRE is further optimized with tailored loss functions to ensure robust learning across diverse inputs and multiple geometric tasks. Extensive experiments demonstrate that MoRE achieves state-of-the-art performance across multiple benchmarks and supports effective downstream applications without extra computation.",
        "arxiv_id": "2510.27234",
        "ARXIVID": "2510.27234",
        "COMMENT": "This paper aligns with criterion 4 as it focuses on a 3D visual foundation model and its architecture, which is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.26887": {
        "authors": [
            "Francisco Villaescusa-Navarro",
            "Boris Bolliet",
            "Pablo Villanueva-Domingo",
            "Adrian E. Bayer",
            "Aidan Acquah",
            "Chetana Amancharla",
            "Almog Barzilay-Siegal",
            "Pablo Bermejo",
            "Camille Bilodeau",
            "Pablo C\\'ardenas Ram\\'irez",
            "Miles Cranmer",
            "Urbano L. Fran\\c{c}a",
            "ChangHoon Hahn",
            "Yan-Fei Jiang",
            "Raul Jimenez",
            "Jun-Young Lee",
            "Antonio Lerario",
            "Osman Mamun",
            "Thomas Meier",
            "Anupam A. Ojha",
            "Pavlos Protopapas",
            "Shimanto Roy",
            "David N. Spergel",
            "Pedro Taranc\\'on-\\'Alvarez",
            "Ujjwal Tiwari",
            "Matteo Viel",
            "Digvijay Wadekar",
            "Chi Wang",
            "Bonny Y. Wang",
            "Licong Xu",
            "Yossi Yovel",
            "Shuwen Yue",
            "Wen-Han Zhou",
            "Qiyao Zhu",
            "Jiajun Zou",
            "\\'I\\~nigo Zubeldia"
        ],
        "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
        "abstract": "arXiv:2510.26887v1 Announce Type: new  Abstract: We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.",
        "arxiv_id": "2510.26887",
        "ARXIVID": "2510.26887",
        "COMMENT": "Does not closely match any specific criteria but is a general-purpose AI system for scientific discovery, which is tangentially related to your friend's interest in AI advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 8
    },
    "2510.27042": {
        "authors": [
            "Michael Kleinman",
            "Matthew Trager",
            "Alessandro Achille",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "title": "e1: Learning Adaptive Control of Reasoning Effort",
        "abstract": "arXiv:2510.27042v1 Announce Type: new  Abstract: Increasing the thinking budget of AI models can significantly improve accuracy, but not all questions warrant the same amount of reasoning. Users may prefer to allocate different amounts of reasoning effort depending on how they value output quality versus latency and cost. To leverage this tradeoff effectively, users need fine-grained control over the amount of thinking used for a particular query, but few approaches enable such control. Existing methods require users to specify the absolute number of desired tokens, but this requires knowing the difficulty of the problem beforehand to appropriately set the token budget for a query. To address these issues, we propose Adaptive Effort Control, a self-adaptive reinforcement learning method that trains models to use a user-specified fraction of tokens relative to the current average chain-of-thought length for each query. This approach eliminates dataset- and phase-specific tuning while producing better cost-accuracy tradeoff curves compared to standard methods. Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time. We observe that the model automatically learns to allocate resources proportionally to the task difficulty and, across model scales ranging from 1.5B to 32B parameters, our approach enables approximately 3x reduction in chain-of-thought length while maintaining or improving performance relative to the base model used for RL training.",
        "arxiv_id": "2510.27042",
        "ARXIVID": "2510.27042",
        "COMMENT": "Does not closely match any specific criteria but is relevant to adaptive control in reasoning effort, which is tangentially related to vision and multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.27128": {
        "authors": [
            "Haonan Wang",
            "Jingyu Lu",
            "Hongrui Li",
            "Xiaomeng Li"
        ],
        "title": "ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding",
        "abstract": "arXiv:2510.27128v1 Announce Type: new  Abstract: Recent advances in neural decoding have enabled the reconstruction of visual experiences from brain activity, positioning fMRI-to-image reconstruction as a promising bridge between neuroscience and computer vision. However, current methods predominantly rely on subject-specific models or require subject-specific fine-tuning, limiting their scalability and real-world applicability. In this work, we introduce ZEBRA, the first zero-shot brain visual decoding framework that eliminates the need for subject-specific adaptation. ZEBRA is built on the key insight that fMRI representations can be decomposed into subject-related and semantic-related components. By leveraging adversarial training, our method explicitly disentangles these components to isolate subject-invariant, semantic-specific representations. This disentanglement allows ZEBRA to generalize to unseen subjects without any additional fMRI data or retraining. Extensive experiments show that ZEBRA significantly outperforms zero-shot baselines and achieves performance comparable to fully finetuned models on several metrics. Our work represents a scalable and practical step toward universal neural decoding. Code and model weights are available at: https://github.com/xmed-lab/ZEBRA.",
        "arxiv_id": "2510.27128",
        "ARXIVID": "2510.27128",
        "COMMENT": "Does not match any specific criteria; focuses on brain visual decoding and fMRI-to-image reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.26854": {
        "authors": [
            "Yu Li",
            "Yuan Huang",
            "Tao Wang",
            "Caiyu Fan",
            "Xiansheng Cai",
            "Sihan Hu",
            "Xinzijian Liu",
            "Cheng Shi",
            "Mingjun Xu",
            "Zhen Wang",
            "Yan Wang",
            "Xiangqi Jin",
            "Tianhan Zhang",
            "Linfeng Zhang",
            "Lei Wang",
            "Youjin Deng",
            "Pan Zhang",
            "Weijie Sun",
            "Xingyu Li",
            "Weinan E",
            "Linfeng Zhang",
            "Zhiyuan Yao",
            "Kun Chen"
        ],
        "title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base",
        "abstract": "arXiv:2510.26854v1 Announce Type: new  Abstract: Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.",
        "arxiv_id": "2510.26854",
        "ARXIVID": "2510.26854",
        "COMMENT": "Does not match any specific criteria; focuses on knowledge synthesis and reasoning chains.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.27213": {
        "authors": [
            "Ren Tasai",
            "Guang Li",
            "Ren Togo",
            "Takahiro Ogawa",
            "Kenji Hirata",
            "Minghui Tang",
            "Takaaki Yoshimura",
            "Hiroyuki Sugimori",
            "Noriko Nishioka",
            "Yukie Shimizu",
            "Kohsuke Kudo",
            "Miki Haseyama"
        ],
        "title": "Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness",
        "abstract": "arXiv:2510.27213v1 Announce Type: new  Abstract: We propose a novel continual self-supervised learning (CSSL) framework for simultaneously learning diverse features from multi-window-obtained chest computed tomography (CT) images and ensuring data privacy. Achieving a robust and highly generalizable model in medical image diagnosis is challenging, mainly because of issues, such as the scarcity of large-scale, accurately annotated datasets and domain shifts inherent to dynamic healthcare environments. Specifically, in chest CT, these domain shifts often arise from differences in window settings, which are optimized for distinct clinical purposes. Previous CSSL frameworks often mitigated domain shift by reusing past data, a typically impractical approach owing to privacy constraints. Our approach addresses these challenges by effectively capturing the relationship between previously learned knowledge and new information across different training stages through continual pretraining on unlabeled images. Specifically, by incorporating a latent replay-based mechanism into CSSL, our method mitigates catastrophic forgetting due to domain shifts during continual pretraining while ensuring data privacy. Additionally, we introduce a feature distillation technique that integrates Wasserstein distance-based knowledge distillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of the model to learn meaningful, domain-shift-robust representations. Finally, we validate our approach using chest CT images obtained across two different window settings, demonstrating superior performance compared with other approaches.",
        "arxiv_id": "2510.27213",
        "ARXIVID": "2510.27213",
        "COMMENT": "Does not match any specific criteria but is related to continual self-supervised learning for medical imaging, which is tangentially relevant to your friend's general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.27245": {
        "authors": [
            "Alik Pramanick",
            "Mayank Bansal",
            "Utkarsh Srivastava",
            "Suklav Ghosh",
            "Arijit Sur"
        ],
        "title": "Trans-defense: Transformer-based Denoiser for Adversarial Defense with Spatial-Frequency Domain Representation",
        "abstract": "arXiv:2510.27245v1 Announce Type: new  Abstract: In recent times, deep neural networks (DNNs) have been successfully adopted for various applications. Despite their notable achievements, it has become evident that DNNs are vulnerable to sophisticated adversarial attacks, restricting their applications in security-critical systems. In this paper, we present two-phase training methods to tackle the attack: first, training the denoising network, and second, the deep classifier model. We propose a novel denoising strategy that integrates both spatial and frequency domain approaches to defend against adversarial attacks on images. Our analysis reveals that high-frequency components of attacked images are more severely corrupted compared to their lower-frequency counterparts. To address this, we leverage Discrete Wavelet Transform (DWT) for frequency analysis and develop a denoising network that combines spatial image features with wavelets through a transformer layer. Next, we retrain the classifier using the denoised images, which enhances the classifier's robustness against adversarial attacks. Experimental results across the MNIST, CIFAR-10, and Fashion-MNIST datasets reveal that the proposed method remarkably elevates classification accuracy, substantially exceeding the performance by utilizing a denoising network and adversarial training approaches. The code is available at https://github.com/Mayank94/Trans-Defense.",
        "arxiv_id": "2510.27245",
        "ARXIVID": "2510.27245",
        "COMMENT": "Does not match any specific criteria but is related to adversarial defense in vision tasks, which is tangentially relevant to your friend's general interest in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.27475": {
        "authors": [
            "Hyemin Boo",
            "Eunsang Lee",
            "Jiyoung Lee"
        ],
        "title": "Referee: Reference-aware Audiovisual Deepfake Detection",
        "abstract": "arXiv:2510.27475v1 Announce Type: new  Abstract: Since deepfakes generated by advanced generative models have rapidly posed serious threats, existing audiovisual deepfake detection approaches struggle to generalize to unseen forgeries. We propose a novel reference-aware audiovisual deepfake detection method, called Referee. Speaker-specific cues from only one-shot examples are leveraged to detect manipulations beyond spatiotemporal artifacts. By matching and aligning identity-related queries from reference and target content into cross-modal features, Referee jointly reasons about audiovisual synchrony and identity consistency. Extensive experiments on FakeAVCeleb, FaceForensics++, and KoDF demonstrate that Referee achieves state-of-the-art performance on cross-dataset and cross-language evaluation protocols. Experimental results highlight the importance of cross-modal identity verification for future deepfake detection. The code is available at https://github.com/ewha-mmai/referee.",
        "arxiv_id": "2510.27475",
        "ARXIVID": "2510.27475",
        "COMMENT": "Does not match any specific criteria but is related to audiovisual deepfake detection, which is tangentially relevant to your friend's general interest in vision and multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.27206": {
        "authors": [
            "Kounianhua Du",
            "Jianxing Liu",
            "Kangning Zhang",
            "Wenxiang Jiao",
            "Yuan Lu",
            "Jiarui Jin",
            "Weiwen Liu",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "title": "Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering",
        "abstract": "arXiv:2510.27206v1 Announce Type: new  Abstract: The rapid evolution of large language models (LLMs) has intensified the demand for effective personalization techniques that can adapt model behavior to individual user preferences. Despite the non-parametric methods utilizing the in-context learning ability of LLMs, recent parametric adaptation methods, including personalized parameter-efficient fine-tuning and reward modeling emerge. However, these methods face limitations in handling dynamic user patterns and high data sparsity scenarios, due to low adaptability and data efficiency. To address these challenges, we propose a fine-grained and instance-tailored steering framework that dynamically generates sample-level interference vectors from user data and injects them into the model's forward pass for personalized adaptation. Our approach introduces two key technical innovations: a fine-grained steering component that captures nuanced signals by hooking activations from attention and MLP layers, and an input-aware aggregation module that synthesizes these signals into contextually relevant enhancements. The method demonstrates high flexibility and data efficiency, excelling in fast-changing distribution and high data sparsity scenarios. In addition, the proposed method is orthogonal to existing methods and operates as a plug-in component compatible with different personalization techniques. Extensive experiments across diverse scenarios--including short-to-long text generation, and web function calling--validate the effectiveness and compatibility of our approach. Results show that our method significantly enhances personalization performance in fast-shifting environments while maintaining robustness across varying interaction modes and context lengths. Implementation is available at https://github.com/KounianhuaDu/Fints.",
        "arxiv_id": "2510.27206",
        "ARXIVID": "2510.27206",
        "COMMENT": "Does not match any specific criteria but is related to personalization techniques for LLMs, which is tangentially relevant to your friend's general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.27236": {
        "authors": [
            "Tianli Liao",
            "Ran Wang",
            "Siqing Zhang",
            "Lei Li",
            "Guangen Liu",
            "Chenyang Zhao",
            "Heling Cao",
            "Peng Li"
        ],
        "title": "Object-IR: Leveraging Object Consistency and Mesh Deformation for Self-Supervised Image Retargeting",
        "abstract": "arXiv:2510.27236v1 Announce Type: new  Abstract: Eliminating geometric distortion in semantically important regions remains an intractable challenge in image retargeting. This paper presents Object-IR, a self-supervised architecture that reformulates image retargeting as a learning-based mesh warping optimization problem, where the mesh deformation is guided by object appearance consistency and geometric-preserving constraints. Given an input image and a target aspect ratio, we initialize a uniform rigid mesh at the output resolution and use a convolutional neural network to predict the motion of each mesh grid and obtain the deformed mesh. The retargeted result is generated by warping the input image according to the rigid mesh in the input image and the deformed mesh in the output resolution. To mitigate geometric distortion, we design a comprehensive objective function incorporating a) object-consistent loss to ensure that the important semantic objects retain their appearance, b) geometric-preserving loss to constrain simple scale transform of the important meshes, and c) boundary loss to enforce a clean rectangular output. Notably, our self-supervised paradigm eliminates the need for manually annotated retargeting datasets by deriving supervision directly from the input's geometric and semantic properties. Extensive evaluations on the RetargetMe benchmark demonstrate that our Object-IR achieves state-of-the-art performance, outperforming existing methods in quantitative metrics and subjective visual quality assessments. The framework efficiently processes arbitrary input resolutions (average inference time: 0.009s for 1024x683 resolution) while maintaining real-time performance on consumer-grade GPUs. The source code will soon be available at https://github.com/tlliao/Object-IR.",
        "arxiv_id": "2510.27236",
        "ARXIVID": "2510.27236",
        "COMMENT": "This paper does not directly match any of the criteria but is related to image retargeting and self-supervised learning, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.27155": {
        "authors": [
            "Yuanhao Tang",
            "Xuechao Zou",
            "Zhengpei Hu",
            "Junliang Xing",
            "Chengkun Zhang",
            "Jianqiang Huang"
        ],
        "title": "AFM-Net: Advanced Fusing Hierarchical CNN Visual Priors with Global Sequence Modeling for Remote Sensing Image Scene Classification",
        "abstract": "arXiv:2510.27155v1 Announce Type: new  Abstract: Remote sensing image scene classification remains a challenging task, primarily due to the complex spatial structures and multi-scale characteristics of ground objects. Existing approaches see CNNs excel at modeling local textures, while Transformers excel at capturing global context. However, efficiently integrating them remains a bottleneck due to the high computational cost of Transformers. To tackle this, we propose AFM-Net, a novel Advanced Hierarchical Fusing framework that achieves effective local and global co-representation through two pathways: a CNN branch for extracting hierarchical visual priors, and a Mamba branch for efficient global sequence modeling. The core innovation of AFM-Net lies in its Hierarchical Fusion Mechanism, which progressively aggregates multi-scale features from both pathways, enabling dynamic cross-level feature interaction and contextual reconstruction to produce highly discriminative representations. These fused features are then adaptively routed through a Mixture-of-Experts classifier module, which dispatches them to the most suitable experts for fine-grained scene recognition. Experiments on AID, NWPU-RESISC45, and UC Merced show that AFM-Net obtains 93.72, 95.54, and 96.92 percent accuracy, surpassing state-of-the-art methods with balanced performance and efficiency. Code is available at https://github.com/tangyuanhao-qhu/AFM-Net.",
        "arxiv_id": "2510.27155",
        "ARXIVID": "2510.27155",
        "COMMENT": "Does not closely match any specific criteria but is relevant to computer vision advancements in remote sensing image classification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.27316": {
        "authors": [
            "Zijia An",
            "Boyu Diao",
            "Ruiqi Liu",
            "Libo Huang",
            "Chuanguang Yang",
            "Fei Wang",
            "Zhulin An",
            "Yongjun Xu"
        ],
        "title": "Overcoming Prompts Pool Confusion via Parameterized Prompt for Incremental Object Detection",
        "abstract": "arXiv:2510.27316v1 Announce Type: new  Abstract: Recent studies have demonstrated that incorporating trainable prompts into pretrained models enables effective incremental learning. However, the application of prompts in incremental object detection (IOD) remains underexplored. Existing prompts pool based approaches assume disjoint class sets across incremental tasks, which are unsuitable for IOD as they overlook the inherent co-occurrence phenomenon in detection images. In co-occurring scenarios, unlabeled objects from previous tasks may appear in current task images, leading to confusion in prompts pool. In this paper, we hold that prompt structures should exhibit adaptive consolidation properties across tasks, with constrained updates to prevent catastrophic forgetting. Motivated by this, we introduce Parameterized Prompts for Incremental Object Detection (P$^2$IOD). Leveraging neural networks global evolution properties, P$^2$IOD employs networks as the parameterized prompts to adaptively consolidate knowledge across tasks. To constrain prompts structure updates, P$^2$IOD further engages a parameterized prompts fusion strategy. Extensive experiments on PASCAL VOC2007 and MS COCO datasets demonstrate that P$^2$IOD's effectiveness in IOD and achieves the state-of-the-art performance among existing baselines.",
        "arxiv_id": "2510.27316",
        "ARXIVID": "2510.27316",
        "COMMENT": "Does not closely match any specific criteria but is relevant to incremental object detection and trainable prompts, which are tangentially related to vision and embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.27419": {
        "authors": [
            "Tian Liang",
            "Wenxiang Jiao",
            "Zhiwei He",
            "Jiahao Xu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "title": "DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains",
        "abstract": "arXiv:2510.27419v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \\textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.",
        "arxiv_id": "2510.27419",
        "ARXIVID": "2510.27419",
        "COMMENT": "Does not match any specific criteria; focuses on reasoning chain compression in large reasoning models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.27186": {
        "authors": [
            "Zixuan Hu",
            "Yongxian Wei",
            "Li Shen",
            "Zhenyi Wang",
            "Lei Li",
            "Chun Yuan",
            "Dacheng Tao"
        ],
        "title": "Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications",
        "abstract": "arXiv:2510.27186v1 Announce Type: new  Abstract: Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the entire image area, making them extremely inefficient when inverting high-resolution images from large-scale Vision Transformers (ViTs). We further identify two underlying causes of this inefficiency: the redundant inversion of noisy backgrounds and the unintended inversion of spurious correlations--a phenomenon we term \"hallucination\" in model inversion. To address these limitations, we propose a novel sparse model inversion strategy, as a plug-and-play extension to speed up existing dense inversion methods with no need for modifying their original loss functions. Specifically, we selectively invert semantic foregrounds while stopping the inversion of noisy backgrounds and potential spurious correlations. Through both theoretical and empirical studies, we validate the efficacy of our approach in achieving significant inversion acceleration (up to 3.79 faster) while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer. Code is available at https://github.com/Egg-Hu/SMI.",
        "arxiv_id": "2510.27186",
        "ARXIVID": "2510.27186",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and model inversion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.27677": {
        "authors": [
            "Bo Li",
            "Duyuan Zheng",
            "Xinyang Liu",
            "Qingwen Li",
            "Hong Li",
            "Hongyan Cui",
            "Ge Gao",
            "Chen Liu"
        ],
        "title": "Vision Transformer for Robust Occluded Person Reidentification in Complex Surveillance Scenes",
        "abstract": "arXiv:2510.27677v1 Announce Type: new  Abstract: Person re-identification (ReID) in surveillance is challenged by occlusion, viewpoint distortion, and poor image quality. Most existing methods rely on complex modules or perform well only on clear frontal images. We propose Sh-ViT (Shuffling Vision Transformer), a lightweight and robust model for occluded person ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a Shuffle module in the final Transformer layer to break spatial correlations and enhance robustness to occlusion and blur; Second, scenario-adapted augmentation (geometric transforms, erasing, blur, and color adjustment) to simulate surveillance conditions; Third, DeiT-based knowledge distillation to improve learning with limited labels.To support real-world evaluation, we construct the MyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base station inspections, with frequent equipment occlusion and camera variations. Experiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT, outperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on Market1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves robustness to occlusion and blur without external modules, offering a practical solution for surveillance-based personnel monitoring.",
        "arxiv_id": "2510.27677",
        "ARXIVID": "2510.27677",
        "COMMENT": "This paper does not directly match any of the criteria but is tangentially related to computer vision and robustness in surveillance scenarios.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}