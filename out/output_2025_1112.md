# Personalized Daily ArXiv Papers 11/12/2025
Total relevant papers: 30

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation](#link0)
**Authors:** Difei Gu, Yunhe Gao, Mu Zhou, Dimitris Metaxas

1. [WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting](#link1)
**Authors:** Kaitao Huang, Yan Yan, Jing-Hao Xue, Hanzi Wang

2. [NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation](#link2)
**Authors:** Kunal Mahatha, Jose Dolz, Christian Desrosiers

3. [ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation](#link3)
**Authors:** Aya Elgebaly, Nikolaos Delopoulos, Juliane H\"orner-Rieber, Carolin Rippke, Sebastian Kl\"uter, Luca Boldrini, Lorenzo Placidi, Riccardo Dal Bello, Nicolaus Andratschke, Michael Baumgartl, Claus Belka, Christopher Kurz, Guillaume Landry, Shadi Albarqouni

4. [Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs](#link4)
**Authors:** Yuezhe Yang, Yiyue Guo, Wenjie Cai, Qingqing Ruan, Siying Wang, Xingbo Dong, Zhe Jin, Yong Dai

5. [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](#link5)
**Authors:** Jun Xu, Xinkai Du, Yu Ao, Peilong Zhao, Yang Li, Ling Zhong, Lin Yuan, Zhongpu Bo, Xiaorui Wang, Mengshu Sun, Zhengke Gui, Dalong Zhang, Zhaoyang Wang, Qiwei Wang, Yangyang Hou, Zhiying Yin, Haofen Wang, Huajun Chen, Lei Liang, Jun Zhou

6. [Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation](#link6)
**Authors:** Jun Sun, Xinxin Zhang, Simin Hong, Jian Zhu, Xiang Gao

7. [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](#link7)
**Authors:** Xuchen Li, Ruitao Wu, Xuanbo Liu, Xukai Wang, Jinbo Hu, Zhixin Bai, Bohan Zeng, Hao Liang, Leheng Chen, Mingrui Chen, Haitian Zhong, Xuanlin Yang, Xu-Yao Zhang, Liu Liu, Jia Li, Kaiqi Huang, Jiahao Xu, Haitao Mi, Wentao Zhang, Bin Dong

8. [Burst Image Quality Assessment: A New Benchmark and Unified Framework for Multiple Downstream Tasks](#link8)
**Authors:** Xiaoye Liang, Lai Jiang, Minglang Qiao, Yichen Guo, Yue Zhang, Xin Deng, Shengxi Li, Yufan Liu, Mai Xu

9. [Beyond Randomness: Understand the Order of the Noise in Diffusion](#link9)
**Authors:** Song Yan, Min Li, Bi Xinliang, Jian Yang, Yusen Zhang, Guanye Xiong, Yunwei Lan, Tao Zhang, Wei Zhai, Zheng-Jun Zha

10. [RAPTR: Radar-based 3D Pose Estimation using Transformer](#link10)
**Authors:** Sorachi Kato, Ryoma Yataka, Pu Perry Wang, Pedro Miraldo, Takuya Fujihashi, Petros Boufounos

11. [Exploring the Underwater World Segmentation without Extra Training](#link11)
**Authors:** Bingyu Li, Tao Huo, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li

12. [Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers](#link12)
**Authors:** Sida Huang, Siqi Huang, Ping Luo, Hongyuan Zhang

13. [Generalized-Scale Object Counting with Gradual Query Aggregation](#link13)
**Authors:** Jer Pelhan, Alan Lukezic, Matej Kristan

14. [Remodeling Semantic Relationships in Vision-Language Fine-Tuning](#link14)
**Authors:** Xiangyang Wu, Liu Liu, Baosheng Yu, Jiayan Qiu, Zhenwei Shi

15. [Revisiting MLLM Based Image Quality Assessment: Errors and Remedy](#link15)
**Authors:** Zhenchen Tang, Songlin Yang, Bo Peng, Zichuan Wang, Jing Dong

16. [SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models](#link16)
**Authors:** Giorgio Piras, Raffaele Mura, Fabio Brau, Luca Oneto, Fabio Roli, Battista Biggio

17. [Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning](#link17)
**Authors:** Valentin Tablan, Scott Taylor, Gabriel Hurtado, Kristoffer Bernhem, Anders Uhrenholt, Gabriele Farei, Karo Moilanen

18. [MADD: Multi-Agent Drug Discovery Orchestra](#link18)
**Authors:** Gleb V. Solovev, Alina B. Zhidkovskaya, Anastasia Orlova, Nina Gubina, Anastasia Vepreva, Rodion Golovinskii, Ilya Tonkii, Ivan Dubrovsky, Ivan Gurev, Dmitry Gilemkhanov, Denis Chistiakov, Timur A. Aliev, Ivan Poddiakov, Galina Zubkova, Ekaterina V. Skorb, Vladimir Vinogradov, Alexander Boukhanovsky, Nikolay Nikitin, Andrei Dmitrenko, Anna Kalyuzhnaya, Andrey Savchenko

19. [PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier](#link19)
**Authors:** Shaomeng Wang, He Wang, Xiaolu Wei, Longquan Dai, Jinhui Tang

20. [Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification](#link20)
**Authors:** Yihang Wu, Ahmad Chaddad

21. [Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning](#link21)
**Authors:** Tianwen Lyu, Xiang Zhuang, Keyan Ding, Xinzhe Cao, Lei Liang, Wei Zhao, Qiang Zhang, Huajun Chen

22. [GAMA: A Neural Neighborhood Search Method with Graph-aware Multi-modal Attention for Vehicle Routing Problem](#link22)
**Authors:** Xiangling Chen, Yi Mei, Mengjie Zhang

23. [Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models](#link23)
**Authors:** Huzaifa Arif, Keerthiram Murugesan, Ching-Yun Ko, Pin-Yu Chen, Payel Das, Alex Gittens

24. [SWAN - Enabling Fast and Mobile Histopathology Image Annotation through Swipeable Interfaces](#link24)
**Authors:** Sweta Banerjee, Timo Gosch, Sara Hester, Viktoria Weiss, Thomas Conrad, Taryn A. Donovan, Nils Porsche, Jonas Ammeling, Christoph Stroblberger, Robert Klopfleisch, Christopher Kaltenecker, Christof A. Bertram, Katharina Breininger, Marc Aubreville

25. [Predicting Coronary Artery Calcium Severity based on Non-Contrast Cardiac CT images using Deep Learning](#link25)
**Authors:** Lachlan Nguyen, Aidan Cousins, Arcot Sowmya, Hugh Dixson, Sonit Singh

26. [Mitigating Negative Flips via Margin Preserving Training](#link26)
**Authors:** Simone Ricci, Niccol\`o Biondi, Federico Pernici, Alberto Del Bimbo

27. [Contrastive Integrated Gradients: A Feature Attribution-Based Method for Explaining Whole Slide Image Classification](#link27)
**Authors:** Anh Mai Vu, Tuan L. Vo, Ngoc Lam Quang Bui, Nam Nguyen Le Binh, Akash Awasthi, Huy Quoc Vo, Thanh-Huy Nguyen, Zhu Han, Chandra Mohan, Hien Van Nguyen

28. [The Impact of Longitudinal Mammogram Alignment on Breast Cancer Risk Assessment](#link28)
**Authors:** Solveig Thrun, Stine Hansen, Zijun Sun, Nele Blum, Suaiba A. Salahuddin, Xin Wang, Kristoffer Wickstr{\o}m, Elisabeth Wetzer, Robert Jenssen, Maik Stille, Michael Kampffmeyer

29. [AI-Powered Data Visualization Platform: An Intelligent Web Application for Automated Dataset Analysis](#link29)
**Authors:** Srihari R, Pallavi M, Tejaswini S, Vaishnavi R C

---
## 0. [Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation](https://arxiv.org/abs/2511.08402) <a id="link0"></a>
**ArXiv ID:** 2511.08402
**Authors:** Difei Gu, Yunhe Gao, Mu Zhou, Dimitris Metaxas

**Abstract:**  Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.

**Comment:** Matches criterion 5 (Integration of Image/Video and Large Language Models) as it integrates fine-grained vision-language modeling for medical interpretation.
**Relevance:** 8
**Novelty:** 7

---

## 1. [WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting](https://arxiv.org/abs/2511.08178) <a id="link1"></a>
**ArXiv ID:** 2511.08178
**Authors:** Kaitao Huang, Yan Yan, Jing-Hao Xue, Hanzi Wang

**Abstract:**  3D GAN inversion projects a single image into the latent space of a pre-trained 3D GAN to achieve single-shot novel view synthesis, which requires visible regions with high fidelity and occluded regions with realism and multi-view consistency. However, existing methods focus on the reconstruction of visible regions, while the generation of occluded regions relies only on the generative prior of 3D GAN. As a result, the generated occluded regions often exhibit poor quality due to the information loss caused by the low bit-rate latent code. To address this, we introduce the warping-and-inpainting strategy to incorporate image inpainting into 3D GAN inversion and propose a novel 3D GAN inversion method, WarpGAN. Specifically, we first employ a 3D GAN inversion encoder to project the single-view image into a latent code that serves as the input to 3D GAN. Then, we perform warping to a novel view using the depth map generated by 3D GAN. Finally, we develop a novel SVINet, which leverages the symmetry prior and multi-view image correspondence w.r.t. the same latent code to perform inpainting of occluded regions in the warped image. Quantitative and qualitative experiments demonstrate that our method consistently outperforms several state-of-the-art methods.

**Comment:** Matches criterion 5 (Integration of Image/Video and Large Language Models) as it combines 3D GAN inversion with novel view inpainting techniques.
**Relevance:** 8
**Novelty:** 7

---

## 2. [NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation](https://arxiv.org/abs/2511.08248) <a id="link2"></a>
**ArXiv ID:** 2511.08248
**Authors:** Kunal Mahatha, Jose Dolz, Christian Desrosiers

**Abstract:**  Despite recent advances in Open-Vocabulary Semantic Segmentation (OVSS), existing training-free methods face several limitations: use of computationally expensive affinity refinement strategies, ineffective fusion of transformer attention maps due to equal weighting or reliance on fixed-size Gaussian kernels to reinforce local spatial smoothness, enforcing isotropic neighborhoods. We propose a strong baseline for training-free OVSS termed as NERVE (Neighbourhood \& Entropy-guided Random-walk for open-Vocabulary sEgmentation), which uniquely integrates global and fine-grained local information, exploiting the neighbourhood structure from the self-attention layer of a stable diffusion model. We also introduce a stochastic random walk for refining the affinity rather than relying on fixed-size Gaussian kernels for local context. This spatial diffusion process encourages propagation across connected and semantically related areas, enabling it to effectively delineate objects with arbitrary shapes. Whereas most existing approaches treat self-attention maps from different transformer heads or layers equally, our method uses entropy-based uncertainty to select the most relevant maps. Notably, our method does not require any conventional post-processing techniques like Conditional Random Fields (CRF) or Pixel-Adaptive Mask Refinement (PAMR). Experiments are performed on 7 popular semantic segmentation benchmarks, yielding an overall state-of-the-art zero-shot segmentation performance, providing an effective approach to open-vocabulary semantic segmentation.

**Comment:** Matches criterion 1 (Spatial Intelligence and Embodied Agents) as it introduces a novel method for spatial reasoning in open-vocabulary segmentation using diffusion processes.
**Relevance:** 8
**Novelty:** 7

---

## 3. [ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation](https://arxiv.org/abs/2511.08046) <a id="link3"></a>
**ArXiv ID:** 2511.08046
**Authors:** Aya Elgebaly, Nikolaos Delopoulos, Juliane H\"orner-Rieber, Carolin Rippke, Sebastian Kl\"uter, Luca Boldrini, Lorenzo Placidi, Riccardo Dal Bello, Nicolaus Andratschke, Michael Baumgartl, Claus Belka, Christopher Kurz, Guillaume Landry, Shadi Albarqouni

**Abstract:**  Automated medical image segmentation suffers from high inter-observer variability, particularly in tasks such as lung nodule delineation, where experts often disagree. Existing approaches either collapse this variability into a consensus mask or rely on separate model branches for each annotator. We introduce ProSona, a two-stage framework that learns a continuous latent space of annotation styles, enabling controllable personalization via natural language prompts. A probabilistic U-Net backbone captures diverse expert hypotheses, while a prompt-guided projection mechanism navigates this latent space to generate personalized segmentations. A multi-level contrastive objective aligns textual and visual representations, promoting disentangled and interpretable expert styles. Across the LIDC-IDRI lung nodule and multi-institutional prostate MRI datasets, ProSona reduces the Generalized Energy Distance by 17% and improves mean Dice by more than one point compared with DPersona. These results demonstrate that natural-language prompts can provide flexible, accurate, and interpretable control over personalized medical image segmentation. Our implementation is available online 1 .

**Comment:** Matches criterion 4 (Vision Foundation Models and Their Applications) as it introduces a personalized framework for medical image segmentation using vision models.
**Relevance:** 7
**Novelty:** 6

---

## 4. [Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs](https://arxiv.org/abs/2511.07748) <a id="link4"></a>
**ArXiv ID:** 2511.07748
**Authors:** Yuezhe Yang, Yiyue Guo, Wenjie Cai, Qingqing Ruan, Siying Wang, Xingbo Dong, Zhe Jin, Yong Dai

**Abstract:**  AI-assisted ultrasound video diagnosis presents new opportunities to enhance the efficiency and accuracy of medical imaging analysis. However, existing research remains limited in terms of dataset diversity, diagnostic performance, and clinical applicability. In this study, we propose \textbf{Auto-US}, an intelligent diagnosis agent that integrates ultrasound video data with clinical diagnostic text. To support this, we constructed \textbf{CUV Dataset} of 495 ultrasound videos spanning five categories and three organs, aggregated from multiple open-access sources. We developed \textbf{CTU-Net}, which achieves state-of-the-art performance in ultrasound video classification, reaching an accuracy of 86.73\% Furthermore, by incorporating large language models, Auto-US is capable of generating clinically meaningful diagnostic suggestions. The final diagnostic scores for each case exceeded 3 out of 5 and were validated by professional clinicians. These results demonstrate the effectiveness and clinical potential of Auto-US in real-world ultrasound applications. Code and data are available at: https://github.com/Bean-Young/Auto-US.

**Comment:** Matches criterion 6 (Video Understanding) as it focuses on ultrasound video classification and diagnosis using a novel framework.
**Relevance:** 7
**Novelty:** 6

---

## 5. [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943) <a id="link5"></a>
**ArXiv ID:** 2511.07943
**Authors:** Jun Xu, Xinkai Du, Yu Ao, Peilong Zhao, Yang Li, Ling Zhong, Lin Yuan, Zhongpu Bo, Xiaorui Wang, Mengshu Sun, Zhengke Gui, Dalong Zhang, Zhaoyang Wang, Qiwei Wang, Yangyang Hou, Zhiying Yin, Haofen Wang, Huajun Chen, Lei Liang, Jun Zhou

**Abstract:**  Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

**Comment:** Matches criterion 2 (Visual and Multimodal Large Language Models) as it explores hierarchical thinking in LLMs for deep search and reasoning.
**Relevance:** 7
**Novelty:** 6

---

## 6. [Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation](https://arxiv.org/abs/2511.08152) <a id="link6"></a>
**ArXiv ID:** 2511.08152
**Authors:** Jun Sun, Xinxin Zhang, Simin Hong, Jian Zhu, Xiang Gao

**Abstract:**  Multimodal learning, while contributing to numerous success stories across various fields, faces the challenge of prohibitively expensive manual annotation. To address the scarcity of annotated data, a popular solution is unsupervised domain adaptation, which has been extensively studied in unimodal settings yet remains less explored in multimodal settings. In this paper, we investigate heterogeneous multimodal domain adaptation, where the primary challenge is the varying domain shifts of different modalities from the source to the target domain. We first introduce the information bottleneck method to learn representations for each modality independently, and then match the source and target domains in the representation space with correlation alignment. To balance the domain alignment of all modalities, we formulate the problem as a multi-objective task, aiming for a Pareto optimal solution. By exploiting the properties specific to our model, the problem can be simplified to a quadratic programming problem. Further approximation yields a closed-form solution, leading to an efficient modality-balanced multimodal domain adaptation algorithm. The proposed method features \textbf{B}alanced multi-\textbf{o}bjective \textbf{o}ptimization for \textbf{m}ultimodal \textbf{d}omain \textbf{a}daptation, termed \textbf{Boomda}. Extensive empirical results showcase the effectiveness of the proposed approach and demonstrate that Boomda outperforms the competing schemes. The code is is available at: https://github.com/sunjunaimer/Boomda.git.

**Comment:** Matches criterion 2 (Visual and Multimodal Large Language Models) as it addresses multimodal domain adaptation with a novel optimization approach.
**Relevance:** 7
**Novelty:** 6

---

## 7. [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151) <a id="link7"></a>
**ArXiv ID:** 2511.08151
**Authors:** Xuchen Li, Ruitao Wu, Xuanbo Liu, Xukai Wang, Jinbo Hu, Zhixin Bai, Bohan Zeng, Hao Liang, Leheng Chen, Mingrui Chen, Haitian Zhong, Xuanlin Yang, Xu-Yao Zhang, Liu Liu, Jia Li, Kaiqi Huang, Jiahao Xu, Haitao Mi, Wentao Zhang, Bin Dong

**Abstract:**  Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

**Comment:** Matches criterion 2 as it explores a multi-agent system for scientific reasoning, which involves multimodal reasoning and integration.
**Relevance:** 5
**Novelty:** 7

---

## 8. [Burst Image Quality Assessment: A New Benchmark and Unified Framework for Multiple Downstream Tasks](https://arxiv.org/abs/2511.07958) <a id="link8"></a>
**ArXiv ID:** 2511.07958
**Authors:** Xiaoye Liang, Lai Jiang, Minglang Qiao, Yichen Guo, Yue Zhang, Xin Deng, Shengxi Li, Yufan Liu, Mai Xu

**Abstract:**  In recent years, the development of burst imaging technology has improved the capture and processing capabilities of visual data, enabling a wide range of applications. However, the redundancy in burst images leads to the increased storage and transmission demands, as well as reduced efficiency of downstream tasks. To address this, we propose a new task of Burst Image Quality Assessment (BuIQA), to evaluate the task-driven quality of each frame within a burst sequence, providing reasonable cues for burst image selection. Specifically, we establish the first benchmark dataset for BuIQA, consisting of $7,346$ burst sequences with $45,827$ images and $191,572$ annotated quality scores for multiple downstream scenarios. Inspired by the data analysis, a unified BuIQA framework is proposed to achieve an efficient adaption for BuIQA under diverse downstream scenarios. Specifically, a task-driven prompt generation network is developed with heterogeneous knowledge distillation, to learn the priors of the downstream task. Then, the task-aware quality assessment network is introduced to assess the burst image quality based on the task prompt. Extensive experiments across 10 downstream scenarios demonstrate the impressive BuIQA performance of the proposed approach, outperforming the state-of-the-art. Furthermore, it can achieve $0.33$ dB PSNR improvement in the downstream tasks of denoising and super-resolution, by applying our approach to select the high-quality burst frames.

**Comment:** Matches criterion 3 (Embodied/Robotic AI: New Benchmarks or Methods) as it introduces a new benchmark and framework for burst image quality assessment.
**Relevance:** 6
**Novelty:** 6

---

## 9. [Beyond Randomness: Understand the Order of the Noise in Diffusion](https://arxiv.org/abs/2511.07756) <a id="link9"></a>
**ArXiv ID:** 2511.07756
**Authors:** Song Yan, Min Li, Bi Xinliang, Jian Yang, Yusen Zhang, Guanye Xiong, Yunwei Lan, Tao Zhang, Wei Zhai, Zheng-Jun Zha

**Abstract:**  In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step "Semantic Erasure-Injection" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.

**Comment:** Matches criterion 5 as it explores semantic manipulation in text-to-content diffusion models, which involves integrating image generation and language models.
**Relevance:** 5
**Novelty:** 7

---

## 10. [RAPTR: Radar-based 3D Pose Estimation using Transformer](https://arxiv.org/abs/2511.08387) <a id="link10"></a>
**ArXiv ID:** 2511.08387
**Authors:** Sorachi Kato, Ryoma Yataka, Pu Perry Wang, Pedro Miraldo, Takuya Fujihashi, Petros Boufounos

**Abstract:**  Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\%$ on HIBER and $76.9\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.

**Comment:** Matches criterion 3 as it introduces a novel method for radar-based 3D pose estimation, which is relevant to embodied AI.
**Relevance:** 5
**Novelty:** 7

---

## 11. [Exploring the Underwater World Segmentation without Extra Training](https://arxiv.org/abs/2511.07923) <a id="link11"></a>
**ArXiv ID:** 2511.07923
**Authors:** Bingyu Li, Tao Huo, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li

**Abstract:**  Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.

**Comment:** Matches criterion 5 as it proposes a method for transferring terrestrial vision-language models to underwater domains, combining image understanding and vision-language models.
**Relevance:** 5
**Novelty:** 6

---

## 12. [Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers](https://arxiv.org/abs/2511.07934) <a id="link12"></a>
**ArXiv ID:** 2511.07934
**Authors:** Sida Huang, Siqi Huang, Ping Luo, Hongyuan Zhang

**Abstract:**  With the development of diffusion models, enhancing spatial controllability in text-to-image generation has become a vital challenge. As a representative task for addressing this challenge, layout-to-image generation aims to generate images that are spatially consistent with the given layout condition. Existing layout-to-image methods typically introduce the layout condition by integrating adapter modules into the base generative model. However, the generated images often exhibit low visual quality and stylistic inconsistency with the base model, indicating a loss of pretrained knowledge. To alleviate this issue, we construct the Layout Synthesis (LaySyn) dataset, which leverages images synthesized by the base model itself to mitigate the distribution shift from the pretraining data. Moreover, we propose the Layout Control (Laytrol) Network, in which parameters are inherited from MM-DiT to preserve the pretrained knowledge of the base model. To effectively activate the copied parameters and avoid disturbance from unstable control conditions, we adopt a dedicated initialization scheme for Laytrol. In this scheme, the layout encoder is initialized as a pure text encoder to ensure that its output tokens remain within the data domain of MM-DiT. Meanwhile, the outputs of the layout control network are initialized to zero. In addition, we apply Object-level Rotary Position Embedding to the layout tokens to provide coarse positional information. Qualitative and quantitative experiments demonstrate the effectiveness of our method.

**Comment:** Matches criterion 5 as it focuses on integrating layout control in multimodal diffusion transformers, which combines image understanding and generation tasks.
**Relevance:** 5
**Novelty:** 6

---

## 13. [Generalized-Scale Object Counting with Gradual Query Aggregation](https://arxiv.org/abs/2511.08048) <a id="link13"></a>
**ArXiv ID:** 2511.08048
**Authors:** Jer Pelhan, Alan Lukezic, Matej Kristan

**Abstract:**  Few-shot detection-based counters estimate the number of instances in the image specified only by a few test-time exemplars. A common approach to localize objects across multiple sizes is to merge backbone features of different resolutions. Furthermore, to enable small object detection in densely populated regions, the input image is commonly upsampled and tiling is applied to cope with the increased computational and memory requirements. Because of these ad-hoc solutions, existing counters struggle with images containing diverse-sized objects and densely populated regions of small objects. We propose GECO2, an end-to-end few-shot counting and detection method that explicitly addresses the object scale issues. A new dense query representation gradually aggregates exemplar-specific feature information across scales that leads to high-resolution dense queries that enable detection of large as well as small objects. GECO2 surpasses state-of-the-art few-shot counters in counting as well as detection accuracy by 10% while running 3x times faster at smaller GPU memory footprint.

**Comment:** Matches criterion 3 as it introduces a novel method for few-shot counting and detection, addressing scale issues in object detection, which is relevant to embodied/robotic AI.
**Relevance:** 5
**Novelty:** 6

---

## 14. [Remodeling Semantic Relationships in Vision-Language Fine-Tuning](https://arxiv.org/abs/2511.08238) <a id="link14"></a>
**ArXiv ID:** 2511.08238
**Authors:** Xiangyang Wu, Liu Liu, Baosheng Yu, Jiayan Qiu, Zhenwei Shi

**Abstract:**  Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.

**Comment:** Matches criterion 2 as it focuses on improving vision-language fine-tuning for multimodal foundation models.
**Relevance:** 5
**Novelty:** 6

---

## 15. [Revisiting MLLM Based Image Quality Assessment: Errors and Remedy](https://arxiv.org/abs/2511.07812) <a id="link15"></a>
**ArXiv ID:** 2511.07812
**Authors:** Zhenchen Tang, Songlin Yang, Bo Peng, Zichuan Wang, Jing Dong

**Abstract:**  The rapid progress of multi-modal large language models (MLLMs) has boosted the task of image quality assessment (IQA). However, a key challenge arises from the inherent mismatch between the discrete token outputs of MLLMs and the continuous nature of quality scores required by IQA tasks. This discrepancy significantly hinders the performance of MLLM-based IQA methods. Previous approaches that convert discrete token predictions into continuous scores often suffer from conversion errors. Moreover, the semantic confusion introduced by level tokens (e.g., ``good'') further constrains the performance of MLLMs on IQA tasks and degrades their original capabilities for related tasks. To tackle these problems, we provide a theoretical analysis of the errors inherent in previous approaches and, motivated by this analysis, propose a simple yet effective framework, Q-Scorer. This framework incorporates a lightweight regression module and IQA-specific score tokens into the MLLM pipeline. Extensive experiments demonstrate that Q-Scorer achieves state-of-the-art performance across multiple IQA benchmarks, generalizes well to mixed datasets, and further improves when combined with other methods.

**Comment:** Matches criterion 2 as it explores multi-modal large language models (MLLMs) and proposes a novel framework for image quality assessment.
**Relevance:** 5
**Novelty:** 6

---

## 16. [SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models](https://arxiv.org/abs/2511.08379) <a id="link16"></a>
**ArXiv ID:** 2511.08379
**Authors:** Giorgio Piras, Raffaele Mura, Fabio Brau, Luca Oneto, Fabio Roli, Battista Biggio

**Abstract:**  Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.

**Comment:** Does not match any specific criteria but is related to mechanistic interpretability and refusal suppression in language models.
**Relevance:** 3
**Novelty:** 6

---

## 17. [Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning](https://arxiv.org/abs/2511.08301) <a id="link17"></a>
**ArXiv ID:** 2511.08301
**Authors:** Valentin Tablan, Scott Taylor, Gabriel Hurtado, Kristoffer Bernhem, Anders Uhrenholt, Gabriele Farei, Karo Moilanen

**Abstract:**  The transition from human-centric to agent-centric software development practices is disrupting existing knowledge sharing environments for software developers. Traditional peer-to-peer repositories and developer communities for shared technical knowledge and best practice have witnessed dramatic drops in participation in a short period of time. At the same time, agentic functional equivalents are yet to emerge leaving AI agents, which already generate a significant proportion of all new software code produced, without access to repositories of valuable shared learning.   In this paper, we introduce Spark, a novel shared agentic memory architecture which is designed to emulate the collective intelligence and know-how of human developer communities. Spark enables AI coding agents to both contribute to and draw from a persistent and continuously evolving experiential memory. Agents operating in the same general problem space use the Spark shared memory as a repository of new knowledge to achieve collective continual learning. We evaluate Spark as a coach for AI coding agents performing software development tasks. We demonstrate that recommendations made by Spark improve the quality of code generated by generic code generation models at varying sizes and capability tiers. Boosted by Spark, a small open-weights model with 30 billion parameters was able to match the code quality afforded by a much larger state-of-the-art model. Separately, we measure the intrinsic quality of recommendations generated by Spark against a wide range of criteria inspired by software development best practice, and achieve helpfulness levels of up to 98.2% in the top two (out of five) qualitative helpfulness bands.

**Comment:** Does not match any specific criteria but is related to shared memory architectures for AI agents.
**Relevance:** 3
**Novelty:** 6

---

## 18. [MADD: Multi-Agent Drug Discovery Orchestra](https://arxiv.org/abs/2511.08217) <a id="link18"></a>
**ArXiv ID:** 2511.08217
**Authors:** Gleb V. Solovev, Alina B. Zhidkovskaya, Anastasia Orlova, Nina Gubina, Anastasia Vepreva, Rodion Golovinskii, Ilya Tonkii, Ivan Dubrovsky, Ivan Gurev, Dmitry Gilemkhanov, Denis Chistiakov, Timur A. Aliev, Ivan Poddiakov, Galina Zubkova, Ekaterina V. Skorb, Vladimir Vinogradov, Alexander Boukhanovsky, Nikolay Nikitin, Andrei Dmitrenko, Anna Kalyuzhnaya, Andrey Savchenko

**Abstract:**  Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.

**Comment:** Does not match any specific criteria but is related to multi-agent systems and AI in drug discovery.
**Relevance:** 3
**Novelty:** 6

---

## 19. [PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier](https://arxiv.org/abs/2511.07806) <a id="link19"></a>
**ArXiv ID:** 2511.07806
**Authors:** Shaomeng Wang, He Wang, Xiaolu Wei, Longquan Dai, Jinhui Tang

**Abstract:**  Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.

**Comment:** Does not match any specific criteria but is related to generative modeling and optimization in diffusion models.
**Relevance:** 3
**Novelty:** 6

---

## 20. [Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification](https://arxiv.org/abs/2511.07929) <a id="link20"></a>
**ArXiv ID:** 2511.07929
**Authors:** Yihang Wu, Ahmad Chaddad

**Abstract:**  Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).

**Comment:** Does not match any specific criteria but is related to federated learning and vision-language models, which is tangentially relevant to your friend's general interest area.
**Relevance:** 3
**Novelty:** 5

---

## 21. [Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning](https://arxiv.org/abs/2511.08024) <a id="link21"></a>
**ArXiv ID:** 2511.08024
**Authors:** Tianwen Lyu, Xiang Zhuang, Keyan Ding, Xinzhe Cao, Lei Liang, Wei Zhao, Qiang Zhang, Huajun Chen

**Abstract:**  Understanding complex biomolecular mechanisms requires multi-step reasoning across molecular interactions, signaling cascades, and metabolic pathways. While large language models(LLMs) show promise in such tasks, their application to biomolecular problems is hindered by logical inconsistencies and the lack of grounding in domain knowledge. Existing approaches often exacerbate these issues: reasoning steps may deviate from biological facts or fail to capture long mechanistic dependencies. To address these challenges, we propose a Knowledge-Augmented Long-CoT Reasoning framework that integrates LLMs with knowledge graph-based multi-hop reasoning chains. The framework constructs mechanistic chains via guided multi-hop traversal and pruning on the knowledge graph; these chains are then incorporated into supervised fine-tuning to improve factual grounding and further refined with reinforcement learning to enhance reasoning reliability and consistency. Furthermore, to overcome the shortcomings of existing benchmarks, which are often restricted in scale and scope and lack annotations for deep reasoning chains, we introduce PrimeKGQA, a comprehensive benchmark for biomolecular question answering. Experimental results on both PrimeKGQA and existing datasets demonstrate that although larger closed-source models still perform well on relatively simple tasks, our method demonstrates clear advantages as reasoning depth increases, achieving state-of-the-art performance on multi-hop tasks that demand traversal of structured biological knowledge. These findings highlight the effectiveness of combining structured knowledge with advanced reasoning strategies for reliable and interpretable biomolecular reasoning.

**Comment:** Does not match any specific criteria but is related to reasoning and knowledge integration, which is tangentially relevant to your friend's general interest area.
**Relevance:** 3
**Novelty:** 5

---

## 22. [GAMA: A Neural Neighborhood Search Method with Graph-aware Multi-modal Attention for Vehicle Routing Problem](https://arxiv.org/abs/2511.07850) <a id="link22"></a>
**ArXiv ID:** 2511.07850
**Authors:** Xiangling Chen, Yi Mei, Mengjie Zhang

**Abstract:**  Recent advances in neural neighborhood search methods have shown potential in tackling Vehicle Routing Problems (VRPs). However, most existing approaches rely on simplistic state representations and fuse heterogeneous information via naive concatenation, limiting their ability to capture rich structural and semantic context. To address these limitations, we propose GAMA, a neural neighborhood search method with Graph-aware Multi-modal Attention model in VRP. GAMA encodes the problem instance and its evolving solution as distinct modalities using graph neural networks, and models their intra- and inter-modal interactions through stacked self- and cross-attention layers. A gated fusion mechanism further integrates the multi-modal representations into a structured state, enabling the policy to make informed and generalizable operator selection decisions. Extensive experiments conducted across various synthetic and benchmark instances demonstrate that the proposed algorithm GAMA significantly outperforms the recent neural baselines. Further ablation studies confirm that both the multi-modal attention mechanism and the gated fusion design play a key role in achieving the observed performance gains.

**Comment:** Does not match any specific criteria but is related to graph neural networks and optimization, which is tangentially relevant to your friend's general interest area.
**Relevance:** 3
**Novelty:** 5

---

## 23. [Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models](https://arxiv.org/abs/2511.08484) <a id="link23"></a>
**ArXiv ID:** 2511.08484
**Authors:** Huzaifa Arif, Keerthiram Murugesan, Ching-Yun Ko, Pin-Yu Chen, Payel Das, Alex Gittens

**Abstract:**  We propose patching for large language models (LLMs) like software versions, a lightweight and modular approach for addressing safety vulnerabilities. While vendors release improved LLM versions, major releases are costly, infrequent, and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This "patch" introduces only 0.003% additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains (toxicity mitigation, bias reduction, and harmfulness refusal) policy patches achieve safety improvements comparable to next-generation safety-aligned models while preserving fluency. Our results demonstrate that LLMs can be "patched" much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.

**Comment:** Does not match any specific criteria but is generally related to LLM safety and patching techniques.
**Relevance:** 3
**Novelty:** 5

---

## 24. [SWAN - Enabling Fast and Mobile Histopathology Image Annotation through Swipeable Interfaces](https://arxiv.org/abs/2511.08271) <a id="link24"></a>
**ArXiv ID:** 2511.08271
**Authors:** Sweta Banerjee, Timo Gosch, Sara Hester, Viktoria Weiss, Thomas Conrad, Taryn A. Donovan, Nils Porsche, Jonas Ammeling, Christoph Stroblberger, Robert Klopfleisch, Christopher Kaltenecker, Christof A. Bertram, Katharina Breininger, Marc Aubreville

**Abstract:**  The annotation of large scale histopathology image datasets remains a major bottleneck in developing robust deep learning models for clinically relevant tasks, such as mitotic figure classification. Folder-based annotation workflows are usually slow, fatiguing, and difficult to scale. To address these challenges, we introduce SWipeable ANnotations (SWAN), an open-source, MIT-licensed web application that enables intuitive image patch classification using a swiping gesture. SWAN supports both desktop and mobile platforms, offers real-time metadata capture, and allows flexible mapping of swipe gestures to class labels. In a pilot study with four pathologists annotating 600 mitotic figure image patches, we compared SWAN against a traditional folder-sorting workflow. SWAN enabled rapid annotations with pairwise percent agreement ranging from 86.52% to 93.68% (Cohen's Kappa = 0.61-0.80), while for the folder-based method, the pairwise percent agreement ranged from 86.98% to 91.32% (Cohen's Kappa = 0.63-0.75) for the task of classifying atypical versus normal mitotic figures, demonstrating high consistency between annotators and comparable performance. Participants rated the tool as highly usable and appreciated the ability to annotate on mobile devices. These results suggest that SWAN can accelerate image annotation while maintaining annotation quality, offering a scalable and user-friendly alternative to conventional workflows.

**Comment:** Does not match any specific criteria but is related to annotation tools for image datasets.
**Relevance:** 3
**Novelty:** 5

---

## 25. [Predicting Coronary Artery Calcium Severity based on Non-Contrast Cardiac CT images using Deep Learning](https://arxiv.org/abs/2511.07695) <a id="link25"></a>
**ArXiv ID:** 2511.07695
**Authors:** Lachlan Nguyen, Aidan Cousins, Arcot Sowmya, Hugh Dixson, Sonit Singh

**Abstract:**  Cardiovascular disease causes high rates of mortality worldwide. Coronary artery calcium (CAC) scoring is a powerful tool to stratify the risk of atherosclerotic cardiovascular disease. Current scoring practices require time-intensive semiautomatic analysis of cardiac computed tomography by radiologists and trained radiographers. The purpose of this study is to develop a deep learning convolutional neural networks (CNN) model to classify the calcium score in cardiac, non-contrast computed tomography images into one of six clinical categories. A total of 68 patient scans were retrospectively obtained together with their respective reported semiautomatic calcium score using an ECG-gated GE Discovery 570 Cardiac SPECT/CT camera. The dataset was divided into training, validation and test sets. Using the semiautomatic CAC score as the reference label, the model demonstrated high performance on a six-class CAC scoring categorisation task. Of the scans analysed, the model misclassified 32 cases, tending towards overestimating the CAC in 26 out of 32 misclassifications. Overall, the model showed high agreement (Cohen's kappa of 0.962), an overall accuracy of 96.5% and high generalisability. The results suggest that the model outputs were accurate and consistent with current semiautomatic practice, with good generalisability to test data. The model demonstrates the viability of a CNN model to stratify the calcium score into an expanded set of six clinical categories.

**Comment:** Does not match any specific criteria but is related to medical imaging and deep learning applications.
**Relevance:** 3
**Novelty:** 5

---

## 26. [Mitigating Negative Flips via Margin Preserving Training](https://arxiv.org/abs/2511.08322) <a id="link26"></a>
**ArXiv ID:** 2511.08322
**Authors:** Simone Ricci, Niccol\`o Biondi, Federico Pernici, Alberto Del Bimbo

**Abstract:**  Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.

**Comment:** Does not match any specific criteria but is related to mitigating inconsistencies in image classification, which is tangentially relevant to your friend's general interest area.
**Relevance:** 3
**Novelty:** 4

---

## 27. [Contrastive Integrated Gradients: A Feature Attribution-Based Method for Explaining Whole Slide Image Classification](https://arxiv.org/abs/2511.08464) <a id="link27"></a>
**ArXiv ID:** 2511.08464
**Authors:** Anh Mai Vu, Tuan L. Vo, Ngoc Lam Quang Bui, Nam Nguyen Le Binh, Akash Awasthi, Huy Quoc Vo, Thanh-Huy Nguyen, Zhu Han, Chandra Mohan, Hien Van Nguyen

**Abstract:**  Interpretability is essential in Whole Slide Image (WSI) analysis for computational pathology, where understanding model predictions helps build trust in AI-assisted diagnostics. While Integrated Gradients (IG) and related attribution methods have shown promise, applying them directly to WSIs introduces challenges due to their high-resolution nature. These methods capture model decision patterns but may overlook class-discriminative signals that are crucial for distinguishing between tumor subtypes. In this work, we introduce Contrastive Integrated Gradients (CIG), a novel attribution method that enhances interpretability by computing contrastive gradients in logit space. First, CIG highlights class-discriminative regions by comparing feature importance relative to a reference class, offering sharper differentiation between tumor and non-tumor areas. Second, CIG satisfies the axioms of integrated attribution, ensuring consistency and theoretical soundness. Third, we propose two attribution quality metrics, MIL-AIC and MIL-SIC, which measure how predictive information and model confidence evolve with access to salient regions, particularly under weak supervision. We validate CIG across three datasets spanning distinct cancer types: CAMELYON16 (breast cancer metastasis in lymph nodes), TCGA-RCC (renal cell carcinoma), and TCGA-Lung (lung cancer). Experimental results demonstrate that CIG yields more informative attributions both quantitatively, using MIL-AIC and MIL-SIC, and qualitatively, through visualizations that align closely with ground truth tumor regions, underscoring its potential for interpretable and trustworthy WSI-based diagnostics

**Comment:** Does not match any specific criteria but is related to interpretability in medical imaging, which is tangentially relevant to your friend's general interest area.
**Relevance:** 3
**Novelty:** 4

---

## 28. [The Impact of Longitudinal Mammogram Alignment on Breast Cancer Risk Assessment](https://arxiv.org/abs/2511.08328) <a id="link28"></a>
**ArXiv ID:** 2511.08328
**Authors:** Solveig Thrun, Stine Hansen, Zijun Sun, Nele Blum, Suaiba A. Salahuddin, Xin Wang, Kristoffer Wickstr{\o}m, Elisabeth Wetzer, Robert Jenssen, Maik Stille, Michael Kampffmeyer

**Abstract:**  Regular mammography screening is crucial for early breast cancer detection. By leveraging deep learning-based risk models, screening intervals can be personalized, especially for high-risk individuals. While recent methods increasingly incorporate longitudinal information from prior mammograms, accurate spatial alignment across time points remains a key challenge. Misalignment can obscure meaningful tissue changes and degrade model performance. In this study, we provide insights into various alignment strategies, image-based registration, feature-level (representation space) alignment with and without regularization, and implicit alignment methods, for their effectiveness in longitudinal deep learning-based risk modeling. Using two large-scale mammography datasets, we assess each method across key metrics, including predictive accuracy, precision, recall, and deformation field quality.   Our results show that image-based registration consistently outperforms the more recently favored feature-based and implicit approaches across all metrics, enabling more accurate, temporally consistent predictions and generating smooth, anatomically plausible deformation fields. Although regularizing the deformation field improves deformation quality, it reduces the risk prediction performance of feature-level alignment. Applying image-based deformation fields within the feature space yields the best risk prediction performance.   These findings underscore the importance of image-based deformation fields for spatial alignment in longitudinal risk modeling, offering improved prediction accuracy and robustness. This approach has strong potential to enhance personalized screening and enable earlier interventions for high-risk individuals. The code is available at https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.git, allowing full reproducibility of the results.

**Comment:** Does not match any specific criteria but is related to medical imaging and deep learning, which is tangentially relevant to your friend's general interest area.
**Relevance:** 3
**Novelty:** 4

---

## 29. [AI-Powered Data Visualization Platform: An Intelligent Web Application for Automated Dataset Analysis](https://arxiv.org/abs/2511.08363) <a id="link29"></a>
**ArXiv ID:** 2511.08363
**Authors:** Srihari R, Pallavi M, Tejaswini S, Vaishnavi R C

**Abstract:**  An AI-powered data visualization platform that automates the entire data analysis process, from uploading a dataset to generating an interactive visualization. Advanced machine learning algorithms are employed to clean and preprocess the data, analyse its features, and automatically select appropriate visualizations. The system establishes the process of automating AI-based analysis and visualization from the context of data-driven environments, and eliminates the challenge of time-consuming manual data analysis. The combination of a Python Flask backend to access the dataset, paired with a React frontend, provides a robust platform that automatically interacts with Firebase Cloud Storage for numerous data processing and data analysis solutions and real-time sources. Key contributions include automatic and intelligent data cleaning, with imputation for missing values, and detection of outliers, via analysis of the data set. AI solutions to intelligently select features, using four different algorithms, and intelligent title generation and visualization are determined by the attributes of the dataset. These contributions were evaluated using two separate datasets to assess the platform's performance. In the process evaluation, the initial analysis was performed in real-time on datasets as large as 100000 rows, while the cloud-based demand platform scales to meet requests from multiple users and processes them simultaneously. In conclusion, the cloud-based data visualization application allowed for a significant reduction of manual inputs to the data analysis process while maintaining a high quality, impactful visual outputs, and user experiences

**Comment:** Does not match any specific criteria but is generally related to AI and data visualization.
**Relevance:** 3
**Novelty:** 4

---


---

## Paper selection prompt
1. Spatial Intelligence and Embodied Agents Papers presenting novel methodological improvements in spatial reasoning or spatial intelligence for embodied agents.
2. Visual and Multimodal Large Language Models Papers exploring Visual Large Language Models (VLLMs), Multi-modal Large Language Models (MLLMs), or Video Large Language Models (Video LLMs). This includes new architectures, training strategies, or applications centered on visionlanguage integration.
3. Embodied/Robotic AI: New Benchmarks or Methods Papers introducing new benchmarks (including simulators) or novel methods in embodied or robotic AI, especially those tackling previously underexplored challenges or providing fresh perspectives on existing problems.
4. Vision Foundation Models and Their Applications Papers focusing on foundation models in computer vision: their core architectures, training objectives, and real-world use cases. 
5.Integration of Image/Video and Large Language Models Papers showcasing techniques that combine image or video understanding tasks and generation tasks and Large Language Models.
6. Video Understanding Papers dedicated to video-based tasks (e.g., classification, captioning, question answering, summarization), including novel methodologies, datasets, or evaluations that push the boundaries of video understanding. 
7.Vision-Focused Survey Papers Comprehensive survey or review papers that synthesize the state of the art in one or more areas of computer vision, highlighting key progress, open challenges, and emerging trends.

In suggesting papers to your friend, remember that he enjoys papers on computer vision and machine learning, and generative modeling in multi-modal learning. Your friend also likes learning about surprising empirical or insightful results in vision-language models or embodied AI, as well as clever statistical tricks.
